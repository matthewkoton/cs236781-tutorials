{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\given}[]{~\\middle\\vert~}$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 1: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "* Basics of supervised learning\n",
    "    - Definitions\n",
    "    - Basics of optimization\n",
    "    - Types of errors\n",
    "* Classic ML example: Binary logistic regression from scratch using numpy (**self study**)\n",
    "* Multiclass logistic regression from scratch with PyTorch and autograd\n",
    "    - Optimization\n",
    "    - Writing training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:05.689208Z",
     "iopub.status.busy": "2022-03-31T05:57:05.688941Z",
     "iopub.status.idle": "2022-03-31T05:57:21.783611Z",
     "shell.execute_reply": "2022-03-31T05:57:21.783281Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:21.785948Z",
     "iopub.status.busy": "2022-03-31T05:57:21.785840Z",
     "iopub.status.idle": "2022-03-31T05:57:21.802646Z",
     "shell.execute_reply": "2022-03-31T05:57:21.802329Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The supervised learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have a labeled dataset of $N$ labelled samples: $\\mathcal{S} = \\{ (\\vec{x}^i,y^i) \\}_{i=1}^N$, where\n",
    "- $\\vec{x}^i = \\left(x^i_1, \\dots, x^i_D\\right) \\in \\mathcal{X}$  is a **sample** or **feature vector**.\n",
    "- $y^i \\in \\mathcal{Y}$ is the **label**.\n",
    "- For classification with $C$ classes, $\\mathcal{Y} = \\{0,\\dots,C-1\\}$, so each $y^i$ is a **class label**.\n",
    "- Probabilistic perspective: assume each labeled sample $(\\vec{x}^i,y^i)\\sim\\mathcal{D}$,\n",
    "  i.e. each labeled sample is drawn from a joint distribution $\\mathcal{D}$ over $\\mathcal{X}\\times\\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our **model** is a parametrized set of functions $\\mathcal{H}\\subseteq \\mathcal{Y}^{\\mathcal{X}}$.\n",
    "\n",
    "($\\mathcal{Y}^{\\mathcal{X}}$ is the set of all functions from $\\mathcal{X}$ to $\\mathcal{Y}$).\n",
    "\n",
    "From a probabilistic perspective, a model is a posterior: $P_{Y|\\vec{X}}(y, \\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the following hypothesis class\n",
    "$$\n",
    "\\mathcal{H} =\n",
    "\\left\\{ h: \\mathcal{X}\\rightarrow\\mathcal{Y}\n",
    "~\\vert~\n",
    "h(\\vec{x}) = \\varphi(\\vectr{w}\\vec{x}+b); \\vec{w}\\in\\set{R}^D,~b\\in\\set{R}\\right\\}\n",
    "$$\n",
    "where $\\varphi(\\cdot)$ is some nonlinear function, is known as the **perceptron** model, and is also called a **neuron** in the context of a neural network (due to it's biologically-inspired origin).\n",
    "\n",
    "\n",
    "| . | . |\n",
    "|---|---|\n",
    "| <img src=\"imgs/perceptron.png\" width=400 /> | <img src=\"imgs/neuron.png\" width=600 /> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **pointwise loss function** is some $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\set{R}_{\\geq 0}$, e.g.\n",
    "- $\\ell(y,\\hat{y})=(y-\\hat{y})^2$\n",
    "- $\\ell(y,\\hat{y})= -y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The task: Given the training set $\\mathcal{S} = \\{ (\\vec{x}^i,y^i) \\}_{i=1}^N \\sim \\mathcal{D}$, find \n",
    "a predictor (hypothesis) $h: \\mathcal{X}\\to\\mathcal{Y}$ which minimizes **population loss**:\n",
    "\n",
    "$$\n",
    "L_{\\mathcal{D}}(h) = \\E[(\\vec{x},y)\\sim\\mathcal{D}]{\\ell(y, h(\\vec{x})}.\n",
    "$$\n",
    "\n",
    "This is also known as the **out-of-sample** loss. It is a deterministic quantity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, it's the true expectation of an RV.\n",
    "\n",
    "Can we solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Joint-distribution $\\mathcal{D}$ is unknown!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, we define an **empirical (in-sample) loss** $L_{\\mathcal{S}}(h)$ as the measure of how well a function $h\\in\\mathcal{H}$ fits the data $\\mathcal{S}$, for example\n",
    "$$\n",
    "L_{\\mathcal{S}}(h) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(h(\\vec{x}^i), y^i) + R(h)\n",
    "$$\n",
    "where\n",
    "\n",
    "- $\\ell(y,\\hat{y})$ is some pointwise loss (depends on the data).\n",
    "- $R(h)$ is a regularization term (depends only on the model).\n",
    "\n",
    "Is this is a deterministic quantity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model is **trained** by updating its parameters to improve its performance on some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We wish to find a parametrization $\\vec{w}^\\ast$ of our model $h^{\\ast}_{\\mathcal{S}}(\\vec{x};\\vec{w}^\\ast)\\in\\mathcal{H}$ such that\n",
    "$$\n",
    "h^{\\ast}_{\\mathcal{S}} = \\arg\\min_{h\\in\\mathcal{H}} L_{\\mathcal{S}}(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usual approach: descent-based optimization (will be covered in next lecture)\n",
    "$$\n",
    "\\vec{w}_{k+1} \\leftarrow \\vec{w}_{k} - \\eta_k \\vec{d}_{k}\n",
    "$$\n",
    "\n",
    "Where $\\vec{d}_k$ is a **descent direction** and $\\eta_k$ is the step size at step $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most common choice for $\\vec{d}_k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gradient descent: $\\vec{d}_k = \\nabla_{\\vec{w}_{k}} L(\\vec{w}_{k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Will we find the optimal solution, $\\vec{w}^\\ast$, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally the loss is non-convex and we have no guarantee of converging to the global optimum!\n",
    "$\\Rightarrow$ **Optimization Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/sgd2d_2.png\" width=800 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient descent (GD) has the advantage of being extremely simple to apply, and requires only first-order derivatives.\n",
    "\n",
    "However it provides no guarantees for convergence on non-convex objectives.\n",
    "\n",
    "Later in the course we'll learn about variants of GD which perform well in practice even on the non-convex objectives we'll face in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalization and Expressiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We've talked about optimization error.\n",
    "\n",
    "What are **other** sources of errors in our learning approach (besides optimization)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We train to minimize our empirical loss $L_\\mathcal{S}$ instead of the population loss $L_\\mathcal{D}$ $\\Rightarrow$ **Generalization Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We only consider a limited set of possible functions, $\\mathcal{H}$ $\\Rightarrow$ **Approximation Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/error_types.png\" width=900 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we mitigate these errors in the practice of machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Optimization error: Mini batches; GD variants like stochastic gradient, Momentum, Adam, LR scheduling, etc (we'll see later in the course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Generalization error: Get more data; get data which better represents $\\mathcal{D}$; train-test splits; cross validation; early stopping; regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Approximation error: Use a powerful hypothesis class (e.g. DNN); more parameters; \"inductive bias\" - tailoring the model to the domain (e.g. CNN for images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary Logistic Regression\n",
    "\n",
    "Actually a **classification** model. We're trying to classify data into 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Domain: $\\vec{x}^i \\in \\set{R}^D$\n",
    "- Target: $y^i \\in \\{0,1\\}$\n",
    "- Model: $\\hat{y} = h(\\vec{x}) = \\sigma(\\vectr{w}\\vec{x}+b)$, where $\\sigma(\\vec{z})$ is the **logistic function**:\n",
    "    $$\\sigma(\\vec{z}) = \\frac{1}{1+e^{-\\vec{z}}}.$$\n",
    "    This function maps the real line onto $[0,1]$.\n",
    "- Probabilistic interpretation: $\\hat{y}=P(\\rvar{Y}=1|\\rvec{X}=\\vec{x})$.\n",
    "\n",
    "  This is a posterior probability function for our target variable $\\rvar{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:21.806138Z",
     "iopub.status.busy": "2022-03-31T05:57:21.806020Z",
     "iopub.status.idle": "2022-03-31T05:57:21.821968Z",
     "shell.execute_reply": "2022-03-31T05:57:21.821667Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "x = np.arange(-5, 5, .01)\n",
    "y_hat = logistic(x)\n",
    "\n",
    "plt.plot(x, y_hat, label='$\\sigma(z)$')\n",
    "plt.grid(True); plt.xlabel('z'); plt.legend(); plt.title('The logistic function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To optimize, we minimize the **negative log-likelihood** (equivalent to maximizing likelihood, i.e. MLE) of the parameters $\\vec{w}$:\n",
    "\n",
    "$$\n",
    "\\bb{w}^\\ast = \\mathrm{arg}\\max_{ \\bb{w}} \\prod_{i=1}^n P(y_i | \\bb{x}_i;\\vec{w}) = \\mathrm{arg}\\min_{ \\bb{w}} \\sum_{i=1}^n -\\log P(y_i | \\bb{x}_i; \\vec{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Based on this we define our loss $L(\\bb{w})$ (what we minimize) as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "L(\\bb{w}) &\\triangleq \\sum_{i=1}^n -\\log P(y_i | \\bb{x}_i; \\vec{w})\\\\\n",
    "&= \\sum_{i=1}^n -y_i \\log P(y_i=1 | \\bb{x}_i; \\vec{w}) - (1-y_i) \\log P(y_i=0 | \\bb{x}_i; \\vec{w}) \\\\\n",
    "&= \\sum_{i=1}^n -y_i \\log \\hat{y}_i - (1-y_i) \\log(1-\\hat{y}_i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting pointwise loss is also known as **cross-entropy**:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell(y, \\hat{y}) &=  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(The \"cross\" here is between the distribution of the samples $y^i$ and the distribution of the predictions $\\hat{y}^i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimization scheme: No closed form solution, but loss is **convex** so gradient-based approach leads to global optimum.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell(y, \\hat{y}) &=  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's plot this loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.046901Z",
     "iopub.status.busy": "2022-03-31T05:57:22.046787Z",
     "iopub.status.idle": "2022-03-31T05:57:22.158748Z",
     "shell.execute_reply": "2022-03-31T05:57:22.158430Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_y0 = -np.log(1-y_hat)\n",
    "loss_y1 = -np.log(y_hat)\n",
    "plt.plot(y_hat, loss_y0, label='$\\ell(y=0,\\hat{y})$')\n",
    "plt.plot(y_hat, loss_y1, label='$\\ell(y=1,\\hat{y})$')\n",
    "plt.grid(True); plt.xlabel('$\\hat{y}$'); plt.legend(); plt.title('Cross entropy loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To apply gradient descent, we need to know the gradient of the loss w.r.t. the parameters $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Prove that for the cross entropy loss with binary logistic regression, it holds that:\n",
    "$$\n",
    "\\pderiv{\\ell(y,\\hat{y})}{\\vec{w}}= (\\hat{y}-y)\\vec{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Proof:\n",
    "\n",
    "First, we apply the chain-rule\n",
    "$$\n",
    "\\pderiv{\\ell(y,\\hat{y})}{\\vec{w}}=\\pderiv{\\ell}{\\hat{y}}\\cdot\\pderiv{\\hat{y}}{z}\\cdot\\pderiv{z}{\\vec{w}},\n",
    "$$\n",
    "where $z=\\vectr{w}\\vec{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, just by definition,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pderiv{\\ell}{\\hat{y}}&=-\\frac{y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}=\\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}\\\\\n",
    "\\pderiv{\\hat{y}}{z}&=\\frac{e^{-z}}{(1+e^{-z})^2}=\n",
    "\\frac{e^{-z}}{1+e^{-z}}\\cdot\\frac{1}{1+e^{-z}}=(1-\\sigma(z))\\sigma(z)=(1-\\hat{y})\\hat{y}\\\\\n",
    "\\pderiv{z}{\\vec{w}}&=\\vec{x}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a warm up exercise, let's see a quick example of implementing this algorithm and training it from scratch using just `numpy` (and a toy dataset from `sklearn`).\n",
    "\n",
    "This is a classic and very simple example of implementing and training a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The `scikit-learn` library comes with a few [toy datasets](http://scikit-learn.org/stable/datasets/index.html#toy-datasets) that are fun to quickly train small models on.\n",
    "\n",
    "As an example we'll load the Wisconsin-breast cancer database:\n",
    "- 569 samples of cancer patients\n",
    "- 30 features: various properties of tumor cells extracted from images\n",
    "- 2 classes: Tumor is either Benign or Malignant\n",
    "\n",
    "We'll apply the basic machine learning approach we saw above: binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.160877Z",
     "iopub.status.busy": "2022-03-31T05:57:22.160761Z",
     "iopub.status.idle": "2022-03-31T05:57:22.305758Z",
     "shell.execute_reply": "2022-03-31T05:57:22.305466Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "ds_cancer = sklearn.datasets.load_breast_cancer()\n",
    "feature_names = ds_cancer.feature_names\n",
    "target_names = ds_cancer.target_names\n",
    "n_features = len(feature_names)\n",
    "\n",
    "print(f'{n_features=}')\n",
    "print(f'feature names: {feature_names}')\n",
    "print(f'target  names: {target_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.307551Z",
     "iopub.status.busy": "2022-03-31T05:57:22.307445Z",
     "iopub.status.idle": "2022-03-31T05:57:22.324556Z",
     "shell.execute_reply": "2022-03-31T05:57:22.324276Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = ds_cancer.data, ds_cancer.target\n",
    "n_samples = len(y)\n",
    "\n",
    "print(f'X: {X.shape}')\n",
    "print(f'y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.326309Z",
     "iopub.status.busy": "2022-03-31T05:57:22.326189Z",
     "iopub.status.idle": "2022-03-31T05:57:22.351440Z",
     "shell.execute_reply": "2022-03-31T05:57:22.351125Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load into a pandas dataframe and show some samples\n",
    "y_names = np.full_like(y, target_names[0].upper(), dtype=target_names.dtype)\n",
    "y_names[y==1] = target_names[1].upper()\n",
    "\n",
    "df_cancer = pd.DataFrame(data=X, columns=ds_cancer.feature_names)\n",
    "df_cancer = df_cancer.assign(CLASS=y_names)\n",
    "df_cancer.iloc[100:110, 0::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.353134Z",
     "iopub.status.busy": "2022-03-31T05:57:22.353029Z",
     "iopub.status.idle": "2022-03-31T05:57:22.426369Z",
     "shell.execute_reply": "2022-03-31T05:57:22.426076Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f'train: X={X_train.shape} y={y_train.shape}')\n",
    "print(f'test : X={X_test.shape} y={y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.428140Z",
     "iopub.status.busy": "2022-03-31T05:57:22.428029Z",
     "iopub.status.idle": "2022-03-31T05:57:22.445197Z",
     "shell.execute_reply": "2022-03-31T05:57:22.444944Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "\n",
    "# Note: each feature is standardized individually:\n",
    "mu_X = np.mean(X_train, axis=0) # (N, D) -> (D,)\n",
    "sigma_X = np.std(X_train, axis=0)\n",
    "\n",
    "# Note: Broadcasting (N, D) with (D,) -> (N, D)\n",
    "X_train_sc = (X_train - mu_X) / sigma_X \n",
    "\n",
    "# Note: Test set must be transformed identically to training set\n",
    "X_test_sc = (X_test - mu_X) / sigma_X\n",
    "\n",
    "print(f'{mu_X.shape=}, {sigma_X.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Implementation\n",
    "\n",
    "We'll implement based on the above definitions.\n",
    "\n",
    "The model will be implemented as a class with an API that conforms to the `sklearn` models, specifically see\n",
    "`sklearn`'s [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.446969Z",
     "iopub.status.busy": "2022-03-31T05:57:22.446870Z",
     "iopub.status.idle": "2022-03-31T05:57:22.465612Z",
     "shell.execute_reply": "2022-03-31T05:57:22.465336Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(object):\n",
    "    def __init__(self, n_iter=100, learn_rate=0.1):\n",
    "        self.n_iter = n_iter\n",
    "        self.learn_rate = learn_rate\n",
    "        self._w = None\n",
    "        \n",
    "    def _add_bias(self, X: np.ndarray):\n",
    "        # Add a bias term column\n",
    "        ones_col = np.ones((X.shape[0], 1))\n",
    "        return np.hstack([ones_col, X])\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray, add_bias=True):\n",
    "        X = self._add_bias(X) if add_bias else X\n",
    "        \n",
    "        # Apply logistic model\n",
    "        z = np.dot(X, self.weights) # (N, D) * (D,)\n",
    "        return logistic(z) # shape (N,)\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        proba = self.predict_proba(X)\n",
    "        \n",
    "        # Apply naive threshold of .5\n",
    "        return np.array(proba > .5, dtype=np.int)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        n, d = X.shape # X is (N, D), y is (N,)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._w = np.random.randn(d + 1) * 0.1\n",
    "        \n",
    "        Xb = self._add_bias(X)\n",
    "\n",
    "        # Training loop\n",
    "        self._losses = []\n",
    "        for i in range(self.n_iter):\n",
    "            # Predicted probabilities\n",
    "            y_hat = self.predict_proba(Xb, add_bias=False)\n",
    "            \n",
    "            # Pointwise loss\n",
    "            loss = -y.dot(np.log(y_hat)) - ((1 - y).dot(np.log(1 - y_hat)))\n",
    "            \n",
    "            # See Exercise for gradient derivation\n",
    "            loss_grad = 1/n * Xb.T.dot(y_hat - y)  # dl/dw: (D+1, N) * (N,)\n",
    "            \n",
    "            # Optimization step\n",
    "            self._w += -self.learn_rate * loss_grad\n",
    "            self._losses.append(loss)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        if self._w is None:\n",
    "            raise ValueError(\"Model is not fitted\")\n",
    "        return self._w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.467358Z",
     "iopub.status.busy": "2022-03-31T05:57:22.467275Z",
     "iopub.status.idle": "2022-03-31T05:57:22.563355Z",
     "shell.execute_reply": "2022-03-31T05:57:22.563067Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "lr_model = BinaryLogisticRegression(n_iter=500, learn_rate=0.01)\n",
    "lr_model.fit(X_train_sc, y_train)\n",
    "\n",
    "plt.plot(lr_model._losses, label='$L_{\\mathcal{S}}$');\n",
    "plt.xlabel('training iteration'); plt.legend(); plt.grid(True); plt.title('Train loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.565187Z",
     "iopub.status.busy": "2022-03-31T05:57:22.565084Z",
     "iopub.status.idle": "2022-03-31T05:57:22.581336Z",
     "shell.execute_reply": "2022-03-31T05:57:22.581061Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred = lr_model.predict(X_train_sc)\n",
    "train_acc = np.mean(y_train_pred == y_train)\n",
    "print(f'train set accuracy: {train_acc*100:.2f}%')\n",
    "\n",
    "y_test_pred = lr_model.predict(X_test_sc)\n",
    "test_acc = np.mean(y_test_pred == y_test)\n",
    "print(f'test set accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we have $C$ classes? Can we still use logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A naïve approach is to train $C$ binary logistic regression classifiers, for example in a One vs. Rest scheme,\n",
    "and then predict based on the classifier returning the greatest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One major drawback of this approach is that it doesn't model the probability distribution over the possible classes, $P_{\\vec{Y}|\\vec{X}}$. \n",
    "\n",
    "For example, a sample might be classified as class A with probability $0.8$ and class $B$ with $0.7$ since nothing constrains the different classifiers.\n",
    "\n",
    "Also, without *calibrating* each model, their raw scores cannont reliably be compared even though they're in the same range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Softmax** is a function which can generate a probability distribution for our $C$ classes given raw prediction scores. It's defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\vec{z}) = \\frac{e^{\\vec{z}}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "note that this is a vector valued, multivariate function. The exponent in the enumerator operates elementwise.\n",
    "\n",
    "The result of softmax is a vector with elements in $[0,1]$ that all sum to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The multiclass model\n",
    "\n",
    "Our model can now be defined as\n",
    "\n",
    "$$\\hat{\\vec{y}} = h(\\vec{x}) = \\mathrm{softmax}(\\mattr{W}\\vec{x}+\\vec{b})$$\n",
    "\n",
    "where,\n",
    "- $\\hat{\\vec{y}}$ is a $C\\times 1$ vector of class probabilities.\n",
    "- ${\\vec{x}}$ is a $D\\times 1$ sample.\n",
    "- $\\mat{W}$ is a $D\\times C$ matrix representing the per-class weights.\n",
    "- $\\vec{b}$ is a per-class bias vector of length $C$.\n",
    "\n",
    "Probabilistic interpretation: $\\hat{y}_j = P(\\rvar{Y}=j|\\rvec{X}=\\vec{x})$.\n",
    "\n",
    "While not very powerful on it's own, this type of model is commonly found at the end of deep neural networks performing classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train such a model we need our labels to also be singleton probability distributions instead of simply the number of the correct class.\n",
    "\n",
    "We'll transform our labels to a 1-hot encoded vector corresponding to a delta distribution.\n",
    "For example, if $y^i=j$ then we'll create\n",
    "$$\n",
    "\\vec{y}^i = [0,\\dots,0,\\underbrace{1}_{j\\mathrm{th\\ component}},0,\\dots,0]^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-Entropy loss\n",
    "\n",
    "After defining the 1-hot label vectors, the multiclass extension of the binary cross-entropy is straightforward:\n",
    "\n",
    "$$\n",
    "\\ell(\\vec{y}, \\hat{\\vec{y}}) = - \\vectr{y} \\log(\\hat{\\vec{y}})\n",
    "$$\n",
    "\n",
    "Note that only the probability assigned to the correct class affects the loss.\n",
    "\n",
    "Minimizing this cross entropy can be interpreted as trying to move the probability distribution of model predictions towards the singleton distribution of the appropriate class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This time we'll tackle an image classification task, the MNIST database of handwritten digits.\n",
    "\n",
    "Today this is also considered a toy dataset, even though it was used in the past to benchmark classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:22.583530Z",
     "iopub.status.busy": "2022-03-31T05:57:22.583432Z",
     "iopub.status.idle": "2022-03-31T05:57:23.673426Z",
     "shell.execute_reply": "2022-03-31T05:57:23.673015Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "import plot_utils\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:23.675467Z",
     "iopub.status.busy": "2022-03-31T05:57:23.675341Z",
     "iopub.status.idle": "2022-03-31T05:57:23.728446Z",
     "shell.execute_reply": "2022-03-31T05:57:23.728149Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transforms that should be applied to each image in the dataset before returning it\n",
    "tf_ds = torchvision.transforms.ToTensor()\n",
    "\n",
    "batch_size = 64\n",
    "data_root = os.path.expanduser(\"~/.pytorch-datasets\")\n",
    "\n",
    "# Training and test datasets\n",
    "ds_train, ds_test = [\n",
    "    torchvision.datasets.MNIST(root=data_root, download=True, train=train, transform=tf_ds)   \n",
    "    for train in [True, False]\n",
    "]\n",
    "\n",
    "# Data loaders\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "x0, y0 = ds_train[0]\n",
    "n_features = torch.numel(x0)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:23.730338Z",
     "iopub.status.busy": "2022-03-31T05:57:23.730231Z",
     "iopub.status.idle": "2022-03-31T05:57:23.909829Z",
     "shell.execute_reply": "2022-03-31T05:57:23.909349Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Show first few samples\n",
    "print(f'x0: {x0.shape}, y0: {y0}')\n",
    "plot_utils.dataset_first_n(ds_train, 10, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that when training, we're actually working with **batches** of samples (we'll learn about SGD in the sebsequent lectures/tutorials):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:23.911944Z",
     "iopub.status.busy": "2022-03-31T05:57:23.911831Z",
     "iopub.status.idle": "2022-03-31T05:57:23.950133Z",
     "shell.execute_reply": "2022-03-31T05:57:23.949826Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x0, y0 = next(iter(dl_train))\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This time we'll use `pytorch` tensors and its [`autograd`](https://pytorch.org/docs/stable/autograd.html) functionality to implement our model.\n",
    "\n",
    "This means we wont have to implement any gradient calculations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's implement $\\mathrm{softmax}(\\cdot)$. We need a small numerical trick to prevent large numbers from exploding the exponentiation. You can verify that this doesn't influence the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:23.952303Z",
     "iopub.status.busy": "2022-03-31T05:57:23.952173Z",
     "iopub.status.idle": "2022-03-31T05:57:23.974439Z",
     "shell.execute_reply": "2022-03-31T05:57:23.974110Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    softmax(z)= e^(z) / sum(e^z)\n",
    "    :param z: A batch of C class scores per N samples, shape (N, C).\n",
    "    :returns: softmax per sample, of shape (N, C).\n",
    "    \"\"\"\n",
    "\n",
    "    # normalization trick to prevent numerical instability:\n",
    "    # shift so that the highest class score (per sample) is 0\n",
    "    zmax, _ = torch.max(z, dim=1, keepdim=True)\n",
    "    z = z - zmax # note broadcasting: (N,C) - (N,1)\n",
    "    \n",
    "    exp_z = torch.exp(z) # (N, C)\n",
    "    sum_exp = torch.sum(exp_z, dim=1, keepdim=True) # (N, 1)\n",
    "    return exp_z / sum_exp # probabilities, (N,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's test our softmax and calculate its derivative with `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:23.976580Z",
     "iopub.status.busy": "2022-03-31T05:57:23.976457Z",
     "iopub.status.idle": "2022-03-31T05:57:24.016133Z",
     "shell.execute_reply": "2022-03-31T05:57:24.015840Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z = torch.randn(size=(4,3), requires_grad=True)\n",
    "y = softmax(z)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.017936Z",
     "iopub.status.busy": "2022-03-31T05:57:24.017817Z",
     "iopub.status.idle": "2022-03-31T05:57:24.046073Z",
     "shell.execute_reply": "2022-03-31T05:57:24.045800Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = softmax(z)\n",
    "L = torch.sum(y) # scalar function of z \n",
    "\n",
    "# Calculate gradient: dL/dz\n",
    "torch.autograd.grad(L, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead of calling `autograd.grad()` directly with specific input tensors, `pytorch` provides us with a way to calculate the derivative of a tensor w.r.t. all the tensors which are **leaves** in it's computation graph (only $\\vec{z}$ in this case).\n",
    "\n",
    "This can be done by calling `.backward()` on a scalar tensor.\n",
    "As a result, the `.grad` property of leaf tensors will be populated with the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.047850Z",
     "iopub.status.busy": "2022-03-31T05:57:24.047737Z",
     "iopub.status.idle": "2022-03-31T05:57:24.069034Z",
     "shell.execute_reply": "2022-03-31T05:57:24.068735Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example with two leaves in the computaion graph\n",
    "z1 = torch.randn(size=(4,3), requires_grad=True)\n",
    "z2 = torch.randn(size=(1,3), requires_grad=True)\n",
    "z = z1 - z2\n",
    "\n",
    "y = softmax(z)\n",
    "L = torch.sum(y) # scalar function of z \n",
    "L.backward()     # Calculate derivative w.r.t. all leaves\n",
    "\n",
    "z1.grad, z2.grad # The leaves z1, z2 will have their .grad populated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the resulting computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.070855Z",
     "iopub.status.busy": "2022-03-31T05:57:24.070744Z",
     "iopub.status.idle": "2022-03-31T05:57:24.189514Z",
     "shell.execute_reply": "2022-03-31T05:57:24.189143Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torchviz\n",
    "torchviz.make_dot(L, params=dict(z1=z1, z2=z2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The next ingredient of the solution is the cross-entropy loss function.\n",
    "\n",
    "Recall that we need to encode our ground-truth labels as one-hot vectors to apply the multiclass cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.191534Z",
     "iopub.status.busy": "2022-03-31T05:57:24.191440Z",
     "iopub.status.idle": "2022-03-31T05:57:24.216881Z",
     "shell.execute_reply": "2022-03-31T05:57:24.216509Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y: Tensor, y_hat: Tensor, eps=1e-6):\n",
    "    \"\"\"\n",
    "    :param y:  Onehot-encoded ground-truth labels, shape (N, C)\n",
    "    :param y_hat: A batch of probabilities, shape (N,C)\n",
    "    :returns: Cross entropy between y and y_hat.\n",
    "    \"\"\"\n",
    "    return torch.sum( - y * torch.log(y_hat + eps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.218798Z",
     "iopub.status.busy": "2022-03-31T05:57:24.218696Z",
     "iopub.status.idle": "2022-03-31T05:57:24.239370Z",
     "shell.execute_reply": "2022-03-31T05:57:24.239057Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def onehot(y: Tensor, n_classes: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Encodes y of shape (N,) containing class labels in the range [0,C-1] as one-hot of shape (N,C).\n",
    "    \"\"\"\n",
    "    y = y.reshape(-1, 1) # Reshape y to (N,1)\n",
    "    zeros = torch.zeros(size=(len(y), n_classes), dtype=torch.float32) # (N,C)\n",
    "    ones = torch.ones_like(y, dtype=torch.float32)\n",
    "    \n",
    "    # scatter: put items from 'src' into 'dest' at indices correspondnig to 'index' along 'dim'\n",
    "    y_onehot = torch.scatter(zeros, dim=1, index=y, src=ones)\n",
    "    \n",
    "    return y_onehot # result has shape (N, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.241335Z",
     "iopub.status.busy": "2022-03-31T05:57:24.241127Z",
     "iopub.status.idle": "2022-03-31T05:57:24.264310Z",
     "shell.execute_reply": "2022-03-31T05:57:24.264005Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "onehot(torch.tensor([1, 3, 5, 0]), n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our model itself will just hold the parameters $\\mat{W}$ and $\\vec{b}$ and apply them to an input batch $\\mat{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.266157Z",
     "iopub.status.busy": "2022-03-31T05:57:24.266051Z",
     "iopub.status.idle": "2022-03-31T05:57:24.287258Z",
     "shell.execute_reply": "2022-03-31T05:57:24.286921Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MCLogisticRegression(object):\n",
    "    def __init__(self, n_features: int, n_classes: int):\n",
    "        # Define our parameter tensors: notice that now W and b are separate\n",
    "        # Specify we want to track their gradients with autograd\n",
    "        self.W = torch.randn(n_features, n_classes, requires_grad=True)\n",
    "        self.b = torch.randn(n_classes, requires_grad=True)\n",
    "        self.params = [self.W, self.b]\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        \"\"\"\n",
    "        :param X: A batch of samples, (N, D)\n",
    "        :return: A batch of class probabilities, (N, C)\n",
    "        \"\"\"\n",
    "        # X is (N, D), W is (D, C), b is (C,)\n",
    "        z = torch.mm(X, self.W) + self.b\n",
    "        y_hat = softmax(z)\n",
    "        return y_hat # (N, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try out the model and loss on the first batch.\n",
    "\n",
    "Note that we naïvely treat each pixel as a separate feature. We'll learn how to properly work with images in a future tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.289076Z",
     "iopub.status.busy": "2022-03-31T05:57:24.288990Z",
     "iopub.status.idle": "2022-03-31T05:57:24.309803Z",
     "shell.execute_reply": "2022-03-31T05:57:24.309511Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = MCLogisticRegression(n_features, n_classes)\n",
    "\n",
    "# Flatten images and convert labels to onehot\n",
    "x0_flat = x0.reshape(-1, n_features)\n",
    "y0_onehot =  onehot(y0, n_classes)\n",
    "\n",
    "print(f'x0_flat: {x0_flat.shape}')\n",
    "print(f'y0_onehot: {y0_onehot.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.311535Z",
     "iopub.status.busy": "2022-03-31T05:57:24.311432Z",
     "iopub.status.idle": "2022-03-31T05:57:24.341704Z",
     "shell.execute_reply": "2022-03-31T05:57:24.341414Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass and compute loss\n",
    "y0_hat = model(x0_flat)\n",
    "loss = cross_entropy_loss(y0_onehot, y0_hat)\n",
    "print('loss = ', loss)\n",
    "\n",
    "# Backward pass to populate .grad on leaf nodes\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we specified `require_grad=True` for our model parameters, every operation performed on these tensors is recorded, and a **computation graph** can be built, which included the model and loss calculation.\n",
    "Notice that the **leaves** in this graph are our parameters $\\mat{W}$ and $\\vec{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.343575Z",
     "iopub.status.busy": "2022-03-31T05:57:24.343462Z",
     "iopub.status.idle": "2022-03-31T05:57:24.403420Z",
     "shell.execute_reply": "2022-03-31T05:57:24.403037Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torchviz\n",
    "torchviz.make_dot(loss, params=dict(W=model.W, b=model.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This graph is what allows efficient implementation of the **back-propagation** algorithm which you'll learn about in the next lecture.\n",
    "\n",
    "By calling `.backward()` from the final loss tensor, pytorch automatically populated the `.grad` property of all leaves in this graph, without us having to explicitly specify them (`W` and `b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The optimization will be as before, but now we'll take the gradients from the `grad` property of our parameter tensors.\n",
    "\n",
    "Therefore, the optimizer only needs access to the parameter tensors from the model.\n",
    "Later you'll see that `pytorch`'s `Optimizer` classes work in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.405910Z",
     "iopub.status.busy": "2022-03-31T05:57:24.405767Z",
     "iopub.status.idle": "2022-03-31T05:57:24.434454Z",
     "shell.execute_reply": "2022-03-31T05:57:24.434135Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class SGDOptimizer:\n",
    "    \"\"\"\n",
    "    A simple gradient descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, params: Sequence[Tensor], learn_rate: float):\n",
    "        self._params = params\n",
    "        self._learn_rate = learn_rate\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Updates parameters in-place based on their gradients.\n",
    "        \"\"\"\n",
    "        with torch.autograd.no_grad(): # Don't track this operation\n",
    "            for param in self._params:\n",
    "                if param.grad is not None:\n",
    "                    param -= self._learn_rate * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Zeros the parameters' gradients if they exist.\n",
    "        \"\"\"\n",
    "        for param in self._params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inference and prediction accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.436398Z",
     "iopub.status.busy": "2022-03-31T05:57:24.436289Z",
     "iopub.status.idle": "2022-03-31T05:57:24.865375Z",
     "shell.execute_reply": "2022-03-31T05:57:24.865010Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(dataloader, model, max_batches=None):\n",
    "    n_correct = 0.\n",
    "    n_total = 0.\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        X = X.reshape(-1, n_features) # flatten images into vectors\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.autograd.no_grad():\n",
    "            y_hat = model(X)\n",
    "        \n",
    "        predictions = torch.argmax(y_hat, dim=1)\n",
    "        n_correct += torch.sum(predictions == y).type(torch.float32)\n",
    "        n_total += X.shape[0]\n",
    "        \n",
    "        if max_batches and i+1 >= max_batches:\n",
    "            break\n",
    "        \n",
    "    return (n_correct / n_total).item()\n",
    "\n",
    "test_set_acc = evaluate_accuracy(dl_test, MCLogisticRegression(n_features, n_classes))\n",
    "print(f'Test set accuracy pre-training: {test_set_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The training loop\n",
    "\n",
    "This is a crucial part of any ML pipeline where model parameters get updated iteratively.\n",
    "\n",
    "One pass over the entire training data is called an **epoch**.\n",
    "When using `pytorch`, your training loop will generally contain the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each epoch:\n",
    "    - Split training data into batches\n",
    "    - For each batch\n",
    "        - Forward pass: Compute predictions and build computation graph\n",
    "        - Calculate loss\n",
    "        - Set existing gradients to zero\n",
    "        - Backward pass: Use back-propagation algorithm to calculate the gradients\n",
    "        - Optimization step: Use the gradients to update the parameters\n",
    "    - Evaluate accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.867454Z",
     "iopub.status.busy": "2022-03-31T05:57:24.867240Z",
     "iopub.status.idle": "2022-03-31T05:57:24.889412Z",
     "shell.execute_reply": "2022-03-31T05:57:24.889080Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some training hyper-parameters\n",
    "epochs = 10\n",
    "max_batches = 50  # limit batches so training is fast (just as a demo)\n",
    "learn_rate = .005\n",
    "num_samples = len(ds_train)\n",
    "\n",
    "# Instantiate the model we'll train\n",
    "model = MCLogisticRegression(n_features, n_classes)\n",
    "\n",
    "# Instantiate the optimizer with model's parameters\n",
    "optimizer = SGDOptimizer(model.params, learn_rate=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T05:57:24.891427Z",
     "iopub.status.busy": "2022-03-31T05:57:24.891315Z",
     "iopub.status.idle": "2022-03-31T05:57:32.282770Z",
     "shell.execute_reply": "2022-03-31T05:57:32.282443Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Epoch: traverse all samples\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    # Loop over randdom batches of training data\n",
    "    for i, (X, y) in enumerate(dl_train):\n",
    "        \n",
    "        X = X.reshape(-1, n_features)\n",
    "        y_onehot = onehot(y, n_classes)\n",
    "        \n",
    "        # Forward pass: predictions and loss\n",
    "        y_hat = model(X)\n",
    "        loss = cross_entropy_loss(y_onehot, y_hat)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass: calculate gradients \n",
    "        loss.backward() \n",
    "        \n",
    "        # Update model using the calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        cumulative_loss += loss.item()\n",
    "        if i+1 > max_batches:\n",
    "            break\n",
    "\n",
    "    # Evaluation\n",
    "    test_accuracy = evaluate_accuracy(dl_test, model, max_batches)\n",
    "    train_accuracy = evaluate_accuracy(dl_train, model, max_batches)\n",
    "    \n",
    "    avg_loss = cumulative_loss/num_samples\n",
    "    print(f\"Epoch {e}. Avg Loss: {avg_loss:.3f}, Train Acc: {train_accuracy*100:.2f}, Test Acc: {test_accuracy*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Final notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is a very naive implementation, for example because\n",
    "    - We didn't treat the images properly.\n",
    "    - We didn't include any regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- PyTorch provides many functions and classes that we could have used, for example:\n",
    "  - Fully connected layer with model parameters\n",
    "  - Softmax\n",
    "  - SGD and many other optimizers\n",
    "  - Cross entropy loss\n",
    "  \n",
    "  however the purpose here was to show an (almost) from-scratch implementation using only tensors,\n",
    "  in order to see whats \"under the hood\" (more or less) of the PyTorch functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by [Aviv A. Rosenberg](https://avivr.net).<br>\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "\n",
    "- MartinThoma [CC0], via Wikimedia Commons https://commons.wikimedia.org/wiki/File:Perceptron-unit.svg\n",
    "- Dr. Nadav Cohen, http://www.cohennadav.com/files/icermw19_slides.pdf\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
