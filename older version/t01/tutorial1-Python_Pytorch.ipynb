{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "\n",
    "# Tutorial 1: Python and PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this tutorial, we will cover:\n",
    "* Course info \n",
    "* Environment setup with `conda`\n",
    "* Jupyter: Using notebooks\n",
    "* Pytorch:\n",
    "  - Tensors basics: indexing, datatypes, math\n",
    "  - Broadcasting\n",
    "  - Intro to automatic differentiation\n",
    "\n",
    "Also in this tutorial, but for self-study:\n",
    "* Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Administration and General Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "My info:\n",
    "- Aviv A. Rosenberg\n",
    "- avivr@cs.technion.ac.il\n",
    "- Office hour: Thursdays, 13:30.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Course:\n",
    "- Website is at https://vistalab-technion.github.io/cs236781/\n",
    "- Updates will be posted there (but emails will be sent via WebCourse)\n",
    "- Post questions on **Piazza** only! (Not email, not facebook)\n",
    "- Any questions about the **homeworks** or **tutorials**: I'll answer on Piazza.\n",
    "- For personal administrative requests/delays: email Chaim.\n",
    "- For appeals/questions about grades: Email Yaniv/Evgenii/Ben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Video Lectures (by Prof. Alex Bronstein)\n",
    "- Provide a high level presentation of most core topics of Deep Learning, including very recent topics.\n",
    "- Give mathematical background and justifications.\n",
    "\n",
    "In-class Lectures\n",
    "- Supplementary material with more in-depth examples or advanced topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tutorials:\n",
    "- Structure is usually a short theory reminders part and then step-by-step technical implementation of a real  problem.\n",
    "- Technical, meant to help you understand the implementation details behind deep learning.\n",
    "- **Highly relevant** for success in the homework assignments.\n",
    "- After this tutorial you should clone the [tutorials repo](https://github.com/vistalab-technion/cs236781-tutorials), install the conda env and play with the code.\n",
    "- Videos are available on course site (by Aviv Rosenberg) - requires a technion account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Homework:\n",
    "- Four HW assignments, quite heavy load. Best to tackle them after you have sufficient programming experience.\n",
    "- Almost entirely \"wet\" i.e. implementation of real algorithms with real data.\n",
    "- Should be done in pairs.\n",
    "- Some will require use of GPUs. We will provide access to course servers - **please register**.\n",
    "- Read the [getting started page](https://vistalab-technion.github.io/cs236781/assignments/getting-started) and [collaboration policy](https://vistalab-technion.github.io/cs236781/info/#administration) carefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To install and manage all the necessary packages and dependencies for the\n",
    "course tutorials and assignments, we use [conda](https://conda.io), a popular package-manager for python.\n",
    "\n",
    "- The tutorial notebooks and homework assignments come with an `environment.yml` file which defines which third-party libraries we depend on.\n",
    "- Conda will use this file to create a virtual environment for you.\n",
    "- This virtual environment includes python and all other packages and tools we specified, separated from any preexisting\n",
    "python installation you may have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Installation\n",
    "\n",
    "1. Install the python3 version of [miniconda](https://conda.io/miniconda.html).\n",
    "Follow the [installation instructions](https://conda.io/docs/user-guide/install/index.html)\n",
    "for your platform.\n",
    "\n",
    "2. Install all dependencies (into a virtual env) with `conda`:\n",
    "\n",
    "    ```shell\n",
    "    conda env update -f environment.yml\n",
    "    ```\n",
    "    \n",
    "    This will also create a new virtual env (`cs236781-tutorials`) if it doesn't already exist.\n",
    "\n",
    "3. To activate the virtual environment (set up `$PATH`):\n",
    "\n",
    "    ```shell\n",
    "    conda activate cs236781-tutorials\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also check what conda environments you have and which is active, run\n",
    "\n",
    "```shell\n",
    "conda env list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short demo of environment setup\n",
    "\n",
    "We'll now do a quick demo of the environment installation and working with `conda`, since usually there are many questions about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running Jupyter\n",
    "\n",
    "From a terminal, enter the folder contaning the tutorial notebooks.\n",
    "1. Make sure that the active conda environment is `cs236781-tutorials`:\n",
    "\n",
    "    ```shell\n",
    "    conda activate cs236781-tutorials\n",
    "    ```\n",
    "\n",
    "2. Run jupyter with\n",
    "\n",
    "    ```shell\n",
    "    jupyter lab\n",
    "    ```\n",
    "    \n",
    "    This will start a [jupyter lab](https://jupyterlab.readthedocs.io/en/stable/)\n",
    "    server and open your browser at the local server's url. You can now start working with the notebooks.\n",
    "\n",
    "If you're new to jupyter notebooks, you can get started by reading the\n",
    "[UI guide](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)\n",
    "and also about how to use notebooks in\n",
    "[JupyterLab](https://jupyterlab.readthedocs.io/en/latest/user/notebook.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Important Note**: The course homework and tutorials use **different** conda envs! Make sure to use the correct one each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Jupyter basics\n",
    "\n",
    "Jupyter notebooks consist mainly of code and markdown cells.\n",
    "The code cells contain code that is run by a `kernel`, an\n",
    "interpreter for some programming language, python in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a code cell; it can contain arbitrary python code.\n",
    "\n",
    "foo = 'bar'\n",
    "print(foo)\n",
    "\n",
    "def the_answer():\n",
    "    return 42\n",
    "\n",
    "# The output of the last expression in a cell is shown\n",
    "2*the_answer()\n",
    "the_answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Variables and functions defined in a code cell are available in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ans = the_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a markdown cell. You can use markdown syntax to format your text, and also include equations\n",
    "written in $\\LaTeX$:\n",
    "\n",
    "$$\n",
    "e^{i\\pi} - 1 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other useful things to know about:\n",
    "* Opening a console for notebook\n",
    "* Restarting kernel\n",
    "* Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.7 ns ± 0.148 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit the_answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python is a great general-purpose programming language on its own and with the addition of a few\n",
    "popular libraries such as `numpy`, `scipy`, `pandas`, `scikit-learn`, `matplotlib` and others it becomes an\n",
    "effective scientific computing environment.\n",
    "\n",
    "Today it is also the most-used language for machine learning both in research and industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recently many **Deep Learning frameworks** have emerged for python.\n",
    "Arguably the most notable ones in 2021 are **TensorFlow** (with the Keras frontend) and **PyTorch**.\n",
    "\n",
    "In this course we'll use PyTorch, which is currently [the leading DL framework](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry) for research.\n",
    "\n",
    "<center><img src=\"https://thegradient.pub/content/images/2019/10/number_medium.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many of you may have some experience with Python and numpy; for the rest of you, this notebook can serve as a quick crash course both on the Python programming language and on the use of PyTorch for scientific computing with tensors.\n",
    "\n",
    "However, we recommend getting up to speed with python using the numerous availble online resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you have previous knowledge in Matlab,\n",
    "we recommend the [numpy for Matlab users](https://docs.scipy.org/doc/numpy-1.15.0/user/numpy-for-matlab-users.html) page as a useful resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Credit: Parts of the Python tutorial here were adapted from the [CS231n Python tutorial](http://cs231n.github.io/python-numpy-tutorial/) by Justin Johnson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "toc-nb-collapsed": true
   },
   "source": [
    "## Basics of Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def quicksort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\n",
    "\n",
    "print(quicksort([3,6,8,10,1,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Python has great [documentation](https://docs.python.org/3)! Use it often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Packages and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A python **module** is simply a python file (`.py`), which can contain functions, classes and even top-level code.\n",
    "\n",
    "A **package** is a collection of modules within a directory. Python comes with a standard library which\n",
    "includes many useful packages.\n",
    "\n",
    "A package must be imported before use. They can be imported like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the python standard library\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can define our own packages and modules. This tutorial comes with a `demo_package`\n",
    "which includes a `demo_module`.\n",
    "\n",
    "Any object can be imported from a module like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import just a specific function from a specific module from a specific package\n",
    "from demo_package.demo_module import demo_func\n",
    "\n",
    "demo_func(math.pi)\n",
    "print('FOO', file=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integers and floats work as you would expect from other languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = 3\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(x + 1)  # Addition;\n",
    "print(x - 1)  # Subtraction;\n",
    "print(x * 2)  # Multiplication;\n",
    "print(x ** 2)  # Exponentiation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "x *= 2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y = 2.5\n",
    "print(type(y))\n",
    "print(y, y + 1, y * 2, y ** 2, y / 2, y // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike many languages, Python does not have unary increment (x++) or decrement (x--) operators.\n",
    "\n",
    "Python also has built-in types for long integers and complex numbers; you can find all of the details in the [documentation](https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-long-complex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booleans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implements all of the usual operators for Boolean logic, but uses English words rather than symbols (`&&`, `||`, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t, f = True, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we let's look at the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(t and f) # Logical AND\n",
    "print(t or f ) # Logical OR\n",
    "print(not t  ) # Logical NOT\n",
    "print(t != f ) # Logical XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hello = 'hello'   # String literals can use single quotes\n",
    "world = \"world\"   # or double quotes; it does not matter.\n",
    "hello, len(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# String concatenation\n",
    "'aaa ' + 'bbb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several way to created formatted strings, here are a couple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'hello'\n",
    "a = [1,2,3]\n",
    "\n",
    "# sprintf style string formatting\n",
    "print('%s %s: pi=%.5f' % (s, a, math.pi))\n",
    "\n",
    "# formatting with f-string literals (python 3.6+)\n",
    "print(f'{s} {a}: pi={math.pi:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String objects have a bunch of useful methods; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = \"hello\"\n",
    "print(s.capitalize() ) # Capitalize a string; prints \"Hello\"\n",
    "print(s.upper()      ) # Convert a string to uppercase; prints \"HELLO\"\n",
    "print(s.rjust(7)     ) # Right-justify a string, padding with spaces; prints \"  hello\"\n",
    "print(s.center(7)    ) # Center a string, padding with spaces; prints \" hello \"\n",
    "print(s.replace('l', '(ell)'))  # Replace all instances of one substring with another\n",
    "print('  world '.strip())  # Strip leading and trailing whitespace; prints \"world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a list of all string methods in the [documentation](https://docs.python.org/3/library/stdtypes.html#string-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python includes several built-in container types: lists, dictionaries, sets, and tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list is the Python equivalent of an array, but is resizeable and can contain elements of different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xs = [3, 1, 2]   # Create a list\n",
    "print(xs)\n",
    "print(xs[2], xs[-1]) # Negative indices count from the end of the list; prints \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xs[2] = 'foo'    # Lists can contain elements of different types\n",
    "print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xs.append('bar') # Add a new element to the end of the list\n",
    "print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = xs.pop()     # Remove and return the last element of the list\n",
    "x, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums = list(range(5))\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums[2:4]    # Get a slice from index 2 to 4 (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums[2:]     # Get a slice from index 2 to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums[:2]     # Get a slice from the start to index 2 (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums[:]      # Get a slice of the whole list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums[:-1]    # Slice indices can be negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums[0:4:2]  # Can also specify slice step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums[2:4] = [8, 9] # Assign a new sublist to a slice\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete elements from a list\n",
    "nums[0:1] = []\n",
    "del nums[-1]\n",
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can loop over the elements of a list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals = ['cat', 'dog', 'monkey']\n",
    "for animal in animals:\n",
    "    print(animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want access to the index of each element within the body of a loop, use the built-in `enumerate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals = ['cat', 'dog', 'monkey']\n",
    "for idx, animal in enumerate(animals):\n",
    "    print(f'#{idx+1}: {animal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums = [0, 1, 2, 3, 4]\n",
    "squares = []\n",
    "for x in nums:\n",
    "    squares.append(x ** 2)\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make this code simpler using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "squares = [x ** 2 for x in nums]\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions can also contain conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "even_squares = [x ** 2 for x in nums if x % 2 == 0]\n",
    "even_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions can be nested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums2 = [-1, 1]\n",
    "[x * y for x in nums for y in nums2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary stores (key, value) pairs. In other languages this is known as a `Map` or `Hash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\n",
    "print(d['cat'])       # Get an entry from a dictionary\n",
    "print('cat' in d)     # Check if a dictionary has a given key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d['fish'] = 'wet'    # Set an entry in a dictionary\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to access a non-existing key raises a KeyError\n",
    "try:\n",
    "    d['monkey']\n",
    "except KeyError as e:\n",
    "    print(e, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(d.get('monkey', 'N/A'))  # Get an element with a default\n",
    "print(d.get('fish', 'N/A'))    # Get an element with a default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "del d['fish']        # Remove an element from a dictionary\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Iteration over keys\n",
    "d = {'person': 2, 'cat': 4, 'spider': 8}\n",
    "for animal in d:\n",
    "    print(f'A {animal} has {d[animal]} legs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over key-value pairs\n",
    "d = {'person': 2, 'cat': 4, 'spider': 8}\n",
    "for animal, num_legs in d.items():\n",
    "    print(f'A {animal} has {num_legs} legs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary using the built-in dict() function\n",
    "dict(foo=1, bar=2, baz=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are similar to list comprehensions, but allow you to easily construct dictionaries. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "nums = [0, 1, 2, 3, 4]\n",
    "even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}\n",
    "even_num_to_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set is an unordered collection of distinct elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals = {'cat', 'dog'}\n",
    "print(animals)\n",
    "print('cat' in animals )  # Check if an element is in a set\n",
    "print('fish' in animals) # prints \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals.add('fish') # Add an element to a set\n",
    "print('fish' in animals)\n",
    "len(animals) # Number of elements in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals.add('cat')       # Adding an element that is already in the set does nothing\n",
    "animals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Loops_: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals = {'cat', 'dog', 'fish'}\n",
    "for idx, animal in enumerate(animals):\n",
    "    print(f'#{idx}: {animal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like lists and dictionaries, we can easily construct sets using set comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "s = {int(sqrt(x)) for x in range(37)}\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tuple is an **immutable** ordered list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (1, 2, 'three')\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used in ways similar to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0:1], t[1:3], t[-1], len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tuple can be used a key in a dictionary and as an element of a sets, while **lists cannot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tuple (and also a list) can be **unpacked**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one, two, three = t\n",
    "one, two, three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when retuning multiple values from a function (or code block in a jupyter notebook, as above)\n",
    "your values get wrapped in a tuple, and the tuple is what's returned.\n",
    "Unpacking the return value of a function can make it seem as if multiple values were returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python functions are defined using the `def` keyword. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 'positive'\n",
    "    elif x < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'zero'\n",
    "\n",
    "for x in [-1, 0, 1]:\n",
    "    print(sign(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often define functions to take optional keyword arguments, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def hello(name, loud=False):\n",
    "    if loud:\n",
    "        print('HELLO, %s' % name.upper())\n",
    "    else:\n",
    "        print('Hello, %s!' % name)\n",
    "\n",
    "hello('Bob')\n",
    "hello('Fred', loud=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional and Keyword arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python functions are very flexible in the way they accept arguments. Both positional (regular) and keyword\n",
    "arguments are supported and can be mixed in the same definition. Additionally, extra arguments can be passed in with the `*args` and `**kwargs` constructs.\n",
    "\n",
    "Here's a function with three positional arguments and three keyword arguments which also accepts extra \n",
    "positional and keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(a1, a2, a3, *extra_args, kw1='foo', kw2='bar', kw3=3, **extra_kwargs):\n",
    "    print(f'Got positional args: {(a1, a2, a3)}')\n",
    "    print(f'Got keyword args   : {dict(kw1=kw1, kw2=kw3, kw3=kw3)}')\n",
    "    print(f'Got extra positional args: {extra_args}')\n",
    "    print(f'Got extra keyword args: {extra_kwargs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be called in many ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc(1,2,3,4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_args = [1,2,3,4]\n",
    "myfunc(*my_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc(1,2,3, kw3=3, kw2=2, foo='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kwargs = dict(kw1=1, kw2=2, kw3=3, kw4=4)\n",
    "myfunc(1,2,3, **my_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that keyword args can be omitted, while positional args cannot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    myfunc(1,2)\n",
    "except TypeError as e:\n",
    "    print(e, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for defining classes in Python is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Greeter:\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # Create an instance variable\n",
    "\n",
    "    # Instance method\n",
    "    def greet(self, loud=False):\n",
    "        if loud:\n",
    "            print('HELLO, %s!' % self.name.upper())\n",
    "        else:\n",
    "            print('Hello, %s' % self.name)\n",
    "\n",
    "g = Greeter('Fred')  # Construct an instance of the Greeter class\n",
    "g.greet()            # Call an instance method\n",
    "g.greet(loud=True)   # Call an instance method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes can implement special **magic functions** that enable them to be integrated nicely with other python code. Magic functions have special names that start and end with `__`.\n",
    "\n",
    "For example, here's a class that can be indexed with `[]` and iterated over with a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleCollection(object):\n",
    "    def __init__(self):\n",
    "        self.items = [100, 200, 300]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        class ExampleIter():\n",
    "            def __init__(self, collection):\n",
    "                self.idx = 0\n",
    "                self.collection = collection\n",
    "                \n",
    "            def __next__(self):\n",
    "                if self.idx >= len(self.collection):\n",
    "                    raise StopIteration()\n",
    "                x = self.collection[self.idx]\n",
    "                self.idx += 1\n",
    "                return x\n",
    "                \n",
    "        return ExampleIter(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ExampleCollection()\n",
    "print('length=', len(example)) # invokes __len__\n",
    "print('example[0]=', example[0]) # invokes __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in example: # invokes __iter__ and it's __next__\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other magic functions exist. Consult the docs and see if you can catch 'em all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "PyTorch is a relatively new yet widely used deep learning framework for python.\n",
    "\n",
    "It can also be used as a general scientific computing library, as it provides a high-performance multidimensional array object, with GPU support.\n",
    "\n",
    "We'll refer to such n-dimentional arrays as **tensors** in accordance with the deep learning terminology.\n",
    "Crucially, pytorch supports **automatic differentiation** through arbitrary computations performed on its tensors.\n",
    "\n",
    "During the course we'll use it extensively and learn many parts of its API.\n",
    "You should also familiarize yourself with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you when implementing your own models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This notebook will show only **a small part** of PyTorch's API, the `Tensor` class.\n",
    "This class is very similar to numpy's `ndarray`, and provides much of the same functionality.\n",
    "However, it also has two important distinctions:\n",
    "- Support for GPU computations.\n",
    "- Can store extra data needed for implementing **automatic differentiation** used for back propagation:\n",
    "    - A tensor of the same dimentions containing the gradient of this tensor w.r.t. some scalar (e.g. loss).\n",
    "    - A node representing an operation in the computational graph that produced this tensor.\n",
    "\n",
    "In the next tutorials we will examine these concepts further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This notebook will show some brief examples, just to get a feel for it and compare it to the usual numpy `ndarray`s.\n",
    "\n",
    "You will be using both PyTorch tensors and numpy `ndarray`s extensively throughout the course homework assignments, and in general when implementing deep learning algorithms.\n",
    "Although we'll mainly use **PyTorch** tensors for implementing our Deep Learning systems, it's still important to be proficient with `numpy`, since:\n",
    "1. They concepts are very similar. Understanting one will help you quickly be proficient with the other.\n",
    "1. You'll find that you need to switch between the two when working with read DL systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To use pytorch, we first need to import the `torch` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tensors: n-dimentional arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An tensor represents an n-dimentional grid of values, all of the same type, and is indexed by a tuple of nonnegative integers.\n",
    "\n",
    "- The the **rank** of the tensor is the number of dimensions it has\n",
    "- The **shape** of a tensor is a tuple of integers giving the number of elements along each dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can initialize tensors from nested Python lists, and access elements using square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3.])  # Create a rank 1 array\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing always returns tensors\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .item() to get a python scalar \n",
    "a[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two very important properties of any tensor are its `shape` and `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_arr(arr, pre_text=''):\n",
    "    print(f'shape={tuple(arr.shape)} dtype={arr.dtype}:')\n",
    "    print(f'{pre_text}{arr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.float32:\n",
      "tensor([1., 2., 3.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2., 3.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] = 5                 # Change an element of the array\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 3) dtype=torch.float32:\n",
      "tensor([[1.0000, 2.0000, 3.0000],\n",
      "        [4.0000, 5.0000, 6.7000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([[1,2,3],[4,5,6.7]])   # Create a rank 2 array\n",
    "print_arr(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(2.), tensor(4.))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0, 0], b[0, 1], b[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Torch also provides many functions to create arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 2))  # Create an array of all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1, 10))   # Create an array of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.2000, 7.2000, 7.2000],\n",
       "        [7.2000, 7.2000, 7.2000],\n",
       "        [7.2000, 7.2000, 7.2000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((3, 3), 7.2) # Create a constant array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4, dtype=torch.int) # Create an identity matrix of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0316, 0.9702, 0.1566],\n",
       "         [0.8995, 0.6036, 0.5985],\n",
       "         [0.2884, 0.0583, 0.7011],\n",
       "         [0.5481, 0.8092, 0.8537]],\n",
       "\n",
       "        [[0.8373, 0.4176, 0.4366],\n",
       "         [0.7384, 0.3080, 0.8267],\n",
       "         [0.7264, 0.1527, 0.4626],\n",
       "         [0.2111, 0.0363, 0.1587]],\n",
       "\n",
       "        [[0.9155, 0.2817, 0.2180],\n",
       "         [0.4676, 0.2991, 0.3171],\n",
       "         [0.7451, 0.8384, 0.0388],\n",
       "         [0.3747, 0.3588, 0.9764]],\n",
       "\n",
       "        [[0.6504, 0.6286, 0.5923],\n",
       "         [0.2936, 0.3447, 0.7068],\n",
       "         [0.0286, 0.7618, 0.9506],\n",
       "         [0.2290, 0.5463, 0.7418]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((4,4,3)) # Create a 3d-array filled with U[0,1] random values\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Array rank\n",
    "\n",
    "In `torch` **rank** means **number of dimensions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**rank-0** arrays are scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=() dtype=torch.int64:\n",
      "17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a0 = torch.tensor(17)\n",
    "print_arr(a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get scalar as a python float\n",
    "a0.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**rank-1** arrays of length `n` have a shape of `(n,)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.int64:\n",
      "tensor([1, 2, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A rank-1 array\n",
    "a1 = torch.tensor([1,2,3])\n",
    "\n",
    "print_arr(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(1,) dtype=torch.float32:\n",
      "tensor([3.1400])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A rank-1 array scalar\n",
    "print_arr(torch.tensor([3.14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**rank-2** arrays have a shape of `(n,m)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 3) dtype=torch.int64:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a2 = torch.tensor([[1,2,3], [4,5,6]])\n",
    "\n",
    "print_arr(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-25T08:17:54.396587Z",
     "iopub.status.busy": "2021-03-25T08:17:54.395962Z",
     "iopub.status.idle": "2021-03-25T08:17:54.415353Z",
     "shell.execute_reply": "2021-03-25T08:17:54.414781Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A column vector is also rank-2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3, 1) dtype=torch.int64:\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_col = a1.reshape(-1, 1)\n",
    "\n",
    "print_arr(a_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-25T08:17:54.418531Z",
     "iopub.status.busy": "2021-03-25T08:17:54.418004Z",
     "iopub.status.idle": "2021-03-25T08:17:54.433799Z",
     "shell.execute_reply": "2021-03-25T08:17:54.434351Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And a row vector is also rank-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(1, 3) dtype=torch.int64:\n",
      "tensor([[1, 2, 3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_row = a1.reshape(1, -1)\n",
    "\n",
    "print_arr(a_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**rank-k** arrays have a shape of `(n1,...,nk)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 3, 4) dtype=torch.float32:\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.zeros((2,3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2, 2, 2) dtype=torch.float32:\n",
      "tensor([[[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.ones((2,2,2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tensor math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Elementwise operations\n",
    "Basic mathematical functions **operate elementwise** on arrays, and are available both as operator overloads and as functions in the `torch` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]], dtype=torch.float)\n",
    "y = torch.tensor([[5,6],[7,8]], dtype=torch.float)\n",
    "\n",
    "# Elementwise basic math\n",
    "print_arr(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(x / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elementwise functions\n",
    "print_arr(torch.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[ 2.7183,  7.3891],\n",
      "        [20.0855, 54.5981]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.float32:\n",
      "tensor([[0.0000, 0.6931],\n",
      "        [1.0986, 1.3863]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are of course many more elementwise operations inmplemented by `torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inner and outer products\n",
    "\n",
    "Unlike e.g. MATLAB, `*` is elementwise multiplication, not matrix multiplication (as we saw above).\n",
    "\n",
    "We can instead use the `torch.matmul()` function to:\n",
    "- compute inner or outer products of vectors,\n",
    "- multiply a vector by a matrix, and to\n",
    "- multiply matrices, and more generally n-d tensors.\n",
    "\n",
    "Note: We can also use python's `@` operator as a shorthand for `matmul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(219)\n",
      "tensor(219)\n",
      "tensor(219)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([9,10])\n",
    "w = torch.tensor([11, 12])\n",
    "\n",
    "# Inner product of vectors; both produce 219\n",
    "print(torch.matmul(v, w))\n",
    "print(torch.dot(v, w))  # dot() is for 1d-tensors only\n",
    "print(v @ w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rank-1 arrays arrays are somewhat special in that `torch` can treat them both as column or as row vectors.\n",
    "Arrays of different rank have different semantics when using them in vector-vector or vector-matrix products, so always make sure you know what shapes you're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.int64:\n",
      "a1\t\ttensor([1, 2, 3])\n",
      "\n",
      "shape=(1, 3) dtype=torch.int64:\n",
      "a_row\t\ttensor([[1, 2, 3]])\n",
      "\n",
      "shape=(3, 1) dtype=torch.int64:\n",
      "a_col\t\ttensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(a1, 'a1\\t\\t')\n",
    "print_arr(a_row, 'a_row\\t\\t')\n",
    "print_arr(a_col, 'a_col\\t\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=() dtype=torch.int64:\n",
      "a1 @ a1 =\t14\n",
      "\n",
      "shape=(1,) dtype=torch.int64:\n",
      "a_row @ a1 =\ttensor([14])\n",
      "\n",
      "shape=(1,) dtype=torch.int64:\n",
      "a1 @ a_col =\ttensor([14])\n",
      "\n",
      "shape=(1, 1) dtype=torch.int64:\n",
      "a_row @ a_col =\ttensor([[14]])\n",
      "\n",
      "shape=(3, 3) dtype=torch.int64:\n",
      "a_col @ a_row =\n",
      "tensor([[1, 2, 3],\n",
      "        [2, 4, 6],\n",
      "        [3, 6, 9]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner products, but output dimenstions are different\n",
    "print_arr(torch.matmul(a1, a1), 'a1 @ a1 =\\t')\n",
    "assert torch.matmul(a1, a1) == a1 @ a1\n",
    "\n",
    "print_arr(torch.matmul(a_row, a1), 'a_row @ a1 =\\t')\n",
    "assert torch.matmul(a_row, a1) == a_row @ a1\n",
    "\n",
    "print_arr(torch.matmul(a1, a_col), 'a1 @ a_col =\\t')\n",
    "assert torch.matmul(a1, a_col) == a1 @ a_col\n",
    "\n",
    "print_arr(torch.matmul(a_row, a_col), 'a_row @ a_col =\\t')\n",
    "assert torch.matmul(a_row, a_col) == a_row @ a_col\n",
    "\n",
    "# Outer product\n",
    "print_arr(torch.matmul(a_col, a_row), 'a_col @ a_row =\\n')\n",
    "assert torch.all(torch.matmul(a_col, a_row) == a_col @ a_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Non-elementwise operations\n",
    "\n",
    "Torch provides many useful functions for performing computations on arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 3) dtype=torch.float32:\n",
      "tensor([[1., 2., 3.],\n",
      "        [3., 4., 5.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[3,4,5]], dtype=torch.float)\n",
    "print_arr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=() dtype=torch.float32:\n",
      "18.0\n",
      "\n",
      "shape=(3,) dtype=torch.float32:\n",
      "tensor([2., 3., 4.])\n",
      "\n",
      "shape=(2,) dtype=torch.float32:\n",
      "tensor([ 6., 60.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.sum(x))  # Compute sum of all elements\n",
    "print_arr(torch.mean(x, dim=0))  # Compute mean of each column\n",
    "print_arr(torch.prod(x, dim=1)) # Compute product of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(1, 3) dtype=torch.float32:\n",
      "tensor([[2., 3., 4.]])\n",
      "\n",
      "shape=(2, 1) dtype=torch.float32:\n",
      "tensor([[ 6.],\n",
      "        [60.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In many cases, it's useful to aggregate but keep the original rank\n",
    "print_arr(torch.mean(x, dim=0, keepdim=True))\n",
    "print_arr(torch.prod(x, dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can find the full list of mathematical functions provided by torch in the [documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pytorch offers several ways to index into arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Slicing**\n",
    "\n",
    "Similar to Python lists, tensors can be sliced. Since they may be multidimensional, you must specify **a slice for each dimension** of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3, 4) dtype=torch.int64:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "\n",
    "print_arr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.int64:\n",
      "tensor([[2, 3],\n",
      "        [6, 7]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = a[:2, 1:3]\n",
    "\n",
    "print_arr(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A slice of an array is a **view** into the same in-memory data, so modifying it will modify the original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 77777,     3,     4],\n",
       "        [    5,     6,     7,     8],\n",
       "        [    9,    10,    11,    12]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing a view\n",
    "b[0, 0] = 77777\n",
    "\n",
    "# ...modifies original\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also mix integer indexing with slice indexing.\n",
    "However, doing so will yield an array of **lower rank** than the original array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Two ways of accessing the data in the middle row of the array.\n",
    "- Mixing integer indexing with slices yields an array of lower rank\n",
    "- Using only slices yields an array of the same rank as the original array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(4,) dtype=torch.int64:\n",
      "tensor([5, 6, 7, 8])\n",
      "\n",
      "shape=(1, 4) dtype=torch.int64:\n",
      "tensor([[5, 6, 7, 8]])\n",
      "\n",
      "shape=(1, 4) dtype=torch.int64:\n",
      "tensor([[5, 6, 7, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_r1 = a[1, :]    # Rank 1 view of the second row of a  \n",
    "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
    "row_r3 = a[[1], :]  # Rank 2 view of the second row of a\n",
    "\n",
    "print_arr(row_r1)\n",
    "print_arr(row_r2)\n",
    "print_arr(row_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.int64:\n",
      "tensor([ 2,  6, 10])\n",
      "\n",
      "shape=(3, 1) dtype=torch.int64:\n",
      "tensor([[ 2],\n",
      "        [ 6],\n",
      "        [10]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can make the same distinction when accessing columns of an array:\n",
    "col_r1 = a[:, 1]\n",
    "col_r2 = a[:, 1:2]\n",
    "\n",
    "print_arr(col_r1)\n",
    "print_arr(col_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Integer-array indexing** \n",
    "\n",
    "- When you slice, the resulting array view will always be a subarray of the original array.\n",
    "- Integer array indexing allows you to construct arbitrary arrays using the data from another array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3, 2) dtype=torch.int64:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
    "print_arr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.int64:\n",
      "tensor([1, 4, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example of integer array indexing.\n",
    "# The returned array will have shape (3,)\n",
    "print_arr(a[ [0, 1, 2], [0, 1, 0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.int64:\n",
      "tensor([1, 4, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The above example of integer array indexing is equivalent to this:\n",
    "print_arr(torch.tensor([a[0, 0], a[1, 1], a[2, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# When using integer array indexing, you can reuse the same\n",
    "# element from the source array:\n",
    "print(a[[0, 0], [1, 1]])\n",
    "\n",
    "# Equivalent to the previous integer array indexing example\n",
    "print(torch.tensor([a[0, 1], a[0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One useful trick with integer array indexing is selecting or mutating one element from each row of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3],\n",
       "        [ 4,  5,  6],\n",
       "        [ 7,  8,  9],\n",
       "        [10, 11, 12]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new array from which we will select elements\n",
    "a = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  6,  7, 11])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an array of column indices (notice it can repeat)\n",
    "col_idx = torch.tensor([0, 2, 0, 1])\n",
    "\n",
    "# Select one element from each row of a using the indices in b\n",
    "a[torch.arange(4), col_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1001,    2,    3],\n",
       "        [   4,    5, 1006],\n",
       "        [1007,    8,    9],\n",
       "        [  10, 1011,   12]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mutate one element from each row of a using the indices in b\n",
    "a[torch.arange(4), col_idx] += 1000\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Boolean array indexing**\n",
    "\n",
    "This type of indexing is used to select the elements of an array that satisfy some condition\n",
    "(similar to MATLAB's logical indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3, 2) dtype=torch.int64:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
    "print_arr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False],\n",
       "        [ True,  True],\n",
       "        [ True,  True]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the elements of a that are bigger than 2;\n",
    "# this returns a numpy array of Booleans of the same\n",
    "# shape as a, where each slot of bool_idx tells\n",
    "# whether that element of a is > 2.\n",
    "bool_idx = (a > 2)\n",
    "bool_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use boolean array indexing to construct a rank 1 array\n",
    "# consisting of the elements of a corresponding to the True values\n",
    "# of bool_idx\n",
    "a[a>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Every tensor is a grid of elements of the same type. `Pytorch` provides a large set of numeric datatypes that you can use to construct arrays.\n",
    "Pytorch tries to guess a datatype when you create an array, but functions that construct arrays usually also include an optional argument to explicitly specify the datatype.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.float32, torch.int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])  # Let torch choose the datatype\n",
    "y = torch.tensor([1.0, 2.0])  # Let torch choose the datatype\n",
    "z = torch.tensor([1, 2], dtype=torch.int64)  # Force a particular datatype\n",
    "\n",
    "x.dtype, y.dtype, z.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Changing and adding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can **transpose** dimensions within an array using arbitrary axis permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(5, 3) dtype=torch.float32:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "shape=(5, 3) dtype=torch.float32:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((3, 5))\n",
    "print_arr(a.transpose(0, 1))\n",
    "print_arr(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use `permute()` to transpose multiple dimensions simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(4, 2, 6) dtype=torch.float32:\n",
      "tensor([[[  1.,   1.,   1.,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,   1.,   1.,   1.]],\n",
      "\n",
      "        [[  1.,   1.,   1.,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,   1.,   1.,   1.]],\n",
      "\n",
      "        [[  1.,   1.,   1.,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1., 777.,   1.,   1.]],\n",
      "\n",
      "        [[  1.,   1.,   1.,   1.,   1.,   1.],\n",
      "         [  1.,   1.,   1.,   1.,   1.,   1.]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 4, 6))\n",
    "a[1,2,3] = 777\n",
    "\n",
    "print_arr(a.permute(1,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that an element `[x,y,z]` moves to position `[y,x,z]` after a transpose with this permutation (1,0,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another important feature is **reshaping** an array into different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3, 6) dtype=torch.int64:\n",
      "tensor([[11, 88, 96,  6,  3, 48],\n",
      "        [ 2, 19,  0,  7, 58, 70],\n",
      "        [45, 51, 41, 37, 72, 29]])\n",
      "\n",
      "shape=(2, 9) dtype=torch.int64:\n",
      "tensor([[11, 88, 96,  6,  3, 48,  2, 19,  0],\n",
      "        [ 7, 58, 70, 45, 51, 41, 37, 72, 29]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 100, (3, 6))\n",
    "print_arr(a)\n",
    "\n",
    "print_arr(torch.reshape(a, (2, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When reshaping, we need to make sure to preserve the same number of elements.\n",
    "Use `-1` in one of the dimensions to tell numpy to \"figure it out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also combine multiple arrays with **concatenation** along an arbitrary axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 2) dtype=torch.int64:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "shape=(1, 2) dtype=torch.int64:\n",
      "tensor([[5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6]])\n",
    "print_arr(a)\n",
    "print_arr(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3, 2) dtype=torch.int64:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.cat((a, b), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(2, 3) dtype=torch.int64:\n",
      "tensor([[1, 2, 5],\n",
      "        [3, 4, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_arr(torch.cat((a, b.T), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Broadcasting is a powerful mechanism that allows pytorch to work with arrays of **different shapes** when performing arithmetic operations. This mechanism also exists in numpy with the same semantics.\n",
    "\n",
    "Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, suppose that we want to add a constant vector to each row of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(4, 3) dtype=torch.int64:\n",
      "x=\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "\n",
      "shape=(3,) dtype=torch.int64:\n",
      "\n",
      "v=tensor([1, 0, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will add the vector v to each row of the matrix x,\n",
    "# storing the result in the matrix y\n",
    "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "\n",
    "print_arr(x,'x=\\n')\n",
    "print_arr(v, '\\nv=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Naïve approach**: Use a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  4],\n",
       "        [ 5,  5,  7],\n",
       "        [ 8,  8, 10],\n",
       "        [11, 11, 13]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.empty_like(x)   # Create an empty matrix with the same shape as x\n",
    "\n",
    "# Add the vector v to each row of the matrix x with an explicit loop\n",
    "for i in range(4):\n",
    "    y[i, :] = x[i, :] + v\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This works; however computing explicit loops in Python is **slow**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Naïve approach 2**: adding the vector v to each row of the matrix `x` is equivalent to forming a matrix `vv` by stacking multiple copies of `v` vertically, then performing elementwise summation of `x` and `vv`.\n",
    "\n",
    "We could implement this approach like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [1, 0, 1]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv = torch.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\n",
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  4],\n",
       "        [ 5,  5,  7],\n",
       "        [ 8,  8, 10],\n",
       "        [11, 11, 13]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + vv  # Add x and vv elementwise\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nice, but:\n",
    "- A new array was allocated and memory was copied.\n",
    "- We had to explicitly define how many times to replicate v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Broadcasting** allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x=(4, 3), v=(3,)\n",
      "\n",
      "shape=(4, 3) dtype=torch.int64:\n",
      "tensor([[ 2,  2,  4],\n",
      "        [ 5,  5,  7],\n",
      "        [ 8,  8, 10],\n",
      "        [11, 11, 13]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "\n",
    "# Add v to each row of x using broadcasting\n",
    "y = x + v  \n",
    "\n",
    "print(f'shapes: x={tuple(x.shape)}, v={tuple(v.shape)}\\n')\n",
    "print_arr(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The line `y = x + v` works even though `x` has shape `(4, 3)` and `v` has shape `(3,)` due to broadcasting; this line works **as if** v actually had shape `(4, 3)`, where each row was a copy of `v`, and the sum was performed elementwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Broadcasting two tensors together follows these rules:\n",
    "\n",
    "1. All input tensors with rank smaller than the input tensor of largest rank, have **1’s prepended to their shapes**.\n",
    "1. The size in each dimension of the **output shape** is the maximum of all the input sizes in that dimension.\n",
    "1. An input can be used in the calculation if its size in a particular **dimension either matches** the output size in that dimension, **or has value exactly 1**.\n",
    "1. If an input has a dimension size of 1 in its shape, the **first data entry in that dimension will be used for all calculations** along that dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our example:\n",
    "- `x` has shape `(4,3)`\n",
    "- `v` has shape `(3,)`.\n",
    "\n",
    "Following the Broadcasting logic, we can say the following is equivalent to what happened:\n",
    "1. `v` has less dims than `x` so a dimension of `1` is **prepended** -> `v` is now `(1, 3)`.\n",
    "1. Output shape will be `(max(1,4), max(3,3)) = (4,3)`.\n",
    "1. Dim 1 of `v` matches exactly (3): so it's clear which data to use.\n",
    "1. Dim 0 is exactly 1 for `v` and 4 for `x`: we can use the first data entry (row 0) of `v` for each time any of its rows is accessed. This is effectively like converting `v` from `(1,3)` to `(4,3)` by replicating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Broadcasting is incredibly useful and necessary for writing **vectorized** code,\n",
    "i.e. code that avoids explicit python loops which are very slow.\n",
    "Instead, this approach leveraged the underlying C implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: Calculating an outer-product with elementsize product and broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=(3,) dtype=torch.int64:\n",
      "tensor([1, 2, 3])\n",
      "\n",
      "shape=(2,) dtype=torch.int64:\n",
      "tensor([4, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute outer product of the vectors\n",
    "v = torch.tensor([1,2,3])  # v has shape (3,)\n",
    "w = torch.tensor([4,5])    # w has shape (2,)\n",
    "print_arr(v)\n",
    "print_arr(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute an outer product, we first reshape `v` to be a column\n",
    "vector of shape `(3, 1)`; we can then broadcast it against `w` to yield\n",
    "an output of shape `(3, 2)`, which is the outer product of `v` and `w`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5],\n",
       "        [ 8, 10],\n",
       "        [12, 15]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3,1) * (2,) -> (3,1) * (1, 2) -> (3, 2) * (3, 2)\n",
    "torch.reshape(v, (3, 1)) * w # note that * is elementwise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply a matrix by a constant:\n",
    "x = torch.ones((2,3))\n",
    "\n",
    "# x has shape (2, 3). Numpy treats scalars as arrays of shape ();\n",
    "# these can be broadcast together to shape (2, 3).\n",
    "\n",
    "# (2,3) * () -> (2,3) * (1,1) -> (2,3) * (2,3)\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Broadcasting typically makes your code more concise and faster, so you should strive to use it where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most important aspect which sets `pytorch` apart from `numpy` is the **automatic differentiation**.\n",
    "\n",
    "Pytorch can automatically *track* computations performed on tensors to build a *computation graph* which can be used for applying the chain rule in order to compute arbitrary derivatives.\n",
    "\n",
    "In the remainder of the course we'll go deeper, and use this ability extensively to train neural networks via the back-propagation algorithm.\n",
    "\n",
    "For now, we'll only show a short demo of this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell pytorch to track operations w.r.t. this tensor\n",
    "w = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9975, -2.0000, -2.0000], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some arbitrary calculations with w\n",
    "x = 2*w\n",
    "y = torch.exp(-x)\n",
    "z = y ** 3 - 2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.9975, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate so that we get a scalar\n",
    "l = torch.sum(z)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice the `grad_fn` property which appeared. Let's see what we can now do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.4873e-02, -3.6865e-05, -9.1380e-08]),)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import autograd\n",
    "\n",
    "# This is where the magic happens\n",
    "w_grad = autograd.grad(l, w)\n",
    "w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What does each element of `w_grad` contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each element $i$ of `w_grad` is actually the derivative of `l` with respect to that element in `w`!\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$\n",
    "\\left[\\mathtt{w_{grad}}\\right]_i=\\frac{\\partial \\ell}{\\partial w_i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How did this happen?\n",
    "\n",
    "When we used `w` for calculations a computation graph was constructed behind the scenes.\n",
    "This graph contains the intermediate results required to compute the derivative via the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\bb{w}} = \\frac{\\partial \\ell}{\\partial \\bb{z}}\\frac{\\partial \\bb{z}}{\\partial \\bb{y}}\\frac{\\partial \\bb{y}}{\\partial \\bb{x}}\\frac{\\partial \\bb{x}}{\\partial \\bb{w}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll explore this in greater detail over the next tutorials.\n",
    "\n",
    "For now, lets look at the computation graph resulting from the above calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"98pt\" height=\"478pt\"\n",
       " viewBox=\"0.00 0.00 98.48 478.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 474)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-474 94.48,-474 94.48,4 -4,4\"/>\n",
       "<!-- 140663339590176 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140663339590176</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"72.24,-28 18.24,-28 18.24,0 72.24,0 72.24,-28\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-16\" font-family=\"monospace\" font-size=\"10.00\">l</text>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-6\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 140662192385136 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140662192385136</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"88.27,-82 2.21,-82 2.21,-64 88.27,-64 88.27,-82\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-70\" font-family=\"monospace\" font-size=\"10.00\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 140662192385136&#45;&gt;140663339590176 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140662192385136&#45;&gt;140663339590176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-63.59C45.24,-56.98 45.24,-47.45 45.24,-38.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-38.31 45.24,-28.31 41.74,-38.31 48.74,-38.31\"/>\n",
       "</g>\n",
       "<!-- 140662192385232 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140662192385232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"87,-136 3.48,-136 3.48,-118 87,-118 87,-136\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-124\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140662192385232&#45;&gt;140662192385136 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140662192385232&#45;&gt;140662192385136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-117.88C45.24,-111.05 45.24,-101.03 45.24,-92.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-92.24 45.24,-82.24 41.74,-92.24 48.74,-92.24\"/>\n",
       "</g>\n",
       "<!-- 140662192385184 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140662192385184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"87.66,-190 2.82,-190 2.82,-172 87.66,-172 87.66,-190\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-178\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 140662192385184&#45;&gt;140662192385232 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140662192385184&#45;&gt;140662192385232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-171.88C45.24,-165.05 45.24,-155.03 45.24,-146.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-146.24 45.24,-136.24 41.74,-146.24 48.74,-146.24\"/>\n",
       "</g>\n",
       "<!-- 140662192385280 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140662192385280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"86.44,-244 4.04,-244 4.04,-226 86.44,-226 86.44,-244\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-232\" font-family=\"monospace\" font-size=\"10.00\">ExpBackward0</text>\n",
       "</g>\n",
       "<!-- 140662192385280&#45;&gt;140662192385184 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140662192385280&#45;&gt;140662192385184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-225.88C45.24,-219.05 45.24,-209.03 45.24,-200.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-200.24 45.24,-190.24 41.74,-200.24 48.74,-200.24\"/>\n",
       "</g>\n",
       "<!-- 140662192384704 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140662192384704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"87.05,-298 3.43,-298 3.43,-280 87.05,-280 87.05,-298\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-286\" font-family=\"monospace\" font-size=\"10.00\">NegBackward0</text>\n",
       "</g>\n",
       "<!-- 140662192384704&#45;&gt;140662192385280 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140662192384704&#45;&gt;140662192385280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-279.88C45.24,-273.05 45.24,-263.03 45.24,-254.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-254.24 45.24,-244.24 41.74,-254.24 48.74,-254.24\"/>\n",
       "</g>\n",
       "<!-- 140662192385424 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140662192385424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"85.82,-352 4.66,-352 4.66,-334 85.82,-334 85.82,-352\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-340\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140662192385424&#45;&gt;140662192384704 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140662192385424&#45;&gt;140662192384704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-333.88C45.24,-327.05 45.24,-317.03 45.24,-308.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-308.24 45.24,-298.24 41.74,-308.24 48.74,-308.24\"/>\n",
       "</g>\n",
       "<!-- 140662192385520 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140662192385520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"90.72,-406 -0.24,-406 -0.24,-388 90.72,-388 90.72,-406\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-394\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140662192385520&#45;&gt;140662192385424 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140662192385520&#45;&gt;140662192385424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-387.88C45.24,-381.05 45.24,-371.03 45.24,-362.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-362.24 45.24,-352.24 41.74,-362.24 48.74,-362.24\"/>\n",
       "</g>\n",
       "<!-- 140663339592016 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140663339592016</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"72.24,-470 18.24,-470 18.24,-442 72.24,-442 72.24,-470\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-458\" font-family=\"monospace\" font-size=\"10.00\">w</text>\n",
       "<text text-anchor=\"middle\" x=\"45.24\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 140663339592016&#45;&gt;140662192385520 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140663339592016&#45;&gt;140662192385520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.24,-441.78C45.24,-434.18 45.24,-424.53 45.24,-416.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.74,-416.05 45.24,-406.05 41.74,-416.05 48.74,-416.05\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7feebc612c10>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "\n",
    "torchviz.make_dot(l, params=dict(w=w, l=l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by [Aviv A. Rosenberg](https://avivr.net).<br>\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "- https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry\n",
    "\n",
    "The Python section here was adapted from:\n",
    "- [CS231n Python tutorial](http://cs231n.github.io/python-numpy-tutorial/) by Justin Johnson."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
