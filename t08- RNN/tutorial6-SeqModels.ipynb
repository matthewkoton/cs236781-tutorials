{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 6: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- What RNNs are and how they work\n",
    "- Implementing basic RNNs models\n",
    "- Application example: sentiment analysis of movie reviews\n",
    "- Temporal Convolution Networks as an RNN alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus far, our models have been composed of fully connected (linear) layers or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fully connected layers\n",
    "    - Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates,\n",
    "        $$\n",
    "        \\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n",
    "        \\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l}.\n",
    "        $$\n",
    "    - FC's have completely pre-fixed input and output dimensions.\n",
    "    \n",
    "    <center><img src=\"img/mlp.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Convolutional layers\n",
    "    - Each layer operates on an input tensor $\\vec{x}$ containing $M$ feature maps. The $k$-th feature map of the output tensor $\\vec{y}$ is:\n",
    "        $$\n",
    "        \\vec{y}^k = \\sum_{m=1}^{M} \\vec{w}^{km}\\ast\\vec{x}^m+b^k,\\ k\\in[1,K]\n",
    "        $$\n",
    "      Where $\\ast$ denotes convolution, and $K$ is the number of output feature maps.\n",
    "      \n",
    "      <center><img src=\"img/cnn_filters.png\" width=\"500\"/></center>\n",
    "      \n",
    "    - This time the weight dimensions are not dependent on the input dimensions.\n",
    "    - Weights are shared across the spatial dimensions of the input.\n",
    "    - Output dimension changes based on input dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However,\n",
    "- Models based on these types of layers lack **persistent state**. \n",
    "- The current output is not affected by **previous inputs** (or outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we model a dynamical system?\n",
    "E.g., a linear system such as\n",
    "\n",
    "$$\\vec{y}_t = a_0 + a_1 \\vec{y}_{t-1}+\\dots+a_P \\vec{y}_{t-P} + b_0 \\vec{x}_t+\\dots+b_{t-Q}\\vec{x}_{t-Q}$$\n",
    "\n",
    "Many use cases and examples: signal processing, text translation, sentiment analysis, scene classification in video, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An RNN layer is similar to a regular FC layer, but it has two inputs:\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{R}^{d_{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/rnn_cell.png\" width=\"400\"/></center>\n",
    "\n",
    "Crucially,\n",
    "- The layer itself is not time-dependent (but is parametrized).\n",
    "- The same layer (function) is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A basic RNN can be defined as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases.\n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1200\" /></center>\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN models are very flexible in terms of input and output meaning.\n",
    "\n",
    "Common applications include image captioning, sentiment analysis, machine translation and signal processing. \n",
    "\n",
    "<center><img src=\"img/rnn_use_cases.jpeg\" width=\"1200\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How would **backpropagation** work, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/bptt.png\" width=\"1000\"></center>\n",
    "\n",
    "1. Calculated loss from each output and accumulate\n",
    "2. Calculate Gradient of loss w.r.t. each parameter at each timestep\n",
    "3. For each parameter, accumulate gradients from all timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is known as **Backpropagation through time**, or BPTT.\n",
    "\n",
    "$$\n",
    "\\pderiv{L_t}{\\mat{W}} = \\sum_{k=1}^{t}\n",
    "\\pderiv{L_t}{\\hat y_t} \\cdot\n",
    "\\pderiv{\\hat y_t}{\\vec{h}_t} \\cdot\n",
    "\\pderiv{\\vec{h}_t}{\\vec{h}_k} \\cdot\n",
    "\\pderiv{\\vec{h}_k}{\\mat{W}}\n",
    "$$\n",
    "\n",
    "But how far back do we go? What's the limiting factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're limited in depth by vanishing and exploding gradients controlled by the eigenvalues of $\\mat{W}$.\n",
    "\n",
    "One pragmatic solution is to limit the number of timesteps involved in the backpropagation.\n",
    "\n",
    "<center><img src=\"img/tbptt.png\" width=\"1000\"></center>\n",
    "\n",
    "This is known as **Truncated backpropagation through time**, or TBPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layered (deep) RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<center><img src=\"img/rnn_layered.png\" width=\"1200\"/></center>\n",
    "\n",
    "- As with MLPs, adding depth allows us to model intricate hierarchical features.\n",
    "- However, now we also have a time dimension which makes the representation time-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the above equations, let's create a simple RNN layer  with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=10, bias=False)\n",
       "  (fc_hh): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc_hy): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 10, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And manually \"run\" a few time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 ((3, 1)):\n",
      "tensor([[0.4863],\n",
      "        [0.4884],\n",
      "        [0.4539]], grad_fn=<SigmoidBackward>)\n",
      "h1 ((3, 10)):\n",
      "tensor([[-0.7211, -0.2087,  0.0559,  0.1973,  0.0059, -0.2591, -0.0868, -0.5072,\n",
      "          0.5213,  0.4355],\n",
      "        [ 0.0483, -0.7419, -0.5230, -0.5456,  0.4723, -0.5015,  0.7743, -0.7416,\n",
      "          0.2145, -0.4240],\n",
      "        [-0.6628,  0.1388, -0.8867,  0.4926,  0.3292,  0.3716,  0.2154,  0.0340,\n",
      "          0.7429,  0.3972]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y2 ((3, 1)):\n",
      "tensor([[0.4161],\n",
      "        [0.3323],\n",
      "        [0.4125]], grad_fn=<SigmoidBackward>)\n",
      "h2 ((3, 10)):\n",
      "tensor([[ 0.6217, -0.5392, -0.0183,  0.7719, -0.4317, -0.3176,  0.0880,  0.6974,\n",
      "         -0.4305,  0.1007],\n",
      "        [-0.3053,  0.3430, -0.0436,  0.3882, -0.5664, -0.6748,  0.7817,  0.5944,\n",
      "         -0.5802, -0.5103],\n",
      "        [ 0.5530, -0.3749,  0.6356,  0.6363,  0.6457, -0.0797,  0.0542,  0.0580,\n",
      "         -0.6065, -0.6338]], grad_fn=<TanhBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t=1\n",
    "x1 = torch.randn(N, in_dim, requires_grad=True) # requiring grad just for torchviz\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1 ({tuple(y1.shape)}):\\n{y1}')\n",
    "print(f'h1 ({tuple(h1.shape)}):\\n{h1}\\n')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim, requires_grad=True)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2 ({tuple(y2.shape)}):\\n{y2}')\n",
    "print(f'h2 ({tuple(h2.shape)}):\\n{h2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, let's visualize the computation graph and see what happened when we used the same RNN block twice, by looking at the graph from both $y_1$ and $y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"569pt\" height=\"580pt\"\n",
       " viewBox=\"0.00 0.00 568.97 580.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 576)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-576 564.97,-576 564.97,4 -4,4\"/>\n",
       "<!-- 140645904529776 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140645904529776</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"398.81,-20 293.17,-20 293.17,0 398.81,0 398.81,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"345.99\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 140645904528144 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140645904528144</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"397.97,-76 294.01,-76 294.01,-56 397.97,-56 397.97,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"345.99\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140645904528144&#45;&gt;140645904529776 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140645904528144&#45;&gt;140645904529776</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.99,-55.59C345.99,-48.7 345.99,-39.1 345.99,-30.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.49,-30.3 345.99,-20.3 342.49,-30.3 349.49,-30.3\"/>\n",
       "</g>\n",
       "<!-- 140645904659504 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140645904659504</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"283.36,-144 218.61,-144 218.61,-112 283.36,-112 283.36,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"250.99\" y=\"-130.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hy.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"250.99\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140645904659504&#45;&gt;140645904528144 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140645904659504&#45;&gt;140645904528144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.96,-111.86C289.57,-102.63 308.15,-90.9 322.64,-81.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"324.91,-84.45 331.49,-76.15 321.17,-78.53 324.91,-84.45\"/>\n",
       "</g>\n",
       "<!-- 140645904659552 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140645904659552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"390.45,-138 301.52,-138 301.52,-118 390.45,-118 390.45,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"345.99\" y=\"-124.4\" font-family=\"Times,serif\" font-size=\"12.00\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 140645904659552&#45;&gt;140645904528144 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140645904659552&#45;&gt;140645904528144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.99,-117.89C345.99,-109.52 345.99,-96.84 345.99,-86.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.49,-86.2 345.99,-76.2 342.49,-86.2 349.49,-86.2\"/>\n",
       "</g>\n",
       "<!-- 140645904659936 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140645904659936</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"389.8,-206 298.18,-206 298.18,-186 389.8,-186 389.8,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"343.99\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140645904659936&#45;&gt;140645904659552 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140645904659936&#45;&gt;140645904659552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M344.27,-185.82C344.56,-176.17 345.03,-160.69 345.4,-148.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"348.91,-148.2 345.71,-138.1 341.91,-147.99 348.91,-148.2\"/>\n",
       "</g>\n",
       "<!-- 140645904659984 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140645904659984</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"163.14,-436 78.84,-436 78.84,-416 163.14,-416 163.14,-436\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.99\" y=\"-422.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 140645904659984&#45;&gt;140645904659936 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140645904659984&#45;&gt;140645904659936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.01,-415.78C163.91,-381.11 284.04,-258.29 327.85,-213.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"330.61,-215.68 335.1,-206.09 325.61,-210.79 330.61,-215.68\"/>\n",
       "</g>\n",
       "<!-- 140645904659792 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140645904659792</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"152.48,-504 89.5,-504 89.5,-472 152.48,-472 152.48,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.99\" y=\"-490.4\" font-family=\"Times,serif\" font-size=\"12.00\">x2</text>\n",
       "<text text-anchor=\"middle\" x=\"120.99\" y=\"-478.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (3, 1024)</text>\n",
       "</g>\n",
       "<!-- 140645904659792&#45;&gt;140645904659984 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140645904659792&#45;&gt;140645904659984</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M120.99,-471.86C120.99,-464.13 120.99,-454.63 120.99,-446.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.49,-446.15 120.99,-436.15 117.49,-446.15 124.49,-446.15\"/>\n",
       "</g>\n",
       "<!-- 140645904659744 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140645904659744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"71.96,-498 0.01,-498 0.01,-478 71.96,-478 71.96,-498\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.99\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140645904659744&#45;&gt;140645904659984 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140645904659744&#45;&gt;140645904659984</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.9,-477.89C62.46,-468.32 83.99,-453.12 100,-441.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"102.08,-444.63 108.23,-436 98.05,-438.91 102.08,-444.63\"/>\n",
       "</g>\n",
       "<!-- 140645904660128 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140645904660128</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"261.47,-572 182.5,-572 182.5,-540 261.47,-540 261.47,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.99\" y=\"-558.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_xh.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"221.99\" y=\"-546.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10, 1024)</text>\n",
       "</g>\n",
       "<!-- 140645904660128&#45;&gt;140645904659744 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140645904660128&#45;&gt;140645904659744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M182.09,-540.84C149.14,-529.15 102.84,-512.72 71.24,-501.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.13,-498.11 61.54,-498.07 69.79,-504.71 72.13,-498.11\"/>\n",
       "</g>\n",
       "<!-- 140645904660656 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>140645904660656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"322.96,-498 251.01,-498 251.01,-478 322.96,-478 322.96,-498\"/>\n",
       "<text text-anchor=\"middle\" x=\"286.99\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660128&#45;&gt;140645904660656 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>140645904660128&#45;&gt;140645904660656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.06,-539.69C247.1,-529.5 260.3,-516.1 270.6,-505.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"273.29,-507.9 277.81,-498.32 268.3,-502.99 273.29,-507.9\"/>\n",
       "</g>\n",
       "<!-- 140645904660032 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140645904660032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"473.97,-268 370.01,-268 370.01,-248 473.97,-248 473.97,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.99\" y=\"-254.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660032&#45;&gt;140645904659936 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140645904660032&#45;&gt;140645904659936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M410.14,-247.89C397.81,-238.41 378.3,-223.4 363.66,-212.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.75,-209.32 355.69,-206 361.48,-214.87 365.75,-209.32\"/>\n",
       "</g>\n",
       "<!-- 140645904660080 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140645904660080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"496.64,-504 431.33,-504 431.33,-472 496.64,-472 496.64,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"463.99\" y=\"-490.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hh.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"463.99\" y=\"-478.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 140645904660080&#45;&gt;140645904660032 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140645904660080&#45;&gt;140645904660032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M462.85,-471.62C460.28,-439.9 453.02,-365.07 437.99,-304 435.81,-295.15 432.55,-285.61 429.55,-277.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.75,-276.18 425.85,-268.14 426.23,-278.73 432.75,-276.18\"/>\n",
       "</g>\n",
       "<!-- 140645904660512 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>140645904660512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"432.97,-436 329.01,-436 329.01,-416 432.97,-416 432.97,-436\"/>\n",
       "<text text-anchor=\"middle\" x=\"380.99\" y=\"-422.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660080&#45;&gt;140645904660512 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>140645904660080&#45;&gt;140645904660512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M443.04,-471.86C430.52,-462.81 414.66,-451.34 402.11,-442.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"403.8,-439.18 393.65,-436.15 399.7,-444.85 403.8,-439.18\"/>\n",
       "</g>\n",
       "<!-- 140645904660176 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140645904660176</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"429.45,-324 340.52,-324 340.52,-304 429.45,-304 429.45,-324\"/>\n",
       "<text text-anchor=\"middle\" x=\"384.99\" y=\"-310.4\" font-family=\"Times,serif\" font-size=\"12.00\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660176&#45;&gt;140645904660032 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140645904660176&#45;&gt;140645904660032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.43,-303.59C396.53,-296.16 403.78,-285.58 409.94,-276.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"412.85,-278.53 415.62,-268.3 407.08,-274.57 412.85,-278.53\"/>\n",
       "</g>\n",
       "<!-- 140645904660320 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140645904660320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"426.8,-380 335.18,-380 335.18,-360 426.8,-360 426.8,-380\"/>\n",
       "<text text-anchor=\"middle\" x=\"380.99\" y=\"-366.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140645904660320&#45;&gt;140645904660176 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140645904660320&#45;&gt;140645904660176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.68,-359.59C382.2,-352.63 382.92,-342.89 383.56,-334.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"387.05,-334.53 384.3,-324.3 380.07,-334.01 387.05,-334.53\"/>\n",
       "</g>\n",
       "<!-- 140645904660416 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>140645904660416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"311.14,-436 226.84,-436 226.84,-416 311.14,-416 311.14,-436\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.99\" y=\"-422.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660416&#45;&gt;140645904660320 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>140645904660416&#45;&gt;140645904660320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287.73,-415.96C305.58,-407.36 332.67,-394.3 353.06,-384.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.69,-387.56 362.18,-380.07 351.65,-381.26 354.69,-387.56\"/>\n",
       "</g>\n",
       "<!-- 140645904660608 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>140645904660608</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"233.48,-504 170.5,-504 170.5,-472 233.48,-472 233.48,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.99\" y=\"-490.4\" font-family=\"Times,serif\" font-size=\"12.00\">x1</text>\n",
       "<text text-anchor=\"middle\" x=\"201.99\" y=\"-478.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (3, 1024)</text>\n",
       "</g>\n",
       "<!-- 140645904660608&#45;&gt;140645904660416 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>140645904660608&#45;&gt;140645904660416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M218.89,-471.86C228.71,-463.07 241.06,-452.01 251.04,-443.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"253.65,-445.43 258.77,-436.15 248.98,-440.22 253.65,-445.43\"/>\n",
       "</g>\n",
       "<!-- 140645904660656&#45;&gt;140645904660416 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>140645904660656&#45;&gt;140645904660416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M284.25,-477.89C281.72,-469.43 277.86,-456.57 274.66,-445.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.97,-444.77 271.75,-436.2 271.27,-446.78 277.97,-444.77\"/>\n",
       "</g>\n",
       "<!-- 140645904660512&#45;&gt;140645904660320 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>140645904660512&#45;&gt;140645904660320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M380.99,-415.59C380.99,-408.7 380.99,-399.1 380.99,-390.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"384.49,-390.3 380.99,-380.3 377.49,-390.3 384.49,-390.3\"/>\n",
       "</g>\n",
       "<!-- 140645904660704 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>140645904660704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"412.96,-498 341.01,-498 341.01,-478 412.96,-478 412.96,-498\"/>\n",
       "<text text-anchor=\"middle\" x=\"376.99\" y=\"-484.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660704&#45;&gt;140645904660512 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>140645904660704&#45;&gt;140645904660512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M377.59,-477.89C378.15,-469.52 379,-456.84 379.7,-446.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"383.2,-446.41 380.37,-436.2 376.22,-445.94 383.2,-446.41\"/>\n",
       "</g>\n",
       "<!-- 140645904660800 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>140645904660800</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"480.47,-572 401.5,-572 401.5,-540 480.47,-540 480.47,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"440.99\" y=\"-558.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hh.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"440.99\" y=\"-546.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10, 10)</text>\n",
       "</g>\n",
       "<!-- 140645904660800&#45;&gt;140645904660704 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>140645904660800&#45;&gt;140645904660704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M426.14,-539.69C416.26,-529.5 403.27,-516.1 393.12,-505.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"395.5,-503.06 386.02,-498.32 390.47,-507.93 395.5,-503.06\"/>\n",
       "</g>\n",
       "<!-- 140645904660224 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>140645904660224</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"560.96,-380 489.01,-380 489.01,-360 560.96,-360 560.96,-380\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.99\" y=\"-366.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140645904660800&#45;&gt;140645904660224 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>140645904660800&#45;&gt;140645904660224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M468.91,-539.95C482.31,-531.25 497.31,-519.03 505.99,-504 526.81,-467.95 527.83,-417.51 526.54,-390.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"530.03,-389.89 525.9,-380.13 523.04,-390.33 530.03,-389.89\"/>\n",
       "</g>\n",
       "<!-- 140645904660224&#45;&gt;140645904660032 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>140645904660224&#45;&gt;140645904660032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M518.76,-359.68C509.81,-346.6 492.47,-322.35 474.99,-304 464.97,-293.49 452.66,-282.92 442.43,-274.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"444.43,-271.79 434.42,-268.33 440.09,-277.28 444.43,-271.79\"/>\n",
       "</g>\n",
       "<!-- 140645904659600 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>140645904659600</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"481.96,-138 410.01,-138 410.01,-118 481.96,-118 481.96,-138\"/>\n",
       "<text text-anchor=\"middle\" x=\"445.99\" y=\"-124.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140645904659600&#45;&gt;140645904528144 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>140645904659600&#45;&gt;140645904528144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M430.8,-117.89C414.55,-108.14 388.57,-92.55 369.64,-81.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"371.37,-78.15 360.99,-76 367.76,-84.15 371.37,-78.15\"/>\n",
       "</g>\n",
       "<!-- 140645904659888 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>140645904659888</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"486.19,-212 407.78,-212 407.78,-180 486.19,-180 486.19,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"446.99\" y=\"-198.4\" font-family=\"Times,serif\" font-size=\"12.00\">fc_hy.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"446.99\" y=\"-186.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 140645904659888&#45;&gt;140645904659600 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>140645904659888&#45;&gt;140645904659600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.76,-179.69C446.61,-170.4 446.43,-158.44 446.28,-148.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.78,-148.26 446.13,-138.32 442.78,-148.37 449.78,-148.26\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7feaad2db880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "\n",
    "torchviz.make_dot(\n",
    "    y2, # Note: Change here to y2 to see the fullly unrolled graph!\n",
    "    params=dict(list(rnn.named_parameters()) + [('x1', x1), ('x2', x2)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Sentiment analysis for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The task: Given a review about a movie written by some user, decide whether it's **positive**, **negative** or **neutral**.\n",
    "\n",
    "<center><img src=\"img/sentiment_analysis.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Classically this is considered a challenging task if approached based on keywords alone.\n",
    "\n",
    "Consider:\n",
    "\n",
    "     \"This movie was actually neither that funny, nor super witty.\"\n",
    "     \n",
    "To comprehend such a sentence, it's intuitive to see that some \"state\" must be kept when \"reading\" it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We'll use the [`torchtext`](https://github.com/pytorch/text) package, which provides useful tools for working ith textual data, and also includes some built-in datasets and dataloaders (similar to `torchvision`).\n",
    "\n",
    "Out dataset will be the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html) (SST) dataset, which contains ~10,000 **labeled** movie reviews.\n",
    "\n",
    "The label of each review is either \"positive\", \"neutral\" or \"negative\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Loading and tokenizing text samples\n",
    "\n",
    "The `torchtext.data.Field` class takes care of splitting text into unique \"tokens\"\n",
    "(~words) and converting it a numerical representation as a sequence of numbers representing\n",
    "the tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torchtext.data\n",
    "\n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "# For this application, tokens ~= words\n",
    "\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n",
      "Number of test     samples: 2210\n"
     ]
    }
   ],
   "source": [
    "import torchtext.datasets\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')\n",
    "print(f'Number of test     samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets print some examples from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0111 [positive]:\n",
      " > the film aims to be funny , uplifting and moving , sometimes all at once .\n",
      "\n",
      "sample#4321 [neutral ]:\n",
      " > the most anti - human big studio picture since 3000 miles to graceland .\n",
      "\n",
      "sample#7777 [negative]:\n",
      " > an ugly , revolting movie .\n",
      "\n",
      "sample#0000 [positive]:\n",
      " > the rock is destined to be the 21st century 's new ` ` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean - claud van damme or steven segal .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([111, 4321, 7777, 0]):\n",
    "    example = ds_train[i]\n",
    "    label = example.label\n",
    "    review = str.join(\" \", example.text)\n",
    "    print(f'sample#{i:04d} [{label:8s}]:\\n > {review}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building a vocabulary\n",
    "\n",
    "The `Field` object can build a **vocabulary** for us,\n",
    "which is simply a bi-directional mapping between a unique index and a token.\n",
    "\n",
    "We'll only include words from the training set in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 15482\n",
      "Number of tokens in training labels: 3\n"
     ]
    }
   ],
   "source": [
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "\n",
    "print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 tokens:\n",
      " ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'the', ',', 'a', 'and', 'of', 'to', '-', 'is', \"'s\", 'it', 'that', 'in', 'as', 'but', 'film']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'first 20 tokens:\\n', review_parser.vocab.itos[:20], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note the **special tokens**, `<unk>`, `<pad>`, `<sos>` and `<eos>` at indexes `0-3`.\n",
    "These were automatically created by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=film            index=19\n",
      "word=actor           index=492\n",
      "word=schwarzenegger  index=3404\n",
      "word=spielberg       index=715\n"
     ]
    }
   ],
   "source": [
    "# Show that some words exist in the vocab\n",
    "for w in ['film', 'actor', 'schwarzenegger', 'spielberg']:\n",
    "    print(f'word={w:15s} index={review_parser.vocab.stoi[w]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels vocab:\n",
      " {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'labels vocab:\\n', dict(label_parser.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data loaders (iterators)\n",
    "\n",
    "The `torchtext` package comes with `Iterator`s, similar to the `DataLoaders` we previously worked with.\n",
    "\n",
    "A key issue when working with text sequences is that each sample is of a different length.\n",
    "\n",
    "So, how can we work with **batches** of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator creates batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets look at a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      " tensor([[    2,     2,     2,     2],\n",
      "        [   56,   108,   364,   656],\n",
      "        [   13,     5,     5,   776],\n",
      "        [  631,   111,   270,   621],\n",
      "        [   36,    19,   184,     6],\n",
      "        [   23,    12,    26,    93],\n",
      "        [ 1949,    69,    89, 10137],\n",
      "        [  193,    38,   736,     5],\n",
      "        [    6,   595,    43,  1616],\n",
      "        [ 5115,    11,     7,  1805],\n",
      "        [  941,  3010,    19, 14235],\n",
      "        [   18,    45,    20,    11],\n",
      "        [   32,    25,    23,     7],\n",
      "        [  450,   848,   351,    11],\n",
      "        [  533,    11,    51, 13313],\n",
      "        [    6,  3784,  1188,     9],\n",
      "        [   12,  3449,    43,     5],\n",
      "        [  667, 12566,   101,   267],\n",
      "        [  114,    11,  2805,   826],\n",
      "        [ 4848,   416,    19,     4],\n",
      "        [    8,  1557,   112,     3],\n",
      "        [ 5625,     6,   439,     1],\n",
      "        [    5,     5,     4,     1],\n",
      "        [  669,   111,     3,     1],\n",
      "        [  411,   167,     1,     1],\n",
      "        [   55,    34,     1,     1],\n",
      "        [   10,     5,     1,     1],\n",
      "        [   29,  5347,     1,     1],\n",
      "        [    4,   643,     1,     1],\n",
      "        [    3, 13705,     1,     1],\n",
      "        [    1,  6722,     1,     1],\n",
      "        [    1,    12,     1,     1],\n",
      "        [    1,    69,     1,     1],\n",
      "        [    1,  5429,     1,     1],\n",
      "        [    1,     4,     1,     1],\n",
      "        [    1,     3,     1,     1]]) torch.Size([36, 4])\n",
      "\n",
      "y = \n",
      " tensor([0, 2, 1, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dl_train))\n",
    "\n",
    "X, y = batch.text, batch.label\n",
    "print('X = \\n', X, X.shape, end='\\n\\n')\n",
    "print('y = \\n', y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are we looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our sample tensor `X` is of shape `(sentence_length, batch_size)`.\n",
    "\n",
    "Note that:\n",
    "1. `sentence_length` changes every batch! You can re-run the previous block to see this.\n",
    "2. Sequence dimension first (not batch). When we implement the model, you'll see why it's easier to work this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "We'll now create our sentiment analysis model based on the simple `RNNLayer` we've implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model will:\n",
    "- Take an input batch of tokenized sentences.\n",
    "- Compute a dense word-embedding of each token.\n",
    "- Process the sentence **sequentially** through the RNN layer.\n",
    "- Produce a `(B, 3)` tensor, which we'll interpret as class probabilities for each sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a **word embedding**? How do we get one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Embeddings encode tokens as tensors in a way that maintain some **semantic** meaning for our task.\n",
    "\n",
    "<center><img src=\"img/word_embeddings.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally we should take a pre-trained embedding depending on our task type.\n",
    "\n",
    "Here we'll train an Embedding together with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 0, 0, 4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2404, -0.5550,  0.6873, -1.0276,  0.5748,  1.3422,  1.5551, -0.9304],\n",
       "        [ 0.3050, -0.6279, -0.0856,  0.6240,  0.0890, -1.5827,  1.2871, -0.6937],\n",
       "        [-0.0246,  0.2887,  0.2568,  0.7967,  0.6089, -0.2071,  0.2202,  1.7748],\n",
       "        [-0.0246,  0.2887,  0.2568,  0.7967,  0.6089, -0.2071,  0.2202,  1.7748],\n",
       "        [ 0.2404, -0.5550,  0.6873, -1.0276,  0.5748,  1.3422,  1.5551, -0.9304],\n",
       "        [-0.1252, -0.3856, -0.9769,  0.5809, -0.7404,  1.2604, -1.4192,  1.0447]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=8)\n",
    "\n",
    "token_idx = torch.randint(low=0, high=5, size=(6,))\n",
    "print(token_idx)\n",
    "embedding_layer(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, model time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        \n",
    "        # Our own Vanilla RNN layer, without phi_y so it outputs a class score\n",
    "        self.rnn = RNNLayer(in_dim=embedding_dim, h_dim=h_dim, out_dim=out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B) Note batch dim is not first!\n",
    "        \n",
    "        embedded = self.embedding(X) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Loop over (batch of) tokens in the sentence(s)\n",
    "        ht = None\n",
    "        for xt in embedded:           # xt is (B, E)\n",
    "            yt, ht = self.rnn(xt, ht) # yt is (B, D_out)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        \n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(review_parser.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test a manual forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(X) = \n",
      " tensor([[-0.9039, -1.0806, -1.3640],\n",
      "        [-0.9976, -1.6040, -0.8437],\n",
      "        [-0.9100, -1.0781, -1.3578],\n",
      "        [-0.9100, -1.0781, -1.3578]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n",
      "labels =  tensor([0, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f'model(X) = \\n', model(X), model(X).shape)\n",
    "print(f'labels = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How big is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN model has 1,577,899 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The RNN model has {count_parameters(model):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why so many? We used only one RNN layer.\n",
    "\n",
    "Where are most of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Let's complete the example by showing the regular pytorch-style train loop with this model.\n",
    "\n",
    "We'll run only a few epochs on a small subset just to test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=100, max_batches=200):\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == max_batches-1:\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches):.3f}, accuracy={num_correct /(max_batches*BATCH_SIZE):.3f}, elapsed={time.time()-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.106, accuracy=0.412, elapsed=4.2 sec\n",
      "Epoch #1, loss=1.085, accuracy=0.409, elapsed=4.0 sec\n",
      "Epoch #2, loss=1.074, accuracy=0.419, elapsed=4.2 sec\n",
      "Epoch #3, loss=1.053, accuracy=0.438, elapsed=4.2 sec\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(rnn_model, optimizer, loss_fn, dl_train, max_epochs=4) # just a demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations\n",
    "\n",
    "As usual this is a very naïve model, just for demonstration.\n",
    "It lacks many tricks of the NLP trade, such was pre-trained embeddings,\n",
    "gated RNN units, deep or bi-directional models, dropout, etc.\n",
    "\n",
    "Don't expect SotA results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Temporal Convolution Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNNs are sometimes useful but they have some serious drawbacks:\n",
    "\n",
    "1. Vanilla RNNs are very hard to train on long sequences (large `S`) and thus are almost never used.\n",
    "Instead, more complex RNNs like LSTMs and GRUs are employed (as you'll see in HW3 :).\n",
    "\n",
    "2. They require processing the input **sequentially** which results in poor performance for both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But CNNs are great! Can we use them instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OK, but how do we deal with:\n",
    "1. Input and output must both be sequences of same length, which should be arbitrary.\n",
    "1. Output at time $t$ only depends on inputs up to time $t$.\n",
    "1. Long-term dependencies across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Input and output must both be sequences of same length, which should be arbitrary.<br>\n",
    "   $\\Longrightarrow$ 1D fully-convolutional architecture with appropriate **padding**.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Output at time $t$ only depends on inputs up to time $t$.<br>\n",
    "    $\\Longrightarrow$ **Causal convolutions**, where an output at time $t$ is convolved only with elements from time $t$ and earlier in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Long-term dependencies across time.<br>\n",
    "    $\\Longrightarrow$ Increase receptive field using **depth** (facilitated by residual connections) and **dilated convolutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Combining these techniques gives us the TCN architecture (Bai et. al, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/tcn.png\" width=\"1400\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement a first a **general** TCN residual block and then a full TCN model for our Sentiment Analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Control padding to maintain causal output size for a fixed kernel size\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        if in_channels != out_channels:\n",
    "            self.channels_adapter = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.channels_adapter = None\n",
    "            \n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        out = self.conv1(x)\n",
    "        out = out[..., :-self.padding] # maintain causality\n",
    "        out = self.conv2(out)\n",
    "        out = out[..., :-self.padding]\n",
    "\n",
    "        # Skip-connection (residual branch)\n",
    "        skip = x if not self.channels_adapter else self.channels_adapter(x)\n",
    "        out = torch.relu(out + skip)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCNBlock(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(2, 4, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (channels_adapter): Conv1d(2, 4, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCNBlock(in_channels=2, out_channels=4, kernel_size=3, dilation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the TCN block maintain causality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/causal_convolution.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentTCN(nn.Module):\n",
    "    def __init__(self, vocab_dim: int, embedding_dim: int, layer_channels: list, out_dim: int, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        assert len(layer_channels) > 0\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        \n",
    "        tcn_channels = [embedding_dim] + layer_channels + [out_dim]\n",
    "        layers = []\n",
    "        for i, (c_in, c_out) in enumerate(zip(tcn_channels[:-1], tcn_channels[1:])):\n",
    "            \n",
    "            # Exponentially-increasing dilation\n",
    "            dilation = 2 ** i\n",
    "            \n",
    "            layers.append(\n",
    "                TCNBlock(c_in, c_out, kernel_size, dilation=dilation, dropout=dropout)\n",
    "            )\n",
    "            \n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, **kw): # x is (S, B)\n",
    "        # First we need to embed our sequence.\n",
    "        # Note how we treat the E as channels for the convulutions.\n",
    "        x_emb = self.embedding(x) # (S, B, E)\n",
    "        x_emb = torch.transpose(x_emb, 0, 1) # (B, S, E)\n",
    "        x_emb = torch.transpose(x_emb, 1, 2) # (B, E, S)\n",
    "        \n",
    "        # Process the entire sequence (at once!)\n",
    "        y_seq = self.blocks(x_emb) # (B, D_out, S)\n",
    "        \n",
    "        # Output predictions\n",
    "        yt = y_seq[..., -1] # (B, D_out)\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentTCN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (blocks): Sequential(\n",
       "    (0): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(100, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(100, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(3, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice where the `channel_adapter`s are. What's their purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TCN model has 1,570,796 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "print(f'The TCN model has {count_parameters(tcn):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try a forward pass with this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0986, -1.0986, -1.0986],\n",
       "        [-1.1709, -0.9898, -1.1450],\n",
       "        [-1.0986, -1.0986, -1.0986],\n",
       "        [-1.1102, -1.1102, -1.0759]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, let's train the new model using the exact same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.075, accuracy=0.390, elapsed=3.7 sec\n",
      "Epoch #1, loss=1.061, accuracy=0.396, elapsed=3.5 sec\n",
      "Epoch #2, loss=1.053, accuracy=0.398, elapsed=3.5 sec\n",
      "Epoch #3, loss=1.034, accuracy=0.427, elapsed=3.8 sec\n"
     ]
    }
   ],
   "source": [
    "tcn_model = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "optimizer = optim.Adam(tcn_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(tcn_model, optimizer, loss_fn, dl_train, max_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "1. The model took sequences of different length every batch, just like an RNN.\n",
    "2. Sequences were processed in **parallel** w.r.t. time, unlike RNN, making it faster.\n",
    "3. Receptive field was controlled by architecture, not by sequence length, unlike RNN.\n",
    "4. No need for BPTT, unlike RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by [Aviv A. Rosenberg](https://avivr.net).<br>\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Sebastian Ruder, \"On word embeddings - Part 1\", 2016, https://ruder.io\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- S. Bai et al. 2018, http://arxiv.org/abs/1803.01271"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
