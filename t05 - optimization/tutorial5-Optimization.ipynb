{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will cover:\n",
    "\n",
    "* Continuous Optimization\n",
    "    + Solving continuous optimization problems\n",
    "* Reminder: multivariate calculus\n",
    "* Gradient Descent\n",
    "    + Why does GD work?\n",
    "    + Selecting the learning rate\n",
    "    + What can go wrong?\n",
    "* Stochastic gradient descent\n",
    "* Advanced optimizers\n",
    "* Working example\n",
    "* PyTorch's optimization API - *torch.optim*\n",
    "* Learning rate scheduling\n",
    "* Projected Gradient Descent (PGD)\n",
    "    + Use case: adversarial attacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continious optimization problems are fundumental in Computer Science.\n",
    "\n",
    "May be either unconstrained:\n",
    "$$ \\min_x f(x) \\\\ f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$$\n",
    "Or constrained:\n",
    "$$ \\min_x f(x)\n",
    "\\text{ subject to } x \\in \\mathcal{K} \\\\\n",
    "f: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\text{, } \\mathcal{K} \\subseteq \\mathbb{R}^d \\text{ is closed and convex} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many problems in CS can be written as a continous optimization problems:\n",
    "* Linear programs (LPs)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/3dpoly.svg/220px-3dpoly.svg.png\">\n",
    "</p>\n",
    "\n",
    "* Linear Regression:\n",
    "\n",
    "$$ \\min_w \\| Xw - y \\|^2 \\\\\n",
    "\\text{where } X \\in \\mathbb{R}^{n \\times d}, y \\in \\mathbb{R}^n $$\n",
    "\n",
    "\n",
    "* Hard SVMs:\n",
    "\n",
    "$$ \\min_{w,b} \\|w\\|^2 \\\\\n",
    "  \\text{subject to } y_i (w^T x_i-b) \\geq 1 $$\n",
    "\n",
    "* **Empirical risk minimization of deep models** \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.fromthegenesis.com/wp-content/uploads/2018/06/Gradie_Desce.jpg\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Continious Optimization Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, continious optimization problems may be solved analytically:\n",
    "* For unconstrained problems, search for stationary points.\n",
    "* For constrained problems, try applying Lagrange multipliers or KKT conditions.\n",
    "\n",
    "However, modern deep architectures include millions (sometimes billions) of parameters...\n",
    "the loss function is summed over all the dataset (**memory burden**)\n",
    "and the loss surface is often very noisy!\n",
    "\n",
    "Therefore, efficient iterative optimization algorithms are required!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://huggingface.co/blog/assets/33_large_language_models/01_model_size.jpg\"  width=\"30%\" height=\"30%\">\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: multivariate calculus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be mainly interested in functions $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "The generalization of the derivative in the multivariate case is denoted as the **gradient**, which is composed of the **partial derivatives**:\n",
    "$$ \\nabla_x f = (\\frac{\\partial f}{\\partial x_1},...,\\frac{\\partial f}{\\partial x_d}) \\in \\mathbb{R}^d $$\n",
    "\n",
    "The gradient gives us local information about the direction of the **largest ascent**:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"paraboloid.png\">\n",
    "</p>\n",
    "\n",
    "If the gradient at some point $x \\in \\mathbb{R}^d$ is $\\vec{0}$ then $x$ is called a **stationary point**.\n",
    "\n",
    "<!---\n",
    "Redundant\n",
    "For functions with multiple outputs $f: \\mathbb{R}^d \\rightarrow \\mathbb{R^m}$ , the gradient is generalized to a **Jacobian**:\n",
    "\n",
    "$$ J_x(f) = \n",
    "\\begin{pmatrix}\n",
    "            \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_d} \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_d} \\\\\n",
    "        \\end{pmatrix}\n",
    "\\in \\mathbb{R}^{m \\times d} $$\n",
    "-->\n",
    "\n",
    "The second derivative of a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is defined by computing the gradient of each of the partial derivatives.\n",
    "\n",
    "The resulting matrix is defined as the **Hessian** of $f$:\n",
    "$$\n",
    "\\nabla^2_x f = \n",
    "\\begin{pmatrix}\n",
    "            \\frac{\\partial^2 f}{\\partial x_1 \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_d} \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            \\frac{\\partial^2 f}{\\partial x_d \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_d \\partial x_d} \\\\\n",
    "        \\end{pmatrix}\n",
    "\\in \\mathbb{R}^{d \\times d} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iterative algorithm for solving continious optimization problems.\n",
    "* Exploit local information from the current guess to produce the next guess.\n",
    "* Idea: move along the anti-gradient direction of the currrent guess:\n",
    "\n",
    "$$ x_{k+1} = x_k - \\eta \\nabla_x f (x_k) $$\n",
    "\n",
    "We denote $ \\eta $, which determines the step size as the **learning rate**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does GD work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does GD work?\n",
    "\n",
    "By using first order Taylor's approximation around $x_k$:\n",
    "$$ f(x_k + \\delta) = f(x_k) + \\nabla_x f(x_k)^T \\delta + o(\\| \\delta\\|)$$\n",
    "Substituting $\\delta = - \\eta \\nabla_x f (x_k)$:\n",
    "$$ f(x_{k+1}) = f(x_k) - \\eta \\| \\nabla_x f(x_k) \\|^2 + o(\\| \\delta\\|)$$\n",
    "If $x_k$ is not a stationary point, then for a small enough $\\eta > 0 $ we have that $f$ strictly decreases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecting the right learning rate is very important!\n",
    "* Selecting too small learning rate would yield to a very slow optimization process (\"**under-damped**\").\n",
    "* Selecting too large learning rate would yield to a jumpy process (\"**over-damped**\").\n",
    "* Selecting a very large learning rate would cause the optimization process to diverge!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"example.gif\" height=60%, width=60%>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the optimal learning rate?\n",
    "* For quadratic objectives, $\\eta_{opt} = \\frac{1}{\\lambda_{max}}$ where $\\lambda_{max}$ is the largest eigenvalue of the (constant) hessian matrix.\n",
    "* For general objectives, computing $\\lambda_{max}$ in every iteration is hard.\n",
    "* In practice: perform manual or black-box tuning.\n",
    "* Check out [optuna](https://optuna.org/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can go wrong?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://nbviewer.org/github/vistalab-technion/cs236781-tutorials/blob/master/t01%20-%20linear%20models/imgs/sgd2d_2.png\" height=25%, width=25%>\n",
    "</p>\n",
    "\n",
    "* The loss surface of DNNs is highly non-convex!\n",
    "* GD depends on initialization. May converge to a **local minimum** rather than a **global minimum**!\n",
    "* Another issue with GD is that it considers all the samples together (memory and computation burdens)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our case the optimization objective can be decomposed as a sum (mean) of objectives on each sample:\n",
    "$$ f(x) = \\frac{1}{n} \\sum_{i=1}^n f_i(x) $$\n",
    "* Recall that $n$ is very large.\n",
    "* Idea: sample an index, and compute the gradient on a single datum:\n",
    "$$ i \\leftarrow Uniform(\\{1,...,n\\}) \\\\ \n",
    "x_{k+1} \\leftarrow x_k - \\eta \\nabla f_i(x_k)$$\n",
    "* In expectation the gradient is exact! However, the variance is very high!\n",
    "* Optimization process becomes very noisy!\n",
    "* Idea: instead of sampling a single datum, sample a **batch(mini-batch)** of samples.\n",
    "* In practice: shuffle the dataset and split it into **mini-batches**. Each iteration over the whole dataset is called an **epoch**.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/batch-1-1024x670.png\" width=25%, height=25%>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced optimizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Heavy ball momentum\n",
    "    + Idea: accumulate velocity from prior iterations!\n",
    "    + Models the physics of a ball that is rolling downhill.\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/1*zVi4ayX9u0MQQwa90CnxVg.gif\", width=25%, height=25%>\n",
    "    </p>\n",
    "    + Momentum is modeled by an exponential moving average of the gradients in the prior steps:\n",
    "    \n",
    "    $$ v_{k+1} \\leftarrow \\gamma v_k + (1-\\gamma) g_k \\\\ x_{k+1} \\leftarrow x_k - \\eta v_{k+1} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AdaGrad\n",
    "    + Stands for Adaptive Gradient.\n",
    "    + Idea: the Hessian matrix may be very unbalanced, so use different effective learning rate for each parameter.\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*WRtvrr9Z0QcokiKlgU7xEw.gif\", width=25%, height=25%>\n",
    "    </p>\n",
    "    \n",
    "    + Mathematically: \n",
    "    $$ G_{k+1} \\leftarrow G_k + g_k \\cdot g_k \\\\ x_{k+1} \\leftarrow x_k - \\frac{\\eta}{\\sqrt{G_{k+1} + \\epsilon}} \\cdot g_k $$\n",
    "\n",
    "    + Note that in the above formulation $\\cdot$ multiplication and the division is done **elementwise**.\n",
    "    + $\\epsilon$ is added to the denominator for numerical stability.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rmsprop\n",
    "    + The problem of Adagrad is that the denominator keeps growing, and hence becomes very slow.\n",
    "    + The solution is to use an EMA of the squared gradients instead:\n",
    "\n",
    "    $$ v_{k+1} \\leftarrow \\beta v_k + (1-\\beta) g_k \\cdot g_k \\\\ x_{k+1} \\leftarrow x_k - \\frac{\\eta}{\\sqrt{v_{k+1} + \\epsilon}} \\cdot g_k $$\n",
    "\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/1*_4zyVpZazh4OSIzprmYQEw.gif\", width=25%, height=25%>\n",
    "    </p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adam\n",
    "    + Stands for Adaptive Moment Estimation.\n",
    "    + Essentially a combination of momentum and rmsprop:\n",
    "    $$\n",
    "        m_{k+1} \\leftarrow \\beta_1 m_k + (1-\\beta_1) g_k \\\\\n",
    "        v_{k+1} \\leftarrow \\beta_2 v_k + (1-\\beta_2) g_k \\cdot g_k \\\\\n",
    "        \\hat{m}_{k+1} \\leftarrow \\frac{m_{k+1}}{1-\\beta_1^{k+1}}, \\quad \\hat{v}_{k+1} \\leftarrow \\frac{v_{k+1}}{1-\\beta_2^{k+1}} \\\\\n",
    "        x_{k+1} \\leftarrow x_k - \\frac{\\eta}{\\sqrt{\\hat{v}_{k+1} + \\epsilon}} \\cdot \\hat{m}_{k+1}\n",
    "    $$\n",
    "    + The most common optimizer today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which optimizer to use?\n",
    "    + Adam would be a good place to start.\n",
    "    + However, **for some tasks it is better to use other optimizers**.\n",
    "    + For instance, simple SGD with momentum works the best for optimizing ResNet!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate SGD for training a simple MLP architecture for performing hand-written digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP architecture\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_dim = 784\n",
    "        self.hidden_dim = 120\n",
    "        self.out_dim = 10\n",
    "\n",
    "        self.flatten = nn.Flatten() # (B,H,W) -> (B,D)\n",
    "        self.linear = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = Net() # Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training dataset\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(), # Convert to tensor\n",
    "            transforms.Normalize((0.1307,), (0.3081,)) # Subtract from values 0.13 then divide by 0.31\n",
    "            ])\n",
    "\n",
    "dataset = datasets.MNIST('./data', train=True, download=True, transform=transform) # MNIST train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloader \n",
    "batch_size = 64\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) # Different order in each epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each batch, optimization can be summarized as follows:\n",
    "* Loss computation on the current batch.\n",
    "* Loss gradient computation w.r.t each of the model params.\n",
    "* perfrom SGD step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] | Batch 1 | loss: 2.2692148685455322\n",
      "Epoch [1/1] | Batch 2 | loss: 2.2100679874420166\n",
      "Epoch [1/1] | Batch 3 | loss: 2.027266263961792\n",
      "Epoch [1/1] | Batch 4 | loss: 1.8630539178848267\n",
      "Epoch [1/1] | Batch 5 | loss: 1.8929814100265503\n",
      "Epoch [1/1] | Batch 6 | loss: 1.6044684648513794\n",
      "Epoch [1/1] | Batch 7 | loss: 1.475663423538208\n",
      "Epoch [1/1] | Batch 8 | loss: 1.3381201028823853\n",
      "Epoch [1/1] | Batch 9 | loss: 1.1293433904647827\n",
      "Epoch [1/1] | Batch 10 | loss: 1.2795214653015137\n",
      "Epoch [1/1] | Batch 11 | loss: 1.0670692920684814\n",
      "Epoch [1/1] | Batch 12 | loss: 0.9152774214744568\n",
      "Epoch [1/1] | Batch 13 | loss: 0.8227441310882568\n",
      "Epoch [1/1] | Batch 14 | loss: 0.8876625895500183\n",
      "Epoch [1/1] | Batch 15 | loss: 0.6743249297142029\n",
      "Epoch [1/1] | Batch 16 | loss: 0.7605699896812439\n",
      "Epoch [1/1] | Batch 17 | loss: 0.7148019671440125\n",
      "Epoch [1/1] | Batch 18 | loss: 0.8863875269889832\n",
      "Epoch [1/1] | Batch 19 | loss: 0.7686529755592346\n",
      "Epoch [1/1] | Batch 20 | loss: 0.729006290435791\n",
      "Epoch [1/1] | Batch 21 | loss: 0.728488028049469\n",
      "Epoch [1/1] | Batch 22 | loss: 0.8528248071670532\n",
      "Epoch [1/1] | Batch 23 | loss: 0.6010146141052246\n",
      "Epoch [1/1] | Batch 24 | loss: 0.7293763160705566\n",
      "Epoch [1/1] | Batch 25 | loss: 0.6482455730438232\n",
      "Epoch [1/1] | Batch 26 | loss: 0.6288968920707703\n",
      "Epoch [1/1] | Batch 27 | loss: 0.5395212769508362\n",
      "Epoch [1/1] | Batch 28 | loss: 0.808894693851471\n",
      "Epoch [1/1] | Batch 29 | loss: 0.7948347926139832\n",
      "Epoch [1/1] | Batch 30 | loss: 0.48381027579307556\n",
      "Epoch [1/1] | Batch 31 | loss: 0.8252214193344116\n",
      "Epoch [1/1] | Batch 32 | loss: 0.6670800447463989\n",
      "Epoch [1/1] | Batch 33 | loss: 0.4697788953781128\n",
      "Epoch [1/1] | Batch 34 | loss: 0.442093163728714\n",
      "Epoch [1/1] | Batch 35 | loss: 0.5057752728462219\n",
      "Epoch [1/1] | Batch 36 | loss: 0.48312291502952576\n",
      "Epoch [1/1] | Batch 37 | loss: 0.42685648798942566\n",
      "Epoch [1/1] | Batch 38 | loss: 0.40095043182373047\n",
      "Epoch [1/1] | Batch 39 | loss: 0.7426704168319702\n",
      "Epoch [1/1] | Batch 40 | loss: 0.34059926867485046\n",
      "Epoch [1/1] | Batch 41 | loss: 0.437120258808136\n",
      "Epoch [1/1] | Batch 42 | loss: 0.6088960766792297\n",
      "Epoch [1/1] | Batch 43 | loss: 0.5458612442016602\n",
      "Epoch [1/1] | Batch 44 | loss: 0.68532794713974\n",
      "Epoch [1/1] | Batch 45 | loss: 0.44103917479515076\n",
      "Epoch [1/1] | Batch 46 | loss: 0.4936816990375519\n",
      "Epoch [1/1] | Batch 47 | loss: 0.46459174156188965\n",
      "Epoch [1/1] | Batch 48 | loss: 0.31443190574645996\n",
      "Epoch [1/1] | Batch 49 | loss: 0.45889726281166077\n",
      "Epoch [1/1] | Batch 50 | loss: 0.4017797112464905\n",
      "Epoch [1/1] | Batch 51 | loss: 0.6157351732254028\n",
      "Epoch [1/1] | Batch 52 | loss: 0.367069810628891\n",
      "Epoch [1/1] | Batch 53 | loss: 0.4621436297893524\n",
      "Epoch [1/1] | Batch 54 | loss: 0.452536940574646\n",
      "Epoch [1/1] | Batch 55 | loss: 0.4478875398635864\n",
      "Epoch [1/1] | Batch 56 | loss: 0.5250715613365173\n",
      "Epoch [1/1] | Batch 57 | loss: 0.5688446164131165\n",
      "Epoch [1/1] | Batch 58 | loss: 0.49000266194343567\n",
      "Epoch [1/1] | Batch 59 | loss: 0.3538033664226532\n",
      "Epoch [1/1] | Batch 60 | loss: 0.5102899670600891\n",
      "Epoch [1/1] | Batch 61 | loss: 0.3224267065525055\n",
      "Epoch [1/1] | Batch 62 | loss: 0.29675230383872986\n",
      "Epoch [1/1] | Batch 63 | loss: 0.589489758014679\n",
      "Epoch [1/1] | Batch 64 | loss: 0.49964430928230286\n",
      "Epoch [1/1] | Batch 65 | loss: 0.5157939791679382\n",
      "Epoch [1/1] | Batch 66 | loss: 0.4192174971103668\n",
      "Epoch [1/1] | Batch 67 | loss: 0.30151328444480896\n",
      "Epoch [1/1] | Batch 68 | loss: 0.276233434677124\n",
      "Epoch [1/1] | Batch 69 | loss: 0.36172086000442505\n",
      "Epoch [1/1] | Batch 70 | loss: 0.4766729176044464\n",
      "Epoch [1/1] | Batch 71 | loss: 0.47543859481811523\n",
      "Epoch [1/1] | Batch 72 | loss: 0.49117758870124817\n",
      "Epoch [1/1] | Batch 73 | loss: 0.27467936277389526\n",
      "Epoch [1/1] | Batch 74 | loss: 0.7185772657394409\n",
      "Epoch [1/1] | Batch 75 | loss: 0.3077230155467987\n",
      "Epoch [1/1] | Batch 76 | loss: 0.3900032937526703\n",
      "Epoch [1/1] | Batch 77 | loss: 0.361768901348114\n",
      "Epoch [1/1] | Batch 78 | loss: 0.25031259655952454\n",
      "Epoch [1/1] | Batch 79 | loss: 0.2579159736633301\n",
      "Epoch [1/1] | Batch 80 | loss: 0.34343817830085754\n",
      "Epoch [1/1] | Batch 81 | loss: 0.419530987739563\n",
      "Epoch [1/1] | Batch 82 | loss: 0.5474551320075989\n",
      "Epoch [1/1] | Batch 83 | loss: 0.5116308331489563\n",
      "Epoch [1/1] | Batch 84 | loss: 0.42575663328170776\n",
      "Epoch [1/1] | Batch 85 | loss: 0.48644840717315674\n",
      "Epoch [1/1] | Batch 86 | loss: 0.4282301366329193\n",
      "Epoch [1/1] | Batch 87 | loss: 0.2848828434944153\n",
      "Epoch [1/1] | Batch 88 | loss: 0.500651478767395\n",
      "Epoch [1/1] | Batch 89 | loss: 0.39744260907173157\n",
      "Epoch [1/1] | Batch 90 | loss: 0.38028475642204285\n",
      "Epoch [1/1] | Batch 91 | loss: 0.39304211735725403\n",
      "Epoch [1/1] | Batch 92 | loss: 0.40760788321495056\n",
      "Epoch [1/1] | Batch 93 | loss: 0.24406342208385468\n",
      "Epoch [1/1] | Batch 94 | loss: 0.39696022868156433\n",
      "Epoch [1/1] | Batch 95 | loss: 0.49246081709861755\n",
      "Epoch [1/1] | Batch 96 | loss: 0.3800676763057709\n",
      "Epoch [1/1] | Batch 97 | loss: 0.2716065049171448\n",
      "Epoch [1/1] | Batch 98 | loss: 0.36443862318992615\n",
      "Epoch [1/1] | Batch 99 | loss: 0.11780057847499847\n",
      "Epoch [1/1] | Batch 100 | loss: 0.12745977938175201\n",
      "Epoch [1/1] | Batch 101 | loss: 0.33177316188812256\n",
      "Epoch [1/1] | Batch 102 | loss: 0.419185072183609\n",
      "Epoch [1/1] | Batch 103 | loss: 0.20601826906204224\n",
      "Epoch [1/1] | Batch 104 | loss: 0.4119439125061035\n",
      "Epoch [1/1] | Batch 105 | loss: 0.5009148716926575\n",
      "Epoch [1/1] | Batch 106 | loss: 0.3734690546989441\n",
      "Epoch [1/1] | Batch 107 | loss: 0.37314534187316895\n",
      "Epoch [1/1] | Batch 108 | loss: 0.29071831703186035\n",
      "Epoch [1/1] | Batch 109 | loss: 0.31608831882476807\n",
      "Epoch [1/1] | Batch 110 | loss: 0.6847937107086182\n",
      "Epoch [1/1] | Batch 111 | loss: 0.31661176681518555\n",
      "Epoch [1/1] | Batch 112 | loss: 0.33421292901039124\n",
      "Epoch [1/1] | Batch 113 | loss: 0.4186474084854126\n",
      "Epoch [1/1] | Batch 114 | loss: 0.23353233933448792\n",
      "Epoch [1/1] | Batch 115 | loss: 0.27124080061912537\n",
      "Epoch [1/1] | Batch 116 | loss: 0.3277783691883087\n",
      "Epoch [1/1] | Batch 117 | loss: 0.25008144974708557\n",
      "Epoch [1/1] | Batch 118 | loss: 0.18249215185642242\n",
      "Epoch [1/1] | Batch 119 | loss: 0.3432999551296234\n",
      "Epoch [1/1] | Batch 120 | loss: 0.2684101462364197\n",
      "Epoch [1/1] | Batch 121 | loss: 0.5271586775779724\n",
      "Epoch [1/1] | Batch 122 | loss: 0.37312522530555725\n",
      "Epoch [1/1] | Batch 123 | loss: 0.31071600317955017\n",
      "Epoch [1/1] | Batch 124 | loss: 0.432390958070755\n",
      "Epoch [1/1] | Batch 125 | loss: 0.46223583817481995\n",
      "Epoch [1/1] | Batch 126 | loss: 0.46492308378219604\n",
      "Epoch [1/1] | Batch 127 | loss: 0.3100569248199463\n",
      "Epoch [1/1] | Batch 128 | loss: 0.4827926456928253\n",
      "Epoch [1/1] | Batch 129 | loss: 0.5269753932952881\n",
      "Epoch [1/1] | Batch 130 | loss: 0.3803631365299225\n",
      "Epoch [1/1] | Batch 131 | loss: 0.36716532707214355\n",
      "Epoch [1/1] | Batch 132 | loss: 0.400774210691452\n",
      "Epoch [1/1] | Batch 133 | loss: 0.3739730715751648\n",
      "Epoch [1/1] | Batch 134 | loss: 0.45652496814727783\n",
      "Epoch [1/1] | Batch 135 | loss: 0.2373468577861786\n",
      "Epoch [1/1] | Batch 136 | loss: 0.21757656335830688\n",
      "Epoch [1/1] | Batch 137 | loss: 0.2891677916049957\n",
      "Epoch [1/1] | Batch 138 | loss: 0.24698039889335632\n",
      "Epoch [1/1] | Batch 139 | loss: 0.2556249797344208\n",
      "Epoch [1/1] | Batch 140 | loss: 0.2882031202316284\n",
      "Epoch [1/1] | Batch 141 | loss: 0.4149017333984375\n",
      "Epoch [1/1] | Batch 142 | loss: 0.39736056327819824\n",
      "Epoch [1/1] | Batch 143 | loss: 0.20213165879249573\n",
      "Epoch [1/1] | Batch 144 | loss: 0.2129868119955063\n",
      "Epoch [1/1] | Batch 145 | loss: 0.45333436131477356\n",
      "Epoch [1/1] | Batch 146 | loss: 0.24418871104717255\n",
      "Epoch [1/1] | Batch 147 | loss: 0.17512674629688263\n",
      "Epoch [1/1] | Batch 148 | loss: 0.3434774875640869\n",
      "Epoch [1/1] | Batch 149 | loss: 0.3691617548465729\n",
      "Epoch [1/1] | Batch 150 | loss: 0.30084699392318726\n",
      "Epoch [1/1] | Batch 151 | loss: 0.4573061764240265\n",
      "Epoch [1/1] | Batch 152 | loss: 0.24102522432804108\n",
      "Epoch [1/1] | Batch 153 | loss: 0.3504702150821686\n",
      "Epoch [1/1] | Batch 154 | loss: 0.5321661233901978\n",
      "Epoch [1/1] | Batch 155 | loss: 0.3428231179714203\n",
      "Epoch [1/1] | Batch 156 | loss: 0.33340081572532654\n",
      "Epoch [1/1] | Batch 157 | loss: 0.24113406240940094\n",
      "Epoch [1/1] | Batch 158 | loss: 0.5129137635231018\n",
      "Epoch [1/1] | Batch 159 | loss: 0.3478209674358368\n",
      "Epoch [1/1] | Batch 160 | loss: 0.4156959354877472\n",
      "Epoch [1/1] | Batch 161 | loss: 0.2991056740283966\n",
      "Epoch [1/1] | Batch 162 | loss: 0.40489763021469116\n",
      "Epoch [1/1] | Batch 163 | loss: 0.3348146975040436\n",
      "Epoch [1/1] | Batch 164 | loss: 0.5455468893051147\n",
      "Epoch [1/1] | Batch 165 | loss: 0.2537307143211365\n",
      "Epoch [1/1] | Batch 166 | loss: 0.3097366392612457\n",
      "Epoch [1/1] | Batch 167 | loss: 0.22757990658283234\n",
      "Epoch [1/1] | Batch 168 | loss: 0.39253467321395874\n",
      "Epoch [1/1] | Batch 169 | loss: 0.2555399239063263\n",
      "Epoch [1/1] | Batch 170 | loss: 0.25816938281059265\n",
      "Epoch [1/1] | Batch 171 | loss: 0.24206985533237457\n",
      "Epoch [1/1] | Batch 172 | loss: 0.2833525240421295\n",
      "Epoch [1/1] | Batch 173 | loss: 0.19056329131126404\n",
      "Epoch [1/1] | Batch 174 | loss: 0.331670880317688\n",
      "Epoch [1/1] | Batch 175 | loss: 0.3216933310031891\n",
      "Epoch [1/1] | Batch 176 | loss: 0.3809948265552521\n",
      "Epoch [1/1] | Batch 177 | loss: 0.360258549451828\n",
      "Epoch [1/1] | Batch 178 | loss: 0.3183709383010864\n",
      "Epoch [1/1] | Batch 179 | loss: 0.25322794914245605\n",
      "Epoch [1/1] | Batch 180 | loss: 0.205093115568161\n",
      "Epoch [1/1] | Batch 181 | loss: 0.39295339584350586\n",
      "Epoch [1/1] | Batch 182 | loss: 0.24636349081993103\n",
      "Epoch [1/1] | Batch 183 | loss: 0.2969978451728821\n",
      "Epoch [1/1] | Batch 184 | loss: 0.3575392961502075\n",
      "Epoch [1/1] | Batch 185 | loss: 0.21186812222003937\n",
      "Epoch [1/1] | Batch 186 | loss: 0.5199079513549805\n",
      "Epoch [1/1] | Batch 187 | loss: 0.2514224052429199\n",
      "Epoch [1/1] | Batch 188 | loss: 0.3113943040370941\n",
      "Epoch [1/1] | Batch 189 | loss: 0.17863863706588745\n",
      "Epoch [1/1] | Batch 190 | loss: 0.19975152611732483\n",
      "Epoch [1/1] | Batch 191 | loss: 0.2570565640926361\n",
      "Epoch [1/1] | Batch 192 | loss: 0.5093363523483276\n",
      "Epoch [1/1] | Batch 193 | loss: 0.3395356833934784\n",
      "Epoch [1/1] | Batch 194 | loss: 0.35685354471206665\n",
      "Epoch [1/1] | Batch 195 | loss: 0.40592285990715027\n",
      "Epoch [1/1] | Batch 196 | loss: 0.3685150444507599\n",
      "Epoch [1/1] | Batch 197 | loss: 0.23112249374389648\n",
      "Epoch [1/1] | Batch 198 | loss: 0.26636677980422974\n",
      "Epoch [1/1] | Batch 199 | loss: 0.24742454290390015\n",
      "Epoch [1/1] | Batch 200 | loss: 0.35648471117019653\n",
      "Epoch [1/1] | Batch 201 | loss: 0.401787132024765\n",
      "Epoch [1/1] | Batch 202 | loss: 0.3087044060230255\n",
      "Epoch [1/1] | Batch 203 | loss: 0.2147236466407776\n",
      "Epoch [1/1] | Batch 204 | loss: 0.39777475595474243\n",
      "Epoch [1/1] | Batch 205 | loss: 0.41978925466537476\n",
      "Epoch [1/1] | Batch 206 | loss: 0.38404759764671326\n",
      "Epoch [1/1] | Batch 207 | loss: 0.26590242981910706\n",
      "Epoch [1/1] | Batch 208 | loss: 0.21940146386623383\n",
      "Epoch [1/1] | Batch 209 | loss: 0.2285776287317276\n",
      "Epoch [1/1] | Batch 210 | loss: 0.20173519849777222\n",
      "Epoch [1/1] | Batch 211 | loss: 0.3372740149497986\n",
      "Epoch [1/1] | Batch 212 | loss: 0.20675650238990784\n",
      "Epoch [1/1] | Batch 213 | loss: 0.3943866193294525\n",
      "Epoch [1/1] | Batch 214 | loss: 0.19046920537948608\n",
      "Epoch [1/1] | Batch 215 | loss: 0.39537355303764343\n",
      "Epoch [1/1] | Batch 216 | loss: 0.2930717170238495\n",
      "Epoch [1/1] | Batch 217 | loss: 0.2729257047176361\n",
      "Epoch [1/1] | Batch 218 | loss: 0.541786253452301\n",
      "Epoch [1/1] | Batch 219 | loss: 0.2942405939102173\n",
      "Epoch [1/1] | Batch 220 | loss: 0.3077932894229889\n",
      "Epoch [1/1] | Batch 221 | loss: 0.37534067034721375\n",
      "Epoch [1/1] | Batch 222 | loss: 0.2209882140159607\n",
      "Epoch [1/1] | Batch 223 | loss: 0.28314509987831116\n",
      "Epoch [1/1] | Batch 224 | loss: 0.10872731357812881\n",
      "Epoch [1/1] | Batch 225 | loss: 0.2564040720462799\n",
      "Epoch [1/1] | Batch 226 | loss: 0.2933441698551178\n",
      "Epoch [1/1] | Batch 227 | loss: 0.2754315137863159\n",
      "Epoch [1/1] | Batch 228 | loss: 0.2379399836063385\n",
      "Epoch [1/1] | Batch 229 | loss: 0.22543463110923767\n",
      "Epoch [1/1] | Batch 230 | loss: 0.2263999730348587\n",
      "Epoch [1/1] | Batch 231 | loss: 0.5372822880744934\n",
      "Epoch [1/1] | Batch 232 | loss: 0.19803577661514282\n",
      "Epoch [1/1] | Batch 233 | loss: 0.2552867829799652\n",
      "Epoch [1/1] | Batch 234 | loss: 0.307576984167099\n",
      "Epoch [1/1] | Batch 235 | loss: 0.31540530920028687\n",
      "Epoch [1/1] | Batch 236 | loss: 0.26328614354133606\n",
      "Epoch [1/1] | Batch 237 | loss: 0.13300873339176178\n",
      "Epoch [1/1] | Batch 238 | loss: 0.36505913734436035\n",
      "Epoch [1/1] | Batch 239 | loss: 0.36232998967170715\n",
      "Epoch [1/1] | Batch 240 | loss: 0.1869489699602127\n",
      "Epoch [1/1] | Batch 241 | loss: 0.18666024506092072\n",
      "Epoch [1/1] | Batch 242 | loss: 0.34641197323799133\n",
      "Epoch [1/1] | Batch 243 | loss: 0.3340684473514557\n",
      "Epoch [1/1] | Batch 244 | loss: 0.23398616909980774\n",
      "Epoch [1/1] | Batch 245 | loss: 0.1057557463645935\n",
      "Epoch [1/1] | Batch 246 | loss: 0.2074327915906906\n",
      "Epoch [1/1] | Batch 247 | loss: 0.31160417199134827\n",
      "Epoch [1/1] | Batch 248 | loss: 0.2696637511253357\n",
      "Epoch [1/1] | Batch 249 | loss: 0.2187841832637787\n",
      "Epoch [1/1] | Batch 250 | loss: 0.18939825892448425\n",
      "Epoch [1/1] | Batch 251 | loss: 0.22779744863510132\n",
      "Epoch [1/1] | Batch 252 | loss: 0.24160046875476837\n",
      "Epoch [1/1] | Batch 253 | loss: 0.3892296850681305\n",
      "Epoch [1/1] | Batch 254 | loss: 0.32869014143943787\n",
      "Epoch [1/1] | Batch 255 | loss: 0.38296008110046387\n",
      "Epoch [1/1] | Batch 256 | loss: 0.19724005460739136\n",
      "Epoch [1/1] | Batch 257 | loss: 0.31575411558151245\n",
      "Epoch [1/1] | Batch 258 | loss: 0.49366533756256104\n",
      "Epoch [1/1] | Batch 259 | loss: 0.16498273611068726\n",
      "Epoch [1/1] | Batch 260 | loss: 0.10804630070924759\n",
      "Epoch [1/1] | Batch 261 | loss: 0.11361269652843475\n",
      "Epoch [1/1] | Batch 262 | loss: 0.18148238956928253\n",
      "Epoch [1/1] | Batch 263 | loss: 0.270607054233551\n",
      "Epoch [1/1] | Batch 264 | loss: 0.24785862863063812\n",
      "Epoch [1/1] | Batch 265 | loss: 0.49650099873542786\n",
      "Epoch [1/1] | Batch 266 | loss: 0.21380431950092316\n",
      "Epoch [1/1] | Batch 267 | loss: 0.3289657235145569\n",
      "Epoch [1/1] | Batch 268 | loss: 0.2014133185148239\n",
      "Epoch [1/1] | Batch 269 | loss: 0.17007167637348175\n",
      "Epoch [1/1] | Batch 270 | loss: 0.43629980087280273\n",
      "Epoch [1/1] | Batch 271 | loss: 0.2598600685596466\n",
      "Epoch [1/1] | Batch 272 | loss: 0.20998498797416687\n",
      "Epoch [1/1] | Batch 273 | loss: 0.18363074958324432\n",
      "Epoch [1/1] | Batch 274 | loss: 0.17402660846710205\n",
      "Epoch [1/1] | Batch 275 | loss: 0.3678610026836395\n",
      "Epoch [1/1] | Batch 276 | loss: 0.2631148397922516\n",
      "Epoch [1/1] | Batch 277 | loss: 0.5717809200286865\n",
      "Epoch [1/1] | Batch 278 | loss: 0.2261303961277008\n",
      "Epoch [1/1] | Batch 279 | loss: 0.16227346658706665\n",
      "Epoch [1/1] | Batch 280 | loss: 0.19024035334587097\n",
      "Epoch [1/1] | Batch 281 | loss: 0.3038388192653656\n",
      "Epoch [1/1] | Batch 282 | loss: 0.2020413875579834\n",
      "Epoch [1/1] | Batch 283 | loss: 0.3018619120121002\n",
      "Epoch [1/1] | Batch 284 | loss: 0.19220976531505585\n",
      "Epoch [1/1] | Batch 285 | loss: 0.33161428570747375\n",
      "Epoch [1/1] | Batch 286 | loss: 0.2562367022037506\n",
      "Epoch [1/1] | Batch 287 | loss: 0.12222796678543091\n",
      "Epoch [1/1] | Batch 288 | loss: 0.3364655673503876\n",
      "Epoch [1/1] | Batch 289 | loss: 0.31036892533302307\n",
      "Epoch [1/1] | Batch 290 | loss: 0.18688830733299255\n",
      "Epoch [1/1] | Batch 291 | loss: 0.2460259646177292\n",
      "Epoch [1/1] | Batch 292 | loss: 0.1969360113143921\n",
      "Epoch [1/1] | Batch 293 | loss: 0.302420437335968\n",
      "Epoch [1/1] | Batch 294 | loss: 0.386014461517334\n",
      "Epoch [1/1] | Batch 295 | loss: 0.1798468679189682\n",
      "Epoch [1/1] | Batch 296 | loss: 0.3722493052482605\n",
      "Epoch [1/1] | Batch 297 | loss: 0.13088208436965942\n",
      "Epoch [1/1] | Batch 298 | loss: 0.43108922243118286\n",
      "Epoch [1/1] | Batch 299 | loss: 0.0828886404633522\n",
      "Epoch [1/1] | Batch 300 | loss: 0.2562170624732971\n",
      "Epoch [1/1] | Batch 301 | loss: 0.2063945084810257\n",
      "Epoch [1/1] | Batch 302 | loss: 0.3615358769893646\n",
      "Epoch [1/1] | Batch 303 | loss: 0.31623002886772156\n",
      "Epoch [1/1] | Batch 304 | loss: 0.15542587637901306\n",
      "Epoch [1/1] | Batch 305 | loss: 0.24913813173770905\n",
      "Epoch [1/1] | Batch 306 | loss: 0.17431065440177917\n",
      "Epoch [1/1] | Batch 307 | loss: 0.4252949655056\n",
      "Epoch [1/1] | Batch 308 | loss: 0.2185228317975998\n",
      "Epoch [1/1] | Batch 309 | loss: 0.25250235199928284\n",
      "Epoch [1/1] | Batch 310 | loss: 0.1277441680431366\n",
      "Epoch [1/1] | Batch 311 | loss: 0.1915241926908493\n",
      "Epoch [1/1] | Batch 312 | loss: 0.36280205845832825\n",
      "Epoch [1/1] | Batch 313 | loss: 0.2805444598197937\n",
      "Epoch [1/1] | Batch 314 | loss: 0.31292590498924255\n",
      "Epoch [1/1] | Batch 315 | loss: 0.19010058045387268\n",
      "Epoch [1/1] | Batch 316 | loss: 0.20291975140571594\n",
      "Epoch [1/1] | Batch 317 | loss: 0.18952570855617523\n",
      "Epoch [1/1] | Batch 318 | loss: 0.12329822778701782\n",
      "Epoch [1/1] | Batch 319 | loss: 0.22869226336479187\n",
      "Epoch [1/1] | Batch 320 | loss: 0.3308769762516022\n",
      "Epoch [1/1] | Batch 321 | loss: 0.17053842544555664\n",
      "Epoch [1/1] | Batch 322 | loss: 0.1484014391899109\n",
      "Epoch [1/1] | Batch 323 | loss: 0.390918105840683\n",
      "Epoch [1/1] | Batch 324 | loss: 0.18705213069915771\n",
      "Epoch [1/1] | Batch 325 | loss: 0.22761096060276031\n",
      "Epoch [1/1] | Batch 326 | loss: 0.2706770896911621\n",
      "Epoch [1/1] | Batch 327 | loss: 0.18925726413726807\n",
      "Epoch [1/1] | Batch 328 | loss: 0.20457397401332855\n",
      "Epoch [1/1] | Batch 329 | loss: 0.21218252182006836\n",
      "Epoch [1/1] | Batch 330 | loss: 0.3099101483821869\n",
      "Epoch [1/1] | Batch 331 | loss: 0.22745825350284576\n",
      "Epoch [1/1] | Batch 332 | loss: 0.22451502084732056\n",
      "Epoch [1/1] | Batch 333 | loss: 0.13974565267562866\n",
      "Epoch [1/1] | Batch 334 | loss: 0.3850253224372864\n",
      "Epoch [1/1] | Batch 335 | loss: 0.16975592076778412\n",
      "Epoch [1/1] | Batch 336 | loss: 0.1501198709011078\n",
      "Epoch [1/1] | Batch 337 | loss: 0.1651969999074936\n",
      "Epoch [1/1] | Batch 338 | loss: 0.15292800962924957\n",
      "Epoch [1/1] | Batch 339 | loss: 0.4232241213321686\n",
      "Epoch [1/1] | Batch 340 | loss: 0.14863504469394684\n",
      "Epoch [1/1] | Batch 341 | loss: 0.17335467040538788\n",
      "Epoch [1/1] | Batch 342 | loss: 0.35130518674850464\n",
      "Epoch [1/1] | Batch 343 | loss: 0.11788006871938705\n",
      "Epoch [1/1] | Batch 344 | loss: 0.1475900262594223\n",
      "Epoch [1/1] | Batch 345 | loss: 0.26947975158691406\n",
      "Epoch [1/1] | Batch 346 | loss: 0.14188046753406525\n",
      "Epoch [1/1] | Batch 347 | loss: 0.1720169335603714\n",
      "Epoch [1/1] | Batch 348 | loss: 0.35118240118026733\n",
      "Epoch [1/1] | Batch 349 | loss: 0.08091478794813156\n",
      "Epoch [1/1] | Batch 350 | loss: 0.20683810114860535\n",
      "Epoch [1/1] | Batch 351 | loss: 0.23524755239486694\n",
      "Epoch [1/1] | Batch 352 | loss: 0.1526544988155365\n",
      "Epoch [1/1] | Batch 353 | loss: 0.1649838387966156\n",
      "Epoch [1/1] | Batch 354 | loss: 0.3781573474407196\n",
      "Epoch [1/1] | Batch 355 | loss: 0.3023834526538849\n",
      "Epoch [1/1] | Batch 356 | loss: 0.13678868114948273\n",
      "Epoch [1/1] | Batch 357 | loss: 0.1751292198896408\n",
      "Epoch [1/1] | Batch 358 | loss: 0.33515262603759766\n",
      "Epoch [1/1] | Batch 359 | loss: 0.220203697681427\n",
      "Epoch [1/1] | Batch 360 | loss: 0.13168101012706757\n",
      "Epoch [1/1] | Batch 361 | loss: 0.5215217471122742\n",
      "Epoch [1/1] | Batch 362 | loss: 0.19189970195293427\n",
      "Epoch [1/1] | Batch 363 | loss: 0.26851382851600647\n",
      "Epoch [1/1] | Batch 364 | loss: 0.13893979787826538\n",
      "Epoch [1/1] | Batch 365 | loss: 0.259128600358963\n",
      "Epoch [1/1] | Batch 366 | loss: 0.1708069145679474\n",
      "Epoch [1/1] | Batch 367 | loss: 0.36257296800613403\n",
      "Epoch [1/1] | Batch 368 | loss: 0.15331609547138214\n",
      "Epoch [1/1] | Batch 369 | loss: 0.14658533036708832\n",
      "Epoch [1/1] | Batch 370 | loss: 0.18169158697128296\n",
      "Epoch [1/1] | Batch 371 | loss: 0.2412542998790741\n",
      "Epoch [1/1] | Batch 372 | loss: 0.2510961592197418\n",
      "Epoch [1/1] | Batch 373 | loss: 0.15822120010852814\n",
      "Epoch [1/1] | Batch 374 | loss: 0.16235260665416718\n",
      "Epoch [1/1] | Batch 375 | loss: 0.12709742784500122\n",
      "Epoch [1/1] | Batch 376 | loss: 0.14946386218070984\n",
      "Epoch [1/1] | Batch 377 | loss: 0.32638877630233765\n",
      "Epoch [1/1] | Batch 378 | loss: 0.2312142550945282\n",
      "Epoch [1/1] | Batch 379 | loss: 0.10351373255252838\n",
      "Epoch [1/1] | Batch 380 | loss: 0.2653007209300995\n",
      "Epoch [1/1] | Batch 381 | loss: 0.3152124881744385\n",
      "Epoch [1/1] | Batch 382 | loss: 0.18218983709812164\n",
      "Epoch [1/1] | Batch 383 | loss: 0.227877676486969\n",
      "Epoch [1/1] | Batch 384 | loss: 0.24440708756446838\n",
      "Epoch [1/1] | Batch 385 | loss: 0.3822828531265259\n",
      "Epoch [1/1] | Batch 386 | loss: 0.17180342972278595\n",
      "Epoch [1/1] | Batch 387 | loss: 0.21110552549362183\n",
      "Epoch [1/1] | Batch 388 | loss: 0.1881893128156662\n",
      "Epoch [1/1] | Batch 389 | loss: 0.18768073618412018\n",
      "Epoch [1/1] | Batch 390 | loss: 0.12883764505386353\n",
      "Epoch [1/1] | Batch 391 | loss: 0.09142417460680008\n",
      "Epoch [1/1] | Batch 392 | loss: 0.2521630823612213\n",
      "Epoch [1/1] | Batch 393 | loss: 0.21867860853672028\n",
      "Epoch [1/1] | Batch 394 | loss: 0.2612239420413971\n",
      "Epoch [1/1] | Batch 395 | loss: 0.13535848259925842\n",
      "Epoch [1/1] | Batch 396 | loss: 0.19741953909397125\n",
      "Epoch [1/1] | Batch 397 | loss: 0.15840765833854675\n",
      "Epoch [1/1] | Batch 398 | loss: 0.43790796399116516\n",
      "Epoch [1/1] | Batch 399 | loss: 0.266965389251709\n",
      "Epoch [1/1] | Batch 400 | loss: 0.23455952107906342\n",
      "Epoch [1/1] | Batch 401 | loss: 0.1625918596982956\n",
      "Epoch [1/1] | Batch 402 | loss: 0.19167903065681458\n",
      "Epoch [1/1] | Batch 403 | loss: 0.32803091406822205\n",
      "Epoch [1/1] | Batch 404 | loss: 0.258282870054245\n",
      "Epoch [1/1] | Batch 405 | loss: 0.20776836574077606\n",
      "Epoch [1/1] | Batch 406 | loss: 0.2992049753665924\n",
      "Epoch [1/1] | Batch 407 | loss: 0.2729441225528717\n",
      "Epoch [1/1] | Batch 408 | loss: 0.1732340008020401\n",
      "Epoch [1/1] | Batch 409 | loss: 0.19986136257648468\n",
      "Epoch [1/1] | Batch 410 | loss: 0.33084094524383545\n",
      "Epoch [1/1] | Batch 411 | loss: 0.2055487334728241\n",
      "Epoch [1/1] | Batch 412 | loss: 0.2506159245967865\n",
      "Epoch [1/1] | Batch 413 | loss: 0.13875794410705566\n",
      "Epoch [1/1] | Batch 414 | loss: 0.21849600970745087\n",
      "Epoch [1/1] | Batch 415 | loss: 0.1498955339193344\n",
      "Epoch [1/1] | Batch 416 | loss: 0.30467095971107483\n",
      "Epoch [1/1] | Batch 417 | loss: 0.3804720938205719\n",
      "Epoch [1/1] | Batch 418 | loss: 0.29625070095062256\n",
      "Epoch [1/1] | Batch 419 | loss: 0.14463520050048828\n",
      "Epoch [1/1] | Batch 420 | loss: 0.21083392202854156\n",
      "Epoch [1/1] | Batch 421 | loss: 0.35273033380508423\n",
      "Epoch [1/1] | Batch 422 | loss: 0.1510644108057022\n",
      "Epoch [1/1] | Batch 423 | loss: 0.11175142973661423\n",
      "Epoch [1/1] | Batch 424 | loss: 0.10640393197536469\n",
      "Epoch [1/1] | Batch 425 | loss: 0.20311015844345093\n",
      "Epoch [1/1] | Batch 426 | loss: 0.381471186876297\n",
      "Epoch [1/1] | Batch 427 | loss: 0.18588095903396606\n",
      "Epoch [1/1] | Batch 428 | loss: 0.26850220561027527\n",
      "Epoch [1/1] | Batch 429 | loss: 0.24366681277751923\n",
      "Epoch [1/1] | Batch 430 | loss: 0.19085028767585754\n",
      "Epoch [1/1] | Batch 431 | loss: 0.4380076825618744\n",
      "Epoch [1/1] | Batch 432 | loss: 0.3082675337791443\n",
      "Epoch [1/1] | Batch 433 | loss: 0.11313004791736603\n",
      "Epoch [1/1] | Batch 434 | loss: 0.35602134466171265\n",
      "Epoch [1/1] | Batch 435 | loss: 0.20325864851474762\n",
      "Epoch [1/1] | Batch 436 | loss: 0.3023557662963867\n",
      "Epoch [1/1] | Batch 437 | loss: 0.12118818610906601\n",
      "Epoch [1/1] | Batch 438 | loss: 0.3764384090900421\n",
      "Epoch [1/1] | Batch 439 | loss: 0.2992079257965088\n",
      "Epoch [1/1] | Batch 440 | loss: 0.10130812972784042\n",
      "Epoch [1/1] | Batch 441 | loss: 0.1580086052417755\n",
      "Epoch [1/1] | Batch 442 | loss: 0.4109881818294525\n",
      "Epoch [1/1] | Batch 443 | loss: 0.3142971694469452\n",
      "Epoch [1/1] | Batch 444 | loss: 0.14502395689487457\n",
      "Epoch [1/1] | Batch 445 | loss: 0.27705106139183044\n",
      "Epoch [1/1] | Batch 446 | loss: 0.187669575214386\n",
      "Epoch [1/1] | Batch 447 | loss: 0.37684816122055054\n",
      "Epoch [1/1] | Batch 448 | loss: 0.2861889600753784\n",
      "Epoch [1/1] | Batch 449 | loss: 0.22804920375347137\n",
      "Epoch [1/1] | Batch 450 | loss: 0.18178221583366394\n",
      "Epoch [1/1] | Batch 451 | loss: 0.23458316922187805\n",
      "Epoch [1/1] | Batch 452 | loss: 0.1657380908727646\n",
      "Epoch [1/1] | Batch 453 | loss: 0.1892821192741394\n",
      "Epoch [1/1] | Batch 454 | loss: 0.14743521809577942\n",
      "Epoch [1/1] | Batch 455 | loss: 0.40714994072914124\n",
      "Epoch [1/1] | Batch 456 | loss: 0.16712000966072083\n",
      "Epoch [1/1] | Batch 457 | loss: 0.13084177672863007\n",
      "Epoch [1/1] | Batch 458 | loss: 0.2715749144554138\n",
      "Epoch [1/1] | Batch 459 | loss: 0.12737977504730225\n",
      "Epoch [1/1] | Batch 460 | loss: 0.1958228200674057\n",
      "Epoch [1/1] | Batch 461 | loss: 0.26108744740486145\n",
      "Epoch [1/1] | Batch 462 | loss: 0.1888938844203949\n",
      "Epoch [1/1] | Batch 463 | loss: 0.2402542680501938\n",
      "Epoch [1/1] | Batch 464 | loss: 0.14501875638961792\n",
      "Epoch [1/1] | Batch 465 | loss: 0.3601958453655243\n",
      "Epoch [1/1] | Batch 466 | loss: 0.17022809386253357\n",
      "Epoch [1/1] | Batch 467 | loss: 0.1449843794107437\n",
      "Epoch [1/1] | Batch 468 | loss: 0.3774867355823517\n",
      "Epoch [1/1] | Batch 469 | loss: 0.3338330388069153\n",
      "Epoch [1/1] | Batch 470 | loss: 0.10578782856464386\n",
      "Epoch [1/1] | Batch 471 | loss: 0.17102742195129395\n",
      "Epoch [1/1] | Batch 472 | loss: 0.1785784363746643\n",
      "Epoch [1/1] | Batch 473 | loss: 0.16923275589942932\n",
      "Epoch [1/1] | Batch 474 | loss: 0.21767085790634155\n",
      "Epoch [1/1] | Batch 475 | loss: 0.147835835814476\n",
      "Epoch [1/1] | Batch 476 | loss: 0.24010597169399261\n",
      "Epoch [1/1] | Batch 477 | loss: 0.11155085265636444\n",
      "Epoch [1/1] | Batch 478 | loss: 0.26622021198272705\n",
      "Epoch [1/1] | Batch 479 | loss: 0.08391204476356506\n",
      "Epoch [1/1] | Batch 480 | loss: 0.1805441677570343\n",
      "Epoch [1/1] | Batch 481 | loss: 0.24152402579784393\n",
      "Epoch [1/1] | Batch 482 | loss: 0.26568108797073364\n",
      "Epoch [1/1] | Batch 483 | loss: 0.13241127133369446\n",
      "Epoch [1/1] | Batch 484 | loss: 0.3194303512573242\n",
      "Epoch [1/1] | Batch 485 | loss: 0.6927952766418457\n",
      "Epoch [1/1] | Batch 486 | loss: 0.158152773976326\n",
      "Epoch [1/1] | Batch 487 | loss: 0.0894244834780693\n",
      "Epoch [1/1] | Batch 488 | loss: 0.0794316977262497\n",
      "Epoch [1/1] | Batch 489 | loss: 0.2961883544921875\n",
      "Epoch [1/1] | Batch 490 | loss: 0.2098640501499176\n",
      "Epoch [1/1] | Batch 491 | loss: 0.23717647790908813\n",
      "Epoch [1/1] | Batch 492 | loss: 0.2579839527606964\n",
      "Epoch [1/1] | Batch 493 | loss: 0.18546703457832336\n",
      "Epoch [1/1] | Batch 494 | loss: 0.23115450143814087\n",
      "Epoch [1/1] | Batch 495 | loss: 0.41004467010498047\n",
      "Epoch [1/1] | Batch 496 | loss: 0.2218250036239624\n",
      "Epoch [1/1] | Batch 497 | loss: 0.1620645523071289\n",
      "Epoch [1/1] | Batch 498 | loss: 0.2872120440006256\n",
      "Epoch [1/1] | Batch 499 | loss: 0.15717671811580658\n",
      "Epoch [1/1] | Batch 500 | loss: 0.086675725877285\n",
      "Epoch [1/1] | Batch 501 | loss: 0.18529383838176727\n",
      "Epoch [1/1] | Batch 502 | loss: 0.08018333464860916\n",
      "Epoch [1/1] | Batch 503 | loss: 0.17608098685741425\n",
      "Epoch [1/1] | Batch 504 | loss: 0.19131124019622803\n",
      "Epoch [1/1] | Batch 505 | loss: 0.35990744829177856\n",
      "Epoch [1/1] | Batch 506 | loss: 0.10013420879840851\n",
      "Epoch [1/1] | Batch 507 | loss: 0.24414731562137604\n",
      "Epoch [1/1] | Batch 508 | loss: 0.2534841299057007\n",
      "Epoch [1/1] | Batch 509 | loss: 0.2743479311466217\n",
      "Epoch [1/1] | Batch 510 | loss: 0.17925475537776947\n",
      "Epoch [1/1] | Batch 511 | loss: 0.1646878719329834\n",
      "Epoch [1/1] | Batch 512 | loss: 0.2083459049463272\n",
      "Epoch [1/1] | Batch 513 | loss: 0.3272492289543152\n",
      "Epoch [1/1] | Batch 514 | loss: 0.2349400371313095\n",
      "Epoch [1/1] | Batch 515 | loss: 0.09260569512844086\n",
      "Epoch [1/1] | Batch 516 | loss: 0.12973837554454803\n",
      "Epoch [1/1] | Batch 517 | loss: 0.14715547859668732\n",
      "Epoch [1/1] | Batch 518 | loss: 0.15990835428237915\n",
      "Epoch [1/1] | Batch 519 | loss: 0.22780704498291016\n",
      "Epoch [1/1] | Batch 520 | loss: 0.2546842694282532\n",
      "Epoch [1/1] | Batch 521 | loss: 0.30128517746925354\n",
      "Epoch [1/1] | Batch 522 | loss: 0.1809777468442917\n",
      "Epoch [1/1] | Batch 523 | loss: 0.33841821551322937\n",
      "Epoch [1/1] | Batch 524 | loss: 0.14417411386966705\n",
      "Epoch [1/1] | Batch 525 | loss: 0.28181853890419006\n",
      "Epoch [1/1] | Batch 526 | loss: 0.15816974639892578\n",
      "Epoch [1/1] | Batch 527 | loss: 0.10893316566944122\n",
      "Epoch [1/1] | Batch 528 | loss: 0.29173046350479126\n",
      "Epoch [1/1] | Batch 529 | loss: 0.31046855449676514\n",
      "Epoch [1/1] | Batch 530 | loss: 0.10296788066625595\n",
      "Epoch [1/1] | Batch 531 | loss: 0.2048274427652359\n",
      "Epoch [1/1] | Batch 532 | loss: 0.2155754566192627\n",
      "Epoch [1/1] | Batch 533 | loss: 0.3322751522064209\n",
      "Epoch [1/1] | Batch 534 | loss: 0.11430498957633972\n",
      "Epoch [1/1] | Batch 535 | loss: 0.38123854994773865\n",
      "Epoch [1/1] | Batch 536 | loss: 0.12669886648654938\n",
      "Epoch [1/1] | Batch 537 | loss: 0.39113345742225647\n",
      "Epoch [1/1] | Batch 538 | loss: 0.3187335133552551\n",
      "Epoch [1/1] | Batch 539 | loss: 0.13498105108737946\n",
      "Epoch [1/1] | Batch 540 | loss: 0.10278785973787308\n",
      "Epoch [1/1] | Batch 541 | loss: 0.07275612652301788\n",
      "Epoch [1/1] | Batch 542 | loss: 0.18332131206989288\n",
      "Epoch [1/1] | Batch 543 | loss: 0.21080145239830017\n",
      "Epoch [1/1] | Batch 544 | loss: 0.3536507189273834\n",
      "Epoch [1/1] | Batch 545 | loss: 0.11717545986175537\n",
      "Epoch [1/1] | Batch 546 | loss: 0.11983132362365723\n",
      "Epoch [1/1] | Batch 547 | loss: 0.30709022283554077\n",
      "Epoch [1/1] | Batch 548 | loss: 0.2405192255973816\n",
      "Epoch [1/1] | Batch 549 | loss: 0.12678615748882294\n",
      "Epoch [1/1] | Batch 550 | loss: 0.12603697180747986\n",
      "Epoch [1/1] | Batch 551 | loss: 0.28938645124435425\n",
      "Epoch [1/1] | Batch 552 | loss: 0.18158186972141266\n",
      "Epoch [1/1] | Batch 553 | loss: 0.08242380619049072\n",
      "Epoch [1/1] | Batch 554 | loss: 0.12679670751094818\n",
      "Epoch [1/1] | Batch 555 | loss: 0.10547935962677002\n",
      "Epoch [1/1] | Batch 556 | loss: 0.3520567715167999\n",
      "Epoch [1/1] | Batch 557 | loss: 0.29172155261039734\n",
      "Epoch [1/1] | Batch 558 | loss: 0.08333025872707367\n",
      "Epoch [1/1] | Batch 559 | loss: 0.15709342062473297\n",
      "Epoch [1/1] | Batch 560 | loss: 0.2161324918270111\n",
      "Epoch [1/1] | Batch 561 | loss: 0.11266057193279266\n",
      "Epoch [1/1] | Batch 562 | loss: 0.19336476922035217\n",
      "Epoch [1/1] | Batch 563 | loss: 0.34384432435035706\n",
      "Epoch [1/1] | Batch 564 | loss: 0.0999152809381485\n",
      "Epoch [1/1] | Batch 565 | loss: 0.13018327951431274\n",
      "Epoch [1/1] | Batch 566 | loss: 0.14412569999694824\n",
      "Epoch [1/1] | Batch 567 | loss: 0.12405617535114288\n",
      "Epoch [1/1] | Batch 568 | loss: 0.2551422417163849\n",
      "Epoch [1/1] | Batch 569 | loss: 0.09739480167627335\n",
      "Epoch [1/1] | Batch 570 | loss: 0.09481485933065414\n",
      "Epoch [1/1] | Batch 571 | loss: 0.09610763937234879\n",
      "Epoch [1/1] | Batch 572 | loss: 0.14200644195079803\n",
      "Epoch [1/1] | Batch 573 | loss: 0.13958171010017395\n",
      "Epoch [1/1] | Batch 574 | loss: 0.26379722356796265\n",
      "Epoch [1/1] | Batch 575 | loss: 0.14078545570373535\n",
      "Epoch [1/1] | Batch 576 | loss: 0.32567471265792847\n",
      "Epoch [1/1] | Batch 577 | loss: 0.03500561788678169\n",
      "Epoch [1/1] | Batch 578 | loss: 0.23732158541679382\n",
      "Epoch [1/1] | Batch 579 | loss: 0.24788886308670044\n",
      "Epoch [1/1] | Batch 580 | loss: 0.13424798846244812\n",
      "Epoch [1/1] | Batch 581 | loss: 0.23752735555171967\n",
      "Epoch [1/1] | Batch 582 | loss: 0.1019994243979454\n",
      "Epoch [1/1] | Batch 583 | loss: 0.22785073518753052\n",
      "Epoch [1/1] | Batch 584 | loss: 0.17688022553920746\n",
      "Epoch [1/1] | Batch 585 | loss: 0.1840931624174118\n",
      "Epoch [1/1] | Batch 586 | loss: 0.21425588428974152\n",
      "Epoch [1/1] | Batch 587 | loss: 0.12991110980510712\n",
      "Epoch [1/1] | Batch 588 | loss: 0.23888304829597473\n",
      "Epoch [1/1] | Batch 589 | loss: 0.24587923288345337\n",
      "Epoch [1/1] | Batch 590 | loss: 0.17636823654174805\n",
      "Epoch [1/1] | Batch 591 | loss: 0.1475762128829956\n",
      "Epoch [1/1] | Batch 592 | loss: 0.2166396528482437\n",
      "Epoch [1/1] | Batch 593 | loss: 0.14935696125030518\n",
      "Epoch [1/1] | Batch 594 | loss: 0.2260339856147766\n",
      "Epoch [1/1] | Batch 595 | loss: 0.22544774413108826\n",
      "Epoch [1/1] | Batch 596 | loss: 0.17360644042491913\n",
      "Epoch [1/1] | Batch 597 | loss: 0.1603403091430664\n",
      "Epoch [1/1] | Batch 598 | loss: 0.1670847237110138\n",
      "Epoch [1/1] | Batch 599 | loss: 0.10416248440742493\n",
      "Epoch [1/1] | Batch 600 | loss: 0.2887878715991974\n",
      "Epoch [1/1] | Batch 601 | loss: 0.17529141902923584\n",
      "Epoch [1/1] | Batch 602 | loss: 0.25678983330726624\n",
      "Epoch [1/1] | Batch 603 | loss: 0.1732243448495865\n",
      "Epoch [1/1] | Batch 604 | loss: 0.1311560720205307\n",
      "Epoch [1/1] | Batch 605 | loss: 0.293285608291626\n",
      "Epoch [1/1] | Batch 606 | loss: 0.04074869677424431\n",
      "Epoch [1/1] | Batch 607 | loss: 0.23173606395721436\n",
      "Epoch [1/1] | Batch 608 | loss: 0.1695265918970108\n",
      "Epoch [1/1] | Batch 609 | loss: 0.184546560049057\n",
      "Epoch [1/1] | Batch 610 | loss: 0.22381827235221863\n",
      "Epoch [1/1] | Batch 611 | loss: 0.23701341450214386\n",
      "Epoch [1/1] | Batch 612 | loss: 0.2099795937538147\n",
      "Epoch [1/1] | Batch 613 | loss: 0.08064961433410645\n",
      "Epoch [1/1] | Batch 614 | loss: 0.1911219358444214\n",
      "Epoch [1/1] | Batch 615 | loss: 0.160532146692276\n",
      "Epoch [1/1] | Batch 616 | loss: 0.20909398794174194\n",
      "Epoch [1/1] | Batch 617 | loss: 0.13985921442508698\n",
      "Epoch [1/1] | Batch 618 | loss: 0.03614654019474983\n",
      "Epoch [1/1] | Batch 619 | loss: 0.05963203310966492\n",
      "Epoch [1/1] | Batch 620 | loss: 0.23728373646736145\n",
      "Epoch [1/1] | Batch 621 | loss: 0.17444010078907013\n",
      "Epoch [1/1] | Batch 622 | loss: 0.12059549987316132\n",
      "Epoch [1/1] | Batch 623 | loss: 0.1998704969882965\n",
      "Epoch [1/1] | Batch 624 | loss: 0.15282844007015228\n",
      "Epoch [1/1] | Batch 625 | loss: 0.20000006258487701\n",
      "Epoch [1/1] | Batch 626 | loss: 0.20479810237884521\n",
      "Epoch [1/1] | Batch 627 | loss: 0.20376645028591156\n",
      "Epoch [1/1] | Batch 628 | loss: 0.20058396458625793\n",
      "Epoch [1/1] | Batch 629 | loss: 0.21221637725830078\n",
      "Epoch [1/1] | Batch 630 | loss: 0.11481745541095734\n",
      "Epoch [1/1] | Batch 631 | loss: 0.16202442348003387\n",
      "Epoch [1/1] | Batch 632 | loss: 0.20762161910533905\n",
      "Epoch [1/1] | Batch 633 | loss: 0.18349163234233856\n",
      "Epoch [1/1] | Batch 634 | loss: 0.06536967307329178\n",
      "Epoch [1/1] | Batch 635 | loss: 0.1662573218345642\n",
      "Epoch [1/1] | Batch 636 | loss: 0.10977482795715332\n",
      "Epoch [1/1] | Batch 637 | loss: 0.15704293549060822\n",
      "Epoch [1/1] | Batch 638 | loss: 0.22759059071540833\n",
      "Epoch [1/1] | Batch 639 | loss: 0.09143110364675522\n",
      "Epoch [1/1] | Batch 640 | loss: 0.27427566051483154\n",
      "Epoch [1/1] | Batch 641 | loss: 0.18695753812789917\n",
      "Epoch [1/1] | Batch 642 | loss: 0.2994166910648346\n",
      "Epoch [1/1] | Batch 643 | loss: 0.08689266443252563\n",
      "Epoch [1/1] | Batch 644 | loss: 0.15036281943321228\n",
      "Epoch [1/1] | Batch 645 | loss: 0.1814609169960022\n",
      "Epoch [1/1] | Batch 646 | loss: 0.15354609489440918\n",
      "Epoch [1/1] | Batch 647 | loss: 0.13493967056274414\n",
      "Epoch [1/1] | Batch 648 | loss: 0.2043994665145874\n",
      "Epoch [1/1] | Batch 649 | loss: 0.22875642776489258\n",
      "Epoch [1/1] | Batch 650 | loss: 0.07331699877977371\n",
      "Epoch [1/1] | Batch 651 | loss: 0.19584855437278748\n",
      "Epoch [1/1] | Batch 652 | loss: 0.31924447417259216\n",
      "Epoch [1/1] | Batch 653 | loss: 0.07864987850189209\n",
      "Epoch [1/1] | Batch 654 | loss: 0.04482371732592583\n",
      "Epoch [1/1] | Batch 655 | loss: 0.12241432070732117\n",
      "Epoch [1/1] | Batch 656 | loss: 0.16579575836658478\n",
      "Epoch [1/1] | Batch 657 | loss: 0.39620208740234375\n",
      "Epoch [1/1] | Batch 658 | loss: 0.13077594339847565\n",
      "Epoch [1/1] | Batch 659 | loss: 0.24822446703910828\n",
      "Epoch [1/1] | Batch 660 | loss: 0.06333184242248535\n",
      "Epoch [1/1] | Batch 661 | loss: 0.14558285474777222\n",
      "Epoch [1/1] | Batch 662 | loss: 0.2337828427553177\n",
      "Epoch [1/1] | Batch 663 | loss: 0.10541819036006927\n",
      "Epoch [1/1] | Batch 664 | loss: 0.22252291440963745\n",
      "Epoch [1/1] | Batch 665 | loss: 0.28047630190849304\n",
      "Epoch [1/1] | Batch 666 | loss: 0.3321245014667511\n",
      "Epoch [1/1] | Batch 667 | loss: 0.25504979491233826\n",
      "Epoch [1/1] | Batch 668 | loss: 0.2536360025405884\n",
      "Epoch [1/1] | Batch 669 | loss: 0.0818619430065155\n",
      "Epoch [1/1] | Batch 670 | loss: 0.1459256410598755\n",
      "Epoch [1/1] | Batch 671 | loss: 0.2623807191848755\n",
      "Epoch [1/1] | Batch 672 | loss: 0.14841529726982117\n",
      "Epoch [1/1] | Batch 673 | loss: 0.26976364850997925\n",
      "Epoch [1/1] | Batch 674 | loss: 0.20205816626548767\n",
      "Epoch [1/1] | Batch 675 | loss: 0.24336089193820953\n",
      "Epoch [1/1] | Batch 676 | loss: 0.117072694003582\n",
      "Epoch [1/1] | Batch 677 | loss: 0.21034502983093262\n",
      "Epoch [1/1] | Batch 678 | loss: 0.08330158144235611\n",
      "Epoch [1/1] | Batch 679 | loss: 0.1396867036819458\n",
      "Epoch [1/1] | Batch 680 | loss: 0.16121387481689453\n",
      "Epoch [1/1] | Batch 681 | loss: 0.0808805376291275\n",
      "Epoch [1/1] | Batch 682 | loss: 0.13294143974781036\n",
      "Epoch [1/1] | Batch 683 | loss: 0.1018061488866806\n",
      "Epoch [1/1] | Batch 684 | loss: 0.1507265418767929\n",
      "Epoch [1/1] | Batch 685 | loss: 0.12225136905908585\n",
      "Epoch [1/1] | Batch 686 | loss: 0.26664629578590393\n",
      "Epoch [1/1] | Batch 687 | loss: 0.17345979809761047\n",
      "Epoch [1/1] | Batch 688 | loss: 0.18119613826274872\n",
      "Epoch [1/1] | Batch 689 | loss: 0.10135230422019958\n",
      "Epoch [1/1] | Batch 690 | loss: 0.13684538006782532\n",
      "Epoch [1/1] | Batch 691 | loss: 0.1561693549156189\n",
      "Epoch [1/1] | Batch 692 | loss: 0.1526496857404709\n",
      "Epoch [1/1] | Batch 693 | loss: 0.12427932769060135\n",
      "Epoch [1/1] | Batch 694 | loss: 0.24791200459003448\n",
      "Epoch [1/1] | Batch 695 | loss: 0.18707992136478424\n",
      "Epoch [1/1] | Batch 696 | loss: 0.32661592960357666\n",
      "Epoch [1/1] | Batch 697 | loss: 0.21332241594791412\n",
      "Epoch [1/1] | Batch 698 | loss: 0.2689145803451538\n",
      "Epoch [1/1] | Batch 699 | loss: 0.2653350532054901\n",
      "Epoch [1/1] | Batch 700 | loss: 0.07395095378160477\n",
      "Epoch [1/1] | Batch 701 | loss: 0.08153177052736282\n",
      "Epoch [1/1] | Batch 702 | loss: 0.13199880719184875\n",
      "Epoch [1/1] | Batch 703 | loss: 0.06109147518873215\n",
      "Epoch [1/1] | Batch 704 | loss: 0.17806486785411835\n",
      "Epoch [1/1] | Batch 705 | loss: 0.32132357358932495\n",
      "Epoch [1/1] | Batch 706 | loss: 0.17635206878185272\n",
      "Epoch [1/1] | Batch 707 | loss: 0.32198140025138855\n",
      "Epoch [1/1] | Batch 708 | loss: 0.21062692999839783\n",
      "Epoch [1/1] | Batch 709 | loss: 0.16485178470611572\n",
      "Epoch [1/1] | Batch 710 | loss: 0.2148972600698471\n",
      "Epoch [1/1] | Batch 711 | loss: 0.3893921971321106\n",
      "Epoch [1/1] | Batch 712 | loss: 0.17660051584243774\n",
      "Epoch [1/1] | Batch 713 | loss: 0.31998470425605774\n",
      "Epoch [1/1] | Batch 714 | loss: 0.13562606275081635\n",
      "Epoch [1/1] | Batch 715 | loss: 0.06495960801839828\n",
      "Epoch [1/1] | Batch 716 | loss: 0.11078359931707382\n",
      "Epoch [1/1] | Batch 717 | loss: 0.06193727254867554\n",
      "Epoch [1/1] | Batch 718 | loss: 0.12756280601024628\n",
      "Epoch [1/1] | Batch 719 | loss: 0.4484579861164093\n",
      "Epoch [1/1] | Batch 720 | loss: 0.18105252087116241\n",
      "Epoch [1/1] | Batch 721 | loss: 0.11262842267751694\n",
      "Epoch [1/1] | Batch 722 | loss: 0.13504964113235474\n",
      "Epoch [1/1] | Batch 723 | loss: 0.11749991029500961\n",
      "Epoch [1/1] | Batch 724 | loss: 0.12720227241516113\n",
      "Epoch [1/1] | Batch 725 | loss: 0.35741138458251953\n",
      "Epoch [1/1] | Batch 726 | loss: 0.09650640189647675\n",
      "Epoch [1/1] | Batch 727 | loss: 0.17544594407081604\n",
      "Epoch [1/1] | Batch 728 | loss: 0.09067995101213455\n",
      "Epoch [1/1] | Batch 729 | loss: 0.22493848204612732\n",
      "Epoch [1/1] | Batch 730 | loss: 0.1270914375782013\n",
      "Epoch [1/1] | Batch 731 | loss: 0.07831599563360214\n",
      "Epoch [1/1] | Batch 732 | loss: 0.14777450263500214\n",
      "Epoch [1/1] | Batch 733 | loss: 0.11540494114160538\n",
      "Epoch [1/1] | Batch 734 | loss: 0.1652669906616211\n",
      "Epoch [1/1] | Batch 735 | loss: 0.174046128988266\n",
      "Epoch [1/1] | Batch 736 | loss: 0.14957693219184875\n",
      "Epoch [1/1] | Batch 737 | loss: 0.308456689119339\n",
      "Epoch [1/1] | Batch 738 | loss: 0.24235442280769348\n",
      "Epoch [1/1] | Batch 739 | loss: 0.10200610011816025\n",
      "Epoch [1/1] | Batch 740 | loss: 0.19332900643348694\n",
      "Epoch [1/1] | Batch 741 | loss: 0.19825391471385956\n",
      "Epoch [1/1] | Batch 742 | loss: 0.2733127176761627\n",
      "Epoch [1/1] | Batch 743 | loss: 0.12927411496639252\n",
      "Epoch [1/1] | Batch 744 | loss: 0.29949864745140076\n",
      "Epoch [1/1] | Batch 745 | loss: 0.15898245573043823\n",
      "Epoch [1/1] | Batch 746 | loss: 0.18499086797237396\n",
      "Epoch [1/1] | Batch 747 | loss: 0.17222875356674194\n",
      "Epoch [1/1] | Batch 748 | loss: 0.16573046147823334\n",
      "Epoch [1/1] | Batch 749 | loss: 0.1800607293844223\n",
      "Epoch [1/1] | Batch 750 | loss: 0.14962542057037354\n",
      "Epoch [1/1] | Batch 751 | loss: 0.11152268946170807\n",
      "Epoch [1/1] | Batch 752 | loss: 0.2972305119037628\n",
      "Epoch [1/1] | Batch 753 | loss: 0.09668388217687607\n",
      "Epoch [1/1] | Batch 754 | loss: 0.2243521511554718\n",
      "Epoch [1/1] | Batch 755 | loss: 0.10294170677661896\n",
      "Epoch [1/1] | Batch 756 | loss: 0.0880773514509201\n",
      "Epoch [1/1] | Batch 757 | loss: 0.13512353599071503\n",
      "Epoch [1/1] | Batch 758 | loss: 0.32059445977211\n",
      "Epoch [1/1] | Batch 759 | loss: 0.06872384250164032\n",
      "Epoch [1/1] | Batch 760 | loss: 0.245191752910614\n",
      "Epoch [1/1] | Batch 761 | loss: 0.05599886551499367\n",
      "Epoch [1/1] | Batch 762 | loss: 0.23664572834968567\n",
      "Epoch [1/1] | Batch 763 | loss: 0.1092730313539505\n",
      "Epoch [1/1] | Batch 764 | loss: 0.16760532557964325\n",
      "Epoch [1/1] | Batch 765 | loss: 0.10993026196956635\n",
      "Epoch [1/1] | Batch 766 | loss: 0.26376914978027344\n",
      "Epoch [1/1] | Batch 767 | loss: 0.26486289501190186\n",
      "Epoch [1/1] | Batch 768 | loss: 0.1664278507232666\n",
      "Epoch [1/1] | Batch 769 | loss: 0.17126935720443726\n",
      "Epoch [1/1] | Batch 770 | loss: 0.21451464295387268\n",
      "Epoch [1/1] | Batch 771 | loss: 0.06370542198419571\n",
      "Epoch [1/1] | Batch 772 | loss: 0.05275613069534302\n",
      "Epoch [1/1] | Batch 773 | loss: 0.15570972859859467\n",
      "Epoch [1/1] | Batch 774 | loss: 0.14104923605918884\n",
      "Epoch [1/1] | Batch 775 | loss: 0.1497320979833603\n",
      "Epoch [1/1] | Batch 776 | loss: 0.08167954534292221\n",
      "Epoch [1/1] | Batch 777 | loss: 0.2564915418624878\n",
      "Epoch [1/1] | Batch 778 | loss: 0.228462815284729\n",
      "Epoch [1/1] | Batch 779 | loss: 0.17524640262126923\n",
      "Epoch [1/1] | Batch 780 | loss: 0.03758176416158676\n",
      "Epoch [1/1] | Batch 781 | loss: 0.06262243539094925\n",
      "Epoch [1/1] | Batch 782 | loss: 0.0695590153336525\n",
      "Epoch [1/1] | Batch 783 | loss: 0.08441656827926636\n",
      "Epoch [1/1] | Batch 784 | loss: 0.092213936150074\n",
      "Epoch [1/1] | Batch 785 | loss: 0.13241395354270935\n",
      "Epoch [1/1] | Batch 786 | loss: 0.12651318311691284\n",
      "Epoch [1/1] | Batch 787 | loss: 0.31323254108428955\n",
      "Epoch [1/1] | Batch 788 | loss: 0.17058826982975006\n",
      "Epoch [1/1] | Batch 789 | loss: 0.07919131219387054\n",
      "Epoch [1/1] | Batch 790 | loss: 0.14717864990234375\n",
      "Epoch [1/1] | Batch 791 | loss: 0.38792869448661804\n",
      "Epoch [1/1] | Batch 792 | loss: 0.11860883980989456\n",
      "Epoch [1/1] | Batch 793 | loss: 0.04808379337191582\n",
      "Epoch [1/1] | Batch 794 | loss: 0.14776967465877533\n",
      "Epoch [1/1] | Batch 795 | loss: 0.24019813537597656\n",
      "Epoch [1/1] | Batch 796 | loss: 0.1364075094461441\n",
      "Epoch [1/1] | Batch 797 | loss: 0.08992195874452591\n",
      "Epoch [1/1] | Batch 798 | loss: 0.13322174549102783\n",
      "Epoch [1/1] | Batch 799 | loss: 0.34265419840812683\n",
      "Epoch [1/1] | Batch 800 | loss: 0.09083552658557892\n",
      "Epoch [1/1] | Batch 801 | loss: 0.11173684895038605\n",
      "Epoch [1/1] | Batch 802 | loss: 0.24111202359199524\n",
      "Epoch [1/1] | Batch 803 | loss: 0.30152004957199097\n",
      "Epoch [1/1] | Batch 804 | loss: 0.14059168100357056\n",
      "Epoch [1/1] | Batch 805 | loss: 0.10359766334295273\n",
      "Epoch [1/1] | Batch 806 | loss: 0.19508817791938782\n",
      "Epoch [1/1] | Batch 807 | loss: 0.13540159165859222\n",
      "Epoch [1/1] | Batch 808 | loss: 0.1299816519021988\n",
      "Epoch [1/1] | Batch 809 | loss: 0.3078802227973938\n",
      "Epoch [1/1] | Batch 810 | loss: 0.11208518594503403\n",
      "Epoch [1/1] | Batch 811 | loss: 0.2494174689054489\n",
      "Epoch [1/1] | Batch 812 | loss: 0.14783307909965515\n",
      "Epoch [1/1] | Batch 813 | loss: 0.16396623849868774\n",
      "Epoch [1/1] | Batch 814 | loss: 0.13361379504203796\n",
      "Epoch [1/1] | Batch 815 | loss: 0.19382059574127197\n",
      "Epoch [1/1] | Batch 816 | loss: 0.09913583099842072\n",
      "Epoch [1/1] | Batch 817 | loss: 0.22362062335014343\n",
      "Epoch [1/1] | Batch 818 | loss: 0.19454096257686615\n",
      "Epoch [1/1] | Batch 819 | loss: 0.16247428953647614\n",
      "Epoch [1/1] | Batch 820 | loss: 0.14148607850074768\n",
      "Epoch [1/1] | Batch 821 | loss: 0.06694658100605011\n",
      "Epoch [1/1] | Batch 822 | loss: 0.04713151231408119\n",
      "Epoch [1/1] | Batch 823 | loss: 0.25335294008255005\n",
      "Epoch [1/1] | Batch 824 | loss: 0.22745028138160706\n",
      "Epoch [1/1] | Batch 825 | loss: 0.10211633145809174\n",
      "Epoch [1/1] | Batch 826 | loss: 0.05593787506222725\n",
      "Epoch [1/1] | Batch 827 | loss: 0.08676399290561676\n",
      "Epoch [1/1] | Batch 828 | loss: 0.1890595555305481\n",
      "Epoch [1/1] | Batch 829 | loss: 0.12550219893455505\n",
      "Epoch [1/1] | Batch 830 | loss: 0.0690189078450203\n",
      "Epoch [1/1] | Batch 831 | loss: 0.18122519552707672\n",
      "Epoch [1/1] | Batch 832 | loss: 0.11120226234197617\n",
      "Epoch [1/1] | Batch 833 | loss: 0.2990839183330536\n",
      "Epoch [1/1] | Batch 834 | loss: 0.09599527716636658\n",
      "Epoch [1/1] | Batch 835 | loss: 0.1280573308467865\n",
      "Epoch [1/1] | Batch 836 | loss: 0.11968068778514862\n",
      "Epoch [1/1] | Batch 837 | loss: 0.21107074618339539\n",
      "Epoch [1/1] | Batch 838 | loss: 0.07880545407533646\n",
      "Epoch [1/1] | Batch 839 | loss: 0.17894543707370758\n",
      "Epoch [1/1] | Batch 840 | loss: 0.11918752640485764\n",
      "Epoch [1/1] | Batch 841 | loss: 0.08186209201812744\n",
      "Epoch [1/1] | Batch 842 | loss: 0.2601378560066223\n",
      "Epoch [1/1] | Batch 843 | loss: 0.06085088849067688\n",
      "Epoch [1/1] | Batch 844 | loss: 0.25491610169410706\n",
      "Epoch [1/1] | Batch 845 | loss: 0.11534161120653152\n",
      "Epoch [1/1] | Batch 846 | loss: 0.023392273113131523\n",
      "Epoch [1/1] | Batch 847 | loss: 0.18174387514591217\n",
      "Epoch [1/1] | Batch 848 | loss: 0.13283349573612213\n",
      "Epoch [1/1] | Batch 849 | loss: 0.11698571592569351\n",
      "Epoch [1/1] | Batch 850 | loss: 0.1314728558063507\n",
      "Epoch [1/1] | Batch 851 | loss: 0.09239564836025238\n",
      "Epoch [1/1] | Batch 852 | loss: 0.19278396666049957\n",
      "Epoch [1/1] | Batch 853 | loss: 0.22687171399593353\n",
      "Epoch [1/1] | Batch 854 | loss: 0.08632487803697586\n",
      "Epoch [1/1] | Batch 855 | loss: 0.12238730490207672\n",
      "Epoch [1/1] | Batch 856 | loss: 0.12559658288955688\n",
      "Epoch [1/1] | Batch 857 | loss: 0.21008172631263733\n",
      "Epoch [1/1] | Batch 858 | loss: 0.21646630764007568\n",
      "Epoch [1/1] | Batch 859 | loss: 0.24514205753803253\n",
      "Epoch [1/1] | Batch 860 | loss: 0.14544238150119781\n",
      "Epoch [1/1] | Batch 861 | loss: 0.1721663773059845\n",
      "Epoch [1/1] | Batch 862 | loss: 0.17493031919002533\n",
      "Epoch [1/1] | Batch 863 | loss: 0.2876283526420593\n",
      "Epoch [1/1] | Batch 864 | loss: 0.09735330939292908\n",
      "Epoch [1/1] | Batch 865 | loss: 0.1503881812095642\n",
      "Epoch [1/1] | Batch 866 | loss: 0.3984472453594208\n",
      "Epoch [1/1] | Batch 867 | loss: 0.2713802456855774\n",
      "Epoch [1/1] | Batch 868 | loss: 0.15721683204174042\n",
      "Epoch [1/1] | Batch 869 | loss: 0.16026027500629425\n",
      "Epoch [1/1] | Batch 870 | loss: 0.25133562088012695\n",
      "Epoch [1/1] | Batch 871 | loss: 0.17042744159698486\n",
      "Epoch [1/1] | Batch 872 | loss: 0.13990518450737\n",
      "Epoch [1/1] | Batch 873 | loss: 0.10922417789697647\n",
      "Epoch [1/1] | Batch 874 | loss: 0.20626631379127502\n",
      "Epoch [1/1] | Batch 875 | loss: 0.04332061484456062\n",
      "Epoch [1/1] | Batch 876 | loss: 0.10943659394979477\n",
      "Epoch [1/1] | Batch 877 | loss: 0.18169745802879333\n",
      "Epoch [1/1] | Batch 878 | loss: 0.21286992728710175\n",
      "Epoch [1/1] | Batch 879 | loss: 0.10187403857707977\n",
      "Epoch [1/1] | Batch 880 | loss: 0.07578469067811966\n",
      "Epoch [1/1] | Batch 881 | loss: 0.11565719544887543\n",
      "Epoch [1/1] | Batch 882 | loss: 0.0363185852766037\n",
      "Epoch [1/1] | Batch 883 | loss: 0.10646093636751175\n",
      "Epoch [1/1] | Batch 884 | loss: 0.05701402574777603\n",
      "Epoch [1/1] | Batch 885 | loss: 0.05521596968173981\n",
      "Epoch [1/1] | Batch 886 | loss: 0.08224821835756302\n",
      "Epoch [1/1] | Batch 887 | loss: 0.17984101176261902\n",
      "Epoch [1/1] | Batch 888 | loss: 0.12095079571008682\n",
      "Epoch [1/1] | Batch 889 | loss: 0.06650238484144211\n",
      "Epoch [1/1] | Batch 890 | loss: 0.06284579634666443\n",
      "Epoch [1/1] | Batch 891 | loss: 0.13050644099712372\n",
      "Epoch [1/1] | Batch 892 | loss: 0.1163935512304306\n",
      "Epoch [1/1] | Batch 893 | loss: 0.09696778655052185\n",
      "Epoch [1/1] | Batch 894 | loss: 0.15789352357387543\n",
      "Epoch [1/1] | Batch 895 | loss: 0.2628822326660156\n",
      "Epoch [1/1] | Batch 896 | loss: 0.13171830773353577\n",
      "Epoch [1/1] | Batch 897 | loss: 0.13572871685028076\n",
      "Epoch [1/1] | Batch 898 | loss: 0.1654776781797409\n",
      "Epoch [1/1] | Batch 899 | loss: 0.11277565360069275\n",
      "Epoch [1/1] | Batch 900 | loss: 0.09644538164138794\n",
      "Epoch [1/1] | Batch 901 | loss: 0.11671141535043716\n",
      "Epoch [1/1] | Batch 902 | loss: 0.06585079431533813\n",
      "Epoch [1/1] | Batch 903 | loss: 0.0750848576426506\n",
      "Epoch [1/1] | Batch 904 | loss: 0.06555646657943726\n",
      "Epoch [1/1] | Batch 905 | loss: 0.10118400305509567\n",
      "Epoch [1/1] | Batch 906 | loss: 0.07398004829883575\n",
      "Epoch [1/1] | Batch 907 | loss: 0.1061672493815422\n",
      "Epoch [1/1] | Batch 908 | loss: 0.12157066911458969\n",
      "Epoch [1/1] | Batch 909 | loss: 0.35914474725723267\n",
      "Epoch [1/1] | Batch 910 | loss: 0.2715359926223755\n",
      "Epoch [1/1] | Batch 911 | loss: 0.08487852662801743\n",
      "Epoch [1/1] | Batch 912 | loss: 0.04180478677153587\n",
      "Epoch [1/1] | Batch 913 | loss: 0.0705309808254242\n",
      "Epoch [1/1] | Batch 914 | loss: 0.13393336534500122\n",
      "Epoch [1/1] | Batch 915 | loss: 0.24634990096092224\n",
      "Epoch [1/1] | Batch 916 | loss: 0.10937348008155823\n",
      "Epoch [1/1] | Batch 917 | loss: 0.25845545530319214\n",
      "Epoch [1/1] | Batch 918 | loss: 0.03693434223532677\n",
      "Epoch [1/1] | Batch 919 | loss: 0.12051532417535782\n",
      "Epoch [1/1] | Batch 920 | loss: 0.10107125341892242\n",
      "Epoch [1/1] | Batch 921 | loss: 0.11057976633310318\n",
      "Epoch [1/1] | Batch 922 | loss: 0.3021475374698639\n",
      "Epoch [1/1] | Batch 923 | loss: 0.14440593123435974\n",
      "Epoch [1/1] | Batch 924 | loss: 0.052030038088560104\n",
      "Epoch [1/1] | Batch 925 | loss: 0.14072705805301666\n",
      "Epoch [1/1] | Batch 926 | loss: 0.1367671936750412\n",
      "Epoch [1/1] | Batch 927 | loss: 0.0603766143321991\n",
      "Epoch [1/1] | Batch 928 | loss: 0.08300270140171051\n",
      "Epoch [1/1] | Batch 929 | loss: 0.38057389855384827\n",
      "Epoch [1/1] | Batch 930 | loss: 0.1504659205675125\n",
      "Epoch [1/1] | Batch 931 | loss: 0.09399235993623734\n",
      "Epoch [1/1] | Batch 932 | loss: 0.27307718992233276\n",
      "Epoch [1/1] | Batch 933 | loss: 0.10415438562631607\n",
      "Epoch [1/1] | Batch 934 | loss: 0.049401480704545975\n",
      "Epoch [1/1] | Batch 935 | loss: 0.10232985764741898\n",
      "Epoch [1/1] | Batch 936 | loss: 0.211587592959404\n",
      "Epoch [1/1] | Batch 937 | loss: 0.1617443561553955\n",
      "Epoch [1/1] | Batch 938 | loss: 0.0460713766515255\n"
     ]
    }
   ],
   "source": [
    "# Actual Training loop\n",
    "num_epochs = 1\n",
    "lr = 1e-1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "losses = [] # For plotting\n",
    "\n",
    "model.train() # Training mode\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "\n",
    "        # 1. Compute loss    \n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # 2. Magically compute gradient\n",
    "        grad = torch.autograd.grad(loss, model.parameters())\n",
    "\n",
    "        # 3. Perform optimization step\n",
    "        for param, g in zip(model.parameters(), grad):\n",
    "            param.grad = g\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Batch {batch_idx+1} | loss: {loss.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the loss over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98a40435b0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we decrease the batch size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] | Batch 1 | loss: 2.550872802734375\n",
      "Epoch [1/1] | Batch 2 | loss: 2.272327184677124\n",
      "Epoch [1/1] | Batch 3 | loss: 2.0798451900482178\n",
      "Epoch [1/1] | Batch 4 | loss: 2.019272565841675\n",
      "Epoch [1/1] | Batch 5 | loss: 2.008283853530884\n",
      "Epoch [1/1] | Batch 6 | loss: 1.8231499195098877\n",
      "Epoch [1/1] | Batch 7 | loss: 1.6534101963043213\n",
      "Epoch [1/1] | Batch 8 | loss: 1.6480196714401245\n",
      "Epoch [1/1] | Batch 9 | loss: 1.5524470806121826\n",
      "Epoch [1/1] | Batch 10 | loss: 1.5912950038909912\n",
      "Epoch [1/1] | Batch 11 | loss: 1.1226023435592651\n",
      "Epoch [1/1] | Batch 12 | loss: 1.4697482585906982\n",
      "Epoch [1/1] | Batch 13 | loss: 1.0499300956726074\n",
      "Epoch [1/1] | Batch 14 | loss: 0.8054142594337463\n",
      "Epoch [1/1] | Batch 15 | loss: 0.9630178809165955\n",
      "Epoch [1/1] | Batch 16 | loss: 1.6144626140594482\n",
      "Epoch [1/1] | Batch 17 | loss: 0.9706082344055176\n",
      "Epoch [1/1] | Batch 18 | loss: 0.9603347182273865\n",
      "Epoch [1/1] | Batch 19 | loss: 0.9181138277053833\n",
      "Epoch [1/1] | Batch 20 | loss: 0.9964801073074341\n",
      "Epoch [1/1] | Batch 21 | loss: 0.9218223094940186\n",
      "Epoch [1/1] | Batch 22 | loss: 0.792879581451416\n",
      "Epoch [1/1] | Batch 23 | loss: 0.7534646987915039\n",
      "Epoch [1/1] | Batch 24 | loss: 0.8607738614082336\n",
      "Epoch [1/1] | Batch 25 | loss: 0.5728683471679688\n",
      "Epoch [1/1] | Batch 26 | loss: 0.9166349768638611\n",
      "Epoch [1/1] | Batch 27 | loss: 0.794952929019928\n",
      "Epoch [1/1] | Batch 28 | loss: 0.905629575252533\n",
      "Epoch [1/1] | Batch 29 | loss: 0.5491656064987183\n",
      "Epoch [1/1] | Batch 30 | loss: 0.7595250010490417\n",
      "Epoch [1/1] | Batch 31 | loss: 0.44258177280426025\n",
      "Epoch [1/1] | Batch 32 | loss: 0.6478436589241028\n",
      "Epoch [1/1] | Batch 33 | loss: 0.9316571354866028\n",
      "Epoch [1/1] | Batch 34 | loss: 0.7138662934303284\n",
      "Epoch [1/1] | Batch 35 | loss: 0.7003101110458374\n",
      "Epoch [1/1] | Batch 36 | loss: 1.1739102602005005\n",
      "Epoch [1/1] | Batch 37 | loss: 0.39093080163002014\n",
      "Epoch [1/1] | Batch 38 | loss: 0.35346999764442444\n",
      "Epoch [1/1] | Batch 39 | loss: 0.519402265548706\n",
      "Epoch [1/1] | Batch 40 | loss: 0.8760745525360107\n",
      "Epoch [1/1] | Batch 41 | loss: 0.6587297320365906\n",
      "Epoch [1/1] | Batch 42 | loss: 0.7763793468475342\n",
      "Epoch [1/1] | Batch 43 | loss: 0.9752394556999207\n",
      "Epoch [1/1] | Batch 44 | loss: 0.31389063596725464\n",
      "Epoch [1/1] | Batch 45 | loss: 0.5307620167732239\n",
      "Epoch [1/1] | Batch 46 | loss: 0.8773272633552551\n",
      "Epoch [1/1] | Batch 47 | loss: 0.9376776814460754\n",
      "Epoch [1/1] | Batch 48 | loss: 0.9408218264579773\n",
      "Epoch [1/1] | Batch 49 | loss: 0.6319009065628052\n",
      "Epoch [1/1] | Batch 50 | loss: 0.9061750769615173\n",
      "Epoch [1/1] | Batch 51 | loss: 0.8810993432998657\n",
      "Epoch [1/1] | Batch 52 | loss: 1.252722978591919\n",
      "Epoch [1/1] | Batch 53 | loss: 1.4988828897476196\n",
      "Epoch [1/1] | Batch 54 | loss: 0.6189529895782471\n",
      "Epoch [1/1] | Batch 55 | loss: 0.2294149398803711\n",
      "Epoch [1/1] | Batch 56 | loss: 0.5658639669418335\n",
      "Epoch [1/1] | Batch 57 | loss: 0.41400429606437683\n",
      "Epoch [1/1] | Batch 58 | loss: 0.3321255147457123\n",
      "Epoch [1/1] | Batch 59 | loss: 0.7916833162307739\n",
      "Epoch [1/1] | Batch 60 | loss: 0.8209172487258911\n",
      "Epoch [1/1] | Batch 61 | loss: 0.7788159251213074\n",
      "Epoch [1/1] | Batch 62 | loss: 0.3199586272239685\n",
      "Epoch [1/1] | Batch 63 | loss: 0.27741169929504395\n",
      "Epoch [1/1] | Batch 64 | loss: 0.5343133807182312\n",
      "Epoch [1/1] | Batch 65 | loss: 0.5212640762329102\n",
      "Epoch [1/1] | Batch 66 | loss: 0.30497246980667114\n",
      "Epoch [1/1] | Batch 67 | loss: 0.6612638235092163\n",
      "Epoch [1/1] | Batch 68 | loss: 0.6878257989883423\n",
      "Epoch [1/1] | Batch 69 | loss: 0.33037084341049194\n",
      "Epoch [1/1] | Batch 70 | loss: 0.39197179675102234\n",
      "Epoch [1/1] | Batch 71 | loss: 0.5760905742645264\n",
      "Epoch [1/1] | Batch 72 | loss: 0.22244222462177277\n",
      "Epoch [1/1] | Batch 73 | loss: 0.636237382888794\n",
      "Epoch [1/1] | Batch 74 | loss: 0.4029034376144409\n",
      "Epoch [1/1] | Batch 75 | loss: 0.14918142557144165\n",
      "Epoch [1/1] | Batch 76 | loss: 0.3618553578853607\n",
      "Epoch [1/1] | Batch 77 | loss: 0.4805525243282318\n",
      "Epoch [1/1] | Batch 78 | loss: 0.7752968072891235\n",
      "Epoch [1/1] | Batch 79 | loss: 0.6322574615478516\n",
      "Epoch [1/1] | Batch 80 | loss: 0.7425093054771423\n",
      "Epoch [1/1] | Batch 81 | loss: 0.3351837694644928\n",
      "Epoch [1/1] | Batch 82 | loss: 0.6692013740539551\n",
      "Epoch [1/1] | Batch 83 | loss: 0.5799654722213745\n",
      "Epoch [1/1] | Batch 84 | loss: 0.810283899307251\n",
      "Epoch [1/1] | Batch 85 | loss: 0.22209608554840088\n",
      "Epoch [1/1] | Batch 86 | loss: 0.2395739108324051\n",
      "Epoch [1/1] | Batch 87 | loss: 0.6475431323051453\n",
      "Epoch [1/1] | Batch 88 | loss: 0.5441766381263733\n",
      "Epoch [1/1] | Batch 89 | loss: 0.5479699373245239\n",
      "Epoch [1/1] | Batch 90 | loss: 0.38341444730758667\n",
      "Epoch [1/1] | Batch 91 | loss: 0.6354466676712036\n",
      "Epoch [1/1] | Batch 92 | loss: 0.5685256123542786\n",
      "Epoch [1/1] | Batch 93 | loss: 0.8308143019676208\n",
      "Epoch [1/1] | Batch 94 | loss: 0.24749881029129028\n",
      "Epoch [1/1] | Batch 95 | loss: 0.22449180483818054\n",
      "Epoch [1/1] | Batch 96 | loss: 0.6601735353469849\n",
      "Epoch [1/1] | Batch 97 | loss: 0.8985243439674377\n",
      "Epoch [1/1] | Batch 98 | loss: 0.829885721206665\n",
      "Epoch [1/1] | Batch 99 | loss: 0.8568345308303833\n",
      "Epoch [1/1] | Batch 100 | loss: 0.7377305626869202\n",
      "Epoch [1/1] | Batch 101 | loss: 0.758476734161377\n",
      "Epoch [1/1] | Batch 102 | loss: 0.7596737146377563\n",
      "Epoch [1/1] | Batch 103 | loss: 0.35313910245895386\n",
      "Epoch [1/1] | Batch 104 | loss: 0.682591438293457\n",
      "Epoch [1/1] | Batch 105 | loss: 0.7752559781074524\n",
      "Epoch [1/1] | Batch 106 | loss: 0.4661974012851715\n",
      "Epoch [1/1] | Batch 107 | loss: 0.6053697466850281\n",
      "Epoch [1/1] | Batch 108 | loss: 0.23109976947307587\n",
      "Epoch [1/1] | Batch 109 | loss: 0.7364614605903625\n",
      "Epoch [1/1] | Batch 110 | loss: 0.24826250970363617\n",
      "Epoch [1/1] | Batch 111 | loss: 0.7813392281532288\n",
      "Epoch [1/1] | Batch 112 | loss: 0.5162479877471924\n",
      "Epoch [1/1] | Batch 113 | loss: 0.44704699516296387\n",
      "Epoch [1/1] | Batch 114 | loss: 1.0161378383636475\n",
      "Epoch [1/1] | Batch 115 | loss: 0.5352137088775635\n",
      "Epoch [1/1] | Batch 116 | loss: 0.7266019582748413\n",
      "Epoch [1/1] | Batch 117 | loss: 0.909155547618866\n",
      "Epoch [1/1] | Batch 118 | loss: 0.4334871172904968\n",
      "Epoch [1/1] | Batch 119 | loss: 1.237783432006836\n",
      "Epoch [1/1] | Batch 120 | loss: 0.37418341636657715\n",
      "Epoch [1/1] | Batch 121 | loss: 0.40869349241256714\n",
      "Epoch [1/1] | Batch 122 | loss: 0.4995991885662079\n",
      "Epoch [1/1] | Batch 123 | loss: 0.694715678691864\n",
      "Epoch [1/1] | Batch 124 | loss: 0.4829219579696655\n",
      "Epoch [1/1] | Batch 125 | loss: 0.3568177819252014\n",
      "Epoch [1/1] | Batch 126 | loss: 0.5295069217681885\n",
      "Epoch [1/1] | Batch 127 | loss: 0.27444055676460266\n",
      "Epoch [1/1] | Batch 128 | loss: 0.6677998304367065\n",
      "Epoch [1/1] | Batch 129 | loss: 0.4186520576477051\n",
      "Epoch [1/1] | Batch 130 | loss: 0.4402659833431244\n",
      "Epoch [1/1] | Batch 131 | loss: 0.35140296816825867\n",
      "Epoch [1/1] | Batch 132 | loss: 0.3855668604373932\n",
      "Epoch [1/1] | Batch 133 | loss: 0.27633124589920044\n",
      "Epoch [1/1] | Batch 134 | loss: 0.6782311201095581\n",
      "Epoch [1/1] | Batch 135 | loss: 0.1309070885181427\n",
      "Epoch [1/1] | Batch 136 | loss: 0.15164561569690704\n",
      "Epoch [1/1] | Batch 137 | loss: 0.36546677350997925\n",
      "Epoch [1/1] | Batch 138 | loss: 0.6762895584106445\n",
      "Epoch [1/1] | Batch 139 | loss: 0.09656636416912079\n",
      "Epoch [1/1] | Batch 140 | loss: 0.16589227318763733\n",
      "Epoch [1/1] | Batch 141 | loss: 0.5307363271713257\n",
      "Epoch [1/1] | Batch 142 | loss: 0.30536001920700073\n",
      "Epoch [1/1] | Batch 143 | loss: 0.5251703858375549\n",
      "Epoch [1/1] | Batch 144 | loss: 0.06285972893238068\n",
      "Epoch [1/1] | Batch 145 | loss: 0.12728968262672424\n",
      "Epoch [1/1] | Batch 146 | loss: 0.3995441794395447\n",
      "Epoch [1/1] | Batch 147 | loss: 0.2831844687461853\n",
      "Epoch [1/1] | Batch 148 | loss: 0.5437442064285278\n",
      "Epoch [1/1] | Batch 149 | loss: 0.5351364016532898\n",
      "Epoch [1/1] | Batch 150 | loss: 0.5170612931251526\n",
      "Epoch [1/1] | Batch 151 | loss: 0.5466024875640869\n",
      "Epoch [1/1] | Batch 152 | loss: 0.6216506958007812\n",
      "Epoch [1/1] | Batch 153 | loss: 0.5421273708343506\n",
      "Epoch [1/1] | Batch 154 | loss: 0.4963732957839966\n",
      "Epoch [1/1] | Batch 155 | loss: 0.3980213403701782\n",
      "Epoch [1/1] | Batch 156 | loss: 0.5373741984367371\n",
      "Epoch [1/1] | Batch 157 | loss: 0.6836877465248108\n",
      "Epoch [1/1] | Batch 158 | loss: 0.7115071415901184\n",
      "Epoch [1/1] | Batch 159 | loss: 0.3034787178039551\n",
      "Epoch [1/1] | Batch 160 | loss: 0.27182069420814514\n",
      "Epoch [1/1] | Batch 161 | loss: 0.4400893449783325\n",
      "Epoch [1/1] | Batch 162 | loss: 0.5250325202941895\n",
      "Epoch [1/1] | Batch 163 | loss: 0.6934951543807983\n",
      "Epoch [1/1] | Batch 164 | loss: 0.5400419235229492\n",
      "Epoch [1/1] | Batch 165 | loss: 0.24790288507938385\n",
      "Epoch [1/1] | Batch 166 | loss: 0.2228211909532547\n",
      "Epoch [1/1] | Batch 167 | loss: 0.13493101298809052\n",
      "Epoch [1/1] | Batch 168 | loss: 0.23672270774841309\n",
      "Epoch [1/1] | Batch 169 | loss: 0.09072475135326385\n",
      "Epoch [1/1] | Batch 170 | loss: 0.7397661805152893\n",
      "Epoch [1/1] | Batch 171 | loss: 0.8494704961776733\n",
      "Epoch [1/1] | Batch 172 | loss: 0.33064180612564087\n",
      "Epoch [1/1] | Batch 173 | loss: 0.49356144666671753\n",
      "Epoch [1/1] | Batch 174 | loss: 0.3551270663738251\n",
      "Epoch [1/1] | Batch 175 | loss: 0.1936756670475006\n",
      "Epoch [1/1] | Batch 176 | loss: 0.5090020298957825\n",
      "Epoch [1/1] | Batch 177 | loss: 0.4604112207889557\n",
      "Epoch [1/1] | Batch 178 | loss: 0.6984365582466125\n",
      "Epoch [1/1] | Batch 179 | loss: 0.5607624650001526\n",
      "Epoch [1/1] | Batch 180 | loss: 0.3464779555797577\n",
      "Epoch [1/1] | Batch 181 | loss: 0.4538990259170532\n",
      "Epoch [1/1] | Batch 182 | loss: 0.08092128485441208\n",
      "Epoch [1/1] | Batch 183 | loss: 0.4459260404109955\n",
      "Epoch [1/1] | Batch 184 | loss: 0.3098675310611725\n",
      "Epoch [1/1] | Batch 185 | loss: 0.26629072427749634\n",
      "Epoch [1/1] | Batch 186 | loss: 0.6462546586990356\n",
      "Epoch [1/1] | Batch 187 | loss: 0.8235872387886047\n",
      "Epoch [1/1] | Batch 188 | loss: 0.5622872114181519\n",
      "Epoch [1/1] | Batch 189 | loss: 0.1753542274236679\n",
      "Epoch [1/1] | Batch 190 | loss: 0.27341708540916443\n",
      "Epoch [1/1] | Batch 191 | loss: 0.5087843537330627\n",
      "Epoch [1/1] | Batch 192 | loss: 0.09893082827329636\n",
      "Epoch [1/1] | Batch 193 | loss: 0.09627310931682587\n",
      "Epoch [1/1] | Batch 194 | loss: 0.09095042943954468\n",
      "Epoch [1/1] | Batch 195 | loss: 0.5960382223129272\n",
      "Epoch [1/1] | Batch 196 | loss: 0.1346336156129837\n",
      "Epoch [1/1] | Batch 197 | loss: 0.6314653158187866\n",
      "Epoch [1/1] | Batch 198 | loss: 0.33323371410369873\n",
      "Epoch [1/1] | Batch 199 | loss: 0.48594558238983154\n",
      "Epoch [1/1] | Batch 200 | loss: 0.510500967502594\n",
      "Epoch [1/1] | Batch 201 | loss: 0.2965846359729767\n",
      "Epoch [1/1] | Batch 202 | loss: 0.23636548221111298\n",
      "Epoch [1/1] | Batch 203 | loss: 0.4170939326286316\n",
      "Epoch [1/1] | Batch 204 | loss: 0.25225764513015747\n",
      "Epoch [1/1] | Batch 205 | loss: 0.8080692291259766\n",
      "Epoch [1/1] | Batch 206 | loss: 0.06503582000732422\n",
      "Epoch [1/1] | Batch 207 | loss: 0.5706907510757446\n",
      "Epoch [1/1] | Batch 208 | loss: 0.17052806913852692\n",
      "Epoch [1/1] | Batch 209 | loss: 0.09020929038524628\n",
      "Epoch [1/1] | Batch 210 | loss: 0.4441836178302765\n",
      "Epoch [1/1] | Batch 211 | loss: 0.2943367660045624\n",
      "Epoch [1/1] | Batch 212 | loss: 0.3524685800075531\n",
      "Epoch [1/1] | Batch 213 | loss: 0.4377436637878418\n",
      "Epoch [1/1] | Batch 214 | loss: 0.6277692317962646\n",
      "Epoch [1/1] | Batch 215 | loss: 0.5133298635482788\n",
      "Epoch [1/1] | Batch 216 | loss: 0.3930477499961853\n",
      "Epoch [1/1] | Batch 217 | loss: 0.1233956515789032\n",
      "Epoch [1/1] | Batch 218 | loss: 0.3027249574661255\n",
      "Epoch [1/1] | Batch 219 | loss: 0.370143324136734\n",
      "Epoch [1/1] | Batch 220 | loss: 0.5631705522537231\n",
      "Epoch [1/1] | Batch 221 | loss: 0.6794397234916687\n",
      "Epoch [1/1] | Batch 222 | loss: 0.32301807403564453\n",
      "Epoch [1/1] | Batch 223 | loss: 0.22354140877723694\n",
      "Epoch [1/1] | Batch 224 | loss: 0.2288704514503479\n",
      "Epoch [1/1] | Batch 225 | loss: 0.875203013420105\n",
      "Epoch [1/1] | Batch 226 | loss: 0.30518728494644165\n",
      "Epoch [1/1] | Batch 227 | loss: 0.21115270256996155\n",
      "Epoch [1/1] | Batch 228 | loss: 0.9646595120429993\n",
      "Epoch [1/1] | Batch 229 | loss: 0.20585279166698456\n",
      "Epoch [1/1] | Batch 230 | loss: 0.24560439586639404\n",
      "Epoch [1/1] | Batch 231 | loss: 0.4285299479961395\n",
      "Epoch [1/1] | Batch 232 | loss: 0.18164129555225372\n",
      "Epoch [1/1] | Batch 233 | loss: 0.8872685432434082\n",
      "Epoch [1/1] | Batch 234 | loss: 0.34028810262680054\n",
      "Epoch [1/1] | Batch 235 | loss: 0.18593566119670868\n",
      "Epoch [1/1] | Batch 236 | loss: 0.4479622542858124\n",
      "Epoch [1/1] | Batch 237 | loss: 0.5011038184165955\n",
      "Epoch [1/1] | Batch 238 | loss: 0.20446991920471191\n",
      "Epoch [1/1] | Batch 239 | loss: 0.1928587406873703\n",
      "Epoch [1/1] | Batch 240 | loss: 0.34876585006713867\n",
      "Epoch [1/1] | Batch 241 | loss: 0.21654434502124786\n",
      "Epoch [1/1] | Batch 242 | loss: 0.10742725431919098\n",
      "Epoch [1/1] | Batch 243 | loss: 0.27838435769081116\n",
      "Epoch [1/1] | Batch 244 | loss: 0.6554228663444519\n",
      "Epoch [1/1] | Batch 245 | loss: 0.19599084556102753\n",
      "Epoch [1/1] | Batch 246 | loss: 0.35181623697280884\n",
      "Epoch [1/1] | Batch 247 | loss: 0.1749582290649414\n",
      "Epoch [1/1] | Batch 248 | loss: 0.07779596000909805\n",
      "Epoch [1/1] | Batch 249 | loss: 0.03589031472802162\n",
      "Epoch [1/1] | Batch 250 | loss: 0.33567649126052856\n",
      "Epoch [1/1] | Batch 251 | loss: 0.09566358476877213\n",
      "Epoch [1/1] | Batch 252 | loss: 0.20186488330364227\n",
      "Epoch [1/1] | Batch 253 | loss: 0.3119976818561554\n",
      "Epoch [1/1] | Batch 254 | loss: 0.4056346118450165\n",
      "Epoch [1/1] | Batch 255 | loss: 0.2099391222000122\n",
      "Epoch [1/1] | Batch 256 | loss: 0.08912044018507004\n",
      "Epoch [1/1] | Batch 257 | loss: 0.15919233858585358\n",
      "Epoch [1/1] | Batch 258 | loss: 0.13996605575084686\n",
      "Epoch [1/1] | Batch 259 | loss: 0.2733016312122345\n",
      "Epoch [1/1] | Batch 260 | loss: 0.15406963229179382\n",
      "Epoch [1/1] | Batch 261 | loss: 0.820622980594635\n",
      "Epoch [1/1] | Batch 262 | loss: 0.693037748336792\n",
      "Epoch [1/1] | Batch 263 | loss: 0.07150011509656906\n",
      "Epoch [1/1] | Batch 264 | loss: 0.4177079498767853\n",
      "Epoch [1/1] | Batch 265 | loss: 0.5601605176925659\n",
      "Epoch [1/1] | Batch 266 | loss: 0.5295495986938477\n",
      "Epoch [1/1] | Batch 267 | loss: 0.08653486520051956\n",
      "Epoch [1/1] | Batch 268 | loss: 0.20960579812526703\n",
      "Epoch [1/1] | Batch 269 | loss: 0.4149118661880493\n",
      "Epoch [1/1] | Batch 270 | loss: 0.48002538084983826\n",
      "Epoch [1/1] | Batch 271 | loss: 0.3110864758491516\n",
      "Epoch [1/1] | Batch 272 | loss: 0.6500290036201477\n",
      "Epoch [1/1] | Batch 273 | loss: 0.20866236090660095\n",
      "Epoch [1/1] | Batch 274 | loss: 0.3953419625759125\n",
      "Epoch [1/1] | Batch 275 | loss: 0.4999224543571472\n",
      "Epoch [1/1] | Batch 276 | loss: 0.9435069561004639\n",
      "Epoch [1/1] | Batch 277 | loss: 0.17063839733600616\n",
      "Epoch [1/1] | Batch 278 | loss: 0.10475277900695801\n",
      "Epoch [1/1] | Batch 279 | loss: 0.4164887070655823\n",
      "Epoch [1/1] | Batch 280 | loss: 0.299679696559906\n",
      "Epoch [1/1] | Batch 281 | loss: 0.16006654500961304\n",
      "Epoch [1/1] | Batch 282 | loss: 0.26727494597435\n",
      "Epoch [1/1] | Batch 283 | loss: 0.22120578587055206\n",
      "Epoch [1/1] | Batch 284 | loss: 0.09731543064117432\n",
      "Epoch [1/1] | Batch 285 | loss: 0.366266667842865\n",
      "Epoch [1/1] | Batch 286 | loss: 0.1363743543624878\n",
      "Epoch [1/1] | Batch 287 | loss: 0.23472003638744354\n",
      "Epoch [1/1] | Batch 288 | loss: 0.261822909116745\n",
      "Epoch [1/1] | Batch 289 | loss: 0.4660322368144989\n",
      "Epoch [1/1] | Batch 290 | loss: 0.48753321170806885\n",
      "Epoch [1/1] | Batch 291 | loss: 0.022012298926711082\n",
      "Epoch [1/1] | Batch 292 | loss: 0.32105597853660583\n",
      "Epoch [1/1] | Batch 293 | loss: 0.32282453775405884\n",
      "Epoch [1/1] | Batch 294 | loss: 0.09991665184497833\n",
      "Epoch [1/1] | Batch 295 | loss: 0.2205248773097992\n",
      "Epoch [1/1] | Batch 296 | loss: 0.09177538007497787\n",
      "Epoch [1/1] | Batch 297 | loss: 0.12496064603328705\n",
      "Epoch [1/1] | Batch 298 | loss: 0.3771577477455139\n",
      "Epoch [1/1] | Batch 299 | loss: 0.36337485909461975\n",
      "Epoch [1/1] | Batch 300 | loss: 0.6505932211875916\n",
      "Epoch [1/1] | Batch 301 | loss: 0.4843127131462097\n",
      "Epoch [1/1] | Batch 302 | loss: 0.6434246301651001\n",
      "Epoch [1/1] | Batch 303 | loss: 0.12525494396686554\n",
      "Epoch [1/1] | Batch 304 | loss: 0.2628878951072693\n",
      "Epoch [1/1] | Batch 305 | loss: 0.11700859665870667\n",
      "Epoch [1/1] | Batch 306 | loss: 0.15301382541656494\n",
      "Epoch [1/1] | Batch 307 | loss: 0.6146649718284607\n",
      "Epoch [1/1] | Batch 308 | loss: 0.5690520405769348\n",
      "Epoch [1/1] | Batch 309 | loss: 0.478600412607193\n",
      "Epoch [1/1] | Batch 310 | loss: 0.5496542453765869\n",
      "Epoch [1/1] | Batch 311 | loss: 0.19496667385101318\n",
      "Epoch [1/1] | Batch 312 | loss: 0.1627916395664215\n",
      "Epoch [1/1] | Batch 313 | loss: 0.24721840023994446\n",
      "Epoch [1/1] | Batch 314 | loss: 0.5399394631385803\n",
      "Epoch [1/1] | Batch 315 | loss: 0.40594539046287537\n",
      "Epoch [1/1] | Batch 316 | loss: 0.22699090838432312\n",
      "Epoch [1/1] | Batch 317 | loss: 0.034815818071365356\n",
      "Epoch [1/1] | Batch 318 | loss: 0.12506861984729767\n",
      "Epoch [1/1] | Batch 319 | loss: 0.19576852023601532\n",
      "Epoch [1/1] | Batch 320 | loss: 0.12688730657100677\n",
      "Epoch [1/1] | Batch 321 | loss: 0.20414131879806519\n",
      "Epoch [1/1] | Batch 322 | loss: 0.099855937063694\n",
      "Epoch [1/1] | Batch 323 | loss: 0.17127563059329987\n",
      "Epoch [1/1] | Batch 324 | loss: 0.8630874156951904\n",
      "Epoch [1/1] | Batch 325 | loss: 0.6963033676147461\n",
      "Epoch [1/1] | Batch 326 | loss: 0.623537003993988\n",
      "Epoch [1/1] | Batch 327 | loss: 0.35878700017929077\n",
      "Epoch [1/1] | Batch 328 | loss: 0.2913382947444916\n",
      "Epoch [1/1] | Batch 329 | loss: 0.11042466014623642\n",
      "Epoch [1/1] | Batch 330 | loss: 0.1061609536409378\n",
      "Epoch [1/1] | Batch 331 | loss: 0.037297528237104416\n",
      "Epoch [1/1] | Batch 332 | loss: 0.28876325488090515\n",
      "Epoch [1/1] | Batch 333 | loss: 0.42670467495918274\n",
      "Epoch [1/1] | Batch 334 | loss: 0.04194938391447067\n",
      "Epoch [1/1] | Batch 335 | loss: 0.20344828069210052\n",
      "Epoch [1/1] | Batch 336 | loss: 0.13563764095306396\n",
      "Epoch [1/1] | Batch 337 | loss: 0.31578829884529114\n",
      "Epoch [1/1] | Batch 338 | loss: 0.5944593548774719\n",
      "Epoch [1/1] | Batch 339 | loss: 0.5951793193817139\n",
      "Epoch [1/1] | Batch 340 | loss: 0.07363790273666382\n",
      "Epoch [1/1] | Batch 341 | loss: 0.3928609788417816\n",
      "Epoch [1/1] | Batch 342 | loss: 0.3270203471183777\n",
      "Epoch [1/1] | Batch 343 | loss: 0.09731485694646835\n",
      "Epoch [1/1] | Batch 344 | loss: 0.37306860089302063\n",
      "Epoch [1/1] | Batch 345 | loss: 0.1049000546336174\n",
      "Epoch [1/1] | Batch 346 | loss: 0.26958131790161133\n",
      "Epoch [1/1] | Batch 347 | loss: 0.9267191290855408\n",
      "Epoch [1/1] | Batch 348 | loss: 0.5862563848495483\n",
      "Epoch [1/1] | Batch 349 | loss: 0.06499247252941132\n",
      "Epoch [1/1] | Batch 350 | loss: 0.3891211450099945\n",
      "Epoch [1/1] | Batch 351 | loss: 0.3926318883895874\n",
      "Epoch [1/1] | Batch 352 | loss: 0.3317462205886841\n",
      "Epoch [1/1] | Batch 353 | loss: 0.1033850908279419\n",
      "Epoch [1/1] | Batch 354 | loss: 0.12779322266578674\n",
      "Epoch [1/1] | Batch 355 | loss: 0.5155696868896484\n",
      "Epoch [1/1] | Batch 356 | loss: 0.19147133827209473\n",
      "Epoch [1/1] | Batch 357 | loss: 0.6145981550216675\n",
      "Epoch [1/1] | Batch 358 | loss: 0.47891879081726074\n",
      "Epoch [1/1] | Batch 359 | loss: 0.18481357395648956\n",
      "Epoch [1/1] | Batch 360 | loss: 0.169564351439476\n",
      "Epoch [1/1] | Batch 361 | loss: 0.13286560773849487\n",
      "Epoch [1/1] | Batch 362 | loss: 0.16265854239463806\n",
      "Epoch [1/1] | Batch 363 | loss: 0.16044190526008606\n",
      "Epoch [1/1] | Batch 364 | loss: 0.3748623728752136\n",
      "Epoch [1/1] | Batch 365 | loss: 0.1588749885559082\n",
      "Epoch [1/1] | Batch 366 | loss: 0.37462592124938965\n",
      "Epoch [1/1] | Batch 367 | loss: 0.37354087829589844\n",
      "Epoch [1/1] | Batch 368 | loss: 0.3968675136566162\n",
      "Epoch [1/1] | Batch 369 | loss: 0.8493216633796692\n",
      "Epoch [1/1] | Batch 370 | loss: 0.1357792615890503\n",
      "Epoch [1/1] | Batch 371 | loss: 0.9212403297424316\n",
      "Epoch [1/1] | Batch 372 | loss: 0.6159811019897461\n",
      "Epoch [1/1] | Batch 373 | loss: 0.45196497440338135\n",
      "Epoch [1/1] | Batch 374 | loss: 0.40861180424690247\n",
      "Epoch [1/1] | Batch 375 | loss: 0.33389338850975037\n",
      "Epoch [1/1] | Batch 376 | loss: 0.08917060494422913\n",
      "Epoch [1/1] | Batch 377 | loss: 0.09426259994506836\n",
      "Epoch [1/1] | Batch 378 | loss: 0.4712485671043396\n",
      "Epoch [1/1] | Batch 379 | loss: 0.13023477792739868\n",
      "Epoch [1/1] | Batch 380 | loss: 0.6025452613830566\n",
      "Epoch [1/1] | Batch 381 | loss: 0.15905991196632385\n",
      "Epoch [1/1] | Batch 382 | loss: 0.37378546595573425\n",
      "Epoch [1/1] | Batch 383 | loss: 0.10884111374616623\n",
      "Epoch [1/1] | Batch 384 | loss: 0.23169885575771332\n",
      "Epoch [1/1] | Batch 385 | loss: 0.41697967052459717\n",
      "Epoch [1/1] | Batch 386 | loss: 0.2196442037820816\n",
      "Epoch [1/1] | Batch 387 | loss: 0.12532226741313934\n",
      "Epoch [1/1] | Batch 388 | loss: 0.11365971714258194\n",
      "Epoch [1/1] | Batch 389 | loss: 0.3192445933818817\n",
      "Epoch [1/1] | Batch 390 | loss: 0.16007301211357117\n",
      "Epoch [1/1] | Batch 391 | loss: 0.1471530646085739\n",
      "Epoch [1/1] | Batch 392 | loss: 0.18030190467834473\n",
      "Epoch [1/1] | Batch 393 | loss: 0.035298846662044525\n",
      "Epoch [1/1] | Batch 394 | loss: 0.03439382463693619\n",
      "Epoch [1/1] | Batch 395 | loss: 0.08745124191045761\n",
      "Epoch [1/1] | Batch 396 | loss: 0.358418732881546\n",
      "Epoch [1/1] | Batch 397 | loss: 0.10619641840457916\n",
      "Epoch [1/1] | Batch 398 | loss: 0.0318499356508255\n",
      "Epoch [1/1] | Batch 399 | loss: 0.5465442538261414\n",
      "Epoch [1/1] | Batch 400 | loss: 0.15752770006656647\n",
      "Epoch [1/1] | Batch 401 | loss: 0.11952674388885498\n",
      "Epoch [1/1] | Batch 402 | loss: 0.9783202409744263\n",
      "Epoch [1/1] | Batch 403 | loss: 0.06495221704244614\n",
      "Epoch [1/1] | Batch 404 | loss: 0.8820730447769165\n",
      "Epoch [1/1] | Batch 405 | loss: 0.17551666498184204\n",
      "Epoch [1/1] | Batch 406 | loss: 0.3540198802947998\n",
      "Epoch [1/1] | Batch 407 | loss: 0.41791072487831116\n",
      "Epoch [1/1] | Batch 408 | loss: 0.3969188332557678\n",
      "Epoch [1/1] | Batch 409 | loss: 0.1628214418888092\n",
      "Epoch [1/1] | Batch 410 | loss: 0.0566069632768631\n",
      "Epoch [1/1] | Batch 411 | loss: 0.0772649347782135\n",
      "Epoch [1/1] | Batch 412 | loss: 0.21603310108184814\n",
      "Epoch [1/1] | Batch 413 | loss: 0.09321048110723495\n",
      "Epoch [1/1] | Batch 414 | loss: 0.1216094121336937\n",
      "Epoch [1/1] | Batch 415 | loss: 0.7571523189544678\n",
      "Epoch [1/1] | Batch 416 | loss: 0.1484929323196411\n",
      "Epoch [1/1] | Batch 417 | loss: 0.5019233822822571\n",
      "Epoch [1/1] | Batch 418 | loss: 0.6125865578651428\n",
      "Epoch [1/1] | Batch 419 | loss: 0.3255147337913513\n",
      "Epoch [1/1] | Batch 420 | loss: 0.2508580982685089\n",
      "Epoch [1/1] | Batch 421 | loss: 0.12456652522087097\n",
      "Epoch [1/1] | Batch 422 | loss: 0.05076652020215988\n",
      "Epoch [1/1] | Batch 423 | loss: 0.381750226020813\n",
      "Epoch [1/1] | Batch 424 | loss: 0.2893626391887665\n",
      "Epoch [1/1] | Batch 425 | loss: 0.053520698100328445\n",
      "Epoch [1/1] | Batch 426 | loss: 0.5979546308517456\n",
      "Epoch [1/1] | Batch 427 | loss: 0.26324501633644104\n",
      "Epoch [1/1] | Batch 428 | loss: 0.3212401866912842\n",
      "Epoch [1/1] | Batch 429 | loss: 0.1759367436170578\n",
      "Epoch [1/1] | Batch 430 | loss: 0.5699160099029541\n",
      "Epoch [1/1] | Batch 431 | loss: 0.37608784437179565\n",
      "Epoch [1/1] | Batch 432 | loss: 0.35672667622566223\n",
      "Epoch [1/1] | Batch 433 | loss: 0.43020081520080566\n",
      "Epoch [1/1] | Batch 434 | loss: 0.3252180516719818\n",
      "Epoch [1/1] | Batch 435 | loss: 0.4480709135532379\n",
      "Epoch [1/1] | Batch 436 | loss: 0.8565984964370728\n",
      "Epoch [1/1] | Batch 437 | loss: 0.40013158321380615\n",
      "Epoch [1/1] | Batch 438 | loss: 0.20679135620594025\n",
      "Epoch [1/1] | Batch 439 | loss: 0.15986822545528412\n",
      "Epoch [1/1] | Batch 440 | loss: 0.12069302797317505\n",
      "Epoch [1/1] | Batch 441 | loss: 0.05918724462389946\n",
      "Epoch [1/1] | Batch 442 | loss: 0.1521879881620407\n",
      "Epoch [1/1] | Batch 443 | loss: 0.06439753621816635\n",
      "Epoch [1/1] | Batch 444 | loss: 0.12928682565689087\n",
      "Epoch [1/1] | Batch 445 | loss: 0.042445506900548935\n",
      "Epoch [1/1] | Batch 446 | loss: 0.15327800810337067\n",
      "Epoch [1/1] | Batch 447 | loss: 0.1571296602487564\n",
      "Epoch [1/1] | Batch 448 | loss: 0.5536820888519287\n",
      "Epoch [1/1] | Batch 449 | loss: 0.6342793703079224\n",
      "Epoch [1/1] | Batch 450 | loss: 0.3044954836368561\n",
      "Epoch [1/1] | Batch 451 | loss: 0.23578964173793793\n",
      "Epoch [1/1] | Batch 452 | loss: 0.8798571825027466\n",
      "Epoch [1/1] | Batch 453 | loss: 0.1098252460360527\n",
      "Epoch [1/1] | Batch 454 | loss: 0.247123122215271\n",
      "Epoch [1/1] | Batch 455 | loss: 0.42321136593818665\n",
      "Epoch [1/1] | Batch 456 | loss: 0.14410842955112457\n",
      "Epoch [1/1] | Batch 457 | loss: 0.6123764514923096\n",
      "Epoch [1/1] | Batch 458 | loss: 0.15243329107761383\n",
      "Epoch [1/1] | Batch 459 | loss: 0.24374477565288544\n",
      "Epoch [1/1] | Batch 460 | loss: 0.2741185128688812\n",
      "Epoch [1/1] | Batch 461 | loss: 0.10994915664196014\n",
      "Epoch [1/1] | Batch 462 | loss: 0.0608835406601429\n",
      "Epoch [1/1] | Batch 463 | loss: 0.485369473695755\n",
      "Epoch [1/1] | Batch 464 | loss: 0.18942248821258545\n",
      "Epoch [1/1] | Batch 465 | loss: 0.08650687336921692\n",
      "Epoch [1/1] | Batch 466 | loss: 0.03757915273308754\n",
      "Epoch [1/1] | Batch 467 | loss: 0.2614953815937042\n",
      "Epoch [1/1] | Batch 468 | loss: 0.05842585489153862\n",
      "Epoch [1/1] | Batch 469 | loss: 0.049086760729551315\n",
      "Epoch [1/1] | Batch 470 | loss: 0.1625363975763321\n",
      "Epoch [1/1] | Batch 471 | loss: 0.04419040307402611\n",
      "Epoch [1/1] | Batch 472 | loss: 0.23586605489253998\n",
      "Epoch [1/1] | Batch 473 | loss: 0.09335058927536011\n",
      "Epoch [1/1] | Batch 474 | loss: 0.3598271906375885\n",
      "Epoch [1/1] | Batch 475 | loss: 0.27802860736846924\n",
      "Epoch [1/1] | Batch 476 | loss: 0.16412238776683807\n",
      "Epoch [1/1] | Batch 477 | loss: 0.2777000665664673\n",
      "Epoch [1/1] | Batch 478 | loss: 0.14497427642345428\n",
      "Epoch [1/1] | Batch 479 | loss: 0.2595689594745636\n",
      "Epoch [1/1] | Batch 480 | loss: 0.11478669941425323\n",
      "Epoch [1/1] | Batch 481 | loss: 0.34776827692985535\n",
      "Epoch [1/1] | Batch 482 | loss: 0.16716986894607544\n",
      "Epoch [1/1] | Batch 483 | loss: 0.34551846981048584\n",
      "Epoch [1/1] | Batch 484 | loss: 0.21845680475234985\n",
      "Epoch [1/1] | Batch 485 | loss: 0.10361943393945694\n",
      "Epoch [1/1] | Batch 486 | loss: 0.4078124463558197\n",
      "Epoch [1/1] | Batch 487 | loss: 0.21427372097969055\n",
      "Epoch [1/1] | Batch 488 | loss: 0.30843910574913025\n",
      "Epoch [1/1] | Batch 489 | loss: 0.31832271814346313\n",
      "Epoch [1/1] | Batch 490 | loss: 0.3599468171596527\n",
      "Epoch [1/1] | Batch 491 | loss: 0.16051004827022552\n",
      "Epoch [1/1] | Batch 492 | loss: 0.24116575717926025\n",
      "Epoch [1/1] | Batch 493 | loss: 0.3569990396499634\n",
      "Epoch [1/1] | Batch 494 | loss: 0.1865784227848053\n",
      "Epoch [1/1] | Batch 495 | loss: 0.38312414288520813\n",
      "Epoch [1/1] | Batch 496 | loss: 0.17061316967010498\n",
      "Epoch [1/1] | Batch 497 | loss: 0.31238675117492676\n",
      "Epoch [1/1] | Batch 498 | loss: 0.47874054312705994\n",
      "Epoch [1/1] | Batch 499 | loss: 0.05044174939393997\n",
      "Epoch [1/1] | Batch 500 | loss: 0.3214983642101288\n",
      "Epoch [1/1] | Batch 501 | loss: 0.050242938101291656\n",
      "Epoch [1/1] | Batch 502 | loss: 0.20660562813282013\n",
      "Epoch [1/1] | Batch 503 | loss: 0.15761731564998627\n",
      "Epoch [1/1] | Batch 504 | loss: 0.2297072857618332\n",
      "Epoch [1/1] | Batch 505 | loss: 0.2021419107913971\n",
      "Epoch [1/1] | Batch 506 | loss: 0.2323441058397293\n",
      "Epoch [1/1] | Batch 507 | loss: 0.14321540296077728\n",
      "Epoch [1/1] | Batch 508 | loss: 0.16528162360191345\n",
      "Epoch [1/1] | Batch 509 | loss: 0.03935365751385689\n",
      "Epoch [1/1] | Batch 510 | loss: 0.5660210847854614\n",
      "Epoch [1/1] | Batch 511 | loss: 0.10760211199522018\n",
      "Epoch [1/1] | Batch 512 | loss: 0.24918679893016815\n",
      "Epoch [1/1] | Batch 513 | loss: 0.10840769857168198\n",
      "Epoch [1/1] | Batch 514 | loss: 0.3383547365665436\n",
      "Epoch [1/1] | Batch 515 | loss: 0.06778472661972046\n",
      "Epoch [1/1] | Batch 516 | loss: 0.5867290496826172\n",
      "Epoch [1/1] | Batch 517 | loss: 0.13886401057243347\n",
      "Epoch [1/1] | Batch 518 | loss: 0.23522666096687317\n",
      "Epoch [1/1] | Batch 519 | loss: 0.11974173039197922\n",
      "Epoch [1/1] | Batch 520 | loss: 0.5542187690734863\n",
      "Epoch [1/1] | Batch 521 | loss: 0.1818261444568634\n",
      "Epoch [1/1] | Batch 522 | loss: 0.21471484005451202\n",
      "Epoch [1/1] | Batch 523 | loss: 0.15304435789585114\n",
      "Epoch [1/1] | Batch 524 | loss: 0.7984214425086975\n",
      "Epoch [1/1] | Batch 525 | loss: 0.044446155428886414\n",
      "Epoch [1/1] | Batch 526 | loss: 0.32002246379852295\n",
      "Epoch [1/1] | Batch 527 | loss: 0.09022265672683716\n",
      "Epoch [1/1] | Batch 528 | loss: 0.19695229828357697\n",
      "Epoch [1/1] | Batch 529 | loss: 0.34332942962646484\n",
      "Epoch [1/1] | Batch 530 | loss: 0.04272661358118057\n",
      "Epoch [1/1] | Batch 531 | loss: 0.23742802441120148\n",
      "Epoch [1/1] | Batch 532 | loss: 0.04287467151880264\n",
      "Epoch [1/1] | Batch 533 | loss: 0.05468905717134476\n",
      "Epoch [1/1] | Batch 534 | loss: 0.0950678139925003\n",
      "Epoch [1/1] | Batch 535 | loss: 0.11982007324695587\n",
      "Epoch [1/1] | Batch 536 | loss: 0.14235486090183258\n",
      "Epoch [1/1] | Batch 537 | loss: 0.11909285187721252\n",
      "Epoch [1/1] | Batch 538 | loss: 0.2194184958934784\n",
      "Epoch [1/1] | Batch 539 | loss: 0.10844332724809647\n",
      "Epoch [1/1] | Batch 540 | loss: 0.5589756965637207\n",
      "Epoch [1/1] | Batch 541 | loss: 0.2781580090522766\n",
      "Epoch [1/1] | Batch 542 | loss: 0.0248674638569355\n",
      "Epoch [1/1] | Batch 543 | loss: 0.12301671504974365\n",
      "Epoch [1/1] | Batch 544 | loss: 0.11977818608283997\n",
      "Epoch [1/1] | Batch 545 | loss: 0.31973356008529663\n",
      "Epoch [1/1] | Batch 546 | loss: 0.10045932233333588\n",
      "Epoch [1/1] | Batch 547 | loss: 0.36298930644989014\n",
      "Epoch [1/1] | Batch 548 | loss: 0.3292193114757538\n",
      "Epoch [1/1] | Batch 549 | loss: 0.04955880343914032\n",
      "Epoch [1/1] | Batch 550 | loss: 0.2742963433265686\n",
      "Epoch [1/1] | Batch 551 | loss: 0.12177223712205887\n",
      "Epoch [1/1] | Batch 552 | loss: 0.42981335520744324\n",
      "Epoch [1/1] | Batch 553 | loss: 0.010395277291536331\n",
      "Epoch [1/1] | Batch 554 | loss: 0.44006529450416565\n",
      "Epoch [1/1] | Batch 555 | loss: 0.05555388703942299\n",
      "Epoch [1/1] | Batch 556 | loss: 0.4375462532043457\n",
      "Epoch [1/1] | Batch 557 | loss: 0.05260098725557327\n",
      "Epoch [1/1] | Batch 558 | loss: 0.926138699054718\n",
      "Epoch [1/1] | Batch 559 | loss: 0.08200138062238693\n",
      "Epoch [1/1] | Batch 560 | loss: 0.019347934052348137\n",
      "Epoch [1/1] | Batch 561 | loss: 0.5311284065246582\n",
      "Epoch [1/1] | Batch 562 | loss: 0.2972494959831238\n",
      "Epoch [1/1] | Batch 563 | loss: 0.08813005685806274\n",
      "Epoch [1/1] | Batch 564 | loss: 0.0437789186835289\n",
      "Epoch [1/1] | Batch 565 | loss: 0.5592406392097473\n",
      "Epoch [1/1] | Batch 566 | loss: 0.0787588357925415\n",
      "Epoch [1/1] | Batch 567 | loss: 0.03861864656209946\n",
      "Epoch [1/1] | Batch 568 | loss: 0.07974602282047272\n",
      "Epoch [1/1] | Batch 569 | loss: 0.30891379714012146\n",
      "Epoch [1/1] | Batch 570 | loss: 0.6711155772209167\n",
      "Epoch [1/1] | Batch 571 | loss: 0.04904309660196304\n",
      "Epoch [1/1] | Batch 572 | loss: 0.23867708444595337\n",
      "Epoch [1/1] | Batch 573 | loss: 0.4213567078113556\n",
      "Epoch [1/1] | Batch 574 | loss: 0.1790546476840973\n",
      "Epoch [1/1] | Batch 575 | loss: 0.2234204262495041\n",
      "Epoch [1/1] | Batch 576 | loss: 0.2214786559343338\n",
      "Epoch [1/1] | Batch 577 | loss: 0.13782110810279846\n",
      "Epoch [1/1] | Batch 578 | loss: 0.18191921710968018\n",
      "Epoch [1/1] | Batch 579 | loss: 0.44633129239082336\n",
      "Epoch [1/1] | Batch 580 | loss: 0.3085339665412903\n",
      "Epoch [1/1] | Batch 581 | loss: 0.2585867643356323\n",
      "Epoch [1/1] | Batch 582 | loss: 0.719720184803009\n",
      "Epoch [1/1] | Batch 583 | loss: 0.02299741469323635\n",
      "Epoch [1/1] | Batch 584 | loss: 0.2543331980705261\n",
      "Epoch [1/1] | Batch 585 | loss: 0.6667707562446594\n",
      "Epoch [1/1] | Batch 586 | loss: 0.17739519476890564\n",
      "Epoch [1/1] | Batch 587 | loss: 0.06352311372756958\n",
      "Epoch [1/1] | Batch 588 | loss: 0.32238301634788513\n",
      "Epoch [1/1] | Batch 589 | loss: 0.06607279181480408\n",
      "Epoch [1/1] | Batch 590 | loss: 0.2533435821533203\n",
      "Epoch [1/1] | Batch 591 | loss: 1.3445942401885986\n",
      "Epoch [1/1] | Batch 592 | loss: 0.23322810232639313\n",
      "Epoch [1/1] | Batch 593 | loss: 0.3340630829334259\n",
      "Epoch [1/1] | Batch 594 | loss: 0.47828468680381775\n",
      "Epoch [1/1] | Batch 595 | loss: 0.3661288321018219\n",
      "Epoch [1/1] | Batch 596 | loss: 0.0335126668214798\n",
      "Epoch [1/1] | Batch 597 | loss: 0.4594399631023407\n",
      "Epoch [1/1] | Batch 598 | loss: 0.2830139398574829\n",
      "Epoch [1/1] | Batch 599 | loss: 0.05609256774187088\n",
      "Epoch [1/1] | Batch 600 | loss: 0.15655334293842316\n",
      "Epoch [1/1] | Batch 601 | loss: 0.14993241429328918\n",
      "Epoch [1/1] | Batch 602 | loss: 0.1539318561553955\n",
      "Epoch [1/1] | Batch 603 | loss: 0.3308731019496918\n",
      "Epoch [1/1] | Batch 604 | loss: 0.04071274772286415\n",
      "Epoch [1/1] | Batch 605 | loss: 0.18840694427490234\n",
      "Epoch [1/1] | Batch 606 | loss: 0.35115864872932434\n",
      "Epoch [1/1] | Batch 607 | loss: 0.05881255865097046\n",
      "Epoch [1/1] | Batch 608 | loss: 0.06113559380173683\n",
      "Epoch [1/1] | Batch 609 | loss: 0.48372477293014526\n",
      "Epoch [1/1] | Batch 610 | loss: 0.10633166879415512\n",
      "Epoch [1/1] | Batch 611 | loss: 0.25411924719810486\n",
      "Epoch [1/1] | Batch 612 | loss: 0.49813833832740784\n",
      "Epoch [1/1] | Batch 613 | loss: 0.2299153208732605\n",
      "Epoch [1/1] | Batch 614 | loss: 0.28311687707901\n",
      "Epoch [1/1] | Batch 615 | loss: 0.21565251052379608\n",
      "Epoch [1/1] | Batch 616 | loss: 0.31896471977233887\n",
      "Epoch [1/1] | Batch 617 | loss: 0.5179459452629089\n",
      "Epoch [1/1] | Batch 618 | loss: 0.28597402572631836\n",
      "Epoch [1/1] | Batch 619 | loss: 0.18429623544216156\n",
      "Epoch [1/1] | Batch 620 | loss: 0.12331164628267288\n",
      "Epoch [1/1] | Batch 621 | loss: 0.3038099408149719\n",
      "Epoch [1/1] | Batch 622 | loss: 0.15771840512752533\n",
      "Epoch [1/1] | Batch 623 | loss: 0.13220685720443726\n",
      "Epoch [1/1] | Batch 624 | loss: 0.17121507227420807\n",
      "Epoch [1/1] | Batch 625 | loss: 0.2086440473794937\n",
      "Epoch [1/1] | Batch 626 | loss: 1.046435832977295\n",
      "Epoch [1/1] | Batch 627 | loss: 0.10868782550096512\n",
      "Epoch [1/1] | Batch 628 | loss: 0.08635375648736954\n",
      "Epoch [1/1] | Batch 629 | loss: 0.1943611055612564\n",
      "Epoch [1/1] | Batch 630 | loss: 0.1443350464105606\n",
      "Epoch [1/1] | Batch 631 | loss: 0.3915838897228241\n",
      "Epoch [1/1] | Batch 632 | loss: 0.10122185945510864\n",
      "Epoch [1/1] | Batch 633 | loss: 0.33235886693000793\n",
      "Epoch [1/1] | Batch 634 | loss: 0.4733476936817169\n",
      "Epoch [1/1] | Batch 635 | loss: 0.4927639067173004\n",
      "Epoch [1/1] | Batch 636 | loss: 0.2143598347902298\n",
      "Epoch [1/1] | Batch 637 | loss: 0.5354536175727844\n",
      "Epoch [1/1] | Batch 638 | loss: 0.08623421937227249\n",
      "Epoch [1/1] | Batch 639 | loss: 0.7931132316589355\n",
      "Epoch [1/1] | Batch 640 | loss: 0.05695406347513199\n",
      "Epoch [1/1] | Batch 641 | loss: 0.10837087035179138\n",
      "Epoch [1/1] | Batch 642 | loss: 0.3308992087841034\n",
      "Epoch [1/1] | Batch 643 | loss: 0.2803657054901123\n",
      "Epoch [1/1] | Batch 644 | loss: 0.1058020368218422\n",
      "Epoch [1/1] | Batch 645 | loss: 0.05070553719997406\n",
      "Epoch [1/1] | Batch 646 | loss: 0.1586698293685913\n",
      "Epoch [1/1] | Batch 647 | loss: 0.03698861971497536\n",
      "Epoch [1/1] | Batch 648 | loss: 0.15693998336791992\n",
      "Epoch [1/1] | Batch 649 | loss: 0.15398669242858887\n",
      "Epoch [1/1] | Batch 650 | loss: 0.12202005833387375\n",
      "Epoch [1/1] | Batch 651 | loss: 0.834308922290802\n",
      "Epoch [1/1] | Batch 652 | loss: 0.5491249561309814\n",
      "Epoch [1/1] | Batch 653 | loss: 0.11216408759355545\n",
      "Epoch [1/1] | Batch 654 | loss: 0.18596936762332916\n",
      "Epoch [1/1] | Batch 655 | loss: 0.07284840196371078\n",
      "Epoch [1/1] | Batch 656 | loss: 0.6239506602287292\n",
      "Epoch [1/1] | Batch 657 | loss: 0.1449965089559555\n",
      "Epoch [1/1] | Batch 658 | loss: 0.023803452029824257\n",
      "Epoch [1/1] | Batch 659 | loss: 0.22744332253932953\n",
      "Epoch [1/1] | Batch 660 | loss: 0.14583411812782288\n",
      "Epoch [1/1] | Batch 661 | loss: 0.22765833139419556\n",
      "Epoch [1/1] | Batch 662 | loss: 0.26312652230262756\n",
      "Epoch [1/1] | Batch 663 | loss: 0.08287395536899567\n",
      "Epoch [1/1] | Batch 664 | loss: 0.09646029770374298\n",
      "Epoch [1/1] | Batch 665 | loss: 0.09494494646787643\n",
      "Epoch [1/1] | Batch 666 | loss: 0.06535451114177704\n",
      "Epoch [1/1] | Batch 667 | loss: 0.0525268092751503\n",
      "Epoch [1/1] | Batch 668 | loss: 0.25059282779693604\n",
      "Epoch [1/1] | Batch 669 | loss: 0.25521162152290344\n",
      "Epoch [1/1] | Batch 670 | loss: 0.1393636167049408\n",
      "Epoch [1/1] | Batch 671 | loss: 0.4255755543708801\n",
      "Epoch [1/1] | Batch 672 | loss: 0.09326350688934326\n",
      "Epoch [1/1] | Batch 673 | loss: 0.5958511233329773\n",
      "Epoch [1/1] | Batch 674 | loss: 0.31361931562423706\n",
      "Epoch [1/1] | Batch 675 | loss: 0.4111286997795105\n",
      "Epoch [1/1] | Batch 676 | loss: 0.04725896939635277\n",
      "Epoch [1/1] | Batch 677 | loss: 0.27318093180656433\n",
      "Epoch [1/1] | Batch 678 | loss: 0.5764721035957336\n",
      "Epoch [1/1] | Batch 679 | loss: 0.21302777528762817\n",
      "Epoch [1/1] | Batch 680 | loss: 0.529509425163269\n",
      "Epoch [1/1] | Batch 681 | loss: 0.08017481863498688\n",
      "Epoch [1/1] | Batch 682 | loss: 0.11026816070079803\n",
      "Epoch [1/1] | Batch 683 | loss: 0.2646048963069916\n",
      "Epoch [1/1] | Batch 684 | loss: 0.27141016721725464\n",
      "Epoch [1/1] | Batch 685 | loss: 0.1293356567621231\n",
      "Epoch [1/1] | Batch 686 | loss: 0.3206399083137512\n",
      "Epoch [1/1] | Batch 687 | loss: 0.05490458011627197\n",
      "Epoch [1/1] | Batch 688 | loss: 0.1025351956486702\n",
      "Epoch [1/1] | Batch 689 | loss: 0.3799057602882385\n",
      "Epoch [1/1] | Batch 690 | loss: 0.04896897077560425\n",
      "Epoch [1/1] | Batch 691 | loss: 0.14260604977607727\n",
      "Epoch [1/1] | Batch 692 | loss: 0.3736058175563812\n",
      "Epoch [1/1] | Batch 693 | loss: 0.32542234659194946\n",
      "Epoch [1/1] | Batch 694 | loss: 0.09847290068864822\n",
      "Epoch [1/1] | Batch 695 | loss: 0.22116674482822418\n",
      "Epoch [1/1] | Batch 696 | loss: 0.1017271876335144\n",
      "Epoch [1/1] | Batch 697 | loss: 0.19095997512340546\n",
      "Epoch [1/1] | Batch 698 | loss: 0.09793582558631897\n",
      "Epoch [1/1] | Batch 699 | loss: 0.18116812407970428\n",
      "Epoch [1/1] | Batch 700 | loss: 0.6515587568283081\n",
      "Epoch [1/1] | Batch 701 | loss: 0.4925294816493988\n",
      "Epoch [1/1] | Batch 702 | loss: 0.08799077570438385\n",
      "Epoch [1/1] | Batch 703 | loss: 0.5164079070091248\n",
      "Epoch [1/1] | Batch 704 | loss: 0.5432873368263245\n",
      "Epoch [1/1] | Batch 705 | loss: 0.5451611280441284\n",
      "Epoch [1/1] | Batch 706 | loss: 0.4456728994846344\n",
      "Epoch [1/1] | Batch 707 | loss: 0.5181991457939148\n",
      "Epoch [1/1] | Batch 708 | loss: 0.05865757167339325\n",
      "Epoch [1/1] | Batch 709 | loss: 0.27498289942741394\n",
      "Epoch [1/1] | Batch 710 | loss: 0.07188770920038223\n",
      "Epoch [1/1] | Batch 711 | loss: 0.28228095173835754\n",
      "Epoch [1/1] | Batch 712 | loss: 0.5029249787330627\n",
      "Epoch [1/1] | Batch 713 | loss: 0.027372069656848907\n",
      "Epoch [1/1] | Batch 714 | loss: 0.03082643449306488\n",
      "Epoch [1/1] | Batch 715 | loss: 0.2506158947944641\n",
      "Epoch [1/1] | Batch 716 | loss: 0.5078571438789368\n",
      "Epoch [1/1] | Batch 717 | loss: 0.29966670274734497\n",
      "Epoch [1/1] | Batch 718 | loss: 0.07963590323925018\n",
      "Epoch [1/1] | Batch 719 | loss: 0.17813827097415924\n",
      "Epoch [1/1] | Batch 720 | loss: 0.46837666630744934\n",
      "Epoch [1/1] | Batch 721 | loss: 0.09408175200223923\n",
      "Epoch [1/1] | Batch 722 | loss: 0.28418824076652527\n",
      "Epoch [1/1] | Batch 723 | loss: 0.045606426894664764\n",
      "Epoch [1/1] | Batch 724 | loss: 0.08470939099788666\n",
      "Epoch [1/1] | Batch 725 | loss: 0.7011522054672241\n",
      "Epoch [1/1] | Batch 726 | loss: 0.19524814188480377\n",
      "Epoch [1/1] | Batch 727 | loss: 0.03146317973732948\n",
      "Epoch [1/1] | Batch 728 | loss: 0.19155411422252655\n",
      "Epoch [1/1] | Batch 729 | loss: 0.12360649555921555\n",
      "Epoch [1/1] | Batch 730 | loss: 0.1246456578373909\n",
      "Epoch [1/1] | Batch 731 | loss: 0.02677825093269348\n",
      "Epoch [1/1] | Batch 732 | loss: 0.1069791316986084\n",
      "Epoch [1/1] | Batch 733 | loss: 0.16187986731529236\n",
      "Epoch [1/1] | Batch 734 | loss: 0.06876640766859055\n",
      "Epoch [1/1] | Batch 735 | loss: 0.13082170486450195\n",
      "Epoch [1/1] | Batch 736 | loss: 0.059161171317100525\n",
      "Epoch [1/1] | Batch 737 | loss: 0.14377613365650177\n",
      "Epoch [1/1] | Batch 738 | loss: 0.051343273371458054\n",
      "Epoch [1/1] | Batch 739 | loss: 0.15656454861164093\n",
      "Epoch [1/1] | Batch 740 | loss: 0.02530653402209282\n",
      "Epoch [1/1] | Batch 741 | loss: 0.09823274612426758\n",
      "Epoch [1/1] | Batch 742 | loss: 0.1854439228773117\n",
      "Epoch [1/1] | Batch 743 | loss: 0.27235397696495056\n",
      "Epoch [1/1] | Batch 744 | loss: 0.3398943245410919\n",
      "Epoch [1/1] | Batch 745 | loss: 0.45506981015205383\n",
      "Epoch [1/1] | Batch 746 | loss: 0.39309409260749817\n",
      "Epoch [1/1] | Batch 747 | loss: 0.5007087588310242\n",
      "Epoch [1/1] | Batch 748 | loss: 0.15037813782691956\n",
      "Epoch [1/1] | Batch 749 | loss: 0.12014786899089813\n",
      "Epoch [1/1] | Batch 750 | loss: 0.046828966587781906\n",
      "Epoch [1/1] | Batch 751 | loss: 0.06499642878770828\n",
      "Epoch [1/1] | Batch 752 | loss: 0.21861785650253296\n",
      "Epoch [1/1] | Batch 753 | loss: 0.09769771248102188\n",
      "Epoch [1/1] | Batch 754 | loss: 0.24528291821479797\n",
      "Epoch [1/1] | Batch 755 | loss: 0.2911840081214905\n",
      "Epoch [1/1] | Batch 756 | loss: 0.15187190473079681\n",
      "Epoch [1/1] | Batch 757 | loss: 0.2855168879032135\n",
      "Epoch [1/1] | Batch 758 | loss: 0.48560523986816406\n",
      "Epoch [1/1] | Batch 759 | loss: 0.1601712703704834\n",
      "Epoch [1/1] | Batch 760 | loss: 0.17865803837776184\n",
      "Epoch [1/1] | Batch 761 | loss: 0.06640744209289551\n",
      "Epoch [1/1] | Batch 762 | loss: 0.02394658699631691\n",
      "Epoch [1/1] | Batch 763 | loss: 0.20372003316879272\n",
      "Epoch [1/1] | Batch 764 | loss: 0.09432587772607803\n",
      "Epoch [1/1] | Batch 765 | loss: 0.09162422269582748\n",
      "Epoch [1/1] | Batch 766 | loss: 0.03927096724510193\n",
      "Epoch [1/1] | Batch 767 | loss: 0.04864950478076935\n",
      "Epoch [1/1] | Batch 768 | loss: 0.05249548330903053\n",
      "Epoch [1/1] | Batch 769 | loss: 0.6211135983467102\n",
      "Epoch [1/1] | Batch 770 | loss: 0.4936721622943878\n",
      "Epoch [1/1] | Batch 771 | loss: 0.04100377485156059\n",
      "Epoch [1/1] | Batch 772 | loss: 0.07105597108602524\n",
      "Epoch [1/1] | Batch 773 | loss: 0.04383792728185654\n",
      "Epoch [1/1] | Batch 774 | loss: 0.3824291527271271\n",
      "Epoch [1/1] | Batch 775 | loss: 0.15295292437076569\n",
      "Epoch [1/1] | Batch 776 | loss: 0.3827250897884369\n",
      "Epoch [1/1] | Batch 777 | loss: 0.3173582851886749\n",
      "Epoch [1/1] | Batch 778 | loss: 0.5068585872650146\n",
      "Epoch [1/1] | Batch 779 | loss: 0.585425078868866\n",
      "Epoch [1/1] | Batch 780 | loss: 0.3704943358898163\n",
      "Epoch [1/1] | Batch 781 | loss: 0.30312591791152954\n",
      "Epoch [1/1] | Batch 782 | loss: 0.21120290458202362\n",
      "Epoch [1/1] | Batch 783 | loss: 0.14084914326667786\n",
      "Epoch [1/1] | Batch 784 | loss: 0.29419732093811035\n",
      "Epoch [1/1] | Batch 785 | loss: 0.07136651128530502\n",
      "Epoch [1/1] | Batch 786 | loss: 0.07031281292438507\n",
      "Epoch [1/1] | Batch 787 | loss: 0.08389051258563995\n",
      "Epoch [1/1] | Batch 788 | loss: 0.05852048844099045\n",
      "Epoch [1/1] | Batch 789 | loss: 0.08411027491092682\n",
      "Epoch [1/1] | Batch 790 | loss: 0.10233462601900101\n",
      "Epoch [1/1] | Batch 791 | loss: 0.3476773500442505\n",
      "Epoch [1/1] | Batch 792 | loss: 0.2767274081707001\n",
      "Epoch [1/1] | Batch 793 | loss: 0.3854930102825165\n",
      "Epoch [1/1] | Batch 794 | loss: 0.12617652118206024\n",
      "Epoch [1/1] | Batch 795 | loss: 0.3181520402431488\n",
      "Epoch [1/1] | Batch 796 | loss: 0.3183310925960541\n",
      "Epoch [1/1] | Batch 797 | loss: 0.29394909739494324\n",
      "Epoch [1/1] | Batch 798 | loss: 0.320299357175827\n",
      "Epoch [1/1] | Batch 799 | loss: 0.14241741597652435\n",
      "Epoch [1/1] | Batch 800 | loss: 0.06691139936447144\n",
      "Epoch [1/1] | Batch 801 | loss: 0.6261486411094666\n",
      "Epoch [1/1] | Batch 802 | loss: 0.3670913875102997\n",
      "Epoch [1/1] | Batch 803 | loss: 0.17478781938552856\n",
      "Epoch [1/1] | Batch 804 | loss: 0.25287169218063354\n",
      "Epoch [1/1] | Batch 805 | loss: 0.2940557599067688\n",
      "Epoch [1/1] | Batch 806 | loss: 0.11621319502592087\n",
      "Epoch [1/1] | Batch 807 | loss: 0.3750882148742676\n",
      "Epoch [1/1] | Batch 808 | loss: 0.23824933171272278\n",
      "Epoch [1/1] | Batch 809 | loss: 0.396757572889328\n",
      "Epoch [1/1] | Batch 810 | loss: 0.39254334568977356\n",
      "Epoch [1/1] | Batch 811 | loss: 0.16825127601623535\n",
      "Epoch [1/1] | Batch 812 | loss: 0.04961176961660385\n",
      "Epoch [1/1] | Batch 813 | loss: 0.3936418890953064\n",
      "Epoch [1/1] | Batch 814 | loss: 0.0035416753962635994\n",
      "Epoch [1/1] | Batch 815 | loss: 0.28183266520500183\n",
      "Epoch [1/1] | Batch 816 | loss: 0.3029174506664276\n",
      "Epoch [1/1] | Batch 817 | loss: 0.570779025554657\n",
      "Epoch [1/1] | Batch 818 | loss: 0.05149922892451286\n",
      "Epoch [1/1] | Batch 819 | loss: 0.2761994004249573\n",
      "Epoch [1/1] | Batch 820 | loss: 0.06157262995839119\n",
      "Epoch [1/1] | Batch 821 | loss: 0.31263047456741333\n",
      "Epoch [1/1] | Batch 822 | loss: 0.1134764701128006\n",
      "Epoch [1/1] | Batch 823 | loss: 0.15381526947021484\n",
      "Epoch [1/1] | Batch 824 | loss: 0.02211751416325569\n",
      "Epoch [1/1] | Batch 825 | loss: 0.1022970974445343\n",
      "Epoch [1/1] | Batch 826 | loss: 0.20641669631004333\n",
      "Epoch [1/1] | Batch 827 | loss: 0.2790929079055786\n",
      "Epoch [1/1] | Batch 828 | loss: 0.14181126654148102\n",
      "Epoch [1/1] | Batch 829 | loss: 0.22699987888336182\n",
      "Epoch [1/1] | Batch 830 | loss: 0.21427321434020996\n",
      "Epoch [1/1] | Batch 831 | loss: 0.07230672985315323\n",
      "Epoch [1/1] | Batch 832 | loss: 0.5500723719596863\n",
      "Epoch [1/1] | Batch 833 | loss: 0.11039705574512482\n",
      "Epoch [1/1] | Batch 834 | loss: 0.37630316615104675\n",
      "Epoch [1/1] | Batch 835 | loss: 0.42991212010383606\n",
      "Epoch [1/1] | Batch 836 | loss: 0.13889624178409576\n",
      "Epoch [1/1] | Batch 837 | loss: 0.12218087166547775\n",
      "Epoch [1/1] | Batch 838 | loss: 0.10971374064683914\n",
      "Epoch [1/1] | Batch 839 | loss: 0.32033103704452515\n",
      "Epoch [1/1] | Batch 840 | loss: 0.35535427927970886\n",
      "Epoch [1/1] | Batch 841 | loss: 0.0900142639875412\n",
      "Epoch [1/1] | Batch 842 | loss: 0.4443221390247345\n",
      "Epoch [1/1] | Batch 843 | loss: 0.6392361521720886\n",
      "Epoch [1/1] | Batch 844 | loss: 0.1881798654794693\n",
      "Epoch [1/1] | Batch 845 | loss: 0.5778357982635498\n",
      "Epoch [1/1] | Batch 846 | loss: 0.5442023873329163\n",
      "Epoch [1/1] | Batch 847 | loss: 0.34259316325187683\n",
      "Epoch [1/1] | Batch 848 | loss: 0.42644816637039185\n",
      "Epoch [1/1] | Batch 849 | loss: 0.15280979871749878\n",
      "Epoch [1/1] | Batch 850 | loss: 0.16186705231666565\n",
      "Epoch [1/1] | Batch 851 | loss: 0.1019638329744339\n",
      "Epoch [1/1] | Batch 852 | loss: 0.42876604199409485\n",
      "Epoch [1/1] | Batch 853 | loss: 0.33701056241989136\n",
      "Epoch [1/1] | Batch 854 | loss: 0.017995817586779594\n",
      "Epoch [1/1] | Batch 855 | loss: 0.24943052232265472\n",
      "Epoch [1/1] | Batch 856 | loss: 0.5323062539100647\n",
      "Epoch [1/1] | Batch 857 | loss: 0.4286431670188904\n",
      "Epoch [1/1] | Batch 858 | loss: 0.20201009511947632\n",
      "Epoch [1/1] | Batch 859 | loss: 0.3887622654438019\n",
      "Epoch [1/1] | Batch 860 | loss: 0.12236250191926956\n",
      "Epoch [1/1] | Batch 861 | loss: 0.15974397957324982\n",
      "Epoch [1/1] | Batch 862 | loss: 0.16740307211875916\n",
      "Epoch [1/1] | Batch 863 | loss: 0.1279369443655014\n",
      "Epoch [1/1] | Batch 864 | loss: 0.04131266474723816\n",
      "Epoch [1/1] | Batch 865 | loss: 0.13124807178974152\n",
      "Epoch [1/1] | Batch 866 | loss: 0.15121352672576904\n",
      "Epoch [1/1] | Batch 867 | loss: 0.05934208631515503\n",
      "Epoch [1/1] | Batch 868 | loss: 0.052499886602163315\n",
      "Epoch [1/1] | Batch 869 | loss: 0.02525963820517063\n",
      "Epoch [1/1] | Batch 870 | loss: 0.07799918204545975\n",
      "Epoch [1/1] | Batch 871 | loss: 0.21068720519542694\n",
      "Epoch [1/1] | Batch 872 | loss: 0.29243987798690796\n",
      "Epoch [1/1] | Batch 873 | loss: 0.18097595870494843\n",
      "Epoch [1/1] | Batch 874 | loss: 0.09668092429637909\n",
      "Epoch [1/1] | Batch 875 | loss: 0.006872545927762985\n",
      "Epoch [1/1] | Batch 876 | loss: 0.048283644020557404\n",
      "Epoch [1/1] | Batch 877 | loss: 0.0748380795121193\n",
      "Epoch [1/1] | Batch 878 | loss: 0.05090641975402832\n",
      "Epoch [1/1] | Batch 879 | loss: 0.17671847343444824\n",
      "Epoch [1/1] | Batch 880 | loss: 0.07922817021608353\n",
      "Epoch [1/1] | Batch 881 | loss: 0.14209406077861786\n",
      "Epoch [1/1] | Batch 882 | loss: 0.013111601583659649\n",
      "Epoch [1/1] | Batch 883 | loss: 0.15567435324192047\n",
      "Epoch [1/1] | Batch 884 | loss: 0.12254799157381058\n",
      "Epoch [1/1] | Batch 885 | loss: 0.32100486755371094\n",
      "Epoch [1/1] | Batch 886 | loss: 0.3028676509857178\n",
      "Epoch [1/1] | Batch 887 | loss: 0.09102434664964676\n",
      "Epoch [1/1] | Batch 888 | loss: 0.1164633259177208\n",
      "Epoch [1/1] | Batch 889 | loss: 0.41512352228164673\n",
      "Epoch [1/1] | Batch 890 | loss: 0.1642889380455017\n",
      "Epoch [1/1] | Batch 891 | loss: 0.7906815409660339\n",
      "Epoch [1/1] | Batch 892 | loss: 0.40975773334503174\n",
      "Epoch [1/1] | Batch 893 | loss: 0.1355697363615036\n",
      "Epoch [1/1] | Batch 894 | loss: 0.1678498536348343\n",
      "Epoch [1/1] | Batch 895 | loss: 0.5914449691772461\n",
      "Epoch [1/1] | Batch 896 | loss: 0.046242523938417435\n",
      "Epoch [1/1] | Batch 897 | loss: 0.40502023696899414\n",
      "Epoch [1/1] | Batch 898 | loss: 0.2530986964702606\n",
      "Epoch [1/1] | Batch 899 | loss: 0.08214685320854187\n",
      "Epoch [1/1] | Batch 900 | loss: 0.10754287242889404\n",
      "Epoch [1/1] | Batch 901 | loss: 0.10366746038198471\n",
      "Epoch [1/1] | Batch 902 | loss: 0.21941588819026947\n",
      "Epoch [1/1] | Batch 903 | loss: 0.17548274993896484\n",
      "Epoch [1/1] | Batch 904 | loss: 0.016861703246831894\n",
      "Epoch [1/1] | Batch 905 | loss: 0.07851265370845795\n",
      "Epoch [1/1] | Batch 906 | loss: 0.0945906788110733\n",
      "Epoch [1/1] | Batch 907 | loss: 0.011334993876516819\n",
      "Epoch [1/1] | Batch 908 | loss: 0.08588498085737228\n",
      "Epoch [1/1] | Batch 909 | loss: 0.5059091448783875\n",
      "Epoch [1/1] | Batch 910 | loss: 0.24857421219348907\n",
      "Epoch [1/1] | Batch 911 | loss: 0.28698092699050903\n",
      "Epoch [1/1] | Batch 912 | loss: 0.03274303674697876\n",
      "Epoch [1/1] | Batch 913 | loss: 0.2308655083179474\n",
      "Epoch [1/1] | Batch 914 | loss: 0.5751683115959167\n",
      "Epoch [1/1] | Batch 915 | loss: 0.18223492801189423\n",
      "Epoch [1/1] | Batch 916 | loss: 0.2798742949962616\n",
      "Epoch [1/1] | Batch 917 | loss: 0.20722070336341858\n",
      "Epoch [1/1] | Batch 918 | loss: 0.13341431319713593\n",
      "Epoch [1/1] | Batch 919 | loss: 0.6342796683311462\n",
      "Epoch [1/1] | Batch 920 | loss: 0.3817887008190155\n",
      "Epoch [1/1] | Batch 921 | loss: 0.04521734640002251\n",
      "Epoch [1/1] | Batch 922 | loss: 0.028169233351945877\n",
      "Epoch [1/1] | Batch 923 | loss: 0.3994910717010498\n",
      "Epoch [1/1] | Batch 924 | loss: 0.4233004152774811\n",
      "Epoch [1/1] | Batch 925 | loss: 0.12309923768043518\n",
      "Epoch [1/1] | Batch 926 | loss: 0.1462288349866867\n",
      "Epoch [1/1] | Batch 927 | loss: 0.10559350997209549\n",
      "Epoch [1/1] | Batch 928 | loss: 0.030534129589796066\n",
      "Epoch [1/1] | Batch 929 | loss: 0.05553232505917549\n",
      "Epoch [1/1] | Batch 930 | loss: 0.10992828756570816\n",
      "Epoch [1/1] | Batch 931 | loss: 0.46438243985176086\n",
      "Epoch [1/1] | Batch 932 | loss: 0.0026840788777917624\n",
      "Epoch [1/1] | Batch 933 | loss: 0.13332991302013397\n",
      "Epoch [1/1] | Batch 934 | loss: 0.09236878156661987\n",
      "Epoch [1/1] | Batch 935 | loss: 0.02519914321601391\n",
      "Epoch [1/1] | Batch 936 | loss: 0.2408563196659088\n",
      "Epoch [1/1] | Batch 937 | loss: 0.3477088510990143\n",
      "Epoch [1/1] | Batch 938 | loss: 0.056645188480615616\n",
      "Epoch [1/1] | Batch 939 | loss: 0.24564340710639954\n",
      "Epoch [1/1] | Batch 940 | loss: 0.012630795128643513\n",
      "Epoch [1/1] | Batch 941 | loss: 0.06540878862142563\n",
      "Epoch [1/1] | Batch 942 | loss: 0.03264767304062843\n",
      "Epoch [1/1] | Batch 943 | loss: 0.02332090400159359\n",
      "Epoch [1/1] | Batch 944 | loss: 0.16267269849777222\n",
      "Epoch [1/1] | Batch 945 | loss: 0.03239886462688446\n",
      "Epoch [1/1] | Batch 946 | loss: 0.015833960846066475\n",
      "Epoch [1/1] | Batch 947 | loss: 0.2141932249069214\n",
      "Epoch [1/1] | Batch 948 | loss: 0.47069457173347473\n",
      "Epoch [1/1] | Batch 949 | loss: 0.2048802524805069\n",
      "Epoch [1/1] | Batch 950 | loss: 0.15337100625038147\n",
      "Epoch [1/1] | Batch 951 | loss: 0.24663347005844116\n",
      "Epoch [1/1] | Batch 952 | loss: 0.4807681739330292\n",
      "Epoch [1/1] | Batch 953 | loss: 0.1151544600725174\n",
      "Epoch [1/1] | Batch 954 | loss: 0.3409360349178314\n",
      "Epoch [1/1] | Batch 955 | loss: 0.4200861155986786\n",
      "Epoch [1/1] | Batch 956 | loss: 0.0878058597445488\n",
      "Epoch [1/1] | Batch 957 | loss: 0.3239787518978119\n",
      "Epoch [1/1] | Batch 958 | loss: 0.17259356379508972\n",
      "Epoch [1/1] | Batch 959 | loss: 0.226955384016037\n",
      "Epoch [1/1] | Batch 960 | loss: 0.2240830957889557\n",
      "Epoch [1/1] | Batch 961 | loss: 0.4617178440093994\n",
      "Epoch [1/1] | Batch 962 | loss: 0.06270714849233627\n",
      "Epoch [1/1] | Batch 963 | loss: 0.09852219372987747\n",
      "Epoch [1/1] | Batch 964 | loss: 0.0436369813978672\n",
      "Epoch [1/1] | Batch 965 | loss: 0.10987356305122375\n",
      "Epoch [1/1] | Batch 966 | loss: 0.33028101921081543\n",
      "Epoch [1/1] | Batch 967 | loss: 0.13824471831321716\n",
      "Epoch [1/1] | Batch 968 | loss: 0.12392838299274445\n",
      "Epoch [1/1] | Batch 969 | loss: 0.11058855056762695\n",
      "Epoch [1/1] | Batch 970 | loss: 0.3419632613658905\n",
      "Epoch [1/1] | Batch 971 | loss: 0.016749896109104156\n",
      "Epoch [1/1] | Batch 972 | loss: 0.23788590729236603\n",
      "Epoch [1/1] | Batch 973 | loss: 0.2506919801235199\n",
      "Epoch [1/1] | Batch 974 | loss: 0.11821897327899933\n",
      "Epoch [1/1] | Batch 975 | loss: 0.3216254413127899\n",
      "Epoch [1/1] | Batch 976 | loss: 0.1549934446811676\n",
      "Epoch [1/1] | Batch 977 | loss: 0.12671253085136414\n",
      "Epoch [1/1] | Batch 978 | loss: 0.1351744681596756\n",
      "Epoch [1/1] | Batch 979 | loss: 0.03614039719104767\n",
      "Epoch [1/1] | Batch 980 | loss: 0.020988592877984047\n",
      "Epoch [1/1] | Batch 981 | loss: 0.02667802758514881\n",
      "Epoch [1/1] | Batch 982 | loss: 0.06795994192361832\n",
      "Epoch [1/1] | Batch 983 | loss: 0.022772083058953285\n",
      "Epoch [1/1] | Batch 984 | loss: 0.041778624057769775\n",
      "Epoch [1/1] | Batch 985 | loss: 0.0051448289304971695\n",
      "Epoch [1/1] | Batch 986 | loss: 0.010024527087807655\n",
      "Epoch [1/1] | Batch 987 | loss: 0.056704625487327576\n",
      "Epoch [1/1] | Batch 988 | loss: 0.01186115201562643\n",
      "Epoch [1/1] | Batch 989 | loss: 0.11569564044475555\n",
      "Epoch [1/1] | Batch 990 | loss: 0.026300504803657532\n",
      "Epoch [1/1] | Batch 991 | loss: 0.1305777132511139\n",
      "Epoch [1/1] | Batch 992 | loss: 0.14122706651687622\n",
      "Epoch [1/1] | Batch 993 | loss: 0.2303360402584076\n",
      "Epoch [1/1] | Batch 994 | loss: 0.26501891016960144\n",
      "Epoch [1/1] | Batch 995 | loss: 0.0738368034362793\n",
      "Epoch [1/1] | Batch 996 | loss: 0.055363573133945465\n",
      "Epoch [1/1] | Batch 997 | loss: 0.0057990895584225655\n",
      "Epoch [1/1] | Batch 998 | loss: 0.41465312242507935\n",
      "Epoch [1/1] | Batch 999 | loss: 0.15728946030139923\n",
      "Epoch [1/1] | Batch 1000 | loss: 0.17854562401771545\n",
      "Epoch [1/1] | Batch 1001 | loss: 0.10218310356140137\n",
      "Epoch [1/1] | Batch 1002 | loss: 0.024219971150159836\n",
      "Epoch [1/1] | Batch 1003 | loss: 0.09530407190322876\n",
      "Epoch [1/1] | Batch 1004 | loss: 0.04751560091972351\n",
      "Epoch [1/1] | Batch 1005 | loss: 0.1512446403503418\n",
      "Epoch [1/1] | Batch 1006 | loss: 0.5819451808929443\n",
      "Epoch [1/1] | Batch 1007 | loss: 0.5085562467575073\n",
      "Epoch [1/1] | Batch 1008 | loss: 1.0330750942230225\n",
      "Epoch [1/1] | Batch 1009 | loss: 0.2090090662240982\n",
      "Epoch [1/1] | Batch 1010 | loss: 0.04722326248884201\n",
      "Epoch [1/1] | Batch 1011 | loss: 0.22351528704166412\n",
      "Epoch [1/1] | Batch 1012 | loss: 0.03820594772696495\n",
      "Epoch [1/1] | Batch 1013 | loss: 0.059301331639289856\n",
      "Epoch [1/1] | Batch 1014 | loss: 0.5015894174575806\n",
      "Epoch [1/1] | Batch 1015 | loss: 0.2398410588502884\n",
      "Epoch [1/1] | Batch 1016 | loss: 0.40911027789115906\n",
      "Epoch [1/1] | Batch 1017 | loss: 0.48594915866851807\n",
      "Epoch [1/1] | Batch 1018 | loss: 0.34400513768196106\n",
      "Epoch [1/1] | Batch 1019 | loss: 0.33302658796310425\n",
      "Epoch [1/1] | Batch 1020 | loss: 0.6784076690673828\n",
      "Epoch [1/1] | Batch 1021 | loss: 0.3076556622982025\n",
      "Epoch [1/1] | Batch 1022 | loss: 0.3480771481990814\n",
      "Epoch [1/1] | Batch 1023 | loss: 0.25808185338974\n",
      "Epoch [1/1] | Batch 1024 | loss: 0.16461318731307983\n",
      "Epoch [1/1] | Batch 1025 | loss: 0.06596311926841736\n",
      "Epoch [1/1] | Batch 1026 | loss: 0.34409409761428833\n",
      "Epoch [1/1] | Batch 1027 | loss: 0.23693689703941345\n",
      "Epoch [1/1] | Batch 1028 | loss: 0.3316161632537842\n",
      "Epoch [1/1] | Batch 1029 | loss: 0.08689745515584946\n",
      "Epoch [1/1] | Batch 1030 | loss: 0.7603010535240173\n",
      "Epoch [1/1] | Batch 1031 | loss: 0.6131805777549744\n",
      "Epoch [1/1] | Batch 1032 | loss: 0.0793832391500473\n",
      "Epoch [1/1] | Batch 1033 | loss: 0.035585977137088776\n",
      "Epoch [1/1] | Batch 1034 | loss: 0.06816114485263824\n",
      "Epoch [1/1] | Batch 1035 | loss: 0.46632641553878784\n",
      "Epoch [1/1] | Batch 1036 | loss: 0.008501802571117878\n",
      "Epoch [1/1] | Batch 1037 | loss: 0.18512119352817535\n",
      "Epoch [1/1] | Batch 1038 | loss: 0.1158546507358551\n",
      "Epoch [1/1] | Batch 1039 | loss: 0.08744792640209198\n",
      "Epoch [1/1] | Batch 1040 | loss: 0.6102308034896851\n",
      "Epoch [1/1] | Batch 1041 | loss: 0.10776045173406601\n",
      "Epoch [1/1] | Batch 1042 | loss: 0.008998713456094265\n",
      "Epoch [1/1] | Batch 1043 | loss: 0.5708904266357422\n",
      "Epoch [1/1] | Batch 1044 | loss: 0.08516645431518555\n",
      "Epoch [1/1] | Batch 1045 | loss: 0.14497365057468414\n",
      "Epoch [1/1] | Batch 1046 | loss: 0.38349196314811707\n",
      "Epoch [1/1] | Batch 1047 | loss: 0.08015470206737518\n",
      "Epoch [1/1] | Batch 1048 | loss: 0.06768884509801865\n",
      "Epoch [1/1] | Batch 1049 | loss: 0.011253672651946545\n",
      "Epoch [1/1] | Batch 1050 | loss: 0.0047581433318555355\n",
      "Epoch [1/1] | Batch 1051 | loss: 0.21798741817474365\n",
      "Epoch [1/1] | Batch 1052 | loss: 0.40726467967033386\n",
      "Epoch [1/1] | Batch 1053 | loss: 0.34934383630752563\n",
      "Epoch [1/1] | Batch 1054 | loss: 0.4868185818195343\n",
      "Epoch [1/1] | Batch 1055 | loss: 0.06167749688029289\n",
      "Epoch [1/1] | Batch 1056 | loss: 0.12850362062454224\n",
      "Epoch [1/1] | Batch 1057 | loss: 0.14149755239486694\n",
      "Epoch [1/1] | Batch 1058 | loss: 0.24228022992610931\n",
      "Epoch [1/1] | Batch 1059 | loss: 0.4273627698421478\n",
      "Epoch [1/1] | Batch 1060 | loss: 0.0355597548186779\n",
      "Epoch [1/1] | Batch 1061 | loss: 0.5644583702087402\n",
      "Epoch [1/1] | Batch 1062 | loss: 0.0412222258746624\n",
      "Epoch [1/1] | Batch 1063 | loss: 0.09131449460983276\n",
      "Epoch [1/1] | Batch 1064 | loss: 0.10618975013494492\n",
      "Epoch [1/1] | Batch 1065 | loss: 0.10653997212648392\n",
      "Epoch [1/1] | Batch 1066 | loss: 0.18560059368610382\n",
      "Epoch [1/1] | Batch 1067 | loss: 0.12805329263210297\n",
      "Epoch [1/1] | Batch 1068 | loss: 0.26646894216537476\n",
      "Epoch [1/1] | Batch 1069 | loss: 0.5096573233604431\n",
      "Epoch [1/1] | Batch 1070 | loss: 0.13578486442565918\n",
      "Epoch [1/1] | Batch 1071 | loss: 0.09908560663461685\n",
      "Epoch [1/1] | Batch 1072 | loss: 0.01755131036043167\n",
      "Epoch [1/1] | Batch 1073 | loss: 0.060118287801742554\n",
      "Epoch [1/1] | Batch 1074 | loss: 0.08869811147451401\n",
      "Epoch [1/1] | Batch 1075 | loss: 0.22934913635253906\n",
      "Epoch [1/1] | Batch 1076 | loss: 0.17042164504528046\n",
      "Epoch [1/1] | Batch 1077 | loss: 0.08684610575437546\n",
      "Epoch [1/1] | Batch 1078 | loss: 0.2460763305425644\n",
      "Epoch [1/1] | Batch 1079 | loss: 0.11398624628782272\n",
      "Epoch [1/1] | Batch 1080 | loss: 0.26399096846580505\n",
      "Epoch [1/1] | Batch 1081 | loss: 0.07759266346693039\n",
      "Epoch [1/1] | Batch 1082 | loss: 0.03301377221941948\n",
      "Epoch [1/1] | Batch 1083 | loss: 0.1584031879901886\n",
      "Epoch [1/1] | Batch 1084 | loss: 0.15252457559108734\n",
      "Epoch [1/1] | Batch 1085 | loss: 0.09926982969045639\n",
      "Epoch [1/1] | Batch 1086 | loss: 0.05123591795563698\n",
      "Epoch [1/1] | Batch 1087 | loss: 0.06837955862283707\n",
      "Epoch [1/1] | Batch 1088 | loss: 0.022399285808205605\n",
      "Epoch [1/1] | Batch 1089 | loss: 0.06812742352485657\n",
      "Epoch [1/1] | Batch 1090 | loss: 0.17819426953792572\n",
      "Epoch [1/1] | Batch 1091 | loss: 0.014882422983646393\n",
      "Epoch [1/1] | Batch 1092 | loss: 0.4571630656719208\n",
      "Epoch [1/1] | Batch 1093 | loss: 0.3189954459667206\n",
      "Epoch [1/1] | Batch 1094 | loss: 0.12867289781570435\n",
      "Epoch [1/1] | Batch 1095 | loss: 0.152170792222023\n",
      "Epoch [1/1] | Batch 1096 | loss: 0.05674726516008377\n",
      "Epoch [1/1] | Batch 1097 | loss: 0.05607437342405319\n",
      "Epoch [1/1] | Batch 1098 | loss: 0.06214343011379242\n",
      "Epoch [1/1] | Batch 1099 | loss: 0.010676952078938484\n",
      "Epoch [1/1] | Batch 1100 | loss: 0.35368049144744873\n",
      "Epoch [1/1] | Batch 1101 | loss: 0.19219821691513062\n",
      "Epoch [1/1] | Batch 1102 | loss: 0.03905267268419266\n",
      "Epoch [1/1] | Batch 1103 | loss: 0.03073487989604473\n",
      "Epoch [1/1] | Batch 1104 | loss: 0.23369528353214264\n",
      "Epoch [1/1] | Batch 1105 | loss: 0.5355210900306702\n",
      "Epoch [1/1] | Batch 1106 | loss: 0.35782164335250854\n",
      "Epoch [1/1] | Batch 1107 | loss: 0.0725366547703743\n",
      "Epoch [1/1] | Batch 1108 | loss: 0.3090652525424957\n",
      "Epoch [1/1] | Batch 1109 | loss: 0.06273268908262253\n",
      "Epoch [1/1] | Batch 1110 | loss: 0.3222421109676361\n",
      "Epoch [1/1] | Batch 1111 | loss: 0.3633827865123749\n",
      "Epoch [1/1] | Batch 1112 | loss: 0.5876879692077637\n",
      "Epoch [1/1] | Batch 1113 | loss: 0.035200174897909164\n",
      "Epoch [1/1] | Batch 1114 | loss: 0.13754308223724365\n",
      "Epoch [1/1] | Batch 1115 | loss: 0.10828812420368195\n",
      "Epoch [1/1] | Batch 1116 | loss: 0.13924019038677216\n",
      "Epoch [1/1] | Batch 1117 | loss: 0.3412550091743469\n",
      "Epoch [1/1] | Batch 1118 | loss: 0.14522682130336761\n",
      "Epoch [1/1] | Batch 1119 | loss: 0.06098271906375885\n",
      "Epoch [1/1] | Batch 1120 | loss: 0.17228886485099792\n",
      "Epoch [1/1] | Batch 1121 | loss: 0.0358307808637619\n",
      "Epoch [1/1] | Batch 1122 | loss: 0.030227988958358765\n",
      "Epoch [1/1] | Batch 1123 | loss: 0.029711881652474403\n",
      "Epoch [1/1] | Batch 1124 | loss: 0.28483298420906067\n",
      "Epoch [1/1] | Batch 1125 | loss: 0.08564020693302155\n",
      "Epoch [1/1] | Batch 1126 | loss: 0.023599039763212204\n",
      "Epoch [1/1] | Batch 1127 | loss: 0.08984778076410294\n",
      "Epoch [1/1] | Batch 1128 | loss: 0.013717408291995525\n",
      "Epoch [1/1] | Batch 1129 | loss: 0.21250896155834198\n",
      "Epoch [1/1] | Batch 1130 | loss: 0.29274073243141174\n",
      "Epoch [1/1] | Batch 1131 | loss: 0.2540242075920105\n",
      "Epoch [1/1] | Batch 1132 | loss: 0.5627002716064453\n",
      "Epoch [1/1] | Batch 1133 | loss: 0.21670782566070557\n",
      "Epoch [1/1] | Batch 1134 | loss: 0.532171905040741\n",
      "Epoch [1/1] | Batch 1135 | loss: 0.40088826417922974\n",
      "Epoch [1/1] | Batch 1136 | loss: 0.035498280078172684\n",
      "Epoch [1/1] | Batch 1137 | loss: 0.5660674571990967\n",
      "Epoch [1/1] | Batch 1138 | loss: 0.7741113305091858\n",
      "Epoch [1/1] | Batch 1139 | loss: 0.20889025926589966\n",
      "Epoch [1/1] | Batch 1140 | loss: 0.30589550733566284\n",
      "Epoch [1/1] | Batch 1141 | loss: 0.2375442087650299\n",
      "Epoch [1/1] | Batch 1142 | loss: 0.02392229624092579\n",
      "Epoch [1/1] | Batch 1143 | loss: 0.033507201820611954\n",
      "Epoch [1/1] | Batch 1144 | loss: 0.04669687896966934\n",
      "Epoch [1/1] | Batch 1145 | loss: 0.15366092324256897\n",
      "Epoch [1/1] | Batch 1146 | loss: 0.09009112417697906\n",
      "Epoch [1/1] | Batch 1147 | loss: 0.10806053876876831\n",
      "Epoch [1/1] | Batch 1148 | loss: 0.320819228887558\n",
      "Epoch [1/1] | Batch 1149 | loss: 0.035991866141557693\n",
      "Epoch [1/1] | Batch 1150 | loss: 0.2445850968360901\n",
      "Epoch [1/1] | Batch 1151 | loss: 0.5059464573860168\n",
      "Epoch [1/1] | Batch 1152 | loss: 0.30320554971694946\n",
      "Epoch [1/1] | Batch 1153 | loss: 0.1413348764181137\n",
      "Epoch [1/1] | Batch 1154 | loss: 0.016818633303046227\n",
      "Epoch [1/1] | Batch 1155 | loss: 0.10996141284704208\n",
      "Epoch [1/1] | Batch 1156 | loss: 0.006945958826690912\n",
      "Epoch [1/1] | Batch 1157 | loss: 0.0212225504219532\n",
      "Epoch [1/1] | Batch 1158 | loss: 0.12580405175685883\n",
      "Epoch [1/1] | Batch 1159 | loss: 0.019710831344127655\n",
      "Epoch [1/1] | Batch 1160 | loss: 0.0337861031293869\n",
      "Epoch [1/1] | Batch 1161 | loss: 0.13133712112903595\n",
      "Epoch [1/1] | Batch 1162 | loss: 0.10620986670255661\n",
      "Epoch [1/1] | Batch 1163 | loss: 0.09252665191888809\n",
      "Epoch [1/1] | Batch 1164 | loss: 0.3216601610183716\n",
      "Epoch [1/1] | Batch 1165 | loss: 0.15378139913082123\n",
      "Epoch [1/1] | Batch 1166 | loss: 0.2543063759803772\n",
      "Epoch [1/1] | Batch 1167 | loss: 0.08555111289024353\n",
      "Epoch [1/1] | Batch 1168 | loss: 0.39235812425613403\n",
      "Epoch [1/1] | Batch 1169 | loss: 0.026923788711428642\n",
      "Epoch [1/1] | Batch 1170 | loss: 0.4924008250236511\n",
      "Epoch [1/1] | Batch 1171 | loss: 0.017929820343852043\n",
      "Epoch [1/1] | Batch 1172 | loss: 0.10910653322935104\n",
      "Epoch [1/1] | Batch 1173 | loss: 0.18904700875282288\n",
      "Epoch [1/1] | Batch 1174 | loss: 0.15373989939689636\n",
      "Epoch [1/1] | Batch 1175 | loss: 0.05716491490602493\n",
      "Epoch [1/1] | Batch 1176 | loss: 0.2989429831504822\n",
      "Epoch [1/1] | Batch 1177 | loss: 0.14678159356117249\n",
      "Epoch [1/1] | Batch 1178 | loss: 0.4315347373485565\n",
      "Epoch [1/1] | Batch 1179 | loss: 0.32975277304649353\n",
      "Epoch [1/1] | Batch 1180 | loss: 0.09810198843479156\n",
      "Epoch [1/1] | Batch 1181 | loss: 0.010188068263232708\n",
      "Epoch [1/1] | Batch 1182 | loss: 0.14402170479297638\n",
      "Epoch [1/1] | Batch 1183 | loss: 0.12444260716438293\n",
      "Epoch [1/1] | Batch 1184 | loss: 0.5511761903762817\n",
      "Epoch [1/1] | Batch 1185 | loss: 0.3005583584308624\n",
      "Epoch [1/1] | Batch 1186 | loss: 0.1398124396800995\n",
      "Epoch [1/1] | Batch 1187 | loss: 0.06219363957643509\n",
      "Epoch [1/1] | Batch 1188 | loss: 0.36757734417915344\n",
      "Epoch [1/1] | Batch 1189 | loss: 0.13868962228298187\n",
      "Epoch [1/1] | Batch 1190 | loss: 0.03296757861971855\n",
      "Epoch [1/1] | Batch 1191 | loss: 0.16212554275989532\n",
      "Epoch [1/1] | Batch 1192 | loss: 0.0321243479847908\n",
      "Epoch [1/1] | Batch 1193 | loss: 0.21424105763435364\n",
      "Epoch [1/1] | Batch 1194 | loss: 0.40431585907936096\n",
      "Epoch [1/1] | Batch 1195 | loss: 0.04426255449652672\n",
      "Epoch [1/1] | Batch 1196 | loss: 0.08753405511379242\n",
      "Epoch [1/1] | Batch 1197 | loss: 0.5464112162590027\n",
      "Epoch [1/1] | Batch 1198 | loss: 0.09370992332696915\n",
      "Epoch [1/1] | Batch 1199 | loss: 0.06323657929897308\n",
      "Epoch [1/1] | Batch 1200 | loss: 0.2501125633716583\n",
      "Epoch [1/1] | Batch 1201 | loss: 0.19623176753520966\n",
      "Epoch [1/1] | Batch 1202 | loss: 0.4683343172073364\n",
      "Epoch [1/1] | Batch 1203 | loss: 0.14877600967884064\n",
      "Epoch [1/1] | Batch 1204 | loss: 0.3486815392971039\n",
      "Epoch [1/1] | Batch 1205 | loss: 0.08869235217571259\n",
      "Epoch [1/1] | Batch 1206 | loss: 0.35407981276512146\n",
      "Epoch [1/1] | Batch 1207 | loss: 0.5407653450965881\n",
      "Epoch [1/1] | Batch 1208 | loss: 0.3476618230342865\n",
      "Epoch [1/1] | Batch 1209 | loss: 0.05579300597310066\n",
      "Epoch [1/1] | Batch 1210 | loss: 0.048747554421424866\n",
      "Epoch [1/1] | Batch 1211 | loss: 0.16090142726898193\n",
      "Epoch [1/1] | Batch 1212 | loss: 0.12761957943439484\n",
      "Epoch [1/1] | Batch 1213 | loss: 0.4605928361415863\n",
      "Epoch [1/1] | Batch 1214 | loss: 0.5521780848503113\n",
      "Epoch [1/1] | Batch 1215 | loss: 0.08184358477592468\n",
      "Epoch [1/1] | Batch 1216 | loss: 0.04012434557080269\n",
      "Epoch [1/1] | Batch 1217 | loss: 0.056392110884189606\n",
      "Epoch [1/1] | Batch 1218 | loss: 0.0348198376595974\n",
      "Epoch [1/1] | Batch 1219 | loss: 0.048541322350502014\n",
      "Epoch [1/1] | Batch 1220 | loss: 0.3839631974697113\n",
      "Epoch [1/1] | Batch 1221 | loss: 0.5584166646003723\n",
      "Epoch [1/1] | Batch 1222 | loss: 0.14035359025001526\n",
      "Epoch [1/1] | Batch 1223 | loss: 0.02712254598736763\n",
      "Epoch [1/1] | Batch 1224 | loss: 0.12253232300281525\n",
      "Epoch [1/1] | Batch 1225 | loss: 0.1383652240037918\n",
      "Epoch [1/1] | Batch 1226 | loss: 0.04596756771206856\n",
      "Epoch [1/1] | Batch 1227 | loss: 0.12997879087924957\n",
      "Epoch [1/1] | Batch 1228 | loss: 0.03614996001124382\n",
      "Epoch [1/1] | Batch 1229 | loss: 0.21521377563476562\n",
      "Epoch [1/1] | Batch 1230 | loss: 0.1810915768146515\n",
      "Epoch [1/1] | Batch 1231 | loss: 0.24523340165615082\n",
      "Epoch [1/1] | Batch 1232 | loss: 0.11262660473585129\n",
      "Epoch [1/1] | Batch 1233 | loss: 0.2224845439195633\n",
      "Epoch [1/1] | Batch 1234 | loss: 0.053015176206827164\n",
      "Epoch [1/1] | Batch 1235 | loss: 0.0230637788772583\n",
      "Epoch [1/1] | Batch 1236 | loss: 0.021553145721554756\n",
      "Epoch [1/1] | Batch 1237 | loss: 0.00868357066065073\n",
      "Epoch [1/1] | Batch 1238 | loss: 0.03988341614603996\n",
      "Epoch [1/1] | Batch 1239 | loss: 0.21862153708934784\n",
      "Epoch [1/1] | Batch 1240 | loss: 0.14625556766986847\n",
      "Epoch [1/1] | Batch 1241 | loss: 0.05408055707812309\n",
      "Epoch [1/1] | Batch 1242 | loss: 0.12241557240486145\n",
      "Epoch [1/1] | Batch 1243 | loss: 0.19189800322055817\n",
      "Epoch [1/1] | Batch 1244 | loss: 0.1390899419784546\n",
      "Epoch [1/1] | Batch 1245 | loss: 0.039285361766815186\n",
      "Epoch [1/1] | Batch 1246 | loss: 0.2208816111087799\n",
      "Epoch [1/1] | Batch 1247 | loss: 0.17068667709827423\n",
      "Epoch [1/1] | Batch 1248 | loss: 0.29166796803474426\n",
      "Epoch [1/1] | Batch 1249 | loss: 0.22379851341247559\n",
      "Epoch [1/1] | Batch 1250 | loss: 0.24507823586463928\n",
      "Epoch [1/1] | Batch 1251 | loss: 0.5956157445907593\n",
      "Epoch [1/1] | Batch 1252 | loss: 0.027237609028816223\n",
      "Epoch [1/1] | Batch 1253 | loss: 0.10632220655679703\n",
      "Epoch [1/1] | Batch 1254 | loss: 0.2062508761882782\n",
      "Epoch [1/1] | Batch 1255 | loss: 0.4126262366771698\n",
      "Epoch [1/1] | Batch 1256 | loss: 0.23429307341575623\n",
      "Epoch [1/1] | Batch 1257 | loss: 0.14747948944568634\n",
      "Epoch [1/1] | Batch 1258 | loss: 0.28008803725242615\n",
      "Epoch [1/1] | Batch 1259 | loss: 0.13089491426944733\n",
      "Epoch [1/1] | Batch 1260 | loss: 0.08080728352069855\n",
      "Epoch [1/1] | Batch 1261 | loss: 0.38378649950027466\n",
      "Epoch [1/1] | Batch 1262 | loss: 0.7500793933868408\n",
      "Epoch [1/1] | Batch 1263 | loss: 0.21870367228984833\n",
      "Epoch [1/1] | Batch 1264 | loss: 1.3982855081558228\n",
      "Epoch [1/1] | Batch 1265 | loss: 0.23732195794582367\n",
      "Epoch [1/1] | Batch 1266 | loss: 0.4411265254020691\n",
      "Epoch [1/1] | Batch 1267 | loss: 0.20141643285751343\n",
      "Epoch [1/1] | Batch 1268 | loss: 0.09159437566995621\n",
      "Epoch [1/1] | Batch 1269 | loss: 0.15636688470840454\n",
      "Epoch [1/1] | Batch 1270 | loss: 0.5804146528244019\n",
      "Epoch [1/1] | Batch 1271 | loss: 0.027820587158203125\n",
      "Epoch [1/1] | Batch 1272 | loss: 0.32495877146720886\n",
      "Epoch [1/1] | Batch 1273 | loss: 0.12045370042324066\n",
      "Epoch [1/1] | Batch 1274 | loss: 0.043805524706840515\n",
      "Epoch [1/1] | Batch 1275 | loss: 0.2844492197036743\n",
      "Epoch [1/1] | Batch 1276 | loss: 0.054864898324012756\n",
      "Epoch [1/1] | Batch 1277 | loss: 0.9726097583770752\n",
      "Epoch [1/1] | Batch 1278 | loss: 0.14675545692443848\n",
      "Epoch [1/1] | Batch 1279 | loss: 0.6442736387252808\n",
      "Epoch [1/1] | Batch 1280 | loss: 0.18245181441307068\n",
      "Epoch [1/1] | Batch 1281 | loss: 0.03248458728194237\n",
      "Epoch [1/1] | Batch 1282 | loss: 0.39240384101867676\n",
      "Epoch [1/1] | Batch 1283 | loss: 0.04552535340189934\n",
      "Epoch [1/1] | Batch 1284 | loss: 0.06508269906044006\n",
      "Epoch [1/1] | Batch 1285 | loss: 0.22937431931495667\n",
      "Epoch [1/1] | Batch 1286 | loss: 0.08330422639846802\n",
      "Epoch [1/1] | Batch 1287 | loss: 0.034799329936504364\n",
      "Epoch [1/1] | Batch 1288 | loss: 0.018542995676398277\n",
      "Epoch [1/1] | Batch 1289 | loss: 0.3935312330722809\n",
      "Epoch [1/1] | Batch 1290 | loss: 0.05493271350860596\n",
      "Epoch [1/1] | Batch 1291 | loss: 0.3725389838218689\n",
      "Epoch [1/1] | Batch 1292 | loss: 0.003572477027773857\n",
      "Epoch [1/1] | Batch 1293 | loss: 0.40060728788375854\n",
      "Epoch [1/1] | Batch 1294 | loss: 0.015580644831061363\n",
      "Epoch [1/1] | Batch 1295 | loss: 0.1952158808708191\n",
      "Epoch [1/1] | Batch 1296 | loss: 0.30268222093582153\n",
      "Epoch [1/1] | Batch 1297 | loss: 0.24548251926898956\n",
      "Epoch [1/1] | Batch 1298 | loss: 0.03951731324195862\n",
      "Epoch [1/1] | Batch 1299 | loss: 0.028607646003365517\n",
      "Epoch [1/1] | Batch 1300 | loss: 0.3875540792942047\n",
      "Epoch [1/1] | Batch 1301 | loss: 0.5537172555923462\n",
      "Epoch [1/1] | Batch 1302 | loss: 0.13845095038414001\n",
      "Epoch [1/1] | Batch 1303 | loss: 0.5554134845733643\n",
      "Epoch [1/1] | Batch 1304 | loss: 0.1321209967136383\n",
      "Epoch [1/1] | Batch 1305 | loss: 0.2816435396671295\n",
      "Epoch [1/1] | Batch 1306 | loss: 0.018985293805599213\n",
      "Epoch [1/1] | Batch 1307 | loss: 0.07248912751674652\n",
      "Epoch [1/1] | Batch 1308 | loss: 0.08873963356018066\n",
      "Epoch [1/1] | Batch 1309 | loss: 0.2777068018913269\n",
      "Epoch [1/1] | Batch 1310 | loss: 0.3016625642776489\n",
      "Epoch [1/1] | Batch 1311 | loss: 0.18620312213897705\n",
      "Epoch [1/1] | Batch 1312 | loss: 0.021621961146593094\n",
      "Epoch [1/1] | Batch 1313 | loss: 0.35173648595809937\n",
      "Epoch [1/1] | Batch 1314 | loss: 0.023418035358190536\n",
      "Epoch [1/1] | Batch 1315 | loss: 0.034785594791173935\n",
      "Epoch [1/1] | Batch 1316 | loss: 0.2083742618560791\n",
      "Epoch [1/1] | Batch 1317 | loss: 0.06230435520410538\n",
      "Epoch [1/1] | Batch 1318 | loss: 0.1357349008321762\n",
      "Epoch [1/1] | Batch 1319 | loss: 0.3626576066017151\n",
      "Epoch [1/1] | Batch 1320 | loss: 0.025595648214221\n",
      "Epoch [1/1] | Batch 1321 | loss: 0.01401602104306221\n",
      "Epoch [1/1] | Batch 1322 | loss: 0.07840906083583832\n",
      "Epoch [1/1] | Batch 1323 | loss: 0.36550962924957275\n",
      "Epoch [1/1] | Batch 1324 | loss: 0.02356642670929432\n",
      "Epoch [1/1] | Batch 1325 | loss: 0.4078995883464813\n",
      "Epoch [1/1] | Batch 1326 | loss: 0.116874098777771\n",
      "Epoch [1/1] | Batch 1327 | loss: 0.5147198438644409\n",
      "Epoch [1/1] | Batch 1328 | loss: 0.40709421038627625\n",
      "Epoch [1/1] | Batch 1329 | loss: 0.030710803344845772\n",
      "Epoch [1/1] | Batch 1330 | loss: 0.3716977536678314\n",
      "Epoch [1/1] | Batch 1331 | loss: 0.11090162396430969\n",
      "Epoch [1/1] | Batch 1332 | loss: 0.5313442349433899\n",
      "Epoch [1/1] | Batch 1333 | loss: 0.164932981133461\n",
      "Epoch [1/1] | Batch 1334 | loss: 0.019851408898830414\n",
      "Epoch [1/1] | Batch 1335 | loss: 0.49467986822128296\n",
      "Epoch [1/1] | Batch 1336 | loss: 0.29835110902786255\n",
      "Epoch [1/1] | Batch 1337 | loss: 0.1705678552389145\n",
      "Epoch [1/1] | Batch 1338 | loss: 0.6230335831642151\n",
      "Epoch [1/1] | Batch 1339 | loss: 0.20277009904384613\n",
      "Epoch [1/1] | Batch 1340 | loss: 0.10226794332265854\n",
      "Epoch [1/1] | Batch 1341 | loss: 0.07502595335245132\n",
      "Epoch [1/1] | Batch 1342 | loss: 0.021477794274687767\n",
      "Epoch [1/1] | Batch 1343 | loss: 0.10223820805549622\n",
      "Epoch [1/1] | Batch 1344 | loss: 0.1501244455575943\n",
      "Epoch [1/1] | Batch 1345 | loss: 0.07270294427871704\n",
      "Epoch [1/1] | Batch 1346 | loss: 0.015531371347606182\n",
      "Epoch [1/1] | Batch 1347 | loss: 0.37630027532577515\n",
      "Epoch [1/1] | Batch 1348 | loss: 0.069522425532341\n",
      "Epoch [1/1] | Batch 1349 | loss: 0.07895076274871826\n",
      "Epoch [1/1] | Batch 1350 | loss: 0.10435611009597778\n",
      "Epoch [1/1] | Batch 1351 | loss: 0.032866112887859344\n",
      "Epoch [1/1] | Batch 1352 | loss: 0.12414161115884781\n",
      "Epoch [1/1] | Batch 1353 | loss: 0.4709765315055847\n",
      "Epoch [1/1] | Batch 1354 | loss: 0.4679821729660034\n",
      "Epoch [1/1] | Batch 1355 | loss: 0.08314307779073715\n",
      "Epoch [1/1] | Batch 1356 | loss: 0.09362676739692688\n",
      "Epoch [1/1] | Batch 1357 | loss: 0.11147170513868332\n",
      "Epoch [1/1] | Batch 1358 | loss: 0.09458060562610626\n",
      "Epoch [1/1] | Batch 1359 | loss: 0.17168891429901123\n",
      "Epoch [1/1] | Batch 1360 | loss: 0.22073180973529816\n",
      "Epoch [1/1] | Batch 1361 | loss: 0.17607803642749786\n",
      "Epoch [1/1] | Batch 1362 | loss: 0.493114173412323\n",
      "Epoch [1/1] | Batch 1363 | loss: 0.016069548204541206\n",
      "Epoch [1/1] | Batch 1364 | loss: 0.3024330735206604\n",
      "Epoch [1/1] | Batch 1365 | loss: 0.0931059867143631\n",
      "Epoch [1/1] | Batch 1366 | loss: 0.07007019221782684\n",
      "Epoch [1/1] | Batch 1367 | loss: 0.17782053351402283\n",
      "Epoch [1/1] | Batch 1368 | loss: 0.03820451349020004\n",
      "Epoch [1/1] | Batch 1369 | loss: 0.017209161072969437\n",
      "Epoch [1/1] | Batch 1370 | loss: 0.050359245389699936\n",
      "Epoch [1/1] | Batch 1371 | loss: 0.2136879414319992\n",
      "Epoch [1/1] | Batch 1372 | loss: 0.02814101241528988\n",
      "Epoch [1/1] | Batch 1373 | loss: 0.48258504271507263\n",
      "Epoch [1/1] | Batch 1374 | loss: 0.12286844104528427\n",
      "Epoch [1/1] | Batch 1375 | loss: 0.05283137038350105\n",
      "Epoch [1/1] | Batch 1376 | loss: 0.19267605245113373\n",
      "Epoch [1/1] | Batch 1377 | loss: 0.09761367738246918\n",
      "Epoch [1/1] | Batch 1378 | loss: 0.09121675789356232\n",
      "Epoch [1/1] | Batch 1379 | loss: 0.04321155324578285\n",
      "Epoch [1/1] | Batch 1380 | loss: 0.04902390390634537\n",
      "Epoch [1/1] | Batch 1381 | loss: 0.4310302436351776\n",
      "Epoch [1/1] | Batch 1382 | loss: 0.27451175451278687\n",
      "Epoch [1/1] | Batch 1383 | loss: 0.09888411313295364\n",
      "Epoch [1/1] | Batch 1384 | loss: 0.09464488923549652\n",
      "Epoch [1/1] | Batch 1385 | loss: 0.24030135571956635\n",
      "Epoch [1/1] | Batch 1386 | loss: 0.16647076606750488\n",
      "Epoch [1/1] | Batch 1387 | loss: 0.03595881164073944\n",
      "Epoch [1/1] | Batch 1388 | loss: 0.6016028523445129\n",
      "Epoch [1/1] | Batch 1389 | loss: 0.043044351041316986\n",
      "Epoch [1/1] | Batch 1390 | loss: 0.1256435066461563\n",
      "Epoch [1/1] | Batch 1391 | loss: 0.0790826752781868\n",
      "Epoch [1/1] | Batch 1392 | loss: 0.011503295972943306\n",
      "Epoch [1/1] | Batch 1393 | loss: 0.03173261135816574\n",
      "Epoch [1/1] | Batch 1394 | loss: 0.13330048322677612\n",
      "Epoch [1/1] | Batch 1395 | loss: 0.1400056630373001\n",
      "Epoch [1/1] | Batch 1396 | loss: 0.010638807900249958\n",
      "Epoch [1/1] | Batch 1397 | loss: 0.3645252585411072\n",
      "Epoch [1/1] | Batch 1398 | loss: 0.2904053032398224\n",
      "Epoch [1/1] | Batch 1399 | loss: 0.22768977284431458\n",
      "Epoch [1/1] | Batch 1400 | loss: 0.1480110138654709\n",
      "Epoch [1/1] | Batch 1401 | loss: 0.7492731213569641\n",
      "Epoch [1/1] | Batch 1402 | loss: 0.07318738102912903\n",
      "Epoch [1/1] | Batch 1403 | loss: 0.22112822532653809\n",
      "Epoch [1/1] | Batch 1404 | loss: 0.10371597856283188\n",
      "Epoch [1/1] | Batch 1405 | loss: 0.1669275164604187\n",
      "Epoch [1/1] | Batch 1406 | loss: 0.05321322754025459\n",
      "Epoch [1/1] | Batch 1407 | loss: 0.377140074968338\n",
      "Epoch [1/1] | Batch 1408 | loss: 0.22580067813396454\n",
      "Epoch [1/1] | Batch 1409 | loss: 0.09003018587827682\n",
      "Epoch [1/1] | Batch 1410 | loss: 0.09392397105693817\n",
      "Epoch [1/1] | Batch 1411 | loss: 0.0077347480691969395\n",
      "Epoch [1/1] | Batch 1412 | loss: 0.12615124881267548\n",
      "Epoch [1/1] | Batch 1413 | loss: 0.1959463357925415\n",
      "Epoch [1/1] | Batch 1414 | loss: 0.010743438266217709\n",
      "Epoch [1/1] | Batch 1415 | loss: 0.023852618411183357\n",
      "Epoch [1/1] | Batch 1416 | loss: 0.1325652301311493\n",
      "Epoch [1/1] | Batch 1417 | loss: 0.10539258271455765\n",
      "Epoch [1/1] | Batch 1418 | loss: 0.02215145342051983\n",
      "Epoch [1/1] | Batch 1419 | loss: 0.21497824788093567\n",
      "Epoch [1/1] | Batch 1420 | loss: 0.39715710282325745\n",
      "Epoch [1/1] | Batch 1421 | loss: 0.0875016525387764\n",
      "Epoch [1/1] | Batch 1422 | loss: 0.019664239138364792\n",
      "Epoch [1/1] | Batch 1423 | loss: 0.22228510677814484\n",
      "Epoch [1/1] | Batch 1424 | loss: 0.15558309853076935\n",
      "Epoch [1/1] | Batch 1425 | loss: 0.005316429305821657\n",
      "Epoch [1/1] | Batch 1426 | loss: 0.20927490293979645\n",
      "Epoch [1/1] | Batch 1427 | loss: 0.020047804340720177\n",
      "Epoch [1/1] | Batch 1428 | loss: 0.2532212436199188\n",
      "Epoch [1/1] | Batch 1429 | loss: 0.10710971802473068\n",
      "Epoch [1/1] | Batch 1430 | loss: 0.7204689383506775\n",
      "Epoch [1/1] | Batch 1431 | loss: 0.17016538977622986\n",
      "Epoch [1/1] | Batch 1432 | loss: 0.025041043758392334\n",
      "Epoch [1/1] | Batch 1433 | loss: 0.04281222075223923\n",
      "Epoch [1/1] | Batch 1434 | loss: 0.4633796215057373\n",
      "Epoch [1/1] | Batch 1435 | loss: 0.15056836605072021\n",
      "Epoch [1/1] | Batch 1436 | loss: 0.13605526089668274\n",
      "Epoch [1/1] | Batch 1437 | loss: 0.023929238319396973\n",
      "Epoch [1/1] | Batch 1438 | loss: 0.027184393256902695\n",
      "Epoch [1/1] | Batch 1439 | loss: 0.08172427117824554\n",
      "Epoch [1/1] | Batch 1440 | loss: 0.030620556324720383\n",
      "Epoch [1/1] | Batch 1441 | loss: 0.3259516656398773\n",
      "Epoch [1/1] | Batch 1442 | loss: 0.058060966432094574\n",
      "Epoch [1/1] | Batch 1443 | loss: 0.021385300904512405\n",
      "Epoch [1/1] | Batch 1444 | loss: 0.010719948448240757\n",
      "Epoch [1/1] | Batch 1445 | loss: 0.051821500062942505\n",
      "Epoch [1/1] | Batch 1446 | loss: 0.03213426470756531\n",
      "Epoch [1/1] | Batch 1447 | loss: 0.03755035623908043\n",
      "Epoch [1/1] | Batch 1448 | loss: 0.5080325603485107\n",
      "Epoch [1/1] | Batch 1449 | loss: 0.024490902200341225\n",
      "Epoch [1/1] | Batch 1450 | loss: 0.45387357473373413\n",
      "Epoch [1/1] | Batch 1451 | loss: 0.5024139285087585\n",
      "Epoch [1/1] | Batch 1452 | loss: 0.10367170721292496\n",
      "Epoch [1/1] | Batch 1453 | loss: 0.022641966119408607\n",
      "Epoch [1/1] | Batch 1454 | loss: 0.1892554759979248\n",
      "Epoch [1/1] | Batch 1455 | loss: 0.4009288251399994\n",
      "Epoch [1/1] | Batch 1456 | loss: 0.33389121294021606\n",
      "Epoch [1/1] | Batch 1457 | loss: 0.00805177353322506\n",
      "Epoch [1/1] | Batch 1458 | loss: 0.04525912180542946\n",
      "Epoch [1/1] | Batch 1459 | loss: 0.138933002948761\n",
      "Epoch [1/1] | Batch 1460 | loss: 0.11463308334350586\n",
      "Epoch [1/1] | Batch 1461 | loss: 0.1354231834411621\n",
      "Epoch [1/1] | Batch 1462 | loss: 0.36229369044303894\n",
      "Epoch [1/1] | Batch 1463 | loss: 0.07523207366466522\n",
      "Epoch [1/1] | Batch 1464 | loss: 0.058816902339458466\n",
      "Epoch [1/1] | Batch 1465 | loss: 0.2757174074649811\n",
      "Epoch [1/1] | Batch 1466 | loss: 0.15799587965011597\n",
      "Epoch [1/1] | Batch 1467 | loss: 0.12447533011436462\n",
      "Epoch [1/1] | Batch 1468 | loss: 0.008409492671489716\n",
      "Epoch [1/1] | Batch 1469 | loss: 0.11005449295043945\n",
      "Epoch [1/1] | Batch 1470 | loss: 0.4853903353214264\n",
      "Epoch [1/1] | Batch 1471 | loss: 0.04341093450784683\n",
      "Epoch [1/1] | Batch 1472 | loss: 0.02547791227698326\n",
      "Epoch [1/1] | Batch 1473 | loss: 0.3669263422489166\n",
      "Epoch [1/1] | Batch 1474 | loss: 0.16695433855056763\n",
      "Epoch [1/1] | Batch 1475 | loss: 0.5104160308837891\n",
      "Epoch [1/1] | Batch 1476 | loss: 0.19880139827728271\n",
      "Epoch [1/1] | Batch 1477 | loss: 0.0377349779009819\n",
      "Epoch [1/1] | Batch 1478 | loss: 0.14945557713508606\n",
      "Epoch [1/1] | Batch 1479 | loss: 0.21205566823482513\n",
      "Epoch [1/1] | Batch 1480 | loss: 0.3559078872203827\n",
      "Epoch [1/1] | Batch 1481 | loss: 0.03673814237117767\n",
      "Epoch [1/1] | Batch 1482 | loss: 0.12100524455308914\n",
      "Epoch [1/1] | Batch 1483 | loss: 0.32352709770202637\n",
      "Epoch [1/1] | Batch 1484 | loss: 0.023763181641697884\n",
      "Epoch [1/1] | Batch 1485 | loss: 0.025288861244916916\n",
      "Epoch [1/1] | Batch 1486 | loss: 0.010476522147655487\n",
      "Epoch [1/1] | Batch 1487 | loss: 0.032633792608976364\n",
      "Epoch [1/1] | Batch 1488 | loss: 0.05697033554315567\n",
      "Epoch [1/1] | Batch 1489 | loss: 0.3485349118709564\n",
      "Epoch [1/1] | Batch 1490 | loss: 0.12614229321479797\n",
      "Epoch [1/1] | Batch 1491 | loss: 0.41088247299194336\n",
      "Epoch [1/1] | Batch 1492 | loss: 0.03629022464156151\n",
      "Epoch [1/1] | Batch 1493 | loss: 0.8365392088890076\n",
      "Epoch [1/1] | Batch 1494 | loss: 0.13457724452018738\n",
      "Epoch [1/1] | Batch 1495 | loss: 0.022181501612067223\n",
      "Epoch [1/1] | Batch 1496 | loss: 0.06109300255775452\n",
      "Epoch [1/1] | Batch 1497 | loss: 0.09402160346508026\n",
      "Epoch [1/1] | Batch 1498 | loss: 0.023468032479286194\n",
      "Epoch [1/1] | Batch 1499 | loss: 0.19507157802581787\n",
      "Epoch [1/1] | Batch 1500 | loss: 0.06652259826660156\n",
      "Epoch [1/1] | Batch 1501 | loss: 0.20254561305046082\n",
      "Epoch [1/1] | Batch 1502 | loss: 0.10160904377698898\n",
      "Epoch [1/1] | Batch 1503 | loss: 0.23731647431850433\n",
      "Epoch [1/1] | Batch 1504 | loss: 0.06542349606752396\n",
      "Epoch [1/1] | Batch 1505 | loss: 0.13535603880882263\n",
      "Epoch [1/1] | Batch 1506 | loss: 0.1991354376077652\n",
      "Epoch [1/1] | Batch 1507 | loss: 0.572282612323761\n",
      "Epoch [1/1] | Batch 1508 | loss: 0.279955118894577\n",
      "Epoch [1/1] | Batch 1509 | loss: 0.23846328258514404\n",
      "Epoch [1/1] | Batch 1510 | loss: 0.18878118693828583\n",
      "Epoch [1/1] | Batch 1511 | loss: 0.31659990549087524\n",
      "Epoch [1/1] | Batch 1512 | loss: 0.470822274684906\n",
      "Epoch [1/1] | Batch 1513 | loss: 0.06078171730041504\n",
      "Epoch [1/1] | Batch 1514 | loss: 0.13209787011146545\n",
      "Epoch [1/1] | Batch 1515 | loss: 0.5591027140617371\n",
      "Epoch [1/1] | Batch 1516 | loss: 0.08633161336183548\n",
      "Epoch [1/1] | Batch 1517 | loss: 0.22897446155548096\n",
      "Epoch [1/1] | Batch 1518 | loss: 0.3854754567146301\n",
      "Epoch [1/1] | Batch 1519 | loss: 0.09171029180288315\n",
      "Epoch [1/1] | Batch 1520 | loss: 0.005359807051718235\n",
      "Epoch [1/1] | Batch 1521 | loss: 0.5213388800621033\n",
      "Epoch [1/1] | Batch 1522 | loss: 0.37524184584617615\n",
      "Epoch [1/1] | Batch 1523 | loss: 0.08641649037599564\n",
      "Epoch [1/1] | Batch 1524 | loss: 0.1984158307313919\n",
      "Epoch [1/1] | Batch 1525 | loss: 0.11410356312990189\n",
      "Epoch [1/1] | Batch 1526 | loss: 0.04584750160574913\n",
      "Epoch [1/1] | Batch 1527 | loss: 0.5852121710777283\n",
      "Epoch [1/1] | Batch 1528 | loss: 0.006887088529765606\n",
      "Epoch [1/1] | Batch 1529 | loss: 0.3080841600894928\n",
      "Epoch [1/1] | Batch 1530 | loss: 0.14330755174160004\n",
      "Epoch [1/1] | Batch 1531 | loss: 0.21787184476852417\n",
      "Epoch [1/1] | Batch 1532 | loss: 0.004729482810944319\n",
      "Epoch [1/1] | Batch 1533 | loss: 0.13847187161445618\n",
      "Epoch [1/1] | Batch 1534 | loss: 0.0883084237575531\n",
      "Epoch [1/1] | Batch 1535 | loss: 0.09361148625612259\n",
      "Epoch [1/1] | Batch 1536 | loss: 0.2311403453350067\n",
      "Epoch [1/1] | Batch 1537 | loss: 0.05952759459614754\n",
      "Epoch [1/1] | Batch 1538 | loss: 0.9149467349052429\n",
      "Epoch [1/1] | Batch 1539 | loss: 0.07839419692754745\n",
      "Epoch [1/1] | Batch 1540 | loss: 0.6534439921379089\n",
      "Epoch [1/1] | Batch 1541 | loss: 0.6194405555725098\n",
      "Epoch [1/1] | Batch 1542 | loss: 0.30268076062202454\n",
      "Epoch [1/1] | Batch 1543 | loss: 0.4574130177497864\n",
      "Epoch [1/1] | Batch 1544 | loss: 0.19315923750400543\n",
      "Epoch [1/1] | Batch 1545 | loss: 0.24499846994876862\n",
      "Epoch [1/1] | Batch 1546 | loss: 0.22416426241397858\n",
      "Epoch [1/1] | Batch 1547 | loss: 0.0588390938937664\n",
      "Epoch [1/1] | Batch 1548 | loss: 0.48142290115356445\n",
      "Epoch [1/1] | Batch 1549 | loss: 0.020924098789691925\n",
      "Epoch [1/1] | Batch 1550 | loss: 0.09234678000211716\n",
      "Epoch [1/1] | Batch 1551 | loss: 0.15459461510181427\n",
      "Epoch [1/1] | Batch 1552 | loss: 0.09033100306987762\n",
      "Epoch [1/1] | Batch 1553 | loss: 0.7840272784233093\n",
      "Epoch [1/1] | Batch 1554 | loss: 0.05211012437939644\n",
      "Epoch [1/1] | Batch 1555 | loss: 0.32162603735923767\n",
      "Epoch [1/1] | Batch 1556 | loss: 0.13249093294143677\n",
      "Epoch [1/1] | Batch 1557 | loss: 0.19991473853588104\n",
      "Epoch [1/1] | Batch 1558 | loss: 0.10202019661664963\n",
      "Epoch [1/1] | Batch 1559 | loss: 0.2388816922903061\n",
      "Epoch [1/1] | Batch 1560 | loss: 0.07154837995767593\n",
      "Epoch [1/1] | Batch 1561 | loss: 0.2969108521938324\n",
      "Epoch [1/1] | Batch 1562 | loss: 0.33947819471359253\n",
      "Epoch [1/1] | Batch 1563 | loss: 0.4216909110546112\n",
      "Epoch [1/1] | Batch 1564 | loss: 0.5642091631889343\n",
      "Epoch [1/1] | Batch 1565 | loss: 0.06044019013643265\n",
      "Epoch [1/1] | Batch 1566 | loss: 0.2769767642021179\n",
      "Epoch [1/1] | Batch 1567 | loss: 0.03567521274089813\n",
      "Epoch [1/1] | Batch 1568 | loss: 0.06770189851522446\n",
      "Epoch [1/1] | Batch 1569 | loss: 0.06291493773460388\n",
      "Epoch [1/1] | Batch 1570 | loss: 0.19414670765399933\n",
      "Epoch [1/1] | Batch 1571 | loss: 0.06524517387151718\n",
      "Epoch [1/1] | Batch 1572 | loss: 0.05962264910340309\n",
      "Epoch [1/1] | Batch 1573 | loss: 0.1350845843553543\n",
      "Epoch [1/1] | Batch 1574 | loss: 0.14380839467048645\n",
      "Epoch [1/1] | Batch 1575 | loss: 0.464937686920166\n",
      "Epoch [1/1] | Batch 1576 | loss: 0.013601924292743206\n",
      "Epoch [1/1] | Batch 1577 | loss: 0.2644515037536621\n",
      "Epoch [1/1] | Batch 1578 | loss: 0.021435024216771126\n",
      "Epoch [1/1] | Batch 1579 | loss: 0.11377400904893875\n",
      "Epoch [1/1] | Batch 1580 | loss: 0.02767508290708065\n",
      "Epoch [1/1] | Batch 1581 | loss: 0.6015135645866394\n",
      "Epoch [1/1] | Batch 1582 | loss: 0.026641029864549637\n",
      "Epoch [1/1] | Batch 1583 | loss: 0.43607228994369507\n",
      "Epoch [1/1] | Batch 1584 | loss: 0.06690368056297302\n",
      "Epoch [1/1] | Batch 1585 | loss: 0.07467454671859741\n",
      "Epoch [1/1] | Batch 1586 | loss: 0.3152947723865509\n",
      "Epoch [1/1] | Batch 1587 | loss: 0.19217929244041443\n",
      "Epoch [1/1] | Batch 1588 | loss: 0.07515981048345566\n",
      "Epoch [1/1] | Batch 1589 | loss: 0.02660447545349598\n",
      "Epoch [1/1] | Batch 1590 | loss: 0.2834945619106293\n",
      "Epoch [1/1] | Batch 1591 | loss: 0.03778164088726044\n",
      "Epoch [1/1] | Batch 1592 | loss: 0.3286614716053009\n",
      "Epoch [1/1] | Batch 1593 | loss: 0.3022303581237793\n",
      "Epoch [1/1] | Batch 1594 | loss: 0.13546161353588104\n",
      "Epoch [1/1] | Batch 1595 | loss: 0.05008048564195633\n",
      "Epoch [1/1] | Batch 1596 | loss: 0.3880741000175476\n",
      "Epoch [1/1] | Batch 1597 | loss: 0.2367837131023407\n",
      "Epoch [1/1] | Batch 1598 | loss: 0.11901987344026566\n",
      "Epoch [1/1] | Batch 1599 | loss: 0.137417733669281\n",
      "Epoch [1/1] | Batch 1600 | loss: 0.16853153705596924\n",
      "Epoch [1/1] | Batch 1601 | loss: 0.09853838384151459\n",
      "Epoch [1/1] | Batch 1602 | loss: 0.02180120348930359\n",
      "Epoch [1/1] | Batch 1603 | loss: 0.35768967866897583\n",
      "Epoch [1/1] | Batch 1604 | loss: 0.036577258259058\n",
      "Epoch [1/1] | Batch 1605 | loss: 0.2058071494102478\n",
      "Epoch [1/1] | Batch 1606 | loss: 0.2270417958498001\n",
      "Epoch [1/1] | Batch 1607 | loss: 0.031236151233315468\n",
      "Epoch [1/1] | Batch 1608 | loss: 0.20518018305301666\n",
      "Epoch [1/1] | Batch 1609 | loss: 0.02922772988677025\n",
      "Epoch [1/1] | Batch 1610 | loss: 0.1150016337633133\n",
      "Epoch [1/1] | Batch 1611 | loss: 0.0715702697634697\n",
      "Epoch [1/1] | Batch 1612 | loss: 0.20627206563949585\n",
      "Epoch [1/1] | Batch 1613 | loss: 0.2564902901649475\n",
      "Epoch [1/1] | Batch 1614 | loss: 0.5079066753387451\n",
      "Epoch [1/1] | Batch 1615 | loss: 0.01555408351123333\n",
      "Epoch [1/1] | Batch 1616 | loss: 0.04904298484325409\n",
      "Epoch [1/1] | Batch 1617 | loss: 0.040219664573669434\n",
      "Epoch [1/1] | Batch 1618 | loss: 0.1730133593082428\n",
      "Epoch [1/1] | Batch 1619 | loss: 0.0476822666823864\n",
      "Epoch [1/1] | Batch 1620 | loss: 0.046997152268886566\n",
      "Epoch [1/1] | Batch 1621 | loss: 0.04402346536517143\n",
      "Epoch [1/1] | Batch 1622 | loss: 0.5010913610458374\n",
      "Epoch [1/1] | Batch 1623 | loss: 0.11215624958276749\n",
      "Epoch [1/1] | Batch 1624 | loss: 0.29994115233421326\n",
      "Epoch [1/1] | Batch 1625 | loss: 0.12688355147838593\n",
      "Epoch [1/1] | Batch 1626 | loss: 0.007249362301081419\n",
      "Epoch [1/1] | Batch 1627 | loss: 0.143872931599617\n",
      "Epoch [1/1] | Batch 1628 | loss: 0.20518645644187927\n",
      "Epoch [1/1] | Batch 1629 | loss: 0.23118866980075836\n",
      "Epoch [1/1] | Batch 1630 | loss: 0.47403261065483093\n",
      "Epoch [1/1] | Batch 1631 | loss: 0.09111954271793365\n",
      "Epoch [1/1] | Batch 1632 | loss: 0.04251885786652565\n",
      "Epoch [1/1] | Batch 1633 | loss: 0.022936265915632248\n",
      "Epoch [1/1] | Batch 1634 | loss: 0.29329991340637207\n",
      "Epoch [1/1] | Batch 1635 | loss: 0.005931971129029989\n",
      "Epoch [1/1] | Batch 1636 | loss: 0.0639844611287117\n",
      "Epoch [1/1] | Batch 1637 | loss: 0.06857910007238388\n",
      "Epoch [1/1] | Batch 1638 | loss: 0.12060369551181793\n",
      "Epoch [1/1] | Batch 1639 | loss: 0.4430278539657593\n",
      "Epoch [1/1] | Batch 1640 | loss: 0.19523808360099792\n",
      "Epoch [1/1] | Batch 1641 | loss: 0.008989214897155762\n",
      "Epoch [1/1] | Batch 1642 | loss: 0.5119723677635193\n",
      "Epoch [1/1] | Batch 1643 | loss: 0.048788923770189285\n",
      "Epoch [1/1] | Batch 1644 | loss: 0.17814022302627563\n",
      "Epoch [1/1] | Batch 1645 | loss: 0.37410417199134827\n",
      "Epoch [1/1] | Batch 1646 | loss: 0.020406000316143036\n",
      "Epoch [1/1] | Batch 1647 | loss: 0.39516305923461914\n",
      "Epoch [1/1] | Batch 1648 | loss: 0.023464780300855637\n",
      "Epoch [1/1] | Batch 1649 | loss: 0.11697545647621155\n",
      "Epoch [1/1] | Batch 1650 | loss: 0.19649992883205414\n",
      "Epoch [1/1] | Batch 1651 | loss: 0.052105922251939774\n",
      "Epoch [1/1] | Batch 1652 | loss: 0.11710332334041595\n",
      "Epoch [1/1] | Batch 1653 | loss: 0.046965472400188446\n",
      "Epoch [1/1] | Batch 1654 | loss: 0.09243273735046387\n",
      "Epoch [1/1] | Batch 1655 | loss: 0.14717036485671997\n",
      "Epoch [1/1] | Batch 1656 | loss: 0.2726125121116638\n",
      "Epoch [1/1] | Batch 1657 | loss: 0.28070902824401855\n",
      "Epoch [1/1] | Batch 1658 | loss: 0.3512200713157654\n",
      "Epoch [1/1] | Batch 1659 | loss: 0.16237756609916687\n",
      "Epoch [1/1] | Batch 1660 | loss: 0.09636996686458588\n",
      "Epoch [1/1] | Batch 1661 | loss: 0.046168986707925797\n",
      "Epoch [1/1] | Batch 1662 | loss: 0.2814798355102539\n",
      "Epoch [1/1] | Batch 1663 | loss: 0.040851935744285583\n",
      "Epoch [1/1] | Batch 1664 | loss: 0.5789217948913574\n",
      "Epoch [1/1] | Batch 1665 | loss: 0.527596652507782\n",
      "Epoch [1/1] | Batch 1666 | loss: 0.058559466153383255\n",
      "Epoch [1/1] | Batch 1667 | loss: 0.9167981147766113\n",
      "Epoch [1/1] | Batch 1668 | loss: 0.13994328677654266\n",
      "Epoch [1/1] | Batch 1669 | loss: 0.03929533436894417\n",
      "Epoch [1/1] | Batch 1670 | loss: 0.38176167011260986\n",
      "Epoch [1/1] | Batch 1671 | loss: 0.02995232120156288\n",
      "Epoch [1/1] | Batch 1672 | loss: 0.057291802018880844\n",
      "Epoch [1/1] | Batch 1673 | loss: 0.014375308528542519\n",
      "Epoch [1/1] | Batch 1674 | loss: 0.13826406002044678\n",
      "Epoch [1/1] | Batch 1675 | loss: 0.17875729501247406\n",
      "Epoch [1/1] | Batch 1676 | loss: 0.13453252613544464\n",
      "Epoch [1/1] | Batch 1677 | loss: 0.02217470295727253\n",
      "Epoch [1/1] | Batch 1678 | loss: 0.018689356744289398\n",
      "Epoch [1/1] | Batch 1679 | loss: 0.10198243707418442\n",
      "Epoch [1/1] | Batch 1680 | loss: 0.5632020235061646\n",
      "Epoch [1/1] | Batch 1681 | loss: 0.07521827518939972\n",
      "Epoch [1/1] | Batch 1682 | loss: 0.35573792457580566\n",
      "Epoch [1/1] | Batch 1683 | loss: 0.2882329821586609\n",
      "Epoch [1/1] | Batch 1684 | loss: 0.09308578073978424\n",
      "Epoch [1/1] | Batch 1685 | loss: 0.018597668036818504\n",
      "Epoch [1/1] | Batch 1686 | loss: 0.1977001279592514\n",
      "Epoch [1/1] | Batch 1687 | loss: 0.40695688128471375\n",
      "Epoch [1/1] | Batch 1688 | loss: 0.31275680661201477\n",
      "Epoch [1/1] | Batch 1689 | loss: 0.0845976397395134\n",
      "Epoch [1/1] | Batch 1690 | loss: 0.03520188853144646\n",
      "Epoch [1/1] | Batch 1691 | loss: 0.4773848354816437\n",
      "Epoch [1/1] | Batch 1692 | loss: 0.4381636083126068\n",
      "Epoch [1/1] | Batch 1693 | loss: 0.22320020198822021\n",
      "Epoch [1/1] | Batch 1694 | loss: 0.19441469013690948\n",
      "Epoch [1/1] | Batch 1695 | loss: 0.17058512568473816\n",
      "Epoch [1/1] | Batch 1696 | loss: 0.04908602684736252\n",
      "Epoch [1/1] | Batch 1697 | loss: 0.027438731864094734\n",
      "Epoch [1/1] | Batch 1698 | loss: 0.08820130676031113\n",
      "Epoch [1/1] | Batch 1699 | loss: 0.6422078013420105\n",
      "Epoch [1/1] | Batch 1700 | loss: 0.0207357257604599\n",
      "Epoch [1/1] | Batch 1701 | loss: 0.024522487074136734\n",
      "Epoch [1/1] | Batch 1702 | loss: 0.03601536154747009\n",
      "Epoch [1/1] | Batch 1703 | loss: 0.07852553576231003\n",
      "Epoch [1/1] | Batch 1704 | loss: 0.012192255817353725\n",
      "Epoch [1/1] | Batch 1705 | loss: 0.2945396602153778\n",
      "Epoch [1/1] | Batch 1706 | loss: 0.011785157024860382\n",
      "Epoch [1/1] | Batch 1707 | loss: 0.43133580684661865\n",
      "Epoch [1/1] | Batch 1708 | loss: 0.15449535846710205\n",
      "Epoch [1/1] | Batch 1709 | loss: 0.19920256733894348\n",
      "Epoch [1/1] | Batch 1710 | loss: 0.5872446298599243\n",
      "Epoch [1/1] | Batch 1711 | loss: 0.11499232053756714\n",
      "Epoch [1/1] | Batch 1712 | loss: 0.16763463616371155\n",
      "Epoch [1/1] | Batch 1713 | loss: 0.05212418735027313\n",
      "Epoch [1/1] | Batch 1714 | loss: 0.017675112932920456\n",
      "Epoch [1/1] | Batch 1715 | loss: 0.04690123721957207\n",
      "Epoch [1/1] | Batch 1716 | loss: 0.014354730024933815\n",
      "Epoch [1/1] | Batch 1717 | loss: 0.0206480510532856\n",
      "Epoch [1/1] | Batch 1718 | loss: 0.025838816538453102\n",
      "Epoch [1/1] | Batch 1719 | loss: 0.18654219806194305\n",
      "Epoch [1/1] | Batch 1720 | loss: 0.4985576868057251\n",
      "Epoch [1/1] | Batch 1721 | loss: 0.027483399957418442\n",
      "Epoch [1/1] | Batch 1722 | loss: 0.0849842056632042\n",
      "Epoch [1/1] | Batch 1723 | loss: 0.08679960668087006\n",
      "Epoch [1/1] | Batch 1724 | loss: 0.12259005755186081\n",
      "Epoch [1/1] | Batch 1725 | loss: 0.03260447084903717\n",
      "Epoch [1/1] | Batch 1726 | loss: 0.03820992261171341\n",
      "Epoch [1/1] | Batch 1727 | loss: 0.09044084697961807\n",
      "Epoch [1/1] | Batch 1728 | loss: 0.07811662554740906\n",
      "Epoch [1/1] | Batch 1729 | loss: 0.08255915343761444\n",
      "Epoch [1/1] | Batch 1730 | loss: 0.005191359668970108\n",
      "Epoch [1/1] | Batch 1731 | loss: 0.24249595403671265\n",
      "Epoch [1/1] | Batch 1732 | loss: 0.02754754014313221\n",
      "Epoch [1/1] | Batch 1733 | loss: 0.02237236127257347\n",
      "Epoch [1/1] | Batch 1734 | loss: 0.05026255175471306\n",
      "Epoch [1/1] | Batch 1735 | loss: 0.20645779371261597\n",
      "Epoch [1/1] | Batch 1736 | loss: 0.2891824543476105\n",
      "Epoch [1/1] | Batch 1737 | loss: 0.016196243464946747\n",
      "Epoch [1/1] | Batch 1738 | loss: 0.028415625914931297\n",
      "Epoch [1/1] | Batch 1739 | loss: 0.4106556475162506\n",
      "Epoch [1/1] | Batch 1740 | loss: 0.5137547850608826\n",
      "Epoch [1/1] | Batch 1741 | loss: 0.053931187838315964\n",
      "Epoch [1/1] | Batch 1742 | loss: 0.004407950676977634\n",
      "Epoch [1/1] | Batch 1743 | loss: 0.026194514706730843\n",
      "Epoch [1/1] | Batch 1744 | loss: 0.08495806902647018\n",
      "Epoch [1/1] | Batch 1745 | loss: 0.7245341539382935\n",
      "Epoch [1/1] | Batch 1746 | loss: 0.061598725616931915\n",
      "Epoch [1/1] | Batch 1747 | loss: 0.2960105836391449\n",
      "Epoch [1/1] | Batch 1748 | loss: 0.06238572299480438\n",
      "Epoch [1/1] | Batch 1749 | loss: 0.04692348837852478\n",
      "Epoch [1/1] | Batch 1750 | loss: 0.051970064640045166\n",
      "Epoch [1/1] | Batch 1751 | loss: 0.186772421002388\n",
      "Epoch [1/1] | Batch 1752 | loss: 0.39463719725608826\n",
      "Epoch [1/1] | Batch 1753 | loss: 0.142106294631958\n",
      "Epoch [1/1] | Batch 1754 | loss: 0.07466425001621246\n",
      "Epoch [1/1] | Batch 1755 | loss: 0.07671114802360535\n",
      "Epoch [1/1] | Batch 1756 | loss: 0.0652003362774849\n",
      "Epoch [1/1] | Batch 1757 | loss: 0.2411513477563858\n",
      "Epoch [1/1] | Batch 1758 | loss: 0.05028403922915459\n",
      "Epoch [1/1] | Batch 1759 | loss: 0.02313067391514778\n",
      "Epoch [1/1] | Batch 1760 | loss: 0.18785861134529114\n",
      "Epoch [1/1] | Batch 1761 | loss: 0.023175494745373726\n",
      "Epoch [1/1] | Batch 1762 | loss: 0.03679663687944412\n",
      "Epoch [1/1] | Batch 1763 | loss: 0.04921489208936691\n",
      "Epoch [1/1] | Batch 1764 | loss: 0.03241099417209625\n",
      "Epoch [1/1] | Batch 1765 | loss: 0.10631345957517624\n",
      "Epoch [1/1] | Batch 1766 | loss: 0.11711537837982178\n",
      "Epoch [1/1] | Batch 1767 | loss: 0.36035385727882385\n",
      "Epoch [1/1] | Batch 1768 | loss: 0.1300794631242752\n",
      "Epoch [1/1] | Batch 1769 | loss: 0.4680350720882416\n",
      "Epoch [1/1] | Batch 1770 | loss: 0.3363777995109558\n",
      "Epoch [1/1] | Batch 1771 | loss: 0.10240596532821655\n",
      "Epoch [1/1] | Batch 1772 | loss: 0.38162869215011597\n",
      "Epoch [1/1] | Batch 1773 | loss: 0.9053839445114136\n",
      "Epoch [1/1] | Batch 1774 | loss: 0.03978497534990311\n",
      "Epoch [1/1] | Batch 1775 | loss: 0.13061948120594025\n",
      "Epoch [1/1] | Batch 1776 | loss: 0.21975275874137878\n",
      "Epoch [1/1] | Batch 1777 | loss: 0.05069299787282944\n",
      "Epoch [1/1] | Batch 1778 | loss: 0.11181189119815826\n",
      "Epoch [1/1] | Batch 1779 | loss: 0.7395818829536438\n",
      "Epoch [1/1] | Batch 1780 | loss: 0.05180944874882698\n",
      "Epoch [1/1] | Batch 1781 | loss: 0.0978572741150856\n",
      "Epoch [1/1] | Batch 1782 | loss: 0.4119614362716675\n",
      "Epoch [1/1] | Batch 1783 | loss: 0.0406358428299427\n",
      "Epoch [1/1] | Batch 1784 | loss: 0.1682673692703247\n",
      "Epoch [1/1] | Batch 1785 | loss: 0.12413160502910614\n",
      "Epoch [1/1] | Batch 1786 | loss: 0.06159920617938042\n",
      "Epoch [1/1] | Batch 1787 | loss: 0.09994787722826004\n",
      "Epoch [1/1] | Batch 1788 | loss: 0.034825317561626434\n",
      "Epoch [1/1] | Batch 1789 | loss: 0.04554697498679161\n",
      "Epoch [1/1] | Batch 1790 | loss: 0.15790456533432007\n",
      "Epoch [1/1] | Batch 1791 | loss: 0.7508887648582458\n",
      "Epoch [1/1] | Batch 1792 | loss: 0.1540515124797821\n",
      "Epoch [1/1] | Batch 1793 | loss: 0.34599223732948303\n",
      "Epoch [1/1] | Batch 1794 | loss: 0.018518025055527687\n",
      "Epoch [1/1] | Batch 1795 | loss: 0.00281176227144897\n",
      "Epoch [1/1] | Batch 1796 | loss: 0.19754455983638763\n",
      "Epoch [1/1] | Batch 1797 | loss: 0.11045388132333755\n",
      "Epoch [1/1] | Batch 1798 | loss: 0.10385669767856598\n",
      "Epoch [1/1] | Batch 1799 | loss: 0.025356629863381386\n",
      "Epoch [1/1] | Batch 1800 | loss: 0.054809413850307465\n",
      "Epoch [1/1] | Batch 1801 | loss: 0.29968327283859253\n",
      "Epoch [1/1] | Batch 1802 | loss: 0.019345691427588463\n",
      "Epoch [1/1] | Batch 1803 | loss: 0.011467153206467628\n",
      "Epoch [1/1] | Batch 1804 | loss: 0.24969518184661865\n",
      "Epoch [1/1] | Batch 1805 | loss: 0.03580457717180252\n",
      "Epoch [1/1] | Batch 1806 | loss: 0.0704139694571495\n",
      "Epoch [1/1] | Batch 1807 | loss: 0.4401676654815674\n",
      "Epoch [1/1] | Batch 1808 | loss: 0.015425406396389008\n",
      "Epoch [1/1] | Batch 1809 | loss: 0.09430299699306488\n",
      "Epoch [1/1] | Batch 1810 | loss: 0.03901126980781555\n",
      "Epoch [1/1] | Batch 1811 | loss: 0.18745245039463043\n",
      "Epoch [1/1] | Batch 1812 | loss: 0.004811320453882217\n",
      "Epoch [1/1] | Batch 1813 | loss: 0.7112627029418945\n",
      "Epoch [1/1] | Batch 1814 | loss: 0.21302641928195953\n",
      "Epoch [1/1] | Batch 1815 | loss: 0.7559524178504944\n",
      "Epoch [1/1] | Batch 1816 | loss: 0.5151223540306091\n",
      "Epoch [1/1] | Batch 1817 | loss: 0.3711734712123871\n",
      "Epoch [1/1] | Batch 1818 | loss: 0.025995273143053055\n",
      "Epoch [1/1] | Batch 1819 | loss: 0.19697338342666626\n",
      "Epoch [1/1] | Batch 1820 | loss: 0.056286752223968506\n",
      "Epoch [1/1] | Batch 1821 | loss: 0.0850147157907486\n",
      "Epoch [1/1] | Batch 1822 | loss: 0.04212234914302826\n",
      "Epoch [1/1] | Batch 1823 | loss: 0.10403398424386978\n",
      "Epoch [1/1] | Batch 1824 | loss: 0.021372880786657333\n",
      "Epoch [1/1] | Batch 1825 | loss: 1.0997470617294312\n",
      "Epoch [1/1] | Batch 1826 | loss: 0.1679111272096634\n",
      "Epoch [1/1] | Batch 1827 | loss: 0.6601267457008362\n",
      "Epoch [1/1] | Batch 1828 | loss: 0.24883179366588593\n",
      "Epoch [1/1] | Batch 1829 | loss: 0.06956588476896286\n",
      "Epoch [1/1] | Batch 1830 | loss: 0.3633216619491577\n",
      "Epoch [1/1] | Batch 1831 | loss: 0.07046321779489517\n",
      "Epoch [1/1] | Batch 1832 | loss: 0.1519613265991211\n",
      "Epoch [1/1] | Batch 1833 | loss: 0.008985890075564384\n",
      "Epoch [1/1] | Batch 1834 | loss: 0.011725300922989845\n",
      "Epoch [1/1] | Batch 1835 | loss: 0.1397608369588852\n",
      "Epoch [1/1] | Batch 1836 | loss: 0.12208607792854309\n",
      "Epoch [1/1] | Batch 1837 | loss: 0.12074137479066849\n",
      "Epoch [1/1] | Batch 1838 | loss: 0.3126843571662903\n",
      "Epoch [1/1] | Batch 1839 | loss: 0.059138622134923935\n",
      "Epoch [1/1] | Batch 1840 | loss: 0.2228790521621704\n",
      "Epoch [1/1] | Batch 1841 | loss: 0.2678968012332916\n",
      "Epoch [1/1] | Batch 1842 | loss: 0.010957630351185799\n",
      "Epoch [1/1] | Batch 1843 | loss: 0.0772024616599083\n",
      "Epoch [1/1] | Batch 1844 | loss: 0.23688849806785583\n",
      "Epoch [1/1] | Batch 1845 | loss: 0.18705329298973083\n",
      "Epoch [1/1] | Batch 1846 | loss: 0.19138678908348083\n",
      "Epoch [1/1] | Batch 1847 | loss: 0.12578976154327393\n",
      "Epoch [1/1] | Batch 1848 | loss: 0.05979277566075325\n",
      "Epoch [1/1] | Batch 1849 | loss: 0.028473392128944397\n",
      "Epoch [1/1] | Batch 1850 | loss: 0.052237313240766525\n",
      "Epoch [1/1] | Batch 1851 | loss: 0.001790219102986157\n",
      "Epoch [1/1] | Batch 1852 | loss: 0.053654275834560394\n",
      "Epoch [1/1] | Batch 1853 | loss: 0.17222106456756592\n",
      "Epoch [1/1] | Batch 1854 | loss: 0.1566959172487259\n",
      "Epoch [1/1] | Batch 1855 | loss: 0.3136552572250366\n",
      "Epoch [1/1] | Batch 1856 | loss: 0.17669302225112915\n",
      "Epoch [1/1] | Batch 1857 | loss: 0.08312749862670898\n",
      "Epoch [1/1] | Batch 1858 | loss: 0.1254936307668686\n",
      "Epoch [1/1] | Batch 1859 | loss: 0.05207625776529312\n",
      "Epoch [1/1] | Batch 1860 | loss: 0.23267868161201477\n",
      "Epoch [1/1] | Batch 1861 | loss: 0.018821626901626587\n",
      "Epoch [1/1] | Batch 1862 | loss: 0.2063804268836975\n",
      "Epoch [1/1] | Batch 1863 | loss: 0.04692491516470909\n",
      "Epoch [1/1] | Batch 1864 | loss: 0.011305107735097408\n",
      "Epoch [1/1] | Batch 1865 | loss: 0.2258414477109909\n",
      "Epoch [1/1] | Batch 1866 | loss: 0.092399463057518\n",
      "Epoch [1/1] | Batch 1867 | loss: 0.4085291028022766\n",
      "Epoch [1/1] | Batch 1868 | loss: 0.1668735146522522\n",
      "Epoch [1/1] | Batch 1869 | loss: 0.07716982066631317\n",
      "Epoch [1/1] | Batch 1870 | loss: 0.040436241775751114\n",
      "Epoch [1/1] | Batch 1871 | loss: 0.0028382637538015842\n",
      "Epoch [1/1] | Batch 1872 | loss: 0.1335458904504776\n",
      "Epoch [1/1] | Batch 1873 | loss: 0.5581544041633606\n",
      "Epoch [1/1] | Batch 1874 | loss: 0.2478395402431488\n",
      "Epoch [1/1] | Batch 1875 | loss: 0.5219075083732605\n",
      "Epoch [1/1] | Batch 1876 | loss: 0.2594170570373535\n",
      "Epoch [1/1] | Batch 1877 | loss: 0.4604076147079468\n",
      "Epoch [1/1] | Batch 1878 | loss: 0.23013260960578918\n",
      "Epoch [1/1] | Batch 1879 | loss: 0.27007371187210083\n",
      "Epoch [1/1] | Batch 1880 | loss: 0.022904112935066223\n",
      "Epoch [1/1] | Batch 1881 | loss: 0.055368609726428986\n",
      "Epoch [1/1] | Batch 1882 | loss: 0.24345241487026215\n",
      "Epoch [1/1] | Batch 1883 | loss: 0.03654579818248749\n",
      "Epoch [1/1] | Batch 1884 | loss: 0.06550071388483047\n",
      "Epoch [1/1] | Batch 1885 | loss: 0.34780770540237427\n",
      "Epoch [1/1] | Batch 1886 | loss: 0.005368110258132219\n",
      "Epoch [1/1] | Batch 1887 | loss: 0.27925437688827515\n",
      "Epoch [1/1] | Batch 1888 | loss: 0.30069780349731445\n",
      "Epoch [1/1] | Batch 1889 | loss: 0.08689813315868378\n",
      "Epoch [1/1] | Batch 1890 | loss: 0.2758903205394745\n",
      "Epoch [1/1] | Batch 1891 | loss: 0.17718449234962463\n",
      "Epoch [1/1] | Batch 1892 | loss: 0.296101450920105\n",
      "Epoch [1/1] | Batch 1893 | loss: 0.44759950041770935\n",
      "Epoch [1/1] | Batch 1894 | loss: 0.16799333691596985\n",
      "Epoch [1/1] | Batch 1895 | loss: 0.37787070870399475\n",
      "Epoch [1/1] | Batch 1896 | loss: 0.25151288509368896\n",
      "Epoch [1/1] | Batch 1897 | loss: 0.06892108172178268\n",
      "Epoch [1/1] | Batch 1898 | loss: 0.03147825598716736\n",
      "Epoch [1/1] | Batch 1899 | loss: 0.04176707565784454\n",
      "Epoch [1/1] | Batch 1900 | loss: 0.12399410456418991\n",
      "Epoch [1/1] | Batch 1901 | loss: 0.22281530499458313\n",
      "Epoch [1/1] | Batch 1902 | loss: 0.016601229086518288\n",
      "Epoch [1/1] | Batch 1903 | loss: 0.6264265775680542\n",
      "Epoch [1/1] | Batch 1904 | loss: 0.20951519906520844\n",
      "Epoch [1/1] | Batch 1905 | loss: 0.03697340190410614\n",
      "Epoch [1/1] | Batch 1906 | loss: 0.028308119624853134\n",
      "Epoch [1/1] | Batch 1907 | loss: 0.009754644706845284\n",
      "Epoch [1/1] | Batch 1908 | loss: 0.07951490581035614\n",
      "Epoch [1/1] | Batch 1909 | loss: 0.6521724462509155\n",
      "Epoch [1/1] | Batch 1910 | loss: 0.4890851676464081\n",
      "Epoch [1/1] | Batch 1911 | loss: 0.13251027464866638\n",
      "Epoch [1/1] | Batch 1912 | loss: 0.010060914792120457\n",
      "Epoch [1/1] | Batch 1913 | loss: 0.12753625214099884\n",
      "Epoch [1/1] | Batch 1914 | loss: 0.011843739077448845\n",
      "Epoch [1/1] | Batch 1915 | loss: 0.4002685248851776\n",
      "Epoch [1/1] | Batch 1916 | loss: 0.4520973563194275\n",
      "Epoch [1/1] | Batch 1917 | loss: 0.017967667430639267\n",
      "Epoch [1/1] | Batch 1918 | loss: 1.2355719804763794\n",
      "Epoch [1/1] | Batch 1919 | loss: 0.5662176609039307\n",
      "Epoch [1/1] | Batch 1920 | loss: 0.04410836845636368\n",
      "Epoch [1/1] | Batch 1921 | loss: 0.024311767891049385\n",
      "Epoch [1/1] | Batch 1922 | loss: 0.2577924132347107\n",
      "Epoch [1/1] | Batch 1923 | loss: 0.10672755539417267\n",
      "Epoch [1/1] | Batch 1924 | loss: 0.2641609311103821\n",
      "Epoch [1/1] | Batch 1925 | loss: 0.21140843629837036\n",
      "Epoch [1/1] | Batch 1926 | loss: 0.052434712648391724\n",
      "Epoch [1/1] | Batch 1927 | loss: 0.2582034468650818\n",
      "Epoch [1/1] | Batch 1928 | loss: 0.1626080423593521\n",
      "Epoch [1/1] | Batch 1929 | loss: 0.01869964972138405\n",
      "Epoch [1/1] | Batch 1930 | loss: 0.06620874255895615\n",
      "Epoch [1/1] | Batch 1931 | loss: 0.2212480902671814\n",
      "Epoch [1/1] | Batch 1932 | loss: 0.043242596089839935\n",
      "Epoch [1/1] | Batch 1933 | loss: 0.3274606168270111\n",
      "Epoch [1/1] | Batch 1934 | loss: 0.08286012709140778\n",
      "Epoch [1/1] | Batch 1935 | loss: 0.2503083348274231\n",
      "Epoch [1/1] | Batch 1936 | loss: 0.12618198990821838\n",
      "Epoch [1/1] | Batch 1937 | loss: 0.318120539188385\n",
      "Epoch [1/1] | Batch 1938 | loss: 0.017562655732035637\n",
      "Epoch [1/1] | Batch 1939 | loss: 0.03127732872962952\n",
      "Epoch [1/1] | Batch 1940 | loss: 0.07300899922847748\n",
      "Epoch [1/1] | Batch 1941 | loss: 1.097797155380249\n",
      "Epoch [1/1] | Batch 1942 | loss: 0.0929960310459137\n",
      "Epoch [1/1] | Batch 1943 | loss: 0.24685436487197876\n",
      "Epoch [1/1] | Batch 1944 | loss: 0.08669696003198624\n",
      "Epoch [1/1] | Batch 1945 | loss: 0.4267958402633667\n",
      "Epoch [1/1] | Batch 1946 | loss: 0.1387709677219391\n",
      "Epoch [1/1] | Batch 1947 | loss: 0.021501706913113594\n",
      "Epoch [1/1] | Batch 1948 | loss: 0.15345603227615356\n",
      "Epoch [1/1] | Batch 1949 | loss: 0.30372434854507446\n",
      "Epoch [1/1] | Batch 1950 | loss: 0.19997695088386536\n",
      "Epoch [1/1] | Batch 1951 | loss: 0.03980637714266777\n",
      "Epoch [1/1] | Batch 1952 | loss: 0.07410494983196259\n",
      "Epoch [1/1] | Batch 1953 | loss: 0.007331137545406818\n",
      "Epoch [1/1] | Batch 1954 | loss: 0.018220821395516396\n",
      "Epoch [1/1] | Batch 1955 | loss: 0.28281864523887634\n",
      "Epoch [1/1] | Batch 1956 | loss: 0.20895840227603912\n",
      "Epoch [1/1] | Batch 1957 | loss: 0.3658769726753235\n",
      "Epoch [1/1] | Batch 1958 | loss: 0.003389387158676982\n",
      "Epoch [1/1] | Batch 1959 | loss: 0.14512164890766144\n",
      "Epoch [1/1] | Batch 1960 | loss: 0.021056896075606346\n",
      "Epoch [1/1] | Batch 1961 | loss: 0.008280646055936813\n",
      "Epoch [1/1] | Batch 1962 | loss: 0.1949952244758606\n",
      "Epoch [1/1] | Batch 1963 | loss: 0.06736002117395401\n",
      "Epoch [1/1] | Batch 1964 | loss: 0.0927492007613182\n",
      "Epoch [1/1] | Batch 1965 | loss: 0.07472437620162964\n",
      "Epoch [1/1] | Batch 1966 | loss: 0.15991874039173126\n",
      "Epoch [1/1] | Batch 1967 | loss: 0.014468710869550705\n",
      "Epoch [1/1] | Batch 1968 | loss: 0.01551769021898508\n",
      "Epoch [1/1] | Batch 1969 | loss: 0.09292349219322205\n",
      "Epoch [1/1] | Batch 1970 | loss: 0.040163274854421616\n",
      "Epoch [1/1] | Batch 1971 | loss: 0.329770565032959\n",
      "Epoch [1/1] | Batch 1972 | loss: 0.07632956653833389\n",
      "Epoch [1/1] | Batch 1973 | loss: 0.1664033830165863\n",
      "Epoch [1/1] | Batch 1974 | loss: 0.17236199975013733\n",
      "Epoch [1/1] | Batch 1975 | loss: 0.008154690265655518\n",
      "Epoch [1/1] | Batch 1976 | loss: 0.010525085963308811\n",
      "Epoch [1/1] | Batch 1977 | loss: 0.016733726486563683\n",
      "Epoch [1/1] | Batch 1978 | loss: 0.04746290668845177\n",
      "Epoch [1/1] | Batch 1979 | loss: 0.23282204568386078\n",
      "Epoch [1/1] | Batch 1980 | loss: 0.511252224445343\n",
      "Epoch [1/1] | Batch 1981 | loss: 0.033327896147966385\n",
      "Epoch [1/1] | Batch 1982 | loss: 0.6427143216133118\n",
      "Epoch [1/1] | Batch 1983 | loss: 0.7015804052352905\n",
      "Epoch [1/1] | Batch 1984 | loss: 0.06310068815946579\n",
      "Epoch [1/1] | Batch 1985 | loss: 0.1949196606874466\n",
      "Epoch [1/1] | Batch 1986 | loss: 0.006207457277923822\n",
      "Epoch [1/1] | Batch 1987 | loss: 0.10000260919332504\n",
      "Epoch [1/1] | Batch 1988 | loss: 0.036523304879665375\n",
      "Epoch [1/1] | Batch 1989 | loss: 0.24314261972904205\n",
      "Epoch [1/1] | Batch 1990 | loss: 0.005885792896151543\n",
      "Epoch [1/1] | Batch 1991 | loss: 0.696721613407135\n",
      "Epoch [1/1] | Batch 1992 | loss: 0.3103056848049164\n",
      "Epoch [1/1] | Batch 1993 | loss: 0.01487772911787033\n",
      "Epoch [1/1] | Batch 1994 | loss: 0.006118176504969597\n",
      "Epoch [1/1] | Batch 1995 | loss: 0.2507747411727905\n",
      "Epoch [1/1] | Batch 1996 | loss: 0.11492333561182022\n",
      "Epoch [1/1] | Batch 1997 | loss: 0.324619323015213\n",
      "Epoch [1/1] | Batch 1998 | loss: 0.037826329469680786\n",
      "Epoch [1/1] | Batch 1999 | loss: 0.08510876446962357\n",
      "Epoch [1/1] | Batch 2000 | loss: 0.1628648191690445\n",
      "Epoch [1/1] | Batch 2001 | loss: 0.06448839604854584\n",
      "Epoch [1/1] | Batch 2002 | loss: 0.4151045083999634\n",
      "Epoch [1/1] | Batch 2003 | loss: 0.342484712600708\n",
      "Epoch [1/1] | Batch 2004 | loss: 0.11809484660625458\n",
      "Epoch [1/1] | Batch 2005 | loss: 0.03135049715638161\n",
      "Epoch [1/1] | Batch 2006 | loss: 0.12589433789253235\n",
      "Epoch [1/1] | Batch 2007 | loss: 0.023667016997933388\n",
      "Epoch [1/1] | Batch 2008 | loss: 0.10002955794334412\n",
      "Epoch [1/1] | Batch 2009 | loss: 0.3664852976799011\n",
      "Epoch [1/1] | Batch 2010 | loss: 0.01920880191028118\n",
      "Epoch [1/1] | Batch 2011 | loss: 0.04963986203074455\n",
      "Epoch [1/1] | Batch 2012 | loss: 0.4413956105709076\n",
      "Epoch [1/1] | Batch 2013 | loss: 0.0360889658331871\n",
      "Epoch [1/1] | Batch 2014 | loss: 0.07095318287611008\n",
      "Epoch [1/1] | Batch 2015 | loss: 0.024466173723340034\n",
      "Epoch [1/1] | Batch 2016 | loss: 0.04453601315617561\n",
      "Epoch [1/1] | Batch 2017 | loss: 0.08007915318012238\n",
      "Epoch [1/1] | Batch 2018 | loss: 0.23255524039268494\n",
      "Epoch [1/1] | Batch 2019 | loss: 0.07901708036661148\n",
      "Epoch [1/1] | Batch 2020 | loss: 0.11603862792253494\n",
      "Epoch [1/1] | Batch 2021 | loss: 0.009369218721985817\n",
      "Epoch [1/1] | Batch 2022 | loss: 0.006148091983050108\n",
      "Epoch [1/1] | Batch 2023 | loss: 0.11609739065170288\n",
      "Epoch [1/1] | Batch 2024 | loss: 0.013070075772702694\n",
      "Epoch [1/1] | Batch 2025 | loss: 0.10593272745609283\n",
      "Epoch [1/1] | Batch 2026 | loss: 0.07485270500183105\n",
      "Epoch [1/1] | Batch 2027 | loss: 0.15074396133422852\n",
      "Epoch [1/1] | Batch 2028 | loss: 0.04888236150145531\n",
      "Epoch [1/1] | Batch 2029 | loss: 0.14045366644859314\n",
      "Epoch [1/1] | Batch 2030 | loss: 0.21297261118888855\n",
      "Epoch [1/1] | Batch 2031 | loss: 0.1945257931947708\n",
      "Epoch [1/1] | Batch 2032 | loss: 0.16434210538864136\n",
      "Epoch [1/1] | Batch 2033 | loss: 0.48073258996009827\n",
      "Epoch [1/1] | Batch 2034 | loss: 0.15053243935108185\n",
      "Epoch [1/1] | Batch 2035 | loss: 0.28804704546928406\n",
      "Epoch [1/1] | Batch 2036 | loss: 0.046219080686569214\n",
      "Epoch [1/1] | Batch 2037 | loss: 0.14631299674510956\n",
      "Epoch [1/1] | Batch 2038 | loss: 0.04406455159187317\n",
      "Epoch [1/1] | Batch 2039 | loss: 0.546164870262146\n",
      "Epoch [1/1] | Batch 2040 | loss: 0.1202576532959938\n",
      "Epoch [1/1] | Batch 2041 | loss: 0.18196015059947968\n",
      "Epoch [1/1] | Batch 2042 | loss: 0.04728942736983299\n",
      "Epoch [1/1] | Batch 2043 | loss: 0.2623070180416107\n",
      "Epoch [1/1] | Batch 2044 | loss: 0.18693533539772034\n",
      "Epoch [1/1] | Batch 2045 | loss: 0.21556812524795532\n",
      "Epoch [1/1] | Batch 2046 | loss: 0.02817496284842491\n",
      "Epoch [1/1] | Batch 2047 | loss: 0.26312851905822754\n",
      "Epoch [1/1] | Batch 2048 | loss: 0.04931041598320007\n",
      "Epoch [1/1] | Batch 2049 | loss: 0.22393006086349487\n",
      "Epoch [1/1] | Batch 2050 | loss: 0.34512874484062195\n",
      "Epoch [1/1] | Batch 2051 | loss: 0.015324557200074196\n",
      "Epoch [1/1] | Batch 2052 | loss: 0.012248444370925426\n",
      "Epoch [1/1] | Batch 2053 | loss: 0.5564115047454834\n",
      "Epoch [1/1] | Batch 2054 | loss: 0.5992079377174377\n",
      "Epoch [1/1] | Batch 2055 | loss: 0.030484957620501518\n",
      "Epoch [1/1] | Batch 2056 | loss: 0.38167980313301086\n",
      "Epoch [1/1] | Batch 2057 | loss: 0.1750512570142746\n",
      "Epoch [1/1] | Batch 2058 | loss: 0.037289638072252274\n",
      "Epoch [1/1] | Batch 2059 | loss: 0.24957406520843506\n",
      "Epoch [1/1] | Batch 2060 | loss: 0.13998308777809143\n",
      "Epoch [1/1] | Batch 2061 | loss: 0.11724554747343063\n",
      "Epoch [1/1] | Batch 2062 | loss: 0.18536774814128876\n",
      "Epoch [1/1] | Batch 2063 | loss: 0.06099271774291992\n",
      "Epoch [1/1] | Batch 2064 | loss: 0.532753050327301\n",
      "Epoch [1/1] | Batch 2065 | loss: 0.3327080309391022\n",
      "Epoch [1/1] | Batch 2066 | loss: 0.4632333517074585\n",
      "Epoch [1/1] | Batch 2067 | loss: 0.10936278104782104\n",
      "Epoch [1/1] | Batch 2068 | loss: 0.04213698208332062\n",
      "Epoch [1/1] | Batch 2069 | loss: 0.12838493287563324\n",
      "Epoch [1/1] | Batch 2070 | loss: 0.10680929571390152\n",
      "Epoch [1/1] | Batch 2071 | loss: 0.08329398185014725\n",
      "Epoch [1/1] | Batch 2072 | loss: 0.008825249038636684\n",
      "Epoch [1/1] | Batch 2073 | loss: 0.021977564319968224\n",
      "Epoch [1/1] | Batch 2074 | loss: 0.5346848368644714\n",
      "Epoch [1/1] | Batch 2075 | loss: 0.83918696641922\n",
      "Epoch [1/1] | Batch 2076 | loss: 0.11594592034816742\n",
      "Epoch [1/1] | Batch 2077 | loss: 0.5029812455177307\n",
      "Epoch [1/1] | Batch 2078 | loss: 0.05832276493310928\n",
      "Epoch [1/1] | Batch 2079 | loss: 0.016241230070590973\n",
      "Epoch [1/1] | Batch 2080 | loss: 0.01715591736137867\n",
      "Epoch [1/1] | Batch 2081 | loss: 0.18099237978458405\n",
      "Epoch [1/1] | Batch 2082 | loss: 0.3138185143470764\n",
      "Epoch [1/1] | Batch 2083 | loss: 0.03576647862792015\n",
      "Epoch [1/1] | Batch 2084 | loss: 0.027475547045469284\n",
      "Epoch [1/1] | Batch 2085 | loss: 0.301383912563324\n",
      "Epoch [1/1] | Batch 2086 | loss: 0.12978854775428772\n",
      "Epoch [1/1] | Batch 2087 | loss: 0.8852629661560059\n",
      "Epoch [1/1] | Batch 2088 | loss: 0.19174163043498993\n",
      "Epoch [1/1] | Batch 2089 | loss: 0.11544768512248993\n",
      "Epoch [1/1] | Batch 2090 | loss: 0.22948108613491058\n",
      "Epoch [1/1] | Batch 2091 | loss: 0.052258096635341644\n",
      "Epoch [1/1] | Batch 2092 | loss: 0.1568336933851242\n",
      "Epoch [1/1] | Batch 2093 | loss: 0.1670066863298416\n",
      "Epoch [1/1] | Batch 2094 | loss: 0.06391921639442444\n",
      "Epoch [1/1] | Batch 2095 | loss: 0.20660710334777832\n",
      "Epoch [1/1] | Batch 2096 | loss: 0.056702546775341034\n",
      "Epoch [1/1] | Batch 2097 | loss: 0.12564323842525482\n",
      "Epoch [1/1] | Batch 2098 | loss: 0.06763637810945511\n",
      "Epoch [1/1] | Batch 2099 | loss: 0.17060935497283936\n",
      "Epoch [1/1] | Batch 2100 | loss: 0.02790887840092182\n",
      "Epoch [1/1] | Batch 2101 | loss: 0.028106775134801865\n",
      "Epoch [1/1] | Batch 2102 | loss: 0.3785686492919922\n",
      "Epoch [1/1] | Batch 2103 | loss: 0.13749495148658752\n",
      "Epoch [1/1] | Batch 2104 | loss: 0.11878718435764313\n",
      "Epoch [1/1] | Batch 2105 | loss: 0.058567363768815994\n",
      "Epoch [1/1] | Batch 2106 | loss: 0.23036326467990875\n",
      "Epoch [1/1] | Batch 2107 | loss: 0.2119687795639038\n",
      "Epoch [1/1] | Batch 2108 | loss: 0.5360473990440369\n",
      "Epoch [1/1] | Batch 2109 | loss: 0.1580299735069275\n",
      "Epoch [1/1] | Batch 2110 | loss: 0.09906046092510223\n",
      "Epoch [1/1] | Batch 2111 | loss: 0.18020251393318176\n",
      "Epoch [1/1] | Batch 2112 | loss: 0.04265059158205986\n",
      "Epoch [1/1] | Batch 2113 | loss: 0.2622919976711273\n",
      "Epoch [1/1] | Batch 2114 | loss: 0.3271210491657257\n",
      "Epoch [1/1] | Batch 2115 | loss: 0.33247795701026917\n",
      "Epoch [1/1] | Batch 2116 | loss: 0.0168317798525095\n",
      "Epoch [1/1] | Batch 2117 | loss: 0.008457205258309841\n",
      "Epoch [1/1] | Batch 2118 | loss: 0.03033633530139923\n",
      "Epoch [1/1] | Batch 2119 | loss: 0.018796104937791824\n",
      "Epoch [1/1] | Batch 2120 | loss: 0.0921025350689888\n",
      "Epoch [1/1] | Batch 2121 | loss: 0.5853321552276611\n",
      "Epoch [1/1] | Batch 2122 | loss: 0.4072398841381073\n",
      "Epoch [1/1] | Batch 2123 | loss: 0.08979449421167374\n",
      "Epoch [1/1] | Batch 2124 | loss: 0.0005874200724065304\n",
      "Epoch [1/1] | Batch 2125 | loss: 0.1737520843744278\n",
      "Epoch [1/1] | Batch 2126 | loss: 0.16768424212932587\n",
      "Epoch [1/1] | Batch 2127 | loss: 0.45418116450309753\n",
      "Epoch [1/1] | Batch 2128 | loss: 0.12468263506889343\n",
      "Epoch [1/1] | Batch 2129 | loss: 0.06965792179107666\n",
      "Epoch [1/1] | Batch 2130 | loss: 0.023687191307544708\n",
      "Epoch [1/1] | Batch 2131 | loss: 0.04557536542415619\n",
      "Epoch [1/1] | Batch 2132 | loss: 0.22376497089862823\n",
      "Epoch [1/1] | Batch 2133 | loss: 0.23684188723564148\n",
      "Epoch [1/1] | Batch 2134 | loss: 0.03894953802227974\n",
      "Epoch [1/1] | Batch 2135 | loss: 0.09831452369689941\n",
      "Epoch [1/1] | Batch 2136 | loss: 0.08258220553398132\n",
      "Epoch [1/1] | Batch 2137 | loss: 0.15988455712795258\n",
      "Epoch [1/1] | Batch 2138 | loss: 0.030726604163646698\n",
      "Epoch [1/1] | Batch 2139 | loss: 0.062071897089481354\n",
      "Epoch [1/1] | Batch 2140 | loss: 0.13493889570236206\n",
      "Epoch [1/1] | Batch 2141 | loss: 0.02984379604458809\n",
      "Epoch [1/1] | Batch 2142 | loss: 0.24123013019561768\n",
      "Epoch [1/1] | Batch 2143 | loss: 0.015260417945683002\n",
      "Epoch [1/1] | Batch 2144 | loss: 0.2331216186285019\n",
      "Epoch [1/1] | Batch 2145 | loss: 0.49921390414237976\n",
      "Epoch [1/1] | Batch 2146 | loss: 0.13178299367427826\n",
      "Epoch [1/1] | Batch 2147 | loss: 0.5120524168014526\n",
      "Epoch [1/1] | Batch 2148 | loss: 0.02967143803834915\n",
      "Epoch [1/1] | Batch 2149 | loss: 0.22866907715797424\n",
      "Epoch [1/1] | Batch 2150 | loss: 0.49837493896484375\n",
      "Epoch [1/1] | Batch 2151 | loss: 0.12529292702674866\n",
      "Epoch [1/1] | Batch 2152 | loss: 0.05826553329825401\n",
      "Epoch [1/1] | Batch 2153 | loss: 0.3072434663772583\n",
      "Epoch [1/1] | Batch 2154 | loss: 0.009723824448883533\n",
      "Epoch [1/1] | Batch 2155 | loss: 0.15579578280448914\n",
      "Epoch [1/1] | Batch 2156 | loss: 0.005170300602912903\n",
      "Epoch [1/1] | Batch 2157 | loss: 0.006024646107107401\n",
      "Epoch [1/1] | Batch 2158 | loss: 0.4795898497104645\n",
      "Epoch [1/1] | Batch 2159 | loss: 0.3921573758125305\n",
      "Epoch [1/1] | Batch 2160 | loss: 0.18786674737930298\n",
      "Epoch [1/1] | Batch 2161 | loss: 0.21815745532512665\n",
      "Epoch [1/1] | Batch 2162 | loss: 0.14425045251846313\n",
      "Epoch [1/1] | Batch 2163 | loss: 0.015287798829376698\n",
      "Epoch [1/1] | Batch 2164 | loss: 0.4529379606246948\n",
      "Epoch [1/1] | Batch 2165 | loss: 0.08888361603021622\n",
      "Epoch [1/1] | Batch 2166 | loss: 0.0077439406886696815\n",
      "Epoch [1/1] | Batch 2167 | loss: 0.1995653510093689\n",
      "Epoch [1/1] | Batch 2168 | loss: 0.011079350486397743\n",
      "Epoch [1/1] | Batch 2169 | loss: 0.016863159835338593\n",
      "Epoch [1/1] | Batch 2170 | loss: 0.014117596670985222\n",
      "Epoch [1/1] | Batch 2171 | loss: 0.030638279393315315\n",
      "Epoch [1/1] | Batch 2172 | loss: 0.008329881355166435\n",
      "Epoch [1/1] | Batch 2173 | loss: 0.03705206885933876\n",
      "Epoch [1/1] | Batch 2174 | loss: 0.017499273642897606\n",
      "Epoch [1/1] | Batch 2175 | loss: 0.07227540761232376\n",
      "Epoch [1/1] | Batch 2176 | loss: 0.4251513183116913\n",
      "Epoch [1/1] | Batch 2177 | loss: 0.22102104127407074\n",
      "Epoch [1/1] | Batch 2178 | loss: 0.2901245653629303\n",
      "Epoch [1/1] | Batch 2179 | loss: 0.030281957238912582\n",
      "Epoch [1/1] | Batch 2180 | loss: 0.006420540157705545\n",
      "Epoch [1/1] | Batch 2181 | loss: 0.3040792942047119\n",
      "Epoch [1/1] | Batch 2182 | loss: 0.14035455882549286\n",
      "Epoch [1/1] | Batch 2183 | loss: 0.17452193796634674\n",
      "Epoch [1/1] | Batch 2184 | loss: 0.0369279608130455\n",
      "Epoch [1/1] | Batch 2185 | loss: 0.3303283154964447\n",
      "Epoch [1/1] | Batch 2186 | loss: 0.09903775900602341\n",
      "Epoch [1/1] | Batch 2187 | loss: 0.015315328724682331\n",
      "Epoch [1/1] | Batch 2188 | loss: 0.09823635220527649\n",
      "Epoch [1/1] | Batch 2189 | loss: 0.013223884627223015\n",
      "Epoch [1/1] | Batch 2190 | loss: 0.2190530151128769\n",
      "Epoch [1/1] | Batch 2191 | loss: 0.019003985449671745\n",
      "Epoch [1/1] | Batch 2192 | loss: 0.027494605630636215\n",
      "Epoch [1/1] | Batch 2193 | loss: 0.2571583092212677\n",
      "Epoch [1/1] | Batch 2194 | loss: 0.14266465604305267\n",
      "Epoch [1/1] | Batch 2195 | loss: 0.01928621716797352\n",
      "Epoch [1/1] | Batch 2196 | loss: 0.2429477721452713\n",
      "Epoch [1/1] | Batch 2197 | loss: 0.08708694577217102\n",
      "Epoch [1/1] | Batch 2198 | loss: 0.017444593831896782\n",
      "Epoch [1/1] | Batch 2199 | loss: 0.0414474792778492\n",
      "Epoch [1/1] | Batch 2200 | loss: 0.05019303783774376\n",
      "Epoch [1/1] | Batch 2201 | loss: 0.0770418643951416\n",
      "Epoch [1/1] | Batch 2202 | loss: 0.45779815316200256\n",
      "Epoch [1/1] | Batch 2203 | loss: 0.555831789970398\n",
      "Epoch [1/1] | Batch 2204 | loss: 0.626427412033081\n",
      "Epoch [1/1] | Batch 2205 | loss: 0.0735693871974945\n",
      "Epoch [1/1] | Batch 2206 | loss: 0.08335848897695541\n",
      "Epoch [1/1] | Batch 2207 | loss: 0.04835611954331398\n",
      "Epoch [1/1] | Batch 2208 | loss: 0.0473233163356781\n",
      "Epoch [1/1] | Batch 2209 | loss: 0.011728654615581036\n",
      "Epoch [1/1] | Batch 2210 | loss: 0.015900492668151855\n",
      "Epoch [1/1] | Batch 2211 | loss: 0.10954515635967255\n",
      "Epoch [1/1] | Batch 2212 | loss: 0.20375043153762817\n",
      "Epoch [1/1] | Batch 2213 | loss: 0.5055921077728271\n",
      "Epoch [1/1] | Batch 2214 | loss: 0.08091656863689423\n",
      "Epoch [1/1] | Batch 2215 | loss: 0.21280385553836823\n",
      "Epoch [1/1] | Batch 2216 | loss: 0.005837501026690006\n",
      "Epoch [1/1] | Batch 2217 | loss: 0.7455806136131287\n",
      "Epoch [1/1] | Batch 2218 | loss: 0.014301974326372147\n",
      "Epoch [1/1] | Batch 2219 | loss: 0.32139852643013\n",
      "Epoch [1/1] | Batch 2220 | loss: 0.03792104870080948\n",
      "Epoch [1/1] | Batch 2221 | loss: 0.253034770488739\n",
      "Epoch [1/1] | Batch 2222 | loss: 0.07265675812959671\n",
      "Epoch [1/1] | Batch 2223 | loss: 0.3056340515613556\n",
      "Epoch [1/1] | Batch 2224 | loss: 0.12214408814907074\n",
      "Epoch [1/1] | Batch 2225 | loss: 0.35996538400650024\n",
      "Epoch [1/1] | Batch 2226 | loss: 0.1836126148700714\n",
      "Epoch [1/1] | Batch 2227 | loss: 0.11037643253803253\n",
      "Epoch [1/1] | Batch 2228 | loss: 0.22697614133358002\n",
      "Epoch [1/1] | Batch 2229 | loss: 0.1019182950258255\n",
      "Epoch [1/1] | Batch 2230 | loss: 0.08264943957328796\n",
      "Epoch [1/1] | Batch 2231 | loss: 0.2523950934410095\n",
      "Epoch [1/1] | Batch 2232 | loss: 0.6337836980819702\n",
      "Epoch [1/1] | Batch 2233 | loss: 0.058930594474077225\n",
      "Epoch [1/1] | Batch 2234 | loss: 0.018631763756275177\n",
      "Epoch [1/1] | Batch 2235 | loss: 0.01458524540066719\n",
      "Epoch [1/1] | Batch 2236 | loss: 0.7992002964019775\n",
      "Epoch [1/1] | Batch 2237 | loss: 0.084448903799057\n",
      "Epoch [1/1] | Batch 2238 | loss: 1.0309125185012817\n",
      "Epoch [1/1] | Batch 2239 | loss: 0.13743677735328674\n",
      "Epoch [1/1] | Batch 2240 | loss: 0.664529025554657\n",
      "Epoch [1/1] | Batch 2241 | loss: 0.1023811474442482\n",
      "Epoch [1/1] | Batch 2242 | loss: 0.20694787800312042\n",
      "Epoch [1/1] | Batch 2243 | loss: 0.1251814216375351\n",
      "Epoch [1/1] | Batch 2244 | loss: 0.03933340683579445\n",
      "Epoch [1/1] | Batch 2245 | loss: 0.08479878306388855\n",
      "Epoch [1/1] | Batch 2246 | loss: 0.15472786128520966\n",
      "Epoch [1/1] | Batch 2247 | loss: 0.21765285730361938\n",
      "Epoch [1/1] | Batch 2248 | loss: 0.14445385336875916\n",
      "Epoch [1/1] | Batch 2249 | loss: 0.15024879574775696\n",
      "Epoch [1/1] | Batch 2250 | loss: 0.10468091070652008\n",
      "Epoch [1/1] | Batch 2251 | loss: 0.12562553584575653\n",
      "Epoch [1/1] | Batch 2252 | loss: 0.31892943382263184\n",
      "Epoch [1/1] | Batch 2253 | loss: 0.3657618761062622\n",
      "Epoch [1/1] | Batch 2254 | loss: 0.04998956620693207\n",
      "Epoch [1/1] | Batch 2255 | loss: 0.43002063035964966\n",
      "Epoch [1/1] | Batch 2256 | loss: 0.03310917317867279\n",
      "Epoch [1/1] | Batch 2257 | loss: 0.07736284285783768\n",
      "Epoch [1/1] | Batch 2258 | loss: 0.0075379847548902035\n",
      "Epoch [1/1] | Batch 2259 | loss: 0.12239555269479752\n",
      "Epoch [1/1] | Batch 2260 | loss: 0.005877944175153971\n",
      "Epoch [1/1] | Batch 2261 | loss: 0.18163716793060303\n",
      "Epoch [1/1] | Batch 2262 | loss: 0.14050090312957764\n",
      "Epoch [1/1] | Batch 2263 | loss: 0.049444638192653656\n",
      "Epoch [1/1] | Batch 2264 | loss: 0.17240241169929504\n",
      "Epoch [1/1] | Batch 2265 | loss: 0.24827857315540314\n",
      "Epoch [1/1] | Batch 2266 | loss: 0.6301851272583008\n",
      "Epoch [1/1] | Batch 2267 | loss: 0.277521550655365\n",
      "Epoch [1/1] | Batch 2268 | loss: 0.21309994161128998\n",
      "Epoch [1/1] | Batch 2269 | loss: 0.05241410434246063\n",
      "Epoch [1/1] | Batch 2270 | loss: 0.08806045353412628\n",
      "Epoch [1/1] | Batch 2271 | loss: 0.07148262858390808\n",
      "Epoch [1/1] | Batch 2272 | loss: 0.028058413416147232\n",
      "Epoch [1/1] | Batch 2273 | loss: 0.01403837651014328\n",
      "Epoch [1/1] | Batch 2274 | loss: 0.011226301081478596\n",
      "Epoch [1/1] | Batch 2275 | loss: 0.07470311969518661\n",
      "Epoch [1/1] | Batch 2276 | loss: 0.012638014741241932\n",
      "Epoch [1/1] | Batch 2277 | loss: 0.009023862890899181\n",
      "Epoch [1/1] | Batch 2278 | loss: 0.10626521706581116\n",
      "Epoch [1/1] | Batch 2279 | loss: 0.14561890065670013\n",
      "Epoch [1/1] | Batch 2280 | loss: 0.2138323336839676\n",
      "Epoch [1/1] | Batch 2281 | loss: 0.09530753642320633\n",
      "Epoch [1/1] | Batch 2282 | loss: 0.15648332238197327\n",
      "Epoch [1/1] | Batch 2283 | loss: 0.33861204981803894\n",
      "Epoch [1/1] | Batch 2284 | loss: 0.5195324420928955\n",
      "Epoch [1/1] | Batch 2285 | loss: 0.15443387627601624\n",
      "Epoch [1/1] | Batch 2286 | loss: 0.008768717758357525\n",
      "Epoch [1/1] | Batch 2287 | loss: 0.048579417169094086\n",
      "Epoch [1/1] | Batch 2288 | loss: 0.7806223630905151\n",
      "Epoch [1/1] | Batch 2289 | loss: 0.22661349177360535\n",
      "Epoch [1/1] | Batch 2290 | loss: 0.34821611642837524\n",
      "Epoch [1/1] | Batch 2291 | loss: 0.226692795753479\n",
      "Epoch [1/1] | Batch 2292 | loss: 0.4162514805793762\n",
      "Epoch [1/1] | Batch 2293 | loss: 0.472547322511673\n",
      "Epoch [1/1] | Batch 2294 | loss: 0.048358820378780365\n",
      "Epoch [1/1] | Batch 2295 | loss: 0.018653081730008125\n",
      "Epoch [1/1] | Batch 2296 | loss: 0.03067728318274021\n",
      "Epoch [1/1] | Batch 2297 | loss: 0.6186656951904297\n",
      "Epoch [1/1] | Batch 2298 | loss: 0.019511539489030838\n",
      "Epoch [1/1] | Batch 2299 | loss: 0.05840772017836571\n",
      "Epoch [1/1] | Batch 2300 | loss: 0.5405575037002563\n",
      "Epoch [1/1] | Batch 2301 | loss: 0.043396200984716415\n",
      "Epoch [1/1] | Batch 2302 | loss: 0.5055064558982849\n",
      "Epoch [1/1] | Batch 2303 | loss: 0.07795047014951706\n",
      "Epoch [1/1] | Batch 2304 | loss: 0.008990693837404251\n",
      "Epoch [1/1] | Batch 2305 | loss: 0.25298747420310974\n",
      "Epoch [1/1] | Batch 2306 | loss: 0.26738205552101135\n",
      "Epoch [1/1] | Batch 2307 | loss: 0.1249801442027092\n",
      "Epoch [1/1] | Batch 2308 | loss: 0.6852352619171143\n",
      "Epoch [1/1] | Batch 2309 | loss: 0.06621631234884262\n",
      "Epoch [1/1] | Batch 2310 | loss: 0.011060155928134918\n",
      "Epoch [1/1] | Batch 2311 | loss: 0.06186424940824509\n",
      "Epoch [1/1] | Batch 2312 | loss: 0.1181817501783371\n",
      "Epoch [1/1] | Batch 2313 | loss: 0.2502879798412323\n",
      "Epoch [1/1] | Batch 2314 | loss: 0.22019188106060028\n",
      "Epoch [1/1] | Batch 2315 | loss: 0.05253376439213753\n",
      "Epoch [1/1] | Batch 2316 | loss: 0.47052645683288574\n",
      "Epoch [1/1] | Batch 2317 | loss: 0.1252286434173584\n",
      "Epoch [1/1] | Batch 2318 | loss: 0.01372066792100668\n",
      "Epoch [1/1] | Batch 2319 | loss: 0.5071254968643188\n",
      "Epoch [1/1] | Batch 2320 | loss: 0.0783936008810997\n",
      "Epoch [1/1] | Batch 2321 | loss: 0.031944192945957184\n",
      "Epoch [1/1] | Batch 2322 | loss: 0.1277594119310379\n",
      "Epoch [1/1] | Batch 2323 | loss: 0.01323755644261837\n",
      "Epoch [1/1] | Batch 2324 | loss: 0.02649357169866562\n",
      "Epoch [1/1] | Batch 2325 | loss: 0.08078517764806747\n",
      "Epoch [1/1] | Batch 2326 | loss: 0.03208337351679802\n",
      "Epoch [1/1] | Batch 2327 | loss: 0.009395280852913857\n",
      "Epoch [1/1] | Batch 2328 | loss: 0.28433382511138916\n",
      "Epoch [1/1] | Batch 2329 | loss: 0.048806026577949524\n",
      "Epoch [1/1] | Batch 2330 | loss: 0.022883839905261993\n",
      "Epoch [1/1] | Batch 2331 | loss: 0.1514836698770523\n",
      "Epoch [1/1] | Batch 2332 | loss: 0.2478565275669098\n",
      "Epoch [1/1] | Batch 2333 | loss: 0.16550889611244202\n",
      "Epoch [1/1] | Batch 2334 | loss: 0.18963128328323364\n",
      "Epoch [1/1] | Batch 2335 | loss: 0.004940091632306576\n",
      "Epoch [1/1] | Batch 2336 | loss: 0.011263505555689335\n",
      "Epoch [1/1] | Batch 2337 | loss: 0.15377220511436462\n",
      "Epoch [1/1] | Batch 2338 | loss: 0.34258028864860535\n",
      "Epoch [1/1] | Batch 2339 | loss: 0.3043110966682434\n",
      "Epoch [1/1] | Batch 2340 | loss: 0.16072414815425873\n",
      "Epoch [1/1] | Batch 2341 | loss: 0.10604768991470337\n",
      "Epoch [1/1] | Batch 2342 | loss: 0.21762005984783173\n",
      "Epoch [1/1] | Batch 2343 | loss: 0.04344398155808449\n",
      "Epoch [1/1] | Batch 2344 | loss: 0.06287284195423126\n",
      "Epoch [1/1] | Batch 2345 | loss: 0.3373579978942871\n",
      "Epoch [1/1] | Batch 2346 | loss: 0.027698276564478874\n",
      "Epoch [1/1] | Batch 2347 | loss: 0.4371981918811798\n",
      "Epoch [1/1] | Batch 2348 | loss: 0.2114877551794052\n",
      "Epoch [1/1] | Batch 2349 | loss: 0.24586106836795807\n",
      "Epoch [1/1] | Batch 2350 | loss: 0.0170070119202137\n",
      "Epoch [1/1] | Batch 2351 | loss: 0.274894118309021\n",
      "Epoch [1/1] | Batch 2352 | loss: 0.3132660984992981\n",
      "Epoch [1/1] | Batch 2353 | loss: 0.04290756955742836\n",
      "Epoch [1/1] | Batch 2354 | loss: 0.1450204700231552\n",
      "Epoch [1/1] | Batch 2355 | loss: 0.24080516397953033\n",
      "Epoch [1/1] | Batch 2356 | loss: 0.049070339649915695\n",
      "Epoch [1/1] | Batch 2357 | loss: 0.06791317462921143\n",
      "Epoch [1/1] | Batch 2358 | loss: 0.1863134503364563\n",
      "Epoch [1/1] | Batch 2359 | loss: 0.30042022466659546\n",
      "Epoch [1/1] | Batch 2360 | loss: 0.03005250357091427\n",
      "Epoch [1/1] | Batch 2361 | loss: 0.12994498014450073\n",
      "Epoch [1/1] | Batch 2362 | loss: 0.08165043592453003\n",
      "Epoch [1/1] | Batch 2363 | loss: 0.415474534034729\n",
      "Epoch [1/1] | Batch 2364 | loss: 0.08876701444387436\n",
      "Epoch [1/1] | Batch 2365 | loss: 0.12888461351394653\n",
      "Epoch [1/1] | Batch 2366 | loss: 0.3117528259754181\n",
      "Epoch [1/1] | Batch 2367 | loss: 0.018365221098065376\n",
      "Epoch [1/1] | Batch 2368 | loss: 0.03805077075958252\n",
      "Epoch [1/1] | Batch 2369 | loss: 0.22853507101535797\n",
      "Epoch [1/1] | Batch 2370 | loss: 0.31004926562309265\n",
      "Epoch [1/1] | Batch 2371 | loss: 0.030091065913438797\n",
      "Epoch [1/1] | Batch 2372 | loss: 0.03380102291703224\n",
      "Epoch [1/1] | Batch 2373 | loss: 0.04109327867627144\n",
      "Epoch [1/1] | Batch 2374 | loss: 0.211442768573761\n",
      "Epoch [1/1] | Batch 2375 | loss: 0.026530100032687187\n",
      "Epoch [1/1] | Batch 2376 | loss: 0.3649738132953644\n",
      "Epoch [1/1] | Batch 2377 | loss: 0.015018940903246403\n",
      "Epoch [1/1] | Batch 2378 | loss: 0.29299086332321167\n",
      "Epoch [1/1] | Batch 2379 | loss: 0.023371722549200058\n",
      "Epoch [1/1] | Batch 2380 | loss: 0.023698879405856133\n",
      "Epoch [1/1] | Batch 2381 | loss: 0.12006241828203201\n",
      "Epoch [1/1] | Batch 2382 | loss: 0.057466357946395874\n",
      "Epoch [1/1] | Batch 2383 | loss: 0.00812811404466629\n",
      "Epoch [1/1] | Batch 2384 | loss: 0.07774427533149719\n",
      "Epoch [1/1] | Batch 2385 | loss: 0.08619143813848495\n",
      "Epoch [1/1] | Batch 2386 | loss: 0.7901914715766907\n",
      "Epoch [1/1] | Batch 2387 | loss: 0.18750213086605072\n",
      "Epoch [1/1] | Batch 2388 | loss: 0.20705966651439667\n",
      "Epoch [1/1] | Batch 2389 | loss: 0.1503099650144577\n",
      "Epoch [1/1] | Batch 2390 | loss: 0.017338957637548447\n",
      "Epoch [1/1] | Batch 2391 | loss: 0.02533029578626156\n",
      "Epoch [1/1] | Batch 2392 | loss: 0.06410522758960724\n",
      "Epoch [1/1] | Batch 2393 | loss: 0.13455045223236084\n",
      "Epoch [1/1] | Batch 2394 | loss: 0.12877966463565826\n",
      "Epoch [1/1] | Batch 2395 | loss: 0.5769244432449341\n",
      "Epoch [1/1] | Batch 2396 | loss: 0.27645236253738403\n",
      "Epoch [1/1] | Batch 2397 | loss: 0.005410061217844486\n",
      "Epoch [1/1] | Batch 2398 | loss: 0.022519057616591454\n",
      "Epoch [1/1] | Batch 2399 | loss: 0.037973444908857346\n",
      "Epoch [1/1] | Batch 2400 | loss: 0.034250855445861816\n",
      "Epoch [1/1] | Batch 2401 | loss: 0.02775900810956955\n",
      "Epoch [1/1] | Batch 2402 | loss: 0.0030127919744700193\n",
      "Epoch [1/1] | Batch 2403 | loss: 0.010254577733576298\n",
      "Epoch [1/1] | Batch 2404 | loss: 0.19497062265872955\n",
      "Epoch [1/1] | Batch 2405 | loss: 0.006596606690436602\n",
      "Epoch [1/1] | Batch 2406 | loss: 0.12410857528448105\n",
      "Epoch [1/1] | Batch 2407 | loss: 0.09250467270612717\n",
      "Epoch [1/1] | Batch 2408 | loss: 0.12865164875984192\n",
      "Epoch [1/1] | Batch 2409 | loss: 0.10908112674951553\n",
      "Epoch [1/1] | Batch 2410 | loss: 0.0066165197640657425\n",
      "Epoch [1/1] | Batch 2411 | loss: 0.09656825661659241\n",
      "Epoch [1/1] | Batch 2412 | loss: 0.0398893728852272\n",
      "Epoch [1/1] | Batch 2413 | loss: 0.004795754328370094\n",
      "Epoch [1/1] | Batch 2414 | loss: 0.07929446548223495\n",
      "Epoch [1/1] | Batch 2415 | loss: 0.004839740693569183\n",
      "Epoch [1/1] | Batch 2416 | loss: 0.04761675372719765\n",
      "Epoch [1/1] | Batch 2417 | loss: 0.12321605533361435\n",
      "Epoch [1/1] | Batch 2418 | loss: 0.06755995005369186\n",
      "Epoch [1/1] | Batch 2419 | loss: 0.012920128181576729\n",
      "Epoch [1/1] | Batch 2420 | loss: 0.48086851835250854\n",
      "Epoch [1/1] | Batch 2421 | loss: 0.022467242553830147\n",
      "Epoch [1/1] | Batch 2422 | loss: 0.5512670874595642\n",
      "Epoch [1/1] | Batch 2423 | loss: 0.05910700559616089\n",
      "Epoch [1/1] | Batch 2424 | loss: 0.10036671161651611\n",
      "Epoch [1/1] | Batch 2425 | loss: 0.17491577565670013\n",
      "Epoch [1/1] | Batch 2426 | loss: 0.02654188685119152\n",
      "Epoch [1/1] | Batch 2427 | loss: 0.03287629038095474\n",
      "Epoch [1/1] | Batch 2428 | loss: 0.5784847736358643\n",
      "Epoch [1/1] | Batch 2429 | loss: 0.1616290807723999\n",
      "Epoch [1/1] | Batch 2430 | loss: 0.44475629925727844\n",
      "Epoch [1/1] | Batch 2431 | loss: 0.014237779192626476\n",
      "Epoch [1/1] | Batch 2432 | loss: 0.05807497352361679\n",
      "Epoch [1/1] | Batch 2433 | loss: 0.33049821853637695\n",
      "Epoch [1/1] | Batch 2434 | loss: 0.12300186604261398\n",
      "Epoch [1/1] | Batch 2435 | loss: 0.02287272736430168\n",
      "Epoch [1/1] | Batch 2436 | loss: 0.005766590125858784\n",
      "Epoch [1/1] | Batch 2437 | loss: 0.20240508019924164\n",
      "Epoch [1/1] | Batch 2438 | loss: 0.1670694798231125\n",
      "Epoch [1/1] | Batch 2439 | loss: 0.4806341230869293\n",
      "Epoch [1/1] | Batch 2440 | loss: 0.6153965592384338\n",
      "Epoch [1/1] | Batch 2441 | loss: 0.013008899055421352\n",
      "Epoch [1/1] | Batch 2442 | loss: 0.004602277651429176\n",
      "Epoch [1/1] | Batch 2443 | loss: 0.021815098822116852\n",
      "Epoch [1/1] | Batch 2444 | loss: 0.2126595377922058\n",
      "Epoch [1/1] | Batch 2445 | loss: 0.03944791108369827\n",
      "Epoch [1/1] | Batch 2446 | loss: 0.446659654378891\n",
      "Epoch [1/1] | Batch 2447 | loss: 0.027066314592957497\n",
      "Epoch [1/1] | Batch 2448 | loss: 0.11515287309885025\n",
      "Epoch [1/1] | Batch 2449 | loss: 0.13756364583969116\n",
      "Epoch [1/1] | Batch 2450 | loss: 0.20237722992897034\n",
      "Epoch [1/1] | Batch 2451 | loss: 0.43050405383110046\n",
      "Epoch [1/1] | Batch 2452 | loss: 0.05971122160553932\n",
      "Epoch [1/1] | Batch 2453 | loss: 0.4108002781867981\n",
      "Epoch [1/1] | Batch 2454 | loss: 0.16903553903102875\n",
      "Epoch [1/1] | Batch 2455 | loss: 0.13323910534381866\n",
      "Epoch [1/1] | Batch 2456 | loss: 0.014724689535796642\n",
      "Epoch [1/1] | Batch 2457 | loss: 0.0072981975972652435\n",
      "Epoch [1/1] | Batch 2458 | loss: 0.013840146362781525\n",
      "Epoch [1/1] | Batch 2459 | loss: 0.1537081003189087\n",
      "Epoch [1/1] | Batch 2460 | loss: 0.02662760578095913\n",
      "Epoch [1/1] | Batch 2461 | loss: 0.3812181353569031\n",
      "Epoch [1/1] | Batch 2462 | loss: 0.25512537360191345\n",
      "Epoch [1/1] | Batch 2463 | loss: 0.032827697694301605\n",
      "Epoch [1/1] | Batch 2464 | loss: 0.22253791987895966\n",
      "Epoch [1/1] | Batch 2465 | loss: 0.207350492477417\n",
      "Epoch [1/1] | Batch 2466 | loss: 0.17623703181743622\n",
      "Epoch [1/1] | Batch 2467 | loss: 0.03902468830347061\n",
      "Epoch [1/1] | Batch 2468 | loss: 0.00925704836845398\n",
      "Epoch [1/1] | Batch 2469 | loss: 0.018753955140709877\n",
      "Epoch [1/1] | Batch 2470 | loss: 0.008813492953777313\n",
      "Epoch [1/1] | Batch 2471 | loss: 0.22767701745033264\n",
      "Epoch [1/1] | Batch 2472 | loss: 0.02252686582505703\n",
      "Epoch [1/1] | Batch 2473 | loss: 0.09634456038475037\n",
      "Epoch [1/1] | Batch 2474 | loss: 0.38188278675079346\n",
      "Epoch [1/1] | Batch 2475 | loss: 0.18268150091171265\n",
      "Epoch [1/1] | Batch 2476 | loss: 0.05730405077338219\n",
      "Epoch [1/1] | Batch 2477 | loss: 0.28872376680374146\n",
      "Epoch [1/1] | Batch 2478 | loss: 0.013727105222642422\n",
      "Epoch [1/1] | Batch 2479 | loss: 0.1427481472492218\n",
      "Epoch [1/1] | Batch 2480 | loss: 0.11737865954637527\n",
      "Epoch [1/1] | Batch 2481 | loss: 0.0708363950252533\n",
      "Epoch [1/1] | Batch 2482 | loss: 0.002767785917967558\n",
      "Epoch [1/1] | Batch 2483 | loss: 0.400511771440506\n",
      "Epoch [1/1] | Batch 2484 | loss: 0.0824989527463913\n",
      "Epoch [1/1] | Batch 2485 | loss: 0.006456105504184961\n",
      "Epoch [1/1] | Batch 2486 | loss: 0.04696686193346977\n",
      "Epoch [1/1] | Batch 2487 | loss: 0.9599890112876892\n",
      "Epoch [1/1] | Batch 2488 | loss: 0.12285155057907104\n",
      "Epoch [1/1] | Batch 2489 | loss: 0.02750249207019806\n",
      "Epoch [1/1] | Batch 2490 | loss: 0.014154214411973953\n",
      "Epoch [1/1] | Batch 2491 | loss: 0.4021478593349457\n",
      "Epoch [1/1] | Batch 2492 | loss: 0.3083038032054901\n",
      "Epoch [1/1] | Batch 2493 | loss: 0.10159008949995041\n",
      "Epoch [1/1] | Batch 2494 | loss: 0.02605002000927925\n",
      "Epoch [1/1] | Batch 2495 | loss: 0.13529188930988312\n",
      "Epoch [1/1] | Batch 2496 | loss: 0.03345715254545212\n",
      "Epoch [1/1] | Batch 2497 | loss: 0.03347887471318245\n",
      "Epoch [1/1] | Batch 2498 | loss: 1.4761273860931396\n",
      "Epoch [1/1] | Batch 2499 | loss: 0.3116726279258728\n",
      "Epoch [1/1] | Batch 2500 | loss: 0.012670096941292286\n",
      "Epoch [1/1] | Batch 2501 | loss: 0.18921326100826263\n",
      "Epoch [1/1] | Batch 2502 | loss: 0.14229868352413177\n",
      "Epoch [1/1] | Batch 2503 | loss: 0.12434373050928116\n",
      "Epoch [1/1] | Batch 2504 | loss: 0.16594836115837097\n",
      "Epoch [1/1] | Batch 2505 | loss: 0.08521207422018051\n",
      "Epoch [1/1] | Batch 2506 | loss: 0.04075911268591881\n",
      "Epoch [1/1] | Batch 2507 | loss: 0.10785886645317078\n",
      "Epoch [1/1] | Batch 2508 | loss: 0.2751431167125702\n",
      "Epoch [1/1] | Batch 2509 | loss: 0.079875148832798\n",
      "Epoch [1/1] | Batch 2510 | loss: 0.09738264232873917\n",
      "Epoch [1/1] | Batch 2511 | loss: 0.016336269676685333\n",
      "Epoch [1/1] | Batch 2512 | loss: 0.3634606599807739\n",
      "Epoch [1/1] | Batch 2513 | loss: 0.2256183922290802\n",
      "Epoch [1/1] | Batch 2514 | loss: 0.10557793080806732\n",
      "Epoch [1/1] | Batch 2515 | loss: 0.09410932660102844\n",
      "Epoch [1/1] | Batch 2516 | loss: 0.017482653260231018\n",
      "Epoch [1/1] | Batch 2517 | loss: 0.023992236703634262\n",
      "Epoch [1/1] | Batch 2518 | loss: 0.039867039769887924\n",
      "Epoch [1/1] | Batch 2519 | loss: 0.002112783258780837\n",
      "Epoch [1/1] | Batch 2520 | loss: 0.6120306253433228\n",
      "Epoch [1/1] | Batch 2521 | loss: 0.3774232268333435\n",
      "Epoch [1/1] | Batch 2522 | loss: 0.04472828656435013\n",
      "Epoch [1/1] | Batch 2523 | loss: 0.3455469608306885\n",
      "Epoch [1/1] | Batch 2524 | loss: 0.16341468691825867\n",
      "Epoch [1/1] | Batch 2525 | loss: 0.0843094065785408\n",
      "Epoch [1/1] | Batch 2526 | loss: 0.005753018893301487\n",
      "Epoch [1/1] | Batch 2527 | loss: 0.019544057548046112\n",
      "Epoch [1/1] | Batch 2528 | loss: 0.1843692809343338\n",
      "Epoch [1/1] | Batch 2529 | loss: 0.06273532658815384\n",
      "Epoch [1/1] | Batch 2530 | loss: 0.04310706630349159\n",
      "Epoch [1/1] | Batch 2531 | loss: 0.05607205256819725\n",
      "Epoch [1/1] | Batch 2532 | loss: 0.7936007380485535\n",
      "Epoch [1/1] | Batch 2533 | loss: 0.036678630858659744\n",
      "Epoch [1/1] | Batch 2534 | loss: 0.10097837448120117\n",
      "Epoch [1/1] | Batch 2535 | loss: 0.060188159346580505\n",
      "Epoch [1/1] | Batch 2536 | loss: 0.023265093564987183\n",
      "Epoch [1/1] | Batch 2537 | loss: 0.3930867314338684\n",
      "Epoch [1/1] | Batch 2538 | loss: 0.10577607899904251\n",
      "Epoch [1/1] | Batch 2539 | loss: 0.01546517014503479\n",
      "Epoch [1/1] | Batch 2540 | loss: 0.08596748113632202\n",
      "Epoch [1/1] | Batch 2541 | loss: 0.07168874889612198\n",
      "Epoch [1/1] | Batch 2542 | loss: 0.009229902178049088\n",
      "Epoch [1/1] | Batch 2543 | loss: 0.2117735594511032\n",
      "Epoch [1/1] | Batch 2544 | loss: 0.03720548003911972\n",
      "Epoch [1/1] | Batch 2545 | loss: 0.0948513001203537\n",
      "Epoch [1/1] | Batch 2546 | loss: 0.15292538702487946\n",
      "Epoch [1/1] | Batch 2547 | loss: 0.27528926730155945\n",
      "Epoch [1/1] | Batch 2548 | loss: 0.09071553498506546\n",
      "Epoch [1/1] | Batch 2549 | loss: 0.23586973547935486\n",
      "Epoch [1/1] | Batch 2550 | loss: 0.1394861787557602\n",
      "Epoch [1/1] | Batch 2551 | loss: 0.051119305193424225\n",
      "Epoch [1/1] | Batch 2552 | loss: 0.02676110528409481\n",
      "Epoch [1/1] | Batch 2553 | loss: 0.04380682855844498\n",
      "Epoch [1/1] | Batch 2554 | loss: 0.1540745347738266\n",
      "Epoch [1/1] | Batch 2555 | loss: 0.2503196597099304\n",
      "Epoch [1/1] | Batch 2556 | loss: 0.013151543214917183\n",
      "Epoch [1/1] | Batch 2557 | loss: 0.009414265863597393\n",
      "Epoch [1/1] | Batch 2558 | loss: 0.06786651909351349\n",
      "Epoch [1/1] | Batch 2559 | loss: 0.2178291529417038\n",
      "Epoch [1/1] | Batch 2560 | loss: 0.025333944708108902\n",
      "Epoch [1/1] | Batch 2561 | loss: 0.04113727807998657\n",
      "Epoch [1/1] | Batch 2562 | loss: 0.16955262422561646\n",
      "Epoch [1/1] | Batch 2563 | loss: 0.056056905537843704\n",
      "Epoch [1/1] | Batch 2564 | loss: 0.1643121838569641\n",
      "Epoch [1/1] | Batch 2565 | loss: 0.008426680229604244\n",
      "Epoch [1/1] | Batch 2566 | loss: 0.03002586029469967\n",
      "Epoch [1/1] | Batch 2567 | loss: 0.20799754559993744\n",
      "Epoch [1/1] | Batch 2568 | loss: 0.11444462835788727\n",
      "Epoch [1/1] | Batch 2569 | loss: 0.022295257076621056\n",
      "Epoch [1/1] | Batch 2570 | loss: 0.017341580241918564\n",
      "Epoch [1/1] | Batch 2571 | loss: 0.04051928594708443\n",
      "Epoch [1/1] | Batch 2572 | loss: 0.03641834482550621\n",
      "Epoch [1/1] | Batch 2573 | loss: 0.1672758013010025\n",
      "Epoch [1/1] | Batch 2574 | loss: 0.00208291900344193\n",
      "Epoch [1/1] | Batch 2575 | loss: 0.021578676998615265\n",
      "Epoch [1/1] | Batch 2576 | loss: 0.2697955369949341\n",
      "Epoch [1/1] | Batch 2577 | loss: 0.06870532780885696\n",
      "Epoch [1/1] | Batch 2578 | loss: 0.016974937170743942\n",
      "Epoch [1/1] | Batch 2579 | loss: 0.2862481474876404\n",
      "Epoch [1/1] | Batch 2580 | loss: 0.004623904824256897\n",
      "Epoch [1/1] | Batch 2581 | loss: 0.06014645844697952\n",
      "Epoch [1/1] | Batch 2582 | loss: 0.007562946528196335\n",
      "Epoch [1/1] | Batch 2583 | loss: 0.06589016318321228\n",
      "Epoch [1/1] | Batch 2584 | loss: 0.08836324512958527\n",
      "Epoch [1/1] | Batch 2585 | loss: 0.15785405039787292\n",
      "Epoch [1/1] | Batch 2586 | loss: 0.04207818582653999\n",
      "Epoch [1/1] | Batch 2587 | loss: 0.07921679317951202\n",
      "Epoch [1/1] | Batch 2588 | loss: 0.07404737919569016\n",
      "Epoch [1/1] | Batch 2589 | loss: 0.11646409332752228\n",
      "Epoch [1/1] | Batch 2590 | loss: 0.07200619578361511\n",
      "Epoch [1/1] | Batch 2591 | loss: 0.4366796910762787\n",
      "Epoch [1/1] | Batch 2592 | loss: 0.2509363293647766\n",
      "Epoch [1/1] | Batch 2593 | loss: 0.2815328538417816\n",
      "Epoch [1/1] | Batch 2594 | loss: 0.15589070320129395\n",
      "Epoch [1/1] | Batch 2595 | loss: 0.44331982731819153\n",
      "Epoch [1/1] | Batch 2596 | loss: 0.08651294559240341\n",
      "Epoch [1/1] | Batch 2597 | loss: 0.3916759788990021\n",
      "Epoch [1/1] | Batch 2598 | loss: 0.6115027666091919\n",
      "Epoch [1/1] | Batch 2599 | loss: 0.07022294402122498\n",
      "Epoch [1/1] | Batch 2600 | loss: 0.1873658001422882\n",
      "Epoch [1/1] | Batch 2601 | loss: 0.22260171175003052\n",
      "Epoch [1/1] | Batch 2602 | loss: 0.03780990466475487\n",
      "Epoch [1/1] | Batch 2603 | loss: 0.15101714432239532\n",
      "Epoch [1/1] | Batch 2604 | loss: 0.18820913136005402\n",
      "Epoch [1/1] | Batch 2605 | loss: 0.3842257559299469\n",
      "Epoch [1/1] | Batch 2606 | loss: 0.4407801032066345\n",
      "Epoch [1/1] | Batch 2607 | loss: 0.16565632820129395\n",
      "Epoch [1/1] | Batch 2608 | loss: 0.04986126720905304\n",
      "Epoch [1/1] | Batch 2609 | loss: 0.020270396023988724\n",
      "Epoch [1/1] | Batch 2610 | loss: 0.02864212542772293\n",
      "Epoch [1/1] | Batch 2611 | loss: 0.6458842158317566\n",
      "Epoch [1/1] | Batch 2612 | loss: 0.13641709089279175\n",
      "Epoch [1/1] | Batch 2613 | loss: 0.2619282007217407\n",
      "Epoch [1/1] | Batch 2614 | loss: 0.18324023485183716\n",
      "Epoch [1/1] | Batch 2615 | loss: 0.3414185643196106\n",
      "Epoch [1/1] | Batch 2616 | loss: 0.2343355417251587\n",
      "Epoch [1/1] | Batch 2617 | loss: 0.14232765138149261\n",
      "Epoch [1/1] | Batch 2618 | loss: 0.0831427052617073\n",
      "Epoch [1/1] | Batch 2619 | loss: 0.00302006839774549\n",
      "Epoch [1/1] | Batch 2620 | loss: 0.5223491787910461\n",
      "Epoch [1/1] | Batch 2621 | loss: 0.09629052132368088\n",
      "Epoch [1/1] | Batch 2622 | loss: 0.37049567699432373\n",
      "Epoch [1/1] | Batch 2623 | loss: 0.023512087762355804\n",
      "Epoch [1/1] | Batch 2624 | loss: 0.1550445854663849\n",
      "Epoch [1/1] | Batch 2625 | loss: 0.10083803534507751\n",
      "Epoch [1/1] | Batch 2626 | loss: 0.04564693942666054\n",
      "Epoch [1/1] | Batch 2627 | loss: 0.11469241976737976\n",
      "Epoch [1/1] | Batch 2628 | loss: 0.5095788240432739\n",
      "Epoch [1/1] | Batch 2629 | loss: 0.23350931704044342\n",
      "Epoch [1/1] | Batch 2630 | loss: 0.17095038294792175\n",
      "Epoch [1/1] | Batch 2631 | loss: 0.24068091809749603\n",
      "Epoch [1/1] | Batch 2632 | loss: 0.29844194650650024\n",
      "Epoch [1/1] | Batch 2633 | loss: 0.13040481507778168\n",
      "Epoch [1/1] | Batch 2634 | loss: 0.04824928566813469\n",
      "Epoch [1/1] | Batch 2635 | loss: 0.19254158437252045\n",
      "Epoch [1/1] | Batch 2636 | loss: 0.013593349605798721\n",
      "Epoch [1/1] | Batch 2637 | loss: 0.29474401473999023\n",
      "Epoch [1/1] | Batch 2638 | loss: 0.3133459985256195\n",
      "Epoch [1/1] | Batch 2639 | loss: 0.024515563622117043\n",
      "Epoch [1/1] | Batch 2640 | loss: 0.20489992201328278\n",
      "Epoch [1/1] | Batch 2641 | loss: 0.027014149352908134\n",
      "Epoch [1/1] | Batch 2642 | loss: 0.15656143426895142\n",
      "Epoch [1/1] | Batch 2643 | loss: 0.11883649230003357\n",
      "Epoch [1/1] | Batch 2644 | loss: 0.028544185683131218\n",
      "Epoch [1/1] | Batch 2645 | loss: 0.015044993720948696\n",
      "Epoch [1/1] | Batch 2646 | loss: 0.37085455656051636\n",
      "Epoch [1/1] | Batch 2647 | loss: 0.6222240924835205\n",
      "Epoch [1/1] | Batch 2648 | loss: 0.046899937093257904\n",
      "Epoch [1/1] | Batch 2649 | loss: 0.015947652980685234\n",
      "Epoch [1/1] | Batch 2650 | loss: 0.005266884341835976\n",
      "Epoch [1/1] | Batch 2651 | loss: 0.045137908309698105\n",
      "Epoch [1/1] | Batch 2652 | loss: 0.012637102976441383\n",
      "Epoch [1/1] | Batch 2653 | loss: 0.18980355560779572\n",
      "Epoch [1/1] | Batch 2654 | loss: 0.0483117438852787\n",
      "Epoch [1/1] | Batch 2655 | loss: 0.03421638906002045\n",
      "Epoch [1/1] | Batch 2656 | loss: 0.35989853739738464\n",
      "Epoch [1/1] | Batch 2657 | loss: 0.13275109231472015\n",
      "Epoch [1/1] | Batch 2658 | loss: 0.043643735349178314\n",
      "Epoch [1/1] | Batch 2659 | loss: 0.5917342901229858\n",
      "Epoch [1/1] | Batch 2660 | loss: 0.017001735046505928\n",
      "Epoch [1/1] | Batch 2661 | loss: 0.15984219312667847\n",
      "Epoch [1/1] | Batch 2662 | loss: 0.21822993457317352\n",
      "Epoch [1/1] | Batch 2663 | loss: 0.02466401271522045\n",
      "Epoch [1/1] | Batch 2664 | loss: 0.30442097783088684\n",
      "Epoch [1/1] | Batch 2665 | loss: 0.05841629207134247\n",
      "Epoch [1/1] | Batch 2666 | loss: 0.1397116631269455\n",
      "Epoch [1/1] | Batch 2667 | loss: 0.08635552227497101\n",
      "Epoch [1/1] | Batch 2668 | loss: 0.28492674231529236\n",
      "Epoch [1/1] | Batch 2669 | loss: 0.06449585407972336\n",
      "Epoch [1/1] | Batch 2670 | loss: 0.12400351464748383\n",
      "Epoch [1/1] | Batch 2671 | loss: 0.008357769809663296\n",
      "Epoch [1/1] | Batch 2672 | loss: 0.16057108342647552\n",
      "Epoch [1/1] | Batch 2673 | loss: 0.012163814157247543\n",
      "Epoch [1/1] | Batch 2674 | loss: 0.011777722276747227\n",
      "Epoch [1/1] | Batch 2675 | loss: 0.13528946042060852\n",
      "Epoch [1/1] | Batch 2676 | loss: 0.0538909025490284\n",
      "Epoch [1/1] | Batch 2677 | loss: 0.00779319740831852\n",
      "Epoch [1/1] | Batch 2678 | loss: 0.15976591408252716\n",
      "Epoch [1/1] | Batch 2679 | loss: 0.11203578859567642\n",
      "Epoch [1/1] | Batch 2680 | loss: 0.14084935188293457\n",
      "Epoch [1/1] | Batch 2681 | loss: 0.0036336397752165794\n",
      "Epoch [1/1] | Batch 2682 | loss: 0.3818275034427643\n",
      "Epoch [1/1] | Batch 2683 | loss: 0.06236996501684189\n",
      "Epoch [1/1] | Batch 2684 | loss: 0.5533226728439331\n",
      "Epoch [1/1] | Batch 2685 | loss: 0.04855012148618698\n",
      "Epoch [1/1] | Batch 2686 | loss: 0.005363245960325003\n",
      "Epoch [1/1] | Batch 2687 | loss: 0.6159981489181519\n",
      "Epoch [1/1] | Batch 2688 | loss: 0.24749888479709625\n",
      "Epoch [1/1] | Batch 2689 | loss: 0.012789513915777206\n",
      "Epoch [1/1] | Batch 2690 | loss: 0.3146285116672516\n",
      "Epoch [1/1] | Batch 2691 | loss: 0.24903230369091034\n",
      "Epoch [1/1] | Batch 2692 | loss: 0.01903819665312767\n",
      "Epoch [1/1] | Batch 2693 | loss: 0.2239225208759308\n",
      "Epoch [1/1] | Batch 2694 | loss: 0.0044279759749770164\n",
      "Epoch [1/1] | Batch 2695 | loss: 0.020116932690143585\n",
      "Epoch [1/1] | Batch 2696 | loss: 0.2798992097377777\n",
      "Epoch [1/1] | Batch 2697 | loss: 0.3094313442707062\n",
      "Epoch [1/1] | Batch 2698 | loss: 0.09843043982982635\n",
      "Epoch [1/1] | Batch 2699 | loss: 0.14855940639972687\n",
      "Epoch [1/1] | Batch 2700 | loss: 0.5634172558784485\n",
      "Epoch [1/1] | Batch 2701 | loss: 0.02620510198175907\n",
      "Epoch [1/1] | Batch 2702 | loss: 0.11760155856609344\n",
      "Epoch [1/1] | Batch 2703 | loss: 0.011424942873418331\n",
      "Epoch [1/1] | Batch 2704 | loss: 0.009263058193027973\n",
      "Epoch [1/1] | Batch 2705 | loss: 0.2193066030740738\n",
      "Epoch [1/1] | Batch 2706 | loss: 0.06452042609453201\n",
      "Epoch [1/1] | Batch 2707 | loss: 0.013150378130376339\n",
      "Epoch [1/1] | Batch 2708 | loss: 0.024025321006774902\n",
      "Epoch [1/1] | Batch 2709 | loss: 0.01815360225737095\n",
      "Epoch [1/1] | Batch 2710 | loss: 0.021697983145713806\n",
      "Epoch [1/1] | Batch 2711 | loss: 0.04804500937461853\n",
      "Epoch [1/1] | Batch 2712 | loss: 0.025622446089982986\n",
      "Epoch [1/1] | Batch 2713 | loss: 0.05177399516105652\n",
      "Epoch [1/1] | Batch 2714 | loss: 0.018963728100061417\n",
      "Epoch [1/1] | Batch 2715 | loss: 0.018255194649100304\n",
      "Epoch [1/1] | Batch 2716 | loss: 0.13287986814975739\n",
      "Epoch [1/1] | Batch 2717 | loss: 0.07682431489229202\n",
      "Epoch [1/1] | Batch 2718 | loss: 0.5776906609535217\n",
      "Epoch [1/1] | Batch 2719 | loss: 0.5685879588127136\n",
      "Epoch [1/1] | Batch 2720 | loss: 0.02247539535164833\n",
      "Epoch [1/1] | Batch 2721 | loss: 0.12381447106599808\n",
      "Epoch [1/1] | Batch 2722 | loss: 0.008553796447813511\n",
      "Epoch [1/1] | Batch 2723 | loss: 0.0770583227276802\n",
      "Epoch [1/1] | Batch 2724 | loss: 0.005912971682846546\n",
      "Epoch [1/1] | Batch 2725 | loss: 0.10907632857561111\n",
      "Epoch [1/1] | Batch 2726 | loss: 0.2556511163711548\n",
      "Epoch [1/1] | Batch 2727 | loss: 0.0038236682303249836\n",
      "Epoch [1/1] | Batch 2728 | loss: 0.014181365258991718\n",
      "Epoch [1/1] | Batch 2729 | loss: 0.2556632459163666\n",
      "Epoch [1/1] | Batch 2730 | loss: 0.00837243627756834\n",
      "Epoch [1/1] | Batch 2731 | loss: 0.11161370575428009\n",
      "Epoch [1/1] | Batch 2732 | loss: 0.1259281188249588\n",
      "Epoch [1/1] | Batch 2733 | loss: 0.24929635226726532\n",
      "Epoch [1/1] | Batch 2734 | loss: 0.011582554318010807\n",
      "Epoch [1/1] | Batch 2735 | loss: 0.07828479260206223\n",
      "Epoch [1/1] | Batch 2736 | loss: 0.010455330833792686\n",
      "Epoch [1/1] | Batch 2737 | loss: 0.022064803168177605\n",
      "Epoch [1/1] | Batch 2738 | loss: 0.011739770881831646\n",
      "Epoch [1/1] | Batch 2739 | loss: 0.011558497324585915\n",
      "Epoch [1/1] | Batch 2740 | loss: 0.1450156569480896\n",
      "Epoch [1/1] | Batch 2741 | loss: 0.7765440940856934\n",
      "Epoch [1/1] | Batch 2742 | loss: 0.12257888168096542\n",
      "Epoch [1/1] | Batch 2743 | loss: 0.00305757112801075\n",
      "Epoch [1/1] | Batch 2744 | loss: 0.14910085499286652\n",
      "Epoch [1/1] | Batch 2745 | loss: 0.09280617535114288\n",
      "Epoch [1/1] | Batch 2746 | loss: 0.13576740026474\n",
      "Epoch [1/1] | Batch 2747 | loss: 0.0684267207980156\n",
      "Epoch [1/1] | Batch 2748 | loss: 0.018001871183514595\n",
      "Epoch [1/1] | Batch 2749 | loss: 0.3800346851348877\n",
      "Epoch [1/1] | Batch 2750 | loss: 0.031065426766872406\n",
      "Epoch [1/1] | Batch 2751 | loss: 0.03337491303682327\n",
      "Epoch [1/1] | Batch 2752 | loss: 0.004684251267462969\n",
      "Epoch [1/1] | Batch 2753 | loss: 0.0834241434931755\n",
      "Epoch [1/1] | Batch 2754 | loss: 0.22005851566791534\n",
      "Epoch [1/1] | Batch 2755 | loss: 0.16593347489833832\n",
      "Epoch [1/1] | Batch 2756 | loss: 0.6061662435531616\n",
      "Epoch [1/1] | Batch 2757 | loss: 0.014677081257104874\n",
      "Epoch [1/1] | Batch 2758 | loss: 0.06942353397607803\n",
      "Epoch [1/1] | Batch 2759 | loss: 0.42956870794296265\n",
      "Epoch [1/1] | Batch 2760 | loss: 0.5369547009468079\n",
      "Epoch [1/1] | Batch 2761 | loss: 0.10618092119693756\n",
      "Epoch [1/1] | Batch 2762 | loss: 0.2641448974609375\n",
      "Epoch [1/1] | Batch 2763 | loss: 0.025740111246705055\n",
      "Epoch [1/1] | Batch 2764 | loss: 0.06816791743040085\n",
      "Epoch [1/1] | Batch 2765 | loss: 0.0065956744365394115\n",
      "Epoch [1/1] | Batch 2766 | loss: 0.10905855894088745\n",
      "Epoch [1/1] | Batch 2767 | loss: 0.050499264150857925\n",
      "Epoch [1/1] | Batch 2768 | loss: 0.007613752502948046\n",
      "Epoch [1/1] | Batch 2769 | loss: 0.07599201053380966\n",
      "Epoch [1/1] | Batch 2770 | loss: 0.030045708641409874\n",
      "Epoch [1/1] | Batch 2771 | loss: 0.13222874701023102\n",
      "Epoch [1/1] | Batch 2772 | loss: 0.0794021412730217\n",
      "Epoch [1/1] | Batch 2773 | loss: 0.017274022102355957\n",
      "Epoch [1/1] | Batch 2774 | loss: 0.145020991563797\n",
      "Epoch [1/1] | Batch 2775 | loss: 0.05716218426823616\n",
      "Epoch [1/1] | Batch 2776 | loss: 0.57598876953125\n",
      "Epoch [1/1] | Batch 2777 | loss: 0.02012677676975727\n",
      "Epoch [1/1] | Batch 2778 | loss: 0.5339561104774475\n",
      "Epoch [1/1] | Batch 2779 | loss: 0.023168129846453667\n",
      "Epoch [1/1] | Batch 2780 | loss: 0.021369874477386475\n",
      "Epoch [1/1] | Batch 2781 | loss: 0.4797160029411316\n",
      "Epoch [1/1] | Batch 2782 | loss: 0.14487862586975098\n",
      "Epoch [1/1] | Batch 2783 | loss: 0.39605212211608887\n",
      "Epoch [1/1] | Batch 2784 | loss: 0.003209650283679366\n",
      "Epoch [1/1] | Batch 2785 | loss: 0.16388089954853058\n",
      "Epoch [1/1] | Batch 2786 | loss: 0.13692300021648407\n",
      "Epoch [1/1] | Batch 2787 | loss: 0.5297530293464661\n",
      "Epoch [1/1] | Batch 2788 | loss: 0.07515663653612137\n",
      "Epoch [1/1] | Batch 2789 | loss: 0.2084280401468277\n",
      "Epoch [1/1] | Batch 2790 | loss: 0.25385138392448425\n",
      "Epoch [1/1] | Batch 2791 | loss: 0.26119670271873474\n",
      "Epoch [1/1] | Batch 2792 | loss: 0.07910840213298798\n",
      "Epoch [1/1] | Batch 2793 | loss: 0.04672161489725113\n",
      "Epoch [1/1] | Batch 2794 | loss: 0.1261022537946701\n",
      "Epoch [1/1] | Batch 2795 | loss: 0.3304988145828247\n",
      "Epoch [1/1] | Batch 2796 | loss: 0.17681778967380524\n",
      "Epoch [1/1] | Batch 2797 | loss: 0.05958113446831703\n",
      "Epoch [1/1] | Batch 2798 | loss: 0.0335196852684021\n",
      "Epoch [1/1] | Batch 2799 | loss: 0.2461639940738678\n",
      "Epoch [1/1] | Batch 2800 | loss: 0.13278251886367798\n",
      "Epoch [1/1] | Batch 2801 | loss: 0.005842201877385378\n",
      "Epoch [1/1] | Batch 2802 | loss: 0.21006932854652405\n",
      "Epoch [1/1] | Batch 2803 | loss: 0.19165091216564178\n",
      "Epoch [1/1] | Batch 2804 | loss: 0.36547526717185974\n",
      "Epoch [1/1] | Batch 2805 | loss: 0.03306177631020546\n",
      "Epoch [1/1] | Batch 2806 | loss: 0.13609011471271515\n",
      "Epoch [1/1] | Batch 2807 | loss: 0.029681555926799774\n",
      "Epoch [1/1] | Batch 2808 | loss: 0.03959745913743973\n",
      "Epoch [1/1] | Batch 2809 | loss: 0.004444516729563475\n",
      "Epoch [1/1] | Batch 2810 | loss: 0.12066500633955002\n",
      "Epoch [1/1] | Batch 2811 | loss: 0.07829487323760986\n",
      "Epoch [1/1] | Batch 2812 | loss: 0.005860191769897938\n",
      "Epoch [1/1] | Batch 2813 | loss: 0.06675337255001068\n",
      "Epoch [1/1] | Batch 2814 | loss: 0.12618261575698853\n",
      "Epoch [1/1] | Batch 2815 | loss: 0.06250280141830444\n",
      "Epoch [1/1] | Batch 2816 | loss: 0.1666446030139923\n",
      "Epoch [1/1] | Batch 2817 | loss: 0.2305406779050827\n",
      "Epoch [1/1] | Batch 2818 | loss: 0.005772486794739962\n",
      "Epoch [1/1] | Batch 2819 | loss: 0.003326184581965208\n",
      "Epoch [1/1] | Batch 2820 | loss: 0.01681748405098915\n",
      "Epoch [1/1] | Batch 2821 | loss: 0.02049156092107296\n",
      "Epoch [1/1] | Batch 2822 | loss: 0.008303910493850708\n",
      "Epoch [1/1] | Batch 2823 | loss: 0.22091272473335266\n",
      "Epoch [1/1] | Batch 2824 | loss: 0.12429061532020569\n",
      "Epoch [1/1] | Batch 2825 | loss: 0.13964861631393433\n",
      "Epoch [1/1] | Batch 2826 | loss: 0.39679068326950073\n",
      "Epoch [1/1] | Batch 2827 | loss: 0.08305232971906662\n",
      "Epoch [1/1] | Batch 2828 | loss: 0.3075636923313141\n",
      "Epoch [1/1] | Batch 2829 | loss: 0.36016935110092163\n",
      "Epoch [1/1] | Batch 2830 | loss: 0.05154718831181526\n",
      "Epoch [1/1] | Batch 2831 | loss: 0.5082705616950989\n",
      "Epoch [1/1] | Batch 2832 | loss: 0.008897013030946255\n",
      "Epoch [1/1] | Batch 2833 | loss: 0.06417754292488098\n",
      "Epoch [1/1] | Batch 2834 | loss: 0.7764147520065308\n",
      "Epoch [1/1] | Batch 2835 | loss: 0.1177484393119812\n",
      "Epoch [1/1] | Batch 2836 | loss: 0.016714919358491898\n",
      "Epoch [1/1] | Batch 2837 | loss: 0.23146690428256989\n",
      "Epoch [1/1] | Batch 2838 | loss: 0.07642864435911179\n",
      "Epoch [1/1] | Batch 2839 | loss: 0.0778823047876358\n",
      "Epoch [1/1] | Batch 2840 | loss: 0.03234241157770157\n",
      "Epoch [1/1] | Batch 2841 | loss: 0.03110540099442005\n",
      "Epoch [1/1] | Batch 2842 | loss: 0.026070715859532356\n",
      "Epoch [1/1] | Batch 2843 | loss: 0.30713576078414917\n",
      "Epoch [1/1] | Batch 2844 | loss: 0.13095588982105255\n",
      "Epoch [1/1] | Batch 2845 | loss: 0.2804051637649536\n",
      "Epoch [1/1] | Batch 2846 | loss: 0.7554726004600525\n",
      "Epoch [1/1] | Batch 2847 | loss: 0.002864276058971882\n",
      "Epoch [1/1] | Batch 2848 | loss: 0.06149594485759735\n",
      "Epoch [1/1] | Batch 2849 | loss: 0.08044499158859253\n",
      "Epoch [1/1] | Batch 2850 | loss: 0.14500966668128967\n",
      "Epoch [1/1] | Batch 2851 | loss: 0.05772343650460243\n",
      "Epoch [1/1] | Batch 2852 | loss: 0.09228311479091644\n",
      "Epoch [1/1] | Batch 2853 | loss: 0.007356950081884861\n",
      "Epoch [1/1] | Batch 2854 | loss: 0.08574890345335007\n",
      "Epoch [1/1] | Batch 2855 | loss: 0.06898633390665054\n",
      "Epoch [1/1] | Batch 2856 | loss: 0.001168984454125166\n",
      "Epoch [1/1] | Batch 2857 | loss: 0.2414175271987915\n",
      "Epoch [1/1] | Batch 2858 | loss: 0.07736742496490479\n",
      "Epoch [1/1] | Batch 2859 | loss: 0.036956436932086945\n",
      "Epoch [1/1] | Batch 2860 | loss: 0.2620021104812622\n",
      "Epoch [1/1] | Batch 2861 | loss: 0.07441262155771255\n",
      "Epoch [1/1] | Batch 2862 | loss: 0.3278242349624634\n",
      "Epoch [1/1] | Batch 2863 | loss: 0.015557399950921535\n",
      "Epoch [1/1] | Batch 2864 | loss: 0.0241312924772501\n",
      "Epoch [1/1] | Batch 2865 | loss: 0.10671555250883102\n",
      "Epoch [1/1] | Batch 2866 | loss: 0.2487756609916687\n",
      "Epoch [1/1] | Batch 2867 | loss: 0.059909429401159286\n",
      "Epoch [1/1] | Batch 2868 | loss: 0.008139323443174362\n",
      "Epoch [1/1] | Batch 2869 | loss: 0.0950302928686142\n",
      "Epoch [1/1] | Batch 2870 | loss: 0.09405530989170074\n",
      "Epoch [1/1] | Batch 2871 | loss: 0.061403412371873856\n",
      "Epoch [1/1] | Batch 2872 | loss: 0.056876011192798615\n",
      "Epoch [1/1] | Batch 2873 | loss: 0.10564488917589188\n",
      "Epoch [1/1] | Batch 2874 | loss: 0.08369680494070053\n",
      "Epoch [1/1] | Batch 2875 | loss: 0.19424894452095032\n",
      "Epoch [1/1] | Batch 2876 | loss: 0.1781875193119049\n",
      "Epoch [1/1] | Batch 2877 | loss: 0.0463034026324749\n",
      "Epoch [1/1] | Batch 2878 | loss: 0.09634089469909668\n",
      "Epoch [1/1] | Batch 2879 | loss: 0.12578238546848297\n",
      "Epoch [1/1] | Batch 2880 | loss: 0.15250122547149658\n",
      "Epoch [1/1] | Batch 2881 | loss: 0.42271888256073\n",
      "Epoch [1/1] | Batch 2882 | loss: 0.05085877701640129\n",
      "Epoch [1/1] | Batch 2883 | loss: 0.06453415751457214\n",
      "Epoch [1/1] | Batch 2884 | loss: 0.013887966983020306\n",
      "Epoch [1/1] | Batch 2885 | loss: 0.01960725151002407\n",
      "Epoch [1/1] | Batch 2886 | loss: 0.2826346457004547\n",
      "Epoch [1/1] | Batch 2887 | loss: 0.04455637186765671\n",
      "Epoch [1/1] | Batch 2888 | loss: 0.008719252422451973\n",
      "Epoch [1/1] | Batch 2889 | loss: 0.36361563205718994\n",
      "Epoch [1/1] | Batch 2890 | loss: 0.06926099956035614\n",
      "Epoch [1/1] | Batch 2891 | loss: 0.011289181187748909\n",
      "Epoch [1/1] | Batch 2892 | loss: 0.10070956498384476\n",
      "Epoch [1/1] | Batch 2893 | loss: 0.05594160780310631\n",
      "Epoch [1/1] | Batch 2894 | loss: 0.017948636785149574\n",
      "Epoch [1/1] | Batch 2895 | loss: 0.0332978218793869\n",
      "Epoch [1/1] | Batch 2896 | loss: 0.0671880766749382\n",
      "Epoch [1/1] | Batch 2897 | loss: 0.7699238657951355\n",
      "Epoch [1/1] | Batch 2898 | loss: 0.13953085243701935\n",
      "Epoch [1/1] | Batch 2899 | loss: 0.05730189383029938\n",
      "Epoch [1/1] | Batch 2900 | loss: 0.2120879739522934\n",
      "Epoch [1/1] | Batch 2901 | loss: 0.18190576136112213\n",
      "Epoch [1/1] | Batch 2902 | loss: 0.08987660706043243\n",
      "Epoch [1/1] | Batch 2903 | loss: 0.21226182579994202\n",
      "Epoch [1/1] | Batch 2904 | loss: 0.055164139717817307\n",
      "Epoch [1/1] | Batch 2905 | loss: 0.20716492831707\n",
      "Epoch [1/1] | Batch 2906 | loss: 0.48593848943710327\n",
      "Epoch [1/1] | Batch 2907 | loss: 0.12605145573616028\n",
      "Epoch [1/1] | Batch 2908 | loss: 0.02092827297747135\n",
      "Epoch [1/1] | Batch 2909 | loss: 0.005528472363948822\n",
      "Epoch [1/1] | Batch 2910 | loss: 0.20040613412857056\n",
      "Epoch [1/1] | Batch 2911 | loss: 0.08965519815683365\n",
      "Epoch [1/1] | Batch 2912 | loss: 0.2706707715988159\n",
      "Epoch [1/1] | Batch 2913 | loss: 0.18174268305301666\n",
      "Epoch [1/1] | Batch 2914 | loss: 0.07008855789899826\n",
      "Epoch [1/1] | Batch 2915 | loss: 0.1311739832162857\n",
      "Epoch [1/1] | Batch 2916 | loss: 0.014945671893656254\n",
      "Epoch [1/1] | Batch 2917 | loss: 0.03965368494391441\n",
      "Epoch [1/1] | Batch 2918 | loss: 0.27088508009910583\n",
      "Epoch [1/1] | Batch 2919 | loss: 0.054321758449077606\n",
      "Epoch [1/1] | Batch 2920 | loss: 0.02125525288283825\n",
      "Epoch [1/1] | Batch 2921 | loss: 0.029987577348947525\n",
      "Epoch [1/1] | Batch 2922 | loss: 0.011219033971428871\n",
      "Epoch [1/1] | Batch 2923 | loss: 0.15904925763607025\n",
      "Epoch [1/1] | Batch 2924 | loss: 0.029990501701831818\n",
      "Epoch [1/1] | Batch 2925 | loss: 0.27383777499198914\n",
      "Epoch [1/1] | Batch 2926 | loss: 0.25506412982940674\n",
      "Epoch [1/1] | Batch 2927 | loss: 0.01784496009349823\n",
      "Epoch [1/1] | Batch 2928 | loss: 0.03908424824476242\n",
      "Epoch [1/1] | Batch 2929 | loss: 0.045402009040117264\n",
      "Epoch [1/1] | Batch 2930 | loss: 0.1821748912334442\n",
      "Epoch [1/1] | Batch 2931 | loss: 0.3183175027370453\n",
      "Epoch [1/1] | Batch 2932 | loss: 0.9591137170791626\n",
      "Epoch [1/1] | Batch 2933 | loss: 0.010191420093178749\n",
      "Epoch [1/1] | Batch 2934 | loss: 0.9612982273101807\n",
      "Epoch [1/1] | Batch 2935 | loss: 0.038974542170763016\n",
      "Epoch [1/1] | Batch 2936 | loss: 0.004994479473680258\n",
      "Epoch [1/1] | Batch 2937 | loss: 0.09022786468267441\n",
      "Epoch [1/1] | Batch 2938 | loss: 0.00940764881670475\n",
      "Epoch [1/1] | Batch 2939 | loss: 0.009402388706803322\n",
      "Epoch [1/1] | Batch 2940 | loss: 0.2007082849740982\n",
      "Epoch [1/1] | Batch 2941 | loss: 0.0723668783903122\n",
      "Epoch [1/1] | Batch 2942 | loss: 0.04111010953783989\n",
      "Epoch [1/1] | Batch 2943 | loss: 0.25647634267807007\n",
      "Epoch [1/1] | Batch 2944 | loss: 0.24521541595458984\n",
      "Epoch [1/1] | Batch 2945 | loss: 0.03640531376004219\n",
      "Epoch [1/1] | Batch 2946 | loss: 0.025855310261249542\n",
      "Epoch [1/1] | Batch 2947 | loss: 0.1162029281258583\n",
      "Epoch [1/1] | Batch 2948 | loss: 0.1974906325340271\n",
      "Epoch [1/1] | Batch 2949 | loss: 0.008020407520234585\n",
      "Epoch [1/1] | Batch 2950 | loss: 0.00867102388292551\n",
      "Epoch [1/1] | Batch 2951 | loss: 0.7547980546951294\n",
      "Epoch [1/1] | Batch 2952 | loss: 0.285032719373703\n",
      "Epoch [1/1] | Batch 2953 | loss: 0.30372607707977295\n",
      "Epoch [1/1] | Batch 2954 | loss: 0.05572304129600525\n",
      "Epoch [1/1] | Batch 2955 | loss: 0.3511156141757965\n",
      "Epoch [1/1] | Batch 2956 | loss: 0.06193399429321289\n",
      "Epoch [1/1] | Batch 2957 | loss: 0.023241335526108742\n",
      "Epoch [1/1] | Batch 2958 | loss: 0.9958376884460449\n",
      "Epoch [1/1] | Batch 2959 | loss: 0.15339542925357819\n",
      "Epoch [1/1] | Batch 2960 | loss: 0.018406249582767487\n",
      "Epoch [1/1] | Batch 2961 | loss: 0.05830778926610947\n",
      "Epoch [1/1] | Batch 2962 | loss: 0.15045438706874847\n",
      "Epoch [1/1] | Batch 2963 | loss: 0.05839865282177925\n",
      "Epoch [1/1] | Batch 2964 | loss: 0.18209001421928406\n",
      "Epoch [1/1] | Batch 2965 | loss: 0.3361143469810486\n",
      "Epoch [1/1] | Batch 2966 | loss: 0.41891735792160034\n",
      "Epoch [1/1] | Batch 2967 | loss: 0.03478577360510826\n",
      "Epoch [1/1] | Batch 2968 | loss: 0.02607913129031658\n",
      "Epoch [1/1] | Batch 2969 | loss: 0.18068280816078186\n",
      "Epoch [1/1] | Batch 2970 | loss: 0.15453611314296722\n",
      "Epoch [1/1] | Batch 2971 | loss: 0.20337601006031036\n",
      "Epoch [1/1] | Batch 2972 | loss: 0.17093294858932495\n",
      "Epoch [1/1] | Batch 2973 | loss: 0.6413406133651733\n",
      "Epoch [1/1] | Batch 2974 | loss: 0.16301032900810242\n",
      "Epoch [1/1] | Batch 2975 | loss: 0.4182165861129761\n",
      "Epoch [1/1] | Batch 2976 | loss: 0.004289631266146898\n",
      "Epoch [1/1] | Batch 2977 | loss: 0.05457030236721039\n",
      "Epoch [1/1] | Batch 2978 | loss: 0.06745035201311111\n",
      "Epoch [1/1] | Batch 2979 | loss: 0.012833663262426853\n",
      "Epoch [1/1] | Batch 2980 | loss: 0.018104150891304016\n",
      "Epoch [1/1] | Batch 2981 | loss: 0.059110064059495926\n",
      "Epoch [1/1] | Batch 2982 | loss: 0.11278000473976135\n",
      "Epoch [1/1] | Batch 2983 | loss: 0.025605009868741035\n",
      "Epoch [1/1] | Batch 2984 | loss: 0.14414916932582855\n",
      "Epoch [1/1] | Batch 2985 | loss: 0.4198892116546631\n",
      "Epoch [1/1] | Batch 2986 | loss: 0.15936285257339478\n",
      "Epoch [1/1] | Batch 2987 | loss: 0.0037759749684482813\n",
      "Epoch [1/1] | Batch 2988 | loss: 0.020777413621544838\n",
      "Epoch [1/1] | Batch 2989 | loss: 0.3483453392982483\n",
      "Epoch [1/1] | Batch 2990 | loss: 0.15743094682693481\n",
      "Epoch [1/1] | Batch 2991 | loss: 0.0036065210588276386\n",
      "Epoch [1/1] | Batch 2992 | loss: 0.33672386407852173\n",
      "Epoch [1/1] | Batch 2993 | loss: 0.3279060125350952\n",
      "Epoch [1/1] | Batch 2994 | loss: 0.022766465321183205\n",
      "Epoch [1/1] | Batch 2995 | loss: 0.023692846298217773\n",
      "Epoch [1/1] | Batch 2996 | loss: 0.3546556532382965\n",
      "Epoch [1/1] | Batch 2997 | loss: 0.027836190536618233\n",
      "Epoch [1/1] | Batch 2998 | loss: 0.13947546482086182\n",
      "Epoch [1/1] | Batch 2999 | loss: 0.16891059279441833\n",
      "Epoch [1/1] | Batch 3000 | loss: 0.12335193902254105\n",
      "Epoch [1/1] | Batch 3001 | loss: 0.02444085106253624\n",
      "Epoch [1/1] | Batch 3002 | loss: 0.11345207691192627\n",
      "Epoch [1/1] | Batch 3003 | loss: 0.028798600658774376\n",
      "Epoch [1/1] | Batch 3004 | loss: 0.002675867872312665\n",
      "Epoch [1/1] | Batch 3005 | loss: 0.013489498756825924\n",
      "Epoch [1/1] | Batch 3006 | loss: 0.022106537595391273\n",
      "Epoch [1/1] | Batch 3007 | loss: 0.06403292715549469\n",
      "Epoch [1/1] | Batch 3008 | loss: 0.4291796088218689\n",
      "Epoch [1/1] | Batch 3009 | loss: 0.168566033244133\n",
      "Epoch [1/1] | Batch 3010 | loss: 0.05364436283707619\n",
      "Epoch [1/1] | Batch 3011 | loss: 0.12717200815677643\n",
      "Epoch [1/1] | Batch 3012 | loss: 0.10856699198484421\n",
      "Epoch [1/1] | Batch 3013 | loss: 0.10079677402973175\n",
      "Epoch [1/1] | Batch 3014 | loss: 0.008926114067435265\n",
      "Epoch [1/1] | Batch 3015 | loss: 0.02738424576818943\n",
      "Epoch [1/1] | Batch 3016 | loss: 0.4949304759502411\n",
      "Epoch [1/1] | Batch 3017 | loss: 0.012709238566458225\n",
      "Epoch [1/1] | Batch 3018 | loss: 0.007427028845995665\n",
      "Epoch [1/1] | Batch 3019 | loss: 0.3118820786476135\n",
      "Epoch [1/1] | Batch 3020 | loss: 0.40929704904556274\n",
      "Epoch [1/1] | Batch 3021 | loss: 0.004804999567568302\n",
      "Epoch [1/1] | Batch 3022 | loss: 0.10106814652681351\n",
      "Epoch [1/1] | Batch 3023 | loss: 0.04474303126335144\n",
      "Epoch [1/1] | Batch 3024 | loss: 0.4613872468471527\n",
      "Epoch [1/1] | Batch 3025 | loss: 0.002878019120544195\n",
      "Epoch [1/1] | Batch 3026 | loss: 0.046178821474313736\n",
      "Epoch [1/1] | Batch 3027 | loss: 0.21594874560832977\n",
      "Epoch [1/1] | Batch 3028 | loss: 0.24851298332214355\n",
      "Epoch [1/1] | Batch 3029 | loss: 0.012178384698927402\n",
      "Epoch [1/1] | Batch 3030 | loss: 0.0037271222099661827\n",
      "Epoch [1/1] | Batch 3031 | loss: 0.009266572073101997\n",
      "Epoch [1/1] | Batch 3032 | loss: 0.15930955111980438\n",
      "Epoch [1/1] | Batch 3033 | loss: 0.010610651224851608\n",
      "Epoch [1/1] | Batch 3034 | loss: 0.21009521186351776\n",
      "Epoch [1/1] | Batch 3035 | loss: 0.09403712302446365\n",
      "Epoch [1/1] | Batch 3036 | loss: 0.21141254901885986\n",
      "Epoch [1/1] | Batch 3037 | loss: 0.4046720266342163\n",
      "Epoch [1/1] | Batch 3038 | loss: 0.07500126957893372\n",
      "Epoch [1/1] | Batch 3039 | loss: 0.25887954235076904\n",
      "Epoch [1/1] | Batch 3040 | loss: 0.009036370553076267\n",
      "Epoch [1/1] | Batch 3041 | loss: 0.9010588526725769\n",
      "Epoch [1/1] | Batch 3042 | loss: 0.43444007635116577\n",
      "Epoch [1/1] | Batch 3043 | loss: 0.10209397971630096\n",
      "Epoch [1/1] | Batch 3044 | loss: 0.08226896077394485\n",
      "Epoch [1/1] | Batch 3045 | loss: 0.020228441804647446\n",
      "Epoch [1/1] | Batch 3046 | loss: 0.015204684808850288\n",
      "Epoch [1/1] | Batch 3047 | loss: 0.028373803943395615\n",
      "Epoch [1/1] | Batch 3048 | loss: 0.026005249470472336\n",
      "Epoch [1/1] | Batch 3049 | loss: 0.00874595157802105\n",
      "Epoch [1/1] | Batch 3050 | loss: 0.3811573088169098\n",
      "Epoch [1/1] | Batch 3051 | loss: 0.4345734417438507\n",
      "Epoch [1/1] | Batch 3052 | loss: 0.11010768264532089\n",
      "Epoch [1/1] | Batch 3053 | loss: 0.0989154502749443\n",
      "Epoch [1/1] | Batch 3054 | loss: 0.36632487177848816\n",
      "Epoch [1/1] | Batch 3055 | loss: 0.06973596662282944\n",
      "Epoch [1/1] | Batch 3056 | loss: 0.29531341791152954\n",
      "Epoch [1/1] | Batch 3057 | loss: 0.3687012195587158\n",
      "Epoch [1/1] | Batch 3058 | loss: 0.012295485474169254\n",
      "Epoch [1/1] | Batch 3059 | loss: 0.3495725095272064\n",
      "Epoch [1/1] | Batch 3060 | loss: 0.333076536655426\n",
      "Epoch [1/1] | Batch 3061 | loss: 0.16961786150932312\n",
      "Epoch [1/1] | Batch 3062 | loss: 0.34425559639930725\n",
      "Epoch [1/1] | Batch 3063 | loss: 0.14015448093414307\n",
      "Epoch [1/1] | Batch 3064 | loss: 0.7832899689674377\n",
      "Epoch [1/1] | Batch 3065 | loss: 0.09226012974977493\n",
      "Epoch [1/1] | Batch 3066 | loss: 0.2593991160392761\n",
      "Epoch [1/1] | Batch 3067 | loss: 0.39565834403038025\n",
      "Epoch [1/1] | Batch 3068 | loss: 0.01253860630095005\n",
      "Epoch [1/1] | Batch 3069 | loss: 0.020524613559246063\n",
      "Epoch [1/1] | Batch 3070 | loss: 0.0060573238879442215\n",
      "Epoch [1/1] | Batch 3071 | loss: 0.14274485409259796\n",
      "Epoch [1/1] | Batch 3072 | loss: 0.17456762492656708\n",
      "Epoch [1/1] | Batch 3073 | loss: 0.046130381524562836\n",
      "Epoch [1/1] | Batch 3074 | loss: 0.011367863044142723\n",
      "Epoch [1/1] | Batch 3075 | loss: 0.12450657784938812\n",
      "Epoch [1/1] | Batch 3076 | loss: 0.006544210482388735\n",
      "Epoch [1/1] | Batch 3077 | loss: 0.10439086705446243\n",
      "Epoch [1/1] | Batch 3078 | loss: 0.10501332581043243\n",
      "Epoch [1/1] | Batch 3079 | loss: 0.020884046331048012\n",
      "Epoch [1/1] | Batch 3080 | loss: 0.025840716436505318\n",
      "Epoch [1/1] | Batch 3081 | loss: 0.5425592064857483\n",
      "Epoch [1/1] | Batch 3082 | loss: 0.12657660245895386\n",
      "Epoch [1/1] | Batch 3083 | loss: 0.028541313484311104\n",
      "Epoch [1/1] | Batch 3084 | loss: 0.4257468581199646\n",
      "Epoch [1/1] | Batch 3085 | loss: 0.3187897205352783\n",
      "Epoch [1/1] | Batch 3086 | loss: 0.12177937477827072\n",
      "Epoch [1/1] | Batch 3087 | loss: 0.04350142180919647\n",
      "Epoch [1/1] | Batch 3088 | loss: 0.0681026503443718\n",
      "Epoch [1/1] | Batch 3089 | loss: 0.16038604080677032\n",
      "Epoch [1/1] | Batch 3090 | loss: 0.24294640123844147\n",
      "Epoch [1/1] | Batch 3091 | loss: 0.016851551830768585\n",
      "Epoch [1/1] | Batch 3092 | loss: 0.05320267379283905\n",
      "Epoch [1/1] | Batch 3093 | loss: 0.06602977961301804\n",
      "Epoch [1/1] | Batch 3094 | loss: 0.08267419040203094\n",
      "Epoch [1/1] | Batch 3095 | loss: 0.003936020657420158\n",
      "Epoch [1/1] | Batch 3096 | loss: 0.010939852334558964\n",
      "Epoch [1/1] | Batch 3097 | loss: 0.07981570065021515\n",
      "Epoch [1/1] | Batch 3098 | loss: 0.016781318932771683\n",
      "Epoch [1/1] | Batch 3099 | loss: 0.019968388602137566\n",
      "Epoch [1/1] | Batch 3100 | loss: 0.14207394421100616\n",
      "Epoch [1/1] | Batch 3101 | loss: 0.13572098314762115\n",
      "Epoch [1/1] | Batch 3102 | loss: 0.04544556513428688\n",
      "Epoch [1/1] | Batch 3103 | loss: 0.07396894693374634\n",
      "Epoch [1/1] | Batch 3104 | loss: 0.02584809437394142\n",
      "Epoch [1/1] | Batch 3105 | loss: 0.12064828723669052\n",
      "Epoch [1/1] | Batch 3106 | loss: 0.0038018394261598587\n",
      "Epoch [1/1] | Batch 3107 | loss: 0.1975451558828354\n",
      "Epoch [1/1] | Batch 3108 | loss: 0.03351787477731705\n",
      "Epoch [1/1] | Batch 3109 | loss: 0.0486283041536808\n",
      "Epoch [1/1] | Batch 3110 | loss: 0.5676040649414062\n",
      "Epoch [1/1] | Batch 3111 | loss: 0.028555572032928467\n",
      "Epoch [1/1] | Batch 3112 | loss: 0.016793835908174515\n",
      "Epoch [1/1] | Batch 3113 | loss: 0.17329634726047516\n",
      "Epoch [1/1] | Batch 3114 | loss: 0.019918890669941902\n",
      "Epoch [1/1] | Batch 3115 | loss: 0.148870050907135\n",
      "Epoch [1/1] | Batch 3116 | loss: 0.013951328583061695\n",
      "Epoch [1/1] | Batch 3117 | loss: 0.06515825539827347\n",
      "Epoch [1/1] | Batch 3118 | loss: 0.6722826957702637\n",
      "Epoch [1/1] | Batch 3119 | loss: 0.09342312812805176\n",
      "Epoch [1/1] | Batch 3120 | loss: 0.028202325105667114\n",
      "Epoch [1/1] | Batch 3121 | loss: 0.5369373559951782\n",
      "Epoch [1/1] | Batch 3122 | loss: 0.08501293510198593\n",
      "Epoch [1/1] | Batch 3123 | loss: 0.34080761671066284\n",
      "Epoch [1/1] | Batch 3124 | loss: 0.34993433952331543\n",
      "Epoch [1/1] | Batch 3125 | loss: 0.05644320696592331\n",
      "Epoch [1/1] | Batch 3126 | loss: 0.03990401700139046\n",
      "Epoch [1/1] | Batch 3127 | loss: 0.1231609508395195\n",
      "Epoch [1/1] | Batch 3128 | loss: 0.11367058753967285\n",
      "Epoch [1/1] | Batch 3129 | loss: 0.07103383541107178\n",
      "Epoch [1/1] | Batch 3130 | loss: 0.045640815049409866\n",
      "Epoch [1/1] | Batch 3131 | loss: 0.2251611053943634\n",
      "Epoch [1/1] | Batch 3132 | loss: 0.19605587422847748\n",
      "Epoch [1/1] | Batch 3133 | loss: 0.009091967716813087\n",
      "Epoch [1/1] | Batch 3134 | loss: 0.06616485118865967\n",
      "Epoch [1/1] | Batch 3135 | loss: 0.10825257748365402\n",
      "Epoch [1/1] | Batch 3136 | loss: 0.02929299883544445\n",
      "Epoch [1/1] | Batch 3137 | loss: 0.008616628125309944\n",
      "Epoch [1/1] | Batch 3138 | loss: 0.01577701047062874\n",
      "Epoch [1/1] | Batch 3139 | loss: 0.24900025129318237\n",
      "Epoch [1/1] | Batch 3140 | loss: 0.10632719099521637\n",
      "Epoch [1/1] | Batch 3141 | loss: 0.009149626828730106\n",
      "Epoch [1/1] | Batch 3142 | loss: 0.2326168268918991\n",
      "Epoch [1/1] | Batch 3143 | loss: 0.0499972440302372\n",
      "Epoch [1/1] | Batch 3144 | loss: 0.011037548072636127\n",
      "Epoch [1/1] | Batch 3145 | loss: 0.07982534915208817\n",
      "Epoch [1/1] | Batch 3146 | loss: 0.007073787972331047\n",
      "Epoch [1/1] | Batch 3147 | loss: 0.013337409123778343\n",
      "Epoch [1/1] | Batch 3148 | loss: 0.12190981209278107\n",
      "Epoch [1/1] | Batch 3149 | loss: 0.03412281349301338\n",
      "Epoch [1/1] | Batch 3150 | loss: 0.0314280241727829\n",
      "Epoch [1/1] | Batch 3151 | loss: 0.3496510684490204\n",
      "Epoch [1/1] | Batch 3152 | loss: 0.5700234174728394\n",
      "Epoch [1/1] | Batch 3153 | loss: 0.4704746901988983\n",
      "Epoch [1/1] | Batch 3154 | loss: 0.059841055423021317\n",
      "Epoch [1/1] | Batch 3155 | loss: 0.09520852565765381\n",
      "Epoch [1/1] | Batch 3156 | loss: 0.39010724425315857\n",
      "Epoch [1/1] | Batch 3157 | loss: 0.036169108003377914\n",
      "Epoch [1/1] | Batch 3158 | loss: 0.1509057879447937\n",
      "Epoch [1/1] | Batch 3159 | loss: 0.039892781525850296\n",
      "Epoch [1/1] | Batch 3160 | loss: 0.4679175615310669\n",
      "Epoch [1/1] | Batch 3161 | loss: 0.004214152693748474\n",
      "Epoch [1/1] | Batch 3162 | loss: 0.036164816468954086\n",
      "Epoch [1/1] | Batch 3163 | loss: 0.0027336981147527695\n",
      "Epoch [1/1] | Batch 3164 | loss: 0.07780174165964127\n",
      "Epoch [1/1] | Batch 3165 | loss: 0.3491065204143524\n",
      "Epoch [1/1] | Batch 3166 | loss: 0.08154401183128357\n",
      "Epoch [1/1] | Batch 3167 | loss: 0.21922679245471954\n",
      "Epoch [1/1] | Batch 3168 | loss: 0.011866649612784386\n",
      "Epoch [1/1] | Batch 3169 | loss: 0.5332344174385071\n",
      "Epoch [1/1] | Batch 3170 | loss: 0.050395648926496506\n",
      "Epoch [1/1] | Batch 3171 | loss: 0.26488080620765686\n",
      "Epoch [1/1] | Batch 3172 | loss: 0.21873050928115845\n",
      "Epoch [1/1] | Batch 3173 | loss: 0.13610434532165527\n",
      "Epoch [1/1] | Batch 3174 | loss: 0.15845359861850739\n",
      "Epoch [1/1] | Batch 3175 | loss: 0.22886116802692413\n",
      "Epoch [1/1] | Batch 3176 | loss: 0.22789816558361053\n",
      "Epoch [1/1] | Batch 3177 | loss: 0.052799735218286514\n",
      "Epoch [1/1] | Batch 3178 | loss: 0.14921772480010986\n",
      "Epoch [1/1] | Batch 3179 | loss: 0.1210215613245964\n",
      "Epoch [1/1] | Batch 3180 | loss: 0.010642367415130138\n",
      "Epoch [1/1] | Batch 3181 | loss: 0.1741328239440918\n",
      "Epoch [1/1] | Batch 3182 | loss: 0.04700872674584389\n",
      "Epoch [1/1] | Batch 3183 | loss: 0.004357611760497093\n",
      "Epoch [1/1] | Batch 3184 | loss: 0.30041566491127014\n",
      "Epoch [1/1] | Batch 3185 | loss: 0.45294928550720215\n",
      "Epoch [1/1] | Batch 3186 | loss: 0.034907709807157516\n",
      "Epoch [1/1] | Batch 3187 | loss: 0.34693315625190735\n",
      "Epoch [1/1] | Batch 3188 | loss: 0.21788187325000763\n",
      "Epoch [1/1] | Batch 3189 | loss: 0.020847084000706673\n",
      "Epoch [1/1] | Batch 3190 | loss: 0.04922741279006004\n",
      "Epoch [1/1] | Batch 3191 | loss: 0.016361497342586517\n",
      "Epoch [1/1] | Batch 3192 | loss: 0.32379257678985596\n",
      "Epoch [1/1] | Batch 3193 | loss: 0.3434849679470062\n",
      "Epoch [1/1] | Batch 3194 | loss: 0.05403291806578636\n",
      "Epoch [1/1] | Batch 3195 | loss: 0.6262429356575012\n",
      "Epoch [1/1] | Batch 3196 | loss: 0.7089351415634155\n",
      "Epoch [1/1] | Batch 3197 | loss: 0.1043078750371933\n",
      "Epoch [1/1] | Batch 3198 | loss: 0.28936508297920227\n",
      "Epoch [1/1] | Batch 3199 | loss: 0.6171121597290039\n",
      "Epoch [1/1] | Batch 3200 | loss: 0.16965603828430176\n",
      "Epoch [1/1] | Batch 3201 | loss: 0.0977218747138977\n",
      "Epoch [1/1] | Batch 3202 | loss: 0.06043344736099243\n",
      "Epoch [1/1] | Batch 3203 | loss: 0.4266676902770996\n",
      "Epoch [1/1] | Batch 3204 | loss: 0.2550891935825348\n",
      "Epoch [1/1] | Batch 3205 | loss: 0.23795558512210846\n",
      "Epoch [1/1] | Batch 3206 | loss: 0.4147779047489166\n",
      "Epoch [1/1] | Batch 3207 | loss: 0.0280094463378191\n",
      "Epoch [1/1] | Batch 3208 | loss: 0.043755508959293365\n",
      "Epoch [1/1] | Batch 3209 | loss: 0.08756551891565323\n",
      "Epoch [1/1] | Batch 3210 | loss: 0.12349893152713776\n",
      "Epoch [1/1] | Batch 3211 | loss: 0.3588407635688782\n",
      "Epoch [1/1] | Batch 3212 | loss: 0.006813326850533485\n",
      "Epoch [1/1] | Batch 3213 | loss: 0.2548542320728302\n",
      "Epoch [1/1] | Batch 3214 | loss: 0.10938148945569992\n",
      "Epoch [1/1] | Batch 3215 | loss: 0.0841573029756546\n",
      "Epoch [1/1] | Batch 3216 | loss: 0.43206480145454407\n",
      "Epoch [1/1] | Batch 3217 | loss: 0.4814222753047943\n",
      "Epoch [1/1] | Batch 3218 | loss: 0.08271706849336624\n",
      "Epoch [1/1] | Batch 3219 | loss: 0.051539190113544464\n",
      "Epoch [1/1] | Batch 3220 | loss: 0.21699097752571106\n",
      "Epoch [1/1] | Batch 3221 | loss: 0.02886902168393135\n",
      "Epoch [1/1] | Batch 3222 | loss: 0.0316372811794281\n",
      "Epoch [1/1] | Batch 3223 | loss: 0.2971964180469513\n",
      "Epoch [1/1] | Batch 3224 | loss: 0.0074887340888381\n",
      "Epoch [1/1] | Batch 3225 | loss: 0.07363931089639664\n",
      "Epoch [1/1] | Batch 3226 | loss: 0.2089150846004486\n",
      "Epoch [1/1] | Batch 3227 | loss: 0.048909302800893784\n",
      "Epoch [1/1] | Batch 3228 | loss: 0.3198244869709015\n",
      "Epoch [1/1] | Batch 3229 | loss: 0.12307406216859818\n",
      "Epoch [1/1] | Batch 3230 | loss: 0.01410868763923645\n",
      "Epoch [1/1] | Batch 3231 | loss: 0.017474615946412086\n",
      "Epoch [1/1] | Batch 3232 | loss: 0.013832257129251957\n",
      "Epoch [1/1] | Batch 3233 | loss: 0.4568086862564087\n",
      "Epoch [1/1] | Batch 3234 | loss: 0.019885698333382607\n",
      "Epoch [1/1] | Batch 3235 | loss: 0.07647953927516937\n",
      "Epoch [1/1] | Batch 3236 | loss: 0.03415725380182266\n",
      "Epoch [1/1] | Batch 3237 | loss: 0.23680654168128967\n",
      "Epoch [1/1] | Batch 3238 | loss: 0.015094637870788574\n",
      "Epoch [1/1] | Batch 3239 | loss: 0.0772179439663887\n",
      "Epoch [1/1] | Batch 3240 | loss: 0.010760794393718243\n",
      "Epoch [1/1] | Batch 3241 | loss: 0.3386495113372803\n",
      "Epoch [1/1] | Batch 3242 | loss: 0.06601299345493317\n",
      "Epoch [1/1] | Batch 3243 | loss: 0.00646845530718565\n",
      "Epoch [1/1] | Batch 3244 | loss: 0.10573221743106842\n",
      "Epoch [1/1] | Batch 3245 | loss: 0.052405815571546555\n",
      "Epoch [1/1] | Batch 3246 | loss: 0.011675016954541206\n",
      "Epoch [1/1] | Batch 3247 | loss: 0.013433992862701416\n",
      "Epoch [1/1] | Batch 3248 | loss: 0.24350324273109436\n",
      "Epoch [1/1] | Batch 3249 | loss: 0.020688483491539955\n",
      "Epoch [1/1] | Batch 3250 | loss: 0.19283728301525116\n",
      "Epoch [1/1] | Batch 3251 | loss: 0.1389472633600235\n",
      "Epoch [1/1] | Batch 3252 | loss: 0.05702295899391174\n",
      "Epoch [1/1] | Batch 3253 | loss: 0.00528104929253459\n",
      "Epoch [1/1] | Batch 3254 | loss: 0.019914744421839714\n",
      "Epoch [1/1] | Batch 3255 | loss: 0.04042777791619301\n",
      "Epoch [1/1] | Batch 3256 | loss: 0.11997221410274506\n",
      "Epoch [1/1] | Batch 3257 | loss: 0.01145412027835846\n",
      "Epoch [1/1] | Batch 3258 | loss: 0.27162888646125793\n",
      "Epoch [1/1] | Batch 3259 | loss: 0.020372483879327774\n",
      "Epoch [1/1] | Batch 3260 | loss: 0.023434825241565704\n",
      "Epoch [1/1] | Batch 3261 | loss: 0.9185089468955994\n",
      "Epoch [1/1] | Batch 3262 | loss: 0.03145889565348625\n",
      "Epoch [1/1] | Batch 3263 | loss: 0.13009591400623322\n",
      "Epoch [1/1] | Batch 3264 | loss: 0.07723671942949295\n",
      "Epoch [1/1] | Batch 3265 | loss: 0.05273733288049698\n",
      "Epoch [1/1] | Batch 3266 | loss: 0.22924773395061493\n",
      "Epoch [1/1] | Batch 3267 | loss: 0.18494939804077148\n",
      "Epoch [1/1] | Batch 3268 | loss: 0.15531973540782928\n",
      "Epoch [1/1] | Batch 3269 | loss: 0.06921757012605667\n",
      "Epoch [1/1] | Batch 3270 | loss: 0.06120193749666214\n",
      "Epoch [1/1] | Batch 3271 | loss: 0.06872046738862991\n",
      "Epoch [1/1] | Batch 3272 | loss: 0.003428687807172537\n",
      "Epoch [1/1] | Batch 3273 | loss: 0.033667176961898804\n",
      "Epoch [1/1] | Batch 3274 | loss: 0.1004490926861763\n",
      "Epoch [1/1] | Batch 3275 | loss: 0.2084910273551941\n",
      "Epoch [1/1] | Batch 3276 | loss: 0.05144072324037552\n",
      "Epoch [1/1] | Batch 3277 | loss: 0.18026190996170044\n",
      "Epoch [1/1] | Batch 3278 | loss: 0.30493414402008057\n",
      "Epoch [1/1] | Batch 3279 | loss: 0.18928317725658417\n",
      "Epoch [1/1] | Batch 3280 | loss: 0.07196973264217377\n",
      "Epoch [1/1] | Batch 3281 | loss: 0.07491130381822586\n",
      "Epoch [1/1] | Batch 3282 | loss: 0.03850345313549042\n",
      "Epoch [1/1] | Batch 3283 | loss: 0.1811487227678299\n",
      "Epoch [1/1] | Batch 3284 | loss: 0.10190621018409729\n",
      "Epoch [1/1] | Batch 3285 | loss: 0.0019513453589752316\n",
      "Epoch [1/1] | Batch 3286 | loss: 0.0830981507897377\n",
      "Epoch [1/1] | Batch 3287 | loss: 0.008752353489398956\n",
      "Epoch [1/1] | Batch 3288 | loss: 0.04836089909076691\n",
      "Epoch [1/1] | Batch 3289 | loss: 0.31037357449531555\n",
      "Epoch [1/1] | Batch 3290 | loss: 0.3552683889865875\n",
      "Epoch [1/1] | Batch 3291 | loss: 0.06740977615118027\n",
      "Epoch [1/1] | Batch 3292 | loss: 0.24616050720214844\n",
      "Epoch [1/1] | Batch 3293 | loss: 0.029345117509365082\n",
      "Epoch [1/1] | Batch 3294 | loss: 0.18646906316280365\n",
      "Epoch [1/1] | Batch 3295 | loss: 0.003401323687285185\n",
      "Epoch [1/1] | Batch 3296 | loss: 0.00689720967784524\n",
      "Epoch [1/1] | Batch 3297 | loss: 0.0011517172679305077\n",
      "Epoch [1/1] | Batch 3298 | loss: 0.46967941522598267\n",
      "Epoch [1/1] | Batch 3299 | loss: 0.09781496226787567\n",
      "Epoch [1/1] | Batch 3300 | loss: 0.7404788732528687\n",
      "Epoch [1/1] | Batch 3301 | loss: 0.30561399459838867\n",
      "Epoch [1/1] | Batch 3302 | loss: 0.3346666395664215\n",
      "Epoch [1/1] | Batch 3303 | loss: 0.04130782559514046\n",
      "Epoch [1/1] | Batch 3304 | loss: 0.08328430354595184\n",
      "Epoch [1/1] | Batch 3305 | loss: 0.005821649916470051\n",
      "Epoch [1/1] | Batch 3306 | loss: 0.29932209849357605\n",
      "Epoch [1/1] | Batch 3307 | loss: 0.018924949690699577\n",
      "Epoch [1/1] | Batch 3308 | loss: 0.010666223242878914\n",
      "Epoch [1/1] | Batch 3309 | loss: 0.015783192589879036\n",
      "Epoch [1/1] | Batch 3310 | loss: 0.14876307547092438\n",
      "Epoch [1/1] | Batch 3311 | loss: 0.01037384569644928\n",
      "Epoch [1/1] | Batch 3312 | loss: 0.00437884172424674\n",
      "Epoch [1/1] | Batch 3313 | loss: 0.12299122661352158\n",
      "Epoch [1/1] | Batch 3314 | loss: 0.019931646063923836\n",
      "Epoch [1/1] | Batch 3315 | loss: 0.042158614844083786\n",
      "Epoch [1/1] | Batch 3316 | loss: 0.05410528928041458\n",
      "Epoch [1/1] | Batch 3317 | loss: 0.02972845546901226\n",
      "Epoch [1/1] | Batch 3318 | loss: 0.14726732671260834\n",
      "Epoch [1/1] | Batch 3319 | loss: 0.26638367772102356\n",
      "Epoch [1/1] | Batch 3320 | loss: 0.10639075189828873\n",
      "Epoch [1/1] | Batch 3321 | loss: 0.3730897009372711\n",
      "Epoch [1/1] | Batch 3322 | loss: 0.013296838849782944\n",
      "Epoch [1/1] | Batch 3323 | loss: 0.19566309452056885\n",
      "Epoch [1/1] | Batch 3324 | loss: 0.3132586181163788\n",
      "Epoch [1/1] | Batch 3325 | loss: 0.0562359094619751\n",
      "Epoch [1/1] | Batch 3326 | loss: 0.010278359055519104\n",
      "Epoch [1/1] | Batch 3327 | loss: 0.023437030613422394\n",
      "Epoch [1/1] | Batch 3328 | loss: 0.20866785943508148\n",
      "Epoch [1/1] | Batch 3329 | loss: 0.15946078300476074\n",
      "Epoch [1/1] | Batch 3330 | loss: 0.06597138941287994\n",
      "Epoch [1/1] | Batch 3331 | loss: 0.17228132486343384\n",
      "Epoch [1/1] | Batch 3332 | loss: 0.0031724683940410614\n",
      "Epoch [1/1] | Batch 3333 | loss: 0.15212906897068024\n",
      "Epoch [1/1] | Batch 3334 | loss: 0.07053998857736588\n",
      "Epoch [1/1] | Batch 3335 | loss: 0.005525451619178057\n",
      "Epoch [1/1] | Batch 3336 | loss: 0.03870449587702751\n",
      "Epoch [1/1] | Batch 3337 | loss: 0.01158869918435812\n",
      "Epoch [1/1] | Batch 3338 | loss: 0.010969443246722221\n",
      "Epoch [1/1] | Batch 3339 | loss: 0.4755494296550751\n",
      "Epoch [1/1] | Batch 3340 | loss: 0.040017466992139816\n",
      "Epoch [1/1] | Batch 3341 | loss: 0.38570818305015564\n",
      "Epoch [1/1] | Batch 3342 | loss: 0.016186729073524475\n",
      "Epoch [1/1] | Batch 3343 | loss: 0.1782481074333191\n",
      "Epoch [1/1] | Batch 3344 | loss: 0.039476606994867325\n",
      "Epoch [1/1] | Batch 3345 | loss: 0.016260655596852303\n",
      "Epoch [1/1] | Batch 3346 | loss: 0.011226503178477287\n",
      "Epoch [1/1] | Batch 3347 | loss: 0.560920238494873\n",
      "Epoch [1/1] | Batch 3348 | loss: 0.009111938998103142\n",
      "Epoch [1/1] | Batch 3349 | loss: 0.05279204621911049\n",
      "Epoch [1/1] | Batch 3350 | loss: 0.0557224340736866\n",
      "Epoch [1/1] | Batch 3351 | loss: 0.030866624787449837\n",
      "Epoch [1/1] | Batch 3352 | loss: 0.027510635554790497\n",
      "Epoch [1/1] | Batch 3353 | loss: 0.008028009906411171\n",
      "Epoch [1/1] | Batch 3354 | loss: 0.003915749955922365\n",
      "Epoch [1/1] | Batch 3355 | loss: 0.3908538818359375\n",
      "Epoch [1/1] | Batch 3356 | loss: 0.005385482683777809\n",
      "Epoch [1/1] | Batch 3357 | loss: 0.13167259097099304\n",
      "Epoch [1/1] | Batch 3358 | loss: 0.0049085780046880245\n",
      "Epoch [1/1] | Batch 3359 | loss: 0.1804402768611908\n",
      "Epoch [1/1] | Batch 3360 | loss: 0.06248198449611664\n",
      "Epoch [1/1] | Batch 3361 | loss: 0.11699597537517548\n",
      "Epoch [1/1] | Batch 3362 | loss: 0.10917782038450241\n",
      "Epoch [1/1] | Batch 3363 | loss: 0.003850550390779972\n",
      "Epoch [1/1] | Batch 3364 | loss: 0.17724360525608063\n",
      "Epoch [1/1] | Batch 3365 | loss: 0.09272100776433945\n",
      "Epoch [1/1] | Batch 3366 | loss: 0.02105526253581047\n",
      "Epoch [1/1] | Batch 3367 | loss: 0.30548378825187683\n",
      "Epoch [1/1] | Batch 3368 | loss: 0.04695737361907959\n",
      "Epoch [1/1] | Batch 3369 | loss: 0.019064508378505707\n",
      "Epoch [1/1] | Batch 3370 | loss: 0.26379555463790894\n",
      "Epoch [1/1] | Batch 3371 | loss: 0.02590824104845524\n",
      "Epoch [1/1] | Batch 3372 | loss: 0.12387710809707642\n",
      "Epoch [1/1] | Batch 3373 | loss: 0.017240304499864578\n",
      "Epoch [1/1] | Batch 3374 | loss: 0.06969927251338959\n",
      "Epoch [1/1] | Batch 3375 | loss: 0.009976653382182121\n",
      "Epoch [1/1] | Batch 3376 | loss: 0.1533513069152832\n",
      "Epoch [1/1] | Batch 3377 | loss: 0.051347069442272186\n",
      "Epoch [1/1] | Batch 3378 | loss: 0.5269631147384644\n",
      "Epoch [1/1] | Batch 3379 | loss: 0.17757846415042877\n",
      "Epoch [1/1] | Batch 3380 | loss: 0.03697992488741875\n",
      "Epoch [1/1] | Batch 3381 | loss: 0.06051620468497276\n",
      "Epoch [1/1] | Batch 3382 | loss: 0.0013221411500126123\n",
      "Epoch [1/1] | Batch 3383 | loss: 0.15863703191280365\n",
      "Epoch [1/1] | Batch 3384 | loss: 0.09178401529788971\n",
      "Epoch [1/1] | Batch 3385 | loss: 0.19192399084568024\n",
      "Epoch [1/1] | Batch 3386 | loss: 0.003233466763049364\n",
      "Epoch [1/1] | Batch 3387 | loss: 0.3333158791065216\n",
      "Epoch [1/1] | Batch 3388 | loss: 0.01835738494992256\n",
      "Epoch [1/1] | Batch 3389 | loss: 0.00575927272439003\n",
      "Epoch [1/1] | Batch 3390 | loss: 0.0177992582321167\n",
      "Epoch [1/1] | Batch 3391 | loss: 0.01525336317718029\n",
      "Epoch [1/1] | Batch 3392 | loss: 0.055145904421806335\n",
      "Epoch [1/1] | Batch 3393 | loss: 0.035793568938970566\n",
      "Epoch [1/1] | Batch 3394 | loss: 0.12724091112613678\n",
      "Epoch [1/1] | Batch 3395 | loss: 0.04756243899464607\n",
      "Epoch [1/1] | Batch 3396 | loss: 0.03323756158351898\n",
      "Epoch [1/1] | Batch 3397 | loss: 0.16825628280639648\n",
      "Epoch [1/1] | Batch 3398 | loss: 0.22015920281410217\n",
      "Epoch [1/1] | Batch 3399 | loss: 0.1326063871383667\n",
      "Epoch [1/1] | Batch 3400 | loss: 0.030631575733423233\n",
      "Epoch [1/1] | Batch 3401 | loss: 0.05009293556213379\n",
      "Epoch [1/1] | Batch 3402 | loss: 0.02123967930674553\n",
      "Epoch [1/1] | Batch 3403 | loss: 0.09909617155790329\n",
      "Epoch [1/1] | Batch 3404 | loss: 0.05222826823592186\n",
      "Epoch [1/1] | Batch 3405 | loss: 0.017566144466400146\n",
      "Epoch [1/1] | Batch 3406 | loss: 0.07656136155128479\n",
      "Epoch [1/1] | Batch 3407 | loss: 0.002770183142274618\n",
      "Epoch [1/1] | Batch 3408 | loss: 0.04152976721525192\n",
      "Epoch [1/1] | Batch 3409 | loss: 0.11196471005678177\n",
      "Epoch [1/1] | Batch 3410 | loss: 0.10270052403211594\n",
      "Epoch [1/1] | Batch 3411 | loss: 0.010125708766281605\n",
      "Epoch [1/1] | Batch 3412 | loss: 0.09719447046518326\n",
      "Epoch [1/1] | Batch 3413 | loss: 0.11993496865034103\n",
      "Epoch [1/1] | Batch 3414 | loss: 0.5300946235656738\n",
      "Epoch [1/1] | Batch 3415 | loss: 0.009563779458403587\n",
      "Epoch [1/1] | Batch 3416 | loss: 0.0022922372445464134\n",
      "Epoch [1/1] | Batch 3417 | loss: 0.008740575984120369\n",
      "Epoch [1/1] | Batch 3418 | loss: 0.020832737907767296\n",
      "Epoch [1/1] | Batch 3419 | loss: 0.048422589898109436\n",
      "Epoch [1/1] | Batch 3420 | loss: 0.012844013050198555\n",
      "Epoch [1/1] | Batch 3421 | loss: 0.12216011434793472\n",
      "Epoch [1/1] | Batch 3422 | loss: 0.006957169156521559\n",
      "Epoch [1/1] | Batch 3423 | loss: 0.06328672915697098\n",
      "Epoch [1/1] | Batch 3424 | loss: 0.16039863228797913\n",
      "Epoch [1/1] | Batch 3425 | loss: 0.2974325716495514\n",
      "Epoch [1/1] | Batch 3426 | loss: 0.09833228588104248\n",
      "Epoch [1/1] | Batch 3427 | loss: 0.10198318958282471\n",
      "Epoch [1/1] | Batch 3428 | loss: 0.019943658262491226\n",
      "Epoch [1/1] | Batch 3429 | loss: 0.2943181097507477\n",
      "Epoch [1/1] | Batch 3430 | loss: 0.031558308750391006\n",
      "Epoch [1/1] | Batch 3431 | loss: 0.07322356849908829\n",
      "Epoch [1/1] | Batch 3432 | loss: 0.8932144045829773\n",
      "Epoch [1/1] | Batch 3433 | loss: 0.016614874824881554\n",
      "Epoch [1/1] | Batch 3434 | loss: 0.31079593300819397\n",
      "Epoch [1/1] | Batch 3435 | loss: 0.30665475130081177\n",
      "Epoch [1/1] | Batch 3436 | loss: 0.006938071921467781\n",
      "Epoch [1/1] | Batch 3437 | loss: 0.14680486917495728\n",
      "Epoch [1/1] | Batch 3438 | loss: 0.21999934315681458\n",
      "Epoch [1/1] | Batch 3439 | loss: 0.01595102809369564\n",
      "Epoch [1/1] | Batch 3440 | loss: 0.01974007673561573\n",
      "Epoch [1/1] | Batch 3441 | loss: 0.025525137782096863\n",
      "Epoch [1/1] | Batch 3442 | loss: 0.007677379529923201\n",
      "Epoch [1/1] | Batch 3443 | loss: 0.0249057374894619\n",
      "Epoch [1/1] | Batch 3444 | loss: 0.042339250445365906\n",
      "Epoch [1/1] | Batch 3445 | loss: 0.7357943058013916\n",
      "Epoch [1/1] | Batch 3446 | loss: 0.5375878214836121\n",
      "Epoch [1/1] | Batch 3447 | loss: 0.05336460471153259\n",
      "Epoch [1/1] | Batch 3448 | loss: 0.15396849811077118\n",
      "Epoch [1/1] | Batch 3449 | loss: 0.4532955586910248\n",
      "Epoch [1/1] | Batch 3450 | loss: 0.2598324716091156\n",
      "Epoch [1/1] | Batch 3451 | loss: 0.007421448361128569\n",
      "Epoch [1/1] | Batch 3452 | loss: 0.0005461718537844718\n",
      "Epoch [1/1] | Batch 3453 | loss: 0.008196843788027763\n",
      "Epoch [1/1] | Batch 3454 | loss: 0.24701912701129913\n",
      "Epoch [1/1] | Batch 3455 | loss: 0.05268314108252525\n",
      "Epoch [1/1] | Batch 3456 | loss: 0.18134558200836182\n",
      "Epoch [1/1] | Batch 3457 | loss: 0.03557475283741951\n",
      "Epoch [1/1] | Batch 3458 | loss: 0.24424542486667633\n",
      "Epoch [1/1] | Batch 3459 | loss: 0.437213659286499\n",
      "Epoch [1/1] | Batch 3460 | loss: 0.0186824519187212\n",
      "Epoch [1/1] | Batch 3461 | loss: 0.06894712150096893\n",
      "Epoch [1/1] | Batch 3462 | loss: 0.026498418301343918\n",
      "Epoch [1/1] | Batch 3463 | loss: 0.06414981931447983\n",
      "Epoch [1/1] | Batch 3464 | loss: 0.268638551235199\n",
      "Epoch [1/1] | Batch 3465 | loss: 0.0072411769069731236\n",
      "Epoch [1/1] | Batch 3466 | loss: 0.42130139470100403\n",
      "Epoch [1/1] | Batch 3467 | loss: 0.0016151131130754948\n",
      "Epoch [1/1] | Batch 3468 | loss: 0.028922542929649353\n",
      "Epoch [1/1] | Batch 3469 | loss: 0.22775541245937347\n",
      "Epoch [1/1] | Batch 3470 | loss: 0.08898461610078812\n",
      "Epoch [1/1] | Batch 3471 | loss: 0.15040713548660278\n",
      "Epoch [1/1] | Batch 3472 | loss: 0.077915258705616\n",
      "Epoch [1/1] | Batch 3473 | loss: 0.16900016367435455\n",
      "Epoch [1/1] | Batch 3474 | loss: 0.8447903394699097\n",
      "Epoch [1/1] | Batch 3475 | loss: 0.04950530081987381\n",
      "Epoch [1/1] | Batch 3476 | loss: 0.03149612247943878\n",
      "Epoch [1/1] | Batch 3477 | loss: 0.02741975337266922\n",
      "Epoch [1/1] | Batch 3478 | loss: 0.11474604904651642\n",
      "Epoch [1/1] | Batch 3479 | loss: 0.12246783077716827\n",
      "Epoch [1/1] | Batch 3480 | loss: 0.3209019601345062\n",
      "Epoch [1/1] | Batch 3481 | loss: 0.07956965267658234\n",
      "Epoch [1/1] | Batch 3482 | loss: 0.049995213747024536\n",
      "Epoch [1/1] | Batch 3483 | loss: 0.20382742583751678\n",
      "Epoch [1/1] | Batch 3484 | loss: 0.05904584005475044\n",
      "Epoch [1/1] | Batch 3485 | loss: 0.40439045429229736\n",
      "Epoch [1/1] | Batch 3486 | loss: 0.005960795562714338\n",
      "Epoch [1/1] | Batch 3487 | loss: 0.4160856604576111\n",
      "Epoch [1/1] | Batch 3488 | loss: 0.010482849553227425\n",
      "Epoch [1/1] | Batch 3489 | loss: 0.01659584231674671\n",
      "Epoch [1/1] | Batch 3490 | loss: 0.4450104534626007\n",
      "Epoch [1/1] | Batch 3491 | loss: 0.05522116646170616\n",
      "Epoch [1/1] | Batch 3492 | loss: 0.14310045540332794\n",
      "Epoch [1/1] | Batch 3493 | loss: 0.14623793959617615\n",
      "Epoch [1/1] | Batch 3494 | loss: 0.21026819944381714\n",
      "Epoch [1/1] | Batch 3495 | loss: 0.19961676001548767\n",
      "Epoch [1/1] | Batch 3496 | loss: 0.003211059607565403\n",
      "Epoch [1/1] | Batch 3497 | loss: 0.04443034157156944\n",
      "Epoch [1/1] | Batch 3498 | loss: 0.01900465041399002\n",
      "Epoch [1/1] | Batch 3499 | loss: 0.030107993632555008\n",
      "Epoch [1/1] | Batch 3500 | loss: 0.07967555522918701\n",
      "Epoch [1/1] | Batch 3501 | loss: 0.08021716773509979\n",
      "Epoch [1/1] | Batch 3502 | loss: 0.031282201409339905\n",
      "Epoch [1/1] | Batch 3503 | loss: 0.026422908529639244\n",
      "Epoch [1/1] | Batch 3504 | loss: 0.03592096269130707\n",
      "Epoch [1/1] | Batch 3505 | loss: 0.589458703994751\n",
      "Epoch [1/1] | Batch 3506 | loss: 0.19096899032592773\n",
      "Epoch [1/1] | Batch 3507 | loss: 0.7173563241958618\n",
      "Epoch [1/1] | Batch 3508 | loss: 0.04723522812128067\n",
      "Epoch [1/1] | Batch 3509 | loss: 0.10374395549297333\n",
      "Epoch [1/1] | Batch 3510 | loss: 0.25837475061416626\n",
      "Epoch [1/1] | Batch 3511 | loss: 0.0050855702720582485\n",
      "Epoch [1/1] | Batch 3512 | loss: 0.03510775789618492\n",
      "Epoch [1/1] | Batch 3513 | loss: 0.043551139533519745\n",
      "Epoch [1/1] | Batch 3514 | loss: 0.035639066249132156\n",
      "Epoch [1/1] | Batch 3515 | loss: 0.10957393050193787\n",
      "Epoch [1/1] | Batch 3516 | loss: 0.29003143310546875\n",
      "Epoch [1/1] | Batch 3517 | loss: 0.08816179633140564\n",
      "Epoch [1/1] | Batch 3518 | loss: 0.3037988245487213\n",
      "Epoch [1/1] | Batch 3519 | loss: 0.22422610223293304\n",
      "Epoch [1/1] | Batch 3520 | loss: 0.006670198403298855\n",
      "Epoch [1/1] | Batch 3521 | loss: 0.32046210765838623\n",
      "Epoch [1/1] | Batch 3522 | loss: 0.003447452560067177\n",
      "Epoch [1/1] | Batch 3523 | loss: 0.09827934950590134\n",
      "Epoch [1/1] | Batch 3524 | loss: 0.025169670581817627\n",
      "Epoch [1/1] | Batch 3525 | loss: 0.41888752579689026\n",
      "Epoch [1/1] | Batch 3526 | loss: 0.08637583255767822\n",
      "Epoch [1/1] | Batch 3527 | loss: 0.03939506784081459\n",
      "Epoch [1/1] | Batch 3528 | loss: 0.3770948052406311\n",
      "Epoch [1/1] | Batch 3529 | loss: 0.026460427790880203\n",
      "Epoch [1/1] | Batch 3530 | loss: 0.36034590005874634\n",
      "Epoch [1/1] | Batch 3531 | loss: 0.07090632617473602\n",
      "Epoch [1/1] | Batch 3532 | loss: 0.2356167584657669\n",
      "Epoch [1/1] | Batch 3533 | loss: 0.03334696590900421\n",
      "Epoch [1/1] | Batch 3534 | loss: 0.26815828680992126\n",
      "Epoch [1/1] | Batch 3535 | loss: 0.004427817650139332\n",
      "Epoch [1/1] | Batch 3536 | loss: 0.10089528560638428\n",
      "Epoch [1/1] | Batch 3537 | loss: 0.024435941129922867\n",
      "Epoch [1/1] | Batch 3538 | loss: 0.021584270521998405\n",
      "Epoch [1/1] | Batch 3539 | loss: 0.05713404342532158\n",
      "Epoch [1/1] | Batch 3540 | loss: 0.3061816990375519\n",
      "Epoch [1/1] | Batch 3541 | loss: 0.1377900391817093\n",
      "Epoch [1/1] | Batch 3542 | loss: 0.0958385095000267\n",
      "Epoch [1/1] | Batch 3543 | loss: 0.004276142455637455\n",
      "Epoch [1/1] | Batch 3544 | loss: 0.03255698084831238\n",
      "Epoch [1/1] | Batch 3545 | loss: 0.11842317879199982\n",
      "Epoch [1/1] | Batch 3546 | loss: 0.20860472321510315\n",
      "Epoch [1/1] | Batch 3547 | loss: 0.47424378991127014\n",
      "Epoch [1/1] | Batch 3548 | loss: 0.12243306636810303\n",
      "Epoch [1/1] | Batch 3549 | loss: 0.45214712619781494\n",
      "Epoch [1/1] | Batch 3550 | loss: 0.1802596002817154\n",
      "Epoch [1/1] | Batch 3551 | loss: 0.08455582708120346\n",
      "Epoch [1/1] | Batch 3552 | loss: 0.20879647135734558\n",
      "Epoch [1/1] | Batch 3553 | loss: 0.02591065689921379\n",
      "Epoch [1/1] | Batch 3554 | loss: 0.001243683393113315\n",
      "Epoch [1/1] | Batch 3555 | loss: 0.27941441535949707\n",
      "Epoch [1/1] | Batch 3556 | loss: 0.02612408995628357\n",
      "Epoch [1/1] | Batch 3557 | loss: 0.14110498130321503\n",
      "Epoch [1/1] | Batch 3558 | loss: 0.014041169546544552\n",
      "Epoch [1/1] | Batch 3559 | loss: 0.009155042469501495\n",
      "Epoch [1/1] | Batch 3560 | loss: 0.04807088151574135\n",
      "Epoch [1/1] | Batch 3561 | loss: 0.05127067118883133\n",
      "Epoch [1/1] | Batch 3562 | loss: 0.01303576398640871\n",
      "Epoch [1/1] | Batch 3563 | loss: 0.07411088794469833\n",
      "Epoch [1/1] | Batch 3564 | loss: 0.11435754597187042\n",
      "Epoch [1/1] | Batch 3565 | loss: 0.04945657402276993\n",
      "Epoch [1/1] | Batch 3566 | loss: 0.006602178327739239\n",
      "Epoch [1/1] | Batch 3567 | loss: 0.06841157376766205\n",
      "Epoch [1/1] | Batch 3568 | loss: 0.29039499163627625\n",
      "Epoch [1/1] | Batch 3569 | loss: 0.7373697757720947\n",
      "Epoch [1/1] | Batch 3570 | loss: 0.0227728933095932\n",
      "Epoch [1/1] | Batch 3571 | loss: 0.07421927899122238\n",
      "Epoch [1/1] | Batch 3572 | loss: 0.3252665400505066\n",
      "Epoch [1/1] | Batch 3573 | loss: 0.3368395268917084\n",
      "Epoch [1/1] | Batch 3574 | loss: 0.027319133281707764\n",
      "Epoch [1/1] | Batch 3575 | loss: 0.1992303729057312\n",
      "Epoch [1/1] | Batch 3576 | loss: 0.010106995701789856\n",
      "Epoch [1/1] | Batch 3577 | loss: 0.11804991215467453\n",
      "Epoch [1/1] | Batch 3578 | loss: 0.045056357979774475\n",
      "Epoch [1/1] | Batch 3579 | loss: 0.15052665770053864\n",
      "Epoch [1/1] | Batch 3580 | loss: 0.08059065043926239\n",
      "Epoch [1/1] | Batch 3581 | loss: 0.03528444468975067\n",
      "Epoch [1/1] | Batch 3582 | loss: 0.08346305787563324\n",
      "Epoch [1/1] | Batch 3583 | loss: 0.15796227753162384\n",
      "Epoch [1/1] | Batch 3584 | loss: 0.14256086945533752\n",
      "Epoch [1/1] | Batch 3585 | loss: 0.32611456513404846\n",
      "Epoch [1/1] | Batch 3586 | loss: 1.0402439832687378\n",
      "Epoch [1/1] | Batch 3587 | loss: 0.21759256720542908\n",
      "Epoch [1/1] | Batch 3588 | loss: 0.0053879800252616405\n",
      "Epoch [1/1] | Batch 3589 | loss: 0.17988187074661255\n",
      "Epoch [1/1] | Batch 3590 | loss: 0.0908169150352478\n",
      "Epoch [1/1] | Batch 3591 | loss: 0.25439953804016113\n",
      "Epoch [1/1] | Batch 3592 | loss: 0.032568320631980896\n",
      "Epoch [1/1] | Batch 3593 | loss: 0.17493002116680145\n",
      "Epoch [1/1] | Batch 3594 | loss: 0.014743667095899582\n",
      "Epoch [1/1] | Batch 3595 | loss: 0.023708581924438477\n",
      "Epoch [1/1] | Batch 3596 | loss: 0.045851632952690125\n",
      "Epoch [1/1] | Batch 3597 | loss: 0.00631724763661623\n",
      "Epoch [1/1] | Batch 3598 | loss: 0.22215469181537628\n",
      "Epoch [1/1] | Batch 3599 | loss: 0.023018725216388702\n",
      "Epoch [1/1] | Batch 3600 | loss: 0.08678073436021805\n",
      "Epoch [1/1] | Batch 3601 | loss: 0.032312169671058655\n",
      "Epoch [1/1] | Batch 3602 | loss: 0.059933219105005264\n",
      "Epoch [1/1] | Batch 3603 | loss: 0.3355950713157654\n",
      "Epoch [1/1] | Batch 3604 | loss: 0.07308460026979446\n",
      "Epoch [1/1] | Batch 3605 | loss: 0.10085903108119965\n",
      "Epoch [1/1] | Batch 3606 | loss: 0.07327352464199066\n",
      "Epoch [1/1] | Batch 3607 | loss: 0.028203830122947693\n",
      "Epoch [1/1] | Batch 3608 | loss: 0.2744980454444885\n",
      "Epoch [1/1] | Batch 3609 | loss: 0.011604352854192257\n",
      "Epoch [1/1] | Batch 3610 | loss: 0.012985723093152046\n",
      "Epoch [1/1] | Batch 3611 | loss: 0.11424609273672104\n",
      "Epoch [1/1] | Batch 3612 | loss: 0.009973191656172276\n",
      "Epoch [1/1] | Batch 3613 | loss: 0.25808918476104736\n",
      "Epoch [1/1] | Batch 3614 | loss: 0.31013548374176025\n",
      "Epoch [1/1] | Batch 3615 | loss: 0.0457056425511837\n",
      "Epoch [1/1] | Batch 3616 | loss: 0.35167166590690613\n",
      "Epoch [1/1] | Batch 3617 | loss: 0.009476548992097378\n",
      "Epoch [1/1] | Batch 3618 | loss: 0.3330608606338501\n",
      "Epoch [1/1] | Batch 3619 | loss: 0.2776201665401459\n",
      "Epoch [1/1] | Batch 3620 | loss: 0.03375166282057762\n",
      "Epoch [1/1] | Batch 3621 | loss: 0.12359488755464554\n",
      "Epoch [1/1] | Batch 3622 | loss: 0.0017271526157855988\n",
      "Epoch [1/1] | Batch 3623 | loss: 0.006687031127512455\n",
      "Epoch [1/1] | Batch 3624 | loss: 0.01198284700512886\n",
      "Epoch [1/1] | Batch 3625 | loss: 0.029281876981258392\n",
      "Epoch [1/1] | Batch 3626 | loss: 0.018488110974431038\n",
      "Epoch [1/1] | Batch 3627 | loss: 0.0024557856377214193\n",
      "Epoch [1/1] | Batch 3628 | loss: 0.3507671654224396\n",
      "Epoch [1/1] | Batch 3629 | loss: 0.021412115544080734\n",
      "Epoch [1/1] | Batch 3630 | loss: 0.0035239371936768293\n",
      "Epoch [1/1] | Batch 3631 | loss: 0.003032271284610033\n",
      "Epoch [1/1] | Batch 3632 | loss: 0.2965027987957001\n",
      "Epoch [1/1] | Batch 3633 | loss: 0.0030824211426079273\n",
      "Epoch [1/1] | Batch 3634 | loss: 0.17712880671024323\n",
      "Epoch [1/1] | Batch 3635 | loss: 0.0485067218542099\n",
      "Epoch [1/1] | Batch 3636 | loss: 0.010807910934090614\n",
      "Epoch [1/1] | Batch 3637 | loss: 0.012747231870889664\n",
      "Epoch [1/1] | Batch 3638 | loss: 0.036400776356458664\n",
      "Epoch [1/1] | Batch 3639 | loss: 0.14860209822654724\n",
      "Epoch [1/1] | Batch 3640 | loss: 0.03048003651201725\n",
      "Epoch [1/1] | Batch 3641 | loss: 0.06624958664178848\n",
      "Epoch [1/1] | Batch 3642 | loss: 0.0059875259175896645\n",
      "Epoch [1/1] | Batch 3643 | loss: 0.023816443979740143\n",
      "Epoch [1/1] | Batch 3644 | loss: 0.2495756447315216\n",
      "Epoch [1/1] | Batch 3645 | loss: 0.0031663556583225727\n",
      "Epoch [1/1] | Batch 3646 | loss: 0.00837888102978468\n",
      "Epoch [1/1] | Batch 3647 | loss: 0.024914808571338654\n",
      "Epoch [1/1] | Batch 3648 | loss: 0.27355921268463135\n",
      "Epoch [1/1] | Batch 3649 | loss: 0.010334795340895653\n",
      "Epoch [1/1] | Batch 3650 | loss: 0.061048462986946106\n",
      "Epoch [1/1] | Batch 3651 | loss: 0.07543419301509857\n",
      "Epoch [1/1] | Batch 3652 | loss: 0.07425722479820251\n",
      "Epoch [1/1] | Batch 3653 | loss: 0.2681794762611389\n",
      "Epoch [1/1] | Batch 3654 | loss: 0.5800291895866394\n",
      "Epoch [1/1] | Batch 3655 | loss: 0.018475137650966644\n",
      "Epoch [1/1] | Batch 3656 | loss: 0.05863068252801895\n",
      "Epoch [1/1] | Batch 3657 | loss: 0.01629025675356388\n",
      "Epoch [1/1] | Batch 3658 | loss: 0.16708333790302277\n",
      "Epoch [1/1] | Batch 3659 | loss: 0.5610806941986084\n",
      "Epoch [1/1] | Batch 3660 | loss: 0.07410580664873123\n",
      "Epoch [1/1] | Batch 3661 | loss: 0.24505667388439178\n",
      "Epoch [1/1] | Batch 3662 | loss: 0.06500203162431717\n",
      "Epoch [1/1] | Batch 3663 | loss: 0.513829231262207\n",
      "Epoch [1/1] | Batch 3664 | loss: 0.010069504380226135\n",
      "Epoch [1/1] | Batch 3665 | loss: 0.13351532816886902\n",
      "Epoch [1/1] | Batch 3666 | loss: 0.019890518859028816\n",
      "Epoch [1/1] | Batch 3667 | loss: 0.005427262280136347\n",
      "Epoch [1/1] | Batch 3668 | loss: 0.3813054859638214\n",
      "Epoch [1/1] | Batch 3669 | loss: 0.20391063392162323\n",
      "Epoch [1/1] | Batch 3670 | loss: 0.2171182930469513\n",
      "Epoch [1/1] | Batch 3671 | loss: 0.11568084359169006\n",
      "Epoch [1/1] | Batch 3672 | loss: 0.5646323561668396\n",
      "Epoch [1/1] | Batch 3673 | loss: 0.15943428874015808\n",
      "Epoch [1/1] | Batch 3674 | loss: 0.17252951860427856\n",
      "Epoch [1/1] | Batch 3675 | loss: 0.09718382358551025\n",
      "Epoch [1/1] | Batch 3676 | loss: 0.01569294184446335\n",
      "Epoch [1/1] | Batch 3677 | loss: 0.10741344094276428\n",
      "Epoch [1/1] | Batch 3678 | loss: 0.07766856998205185\n",
      "Epoch [1/1] | Batch 3679 | loss: 0.014834124594926834\n",
      "Epoch [1/1] | Batch 3680 | loss: 0.00685615511611104\n",
      "Epoch [1/1] | Batch 3681 | loss: 0.04476020857691765\n",
      "Epoch [1/1] | Batch 3682 | loss: 0.16978131234645844\n",
      "Epoch [1/1] | Batch 3683 | loss: 0.3311733603477478\n",
      "Epoch [1/1] | Batch 3684 | loss: 0.0482640415430069\n",
      "Epoch [1/1] | Batch 3685 | loss: 0.06471336632966995\n",
      "Epoch [1/1] | Batch 3686 | loss: 0.011339818127453327\n",
      "Epoch [1/1] | Batch 3687 | loss: 0.015962718054652214\n",
      "Epoch [1/1] | Batch 3688 | loss: 0.36610153317451477\n",
      "Epoch [1/1] | Batch 3689 | loss: 0.23689162731170654\n",
      "Epoch [1/1] | Batch 3690 | loss: 0.0266861692070961\n",
      "Epoch [1/1] | Batch 3691 | loss: 0.07013552635908127\n",
      "Epoch [1/1] | Batch 3692 | loss: 0.005907122977077961\n",
      "Epoch [1/1] | Batch 3693 | loss: 0.004631520714610815\n",
      "Epoch [1/1] | Batch 3694 | loss: 0.007923515513539314\n",
      "Epoch [1/1] | Batch 3695 | loss: 0.10796681046485901\n",
      "Epoch [1/1] | Batch 3696 | loss: 0.30672022700309753\n",
      "Epoch [1/1] | Batch 3697 | loss: 0.004523212090134621\n",
      "Epoch [1/1] | Batch 3698 | loss: 0.01091562956571579\n",
      "Epoch [1/1] | Batch 3699 | loss: 0.0536811538040638\n",
      "Epoch [1/1] | Batch 3700 | loss: 0.09689616411924362\n",
      "Epoch [1/1] | Batch 3701 | loss: 0.13092350959777832\n",
      "Epoch [1/1] | Batch 3702 | loss: 0.10755126178264618\n",
      "Epoch [1/1] | Batch 3703 | loss: 0.0716799795627594\n",
      "Epoch [1/1] | Batch 3704 | loss: 0.34961190819740295\n",
      "Epoch [1/1] | Batch 3705 | loss: 0.009769001044332981\n",
      "Epoch [1/1] | Batch 3706 | loss: 0.10994633287191391\n",
      "Epoch [1/1] | Batch 3707 | loss: 0.0027062545996159315\n",
      "Epoch [1/1] | Batch 3708 | loss: 0.10070303082466125\n",
      "Epoch [1/1] | Batch 3709 | loss: 0.029975194483995438\n",
      "Epoch [1/1] | Batch 3710 | loss: 0.0077733807265758514\n",
      "Epoch [1/1] | Batch 3711 | loss: 0.04248867183923721\n",
      "Epoch [1/1] | Batch 3712 | loss: 0.00984315387904644\n",
      "Epoch [1/1] | Batch 3713 | loss: 0.02418302372097969\n",
      "Epoch [1/1] | Batch 3714 | loss: 0.008985457010567188\n",
      "Epoch [1/1] | Batch 3715 | loss: 0.111679807305336\n",
      "Epoch [1/1] | Batch 3716 | loss: 0.4615560472011566\n",
      "Epoch [1/1] | Batch 3717 | loss: 0.006402417551726103\n",
      "Epoch [1/1] | Batch 3718 | loss: 0.20998047292232513\n",
      "Epoch [1/1] | Batch 3719 | loss: 0.012976332567632198\n",
      "Epoch [1/1] | Batch 3720 | loss: 0.0073423488065600395\n",
      "Epoch [1/1] | Batch 3721 | loss: 0.18521490693092346\n",
      "Epoch [1/1] | Batch 3722 | loss: 0.017472021281719208\n",
      "Epoch [1/1] | Batch 3723 | loss: 0.05303526669740677\n",
      "Epoch [1/1] | Batch 3724 | loss: 0.12228839844465256\n",
      "Epoch [1/1] | Batch 3725 | loss: 0.004268575459718704\n",
      "Epoch [1/1] | Batch 3726 | loss: 0.03463760018348694\n",
      "Epoch [1/1] | Batch 3727 | loss: 0.07209344953298569\n",
      "Epoch [1/1] | Batch 3728 | loss: 0.004303703084588051\n",
      "Epoch [1/1] | Batch 3729 | loss: 0.050374288111925125\n",
      "Epoch [1/1] | Batch 3730 | loss: 0.0279843732714653\n",
      "Epoch [1/1] | Batch 3731 | loss: 0.03280356898903847\n",
      "Epoch [1/1] | Batch 3732 | loss: 0.00109076127409935\n",
      "Epoch [1/1] | Batch 3733 | loss: 0.15631529688835144\n",
      "Epoch [1/1] | Batch 3734 | loss: 0.13048836588859558\n",
      "Epoch [1/1] | Batch 3735 | loss: 0.36769163608551025\n",
      "Epoch [1/1] | Batch 3736 | loss: 0.0440368726849556\n",
      "Epoch [1/1] | Batch 3737 | loss: 0.08035611361265182\n",
      "Epoch [1/1] | Batch 3738 | loss: 0.051268015056848526\n",
      "Epoch [1/1] | Batch 3739 | loss: 0.7776078581809998\n",
      "Epoch [1/1] | Batch 3740 | loss: 0.6437838077545166\n",
      "Epoch [1/1] | Batch 3741 | loss: 0.07628215849399567\n",
      "Epoch [1/1] | Batch 3742 | loss: 0.2413100153207779\n",
      "Epoch [1/1] | Batch 3743 | loss: 0.056645575910806656\n",
      "Epoch [1/1] | Batch 3744 | loss: 0.29434898495674133\n",
      "Epoch [1/1] | Batch 3745 | loss: 0.000858721265103668\n",
      "Epoch [1/1] | Batch 3746 | loss: 0.07999980449676514\n",
      "Epoch [1/1] | Batch 3747 | loss: 0.21537910401821136\n",
      "Epoch [1/1] | Batch 3748 | loss: 0.1276383101940155\n",
      "Epoch [1/1] | Batch 3749 | loss: 0.3352293074131012\n",
      "Epoch [1/1] | Batch 3750 | loss: 0.11629968136548996\n"
     ]
    }
   ],
   "source": [
    "# This time, let's try with a smaller batch size!\n",
    "\n",
    "model = Net() # re-initialize net\n",
    "\n",
    "# re-define dataloader \n",
    "batch_size = 16\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Actual Training loop\n",
    "num_epochs = 1\n",
    "lr = 1e-1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "losses = [] # For plotting\n",
    "\n",
    "model.train() # Training mode\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "\n",
    "        # 1. Compute loss    \n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # 2. Magically compute gradient\n",
    "        grad = torch.autograd.grad(loss, model.parameters())\n",
    "\n",
    "        # 3. Perform optimization step\n",
    "        for param, g in zip(model.parameters(), grad):\n",
    "            param.grad = g\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Batch {batch_idx+1} | loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98a3f32340>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, smaller batch yields to a more noisy optimization process.\n",
    "This is due to high gradient variance!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's optimization API - *torch.optim*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For performing optimization with ease, PyTorch includes an optimization interface named torch.optim.\n",
    "* Supports numerous optimization algorithms!\n",
    "* We will demonstrate the API by replacing the above training procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] | Batch 1 | loss: 2.3790676593780518\n",
      "Epoch [1/1] | Batch 2 | loss: 2.1210269927978516\n",
      "Epoch [1/1] | Batch 3 | loss: 1.940892219543457\n",
      "Epoch [1/1] | Batch 4 | loss: 1.8131083250045776\n",
      "Epoch [1/1] | Batch 5 | loss: 1.8365105390548706\n",
      "Epoch [1/1] | Batch 6 | loss: 1.5536680221557617\n",
      "Epoch [1/1] | Batch 7 | loss: 1.6472917795181274\n",
      "Epoch [1/1] | Batch 8 | loss: 1.531247854232788\n",
      "Epoch [1/1] | Batch 9 | loss: 1.2547249794006348\n",
      "Epoch [1/1] | Batch 10 | loss: 1.353220820426941\n",
      "Epoch [1/1] | Batch 11 | loss: 1.1045948266983032\n",
      "Epoch [1/1] | Batch 12 | loss: 1.0411723852157593\n",
      "Epoch [1/1] | Batch 13 | loss: 0.9999468326568604\n",
      "Epoch [1/1] | Batch 14 | loss: 0.819925844669342\n",
      "Epoch [1/1] | Batch 15 | loss: 0.7688719630241394\n",
      "Epoch [1/1] | Batch 16 | loss: 0.7433228492736816\n",
      "Epoch [1/1] | Batch 17 | loss: 0.5915358662605286\n",
      "Epoch [1/1] | Batch 18 | loss: 0.8329132199287415\n",
      "Epoch [1/1] | Batch 19 | loss: 0.832962691783905\n",
      "Epoch [1/1] | Batch 20 | loss: 0.7877292037010193\n",
      "Epoch [1/1] | Batch 21 | loss: 0.5693066120147705\n",
      "Epoch [1/1] | Batch 22 | loss: 0.6105296611785889\n",
      "Epoch [1/1] | Batch 23 | loss: 0.6378671526908875\n",
      "Epoch [1/1] | Batch 24 | loss: 0.5783095955848694\n",
      "Epoch [1/1] | Batch 25 | loss: 0.6316388845443726\n",
      "Epoch [1/1] | Batch 26 | loss: 0.7033675909042358\n",
      "Epoch [1/1] | Batch 27 | loss: 0.7957795262336731\n",
      "Epoch [1/1] | Batch 28 | loss: 0.6816235184669495\n",
      "Epoch [1/1] | Batch 29 | loss: 0.6138924956321716\n",
      "Epoch [1/1] | Batch 30 | loss: 0.5231571197509766\n",
      "Epoch [1/1] | Batch 31 | loss: 0.4420876204967499\n",
      "Epoch [1/1] | Batch 32 | loss: 0.6449339389801025\n",
      "Epoch [1/1] | Batch 33 | loss: 0.538503110408783\n",
      "Epoch [1/1] | Batch 34 | loss: 0.4894322156906128\n",
      "Epoch [1/1] | Batch 35 | loss: 0.4713779389858246\n",
      "Epoch [1/1] | Batch 36 | loss: 0.6414483189582825\n",
      "Epoch [1/1] | Batch 37 | loss: 0.562211811542511\n",
      "Epoch [1/1] | Batch 38 | loss: 0.4210209250450134\n",
      "Epoch [1/1] | Batch 39 | loss: 0.48705264925956726\n",
      "Epoch [1/1] | Batch 40 | loss: 0.5684853792190552\n",
      "Epoch [1/1] | Batch 41 | loss: 0.4877517521381378\n",
      "Epoch [1/1] | Batch 42 | loss: 0.4909411370754242\n",
      "Epoch [1/1] | Batch 43 | loss: 0.7512537240982056\n",
      "Epoch [1/1] | Batch 44 | loss: 0.6230435371398926\n",
      "Epoch [1/1] | Batch 45 | loss: 0.3856610655784607\n",
      "Epoch [1/1] | Batch 46 | loss: 0.41640010476112366\n",
      "Epoch [1/1] | Batch 47 | loss: 0.6241115927696228\n",
      "Epoch [1/1] | Batch 48 | loss: 0.43358081579208374\n",
      "Epoch [1/1] | Batch 49 | loss: 0.4793809652328491\n",
      "Epoch [1/1] | Batch 50 | loss: 0.5518673658370972\n",
      "Epoch [1/1] | Batch 51 | loss: 0.349568635225296\n",
      "Epoch [1/1] | Batch 52 | loss: 0.6494529247283936\n",
      "Epoch [1/1] | Batch 53 | loss: 0.34546366333961487\n",
      "Epoch [1/1] | Batch 54 | loss: 0.4902801215648651\n",
      "Epoch [1/1] | Batch 55 | loss: 0.39637497067451477\n",
      "Epoch [1/1] | Batch 56 | loss: 0.37942469120025635\n",
      "Epoch [1/1] | Batch 57 | loss: 0.31243249773979187\n",
      "Epoch [1/1] | Batch 58 | loss: 0.468966007232666\n",
      "Epoch [1/1] | Batch 59 | loss: 0.5940530896186829\n",
      "Epoch [1/1] | Batch 60 | loss: 0.42894086241722107\n",
      "Epoch [1/1] | Batch 61 | loss: 0.35724833607673645\n",
      "Epoch [1/1] | Batch 62 | loss: 0.34488850831985474\n",
      "Epoch [1/1] | Batch 63 | loss: 0.500744640827179\n",
      "Epoch [1/1] | Batch 64 | loss: 0.4174988269805908\n",
      "Epoch [1/1] | Batch 65 | loss: 0.38399049639701843\n",
      "Epoch [1/1] | Batch 66 | loss: 0.5049076676368713\n",
      "Epoch [1/1] | Batch 67 | loss: 0.4027557671070099\n",
      "Epoch [1/1] | Batch 68 | loss: 0.38171616196632385\n",
      "Epoch [1/1] | Batch 69 | loss: 0.43589478731155396\n",
      "Epoch [1/1] | Batch 70 | loss: 0.42102187871932983\n",
      "Epoch [1/1] | Batch 71 | loss: 0.4426119923591614\n",
      "Epoch [1/1] | Batch 72 | loss: 0.3655778467655182\n",
      "Epoch [1/1] | Batch 73 | loss: 0.4479084312915802\n",
      "Epoch [1/1] | Batch 74 | loss: 0.4989163279533386\n",
      "Epoch [1/1] | Batch 75 | loss: 0.21131600439548492\n",
      "Epoch [1/1] | Batch 76 | loss: 0.2606370151042938\n",
      "Epoch [1/1] | Batch 77 | loss: 0.24350447952747345\n",
      "Epoch [1/1] | Batch 78 | loss: 0.531944990158081\n",
      "Epoch [1/1] | Batch 79 | loss: 0.3340666592121124\n",
      "Epoch [1/1] | Batch 80 | loss: 0.5776289105415344\n",
      "Epoch [1/1] | Batch 81 | loss: 0.25668036937713623\n",
      "Epoch [1/1] | Batch 82 | loss: 0.40114155411720276\n",
      "Epoch [1/1] | Batch 83 | loss: 0.24736444652080536\n",
      "Epoch [1/1] | Batch 84 | loss: 0.12944623827934265\n",
      "Epoch [1/1] | Batch 85 | loss: 0.19141675531864166\n",
      "Epoch [1/1] | Batch 86 | loss: 0.2846975028514862\n",
      "Epoch [1/1] | Batch 87 | loss: 0.37071821093559265\n",
      "Epoch [1/1] | Batch 88 | loss: 0.29630064964294434\n",
      "Epoch [1/1] | Batch 89 | loss: 0.32220956683158875\n",
      "Epoch [1/1] | Batch 90 | loss: 0.32384708523750305\n",
      "Epoch [1/1] | Batch 91 | loss: 0.22800344228744507\n",
      "Epoch [1/1] | Batch 92 | loss: 0.47643017768859863\n",
      "Epoch [1/1] | Batch 93 | loss: 0.4012835621833801\n",
      "Epoch [1/1] | Batch 94 | loss: 0.4517427086830139\n",
      "Epoch [1/1] | Batch 95 | loss: 0.35699933767318726\n",
      "Epoch [1/1] | Batch 96 | loss: 0.27914148569107056\n",
      "Epoch [1/1] | Batch 97 | loss: 0.2239447981119156\n",
      "Epoch [1/1] | Batch 98 | loss: 0.16458046436309814\n",
      "Epoch [1/1] | Batch 99 | loss: 0.15193961560726166\n",
      "Epoch [1/1] | Batch 100 | loss: 0.34192848205566406\n",
      "Epoch [1/1] | Batch 101 | loss: 0.36541080474853516\n",
      "Epoch [1/1] | Batch 102 | loss: 0.2806699275970459\n",
      "Epoch [1/1] | Batch 103 | loss: 0.49383416771888733\n",
      "Epoch [1/1] | Batch 104 | loss: 0.5313491821289062\n",
      "Epoch [1/1] | Batch 105 | loss: 0.2845012843608856\n",
      "Epoch [1/1] | Batch 106 | loss: 0.45771780610084534\n",
      "Epoch [1/1] | Batch 107 | loss: 0.2904300391674042\n",
      "Epoch [1/1] | Batch 108 | loss: 0.3526637554168701\n",
      "Epoch [1/1] | Batch 109 | loss: 0.4303109645843506\n",
      "Epoch [1/1] | Batch 110 | loss: 0.2039753496646881\n",
      "Epoch [1/1] | Batch 111 | loss: 0.41659510135650635\n",
      "Epoch [1/1] | Batch 112 | loss: 0.3913200795650482\n",
      "Epoch [1/1] | Batch 113 | loss: 0.36458730697631836\n",
      "Epoch [1/1] | Batch 114 | loss: 0.3879132866859436\n",
      "Epoch [1/1] | Batch 115 | loss: 0.38602104783058167\n",
      "Epoch [1/1] | Batch 116 | loss: 0.41939595341682434\n",
      "Epoch [1/1] | Batch 117 | loss: 0.537359356880188\n",
      "Epoch [1/1] | Batch 118 | loss: 0.4017682671546936\n",
      "Epoch [1/1] | Batch 119 | loss: 0.26637768745422363\n",
      "Epoch [1/1] | Batch 120 | loss: 0.26923850178718567\n",
      "Epoch [1/1] | Batch 121 | loss: 0.36880919337272644\n",
      "Epoch [1/1] | Batch 122 | loss: 0.39862746000289917\n",
      "Epoch [1/1] | Batch 123 | loss: 0.3065131604671478\n",
      "Epoch [1/1] | Batch 124 | loss: 0.655264675617218\n",
      "Epoch [1/1] | Batch 125 | loss: 0.5418389439582825\n",
      "Epoch [1/1] | Batch 126 | loss: 0.2660790979862213\n",
      "Epoch [1/1] | Batch 127 | loss: 0.27557381987571716\n",
      "Epoch [1/1] | Batch 128 | loss: 0.31048834323883057\n",
      "Epoch [1/1] | Batch 129 | loss: 0.27757731080055237\n",
      "Epoch [1/1] | Batch 130 | loss: 0.3940155506134033\n",
      "Epoch [1/1] | Batch 131 | loss: 0.31366604566574097\n",
      "Epoch [1/1] | Batch 132 | loss: 0.29776787757873535\n",
      "Epoch [1/1] | Batch 133 | loss: 0.23288629949092865\n",
      "Epoch [1/1] | Batch 134 | loss: 0.33240774273872375\n",
      "Epoch [1/1] | Batch 135 | loss: 0.23462019860744476\n",
      "Epoch [1/1] | Batch 136 | loss: 0.289823055267334\n",
      "Epoch [1/1] | Batch 137 | loss: 0.30890458822250366\n",
      "Epoch [1/1] | Batch 138 | loss: 0.3956716060638428\n",
      "Epoch [1/1] | Batch 139 | loss: 0.4170612394809723\n",
      "Epoch [1/1] | Batch 140 | loss: 0.24092763662338257\n",
      "Epoch [1/1] | Batch 141 | loss: 0.39106833934783936\n",
      "Epoch [1/1] | Batch 142 | loss: 0.26365992426872253\n",
      "Epoch [1/1] | Batch 143 | loss: 0.38018083572387695\n",
      "Epoch [1/1] | Batch 144 | loss: 0.20557495951652527\n",
      "Epoch [1/1] | Batch 145 | loss: 0.35082992911338806\n",
      "Epoch [1/1] | Batch 146 | loss: 0.23617707192897797\n",
      "Epoch [1/1] | Batch 147 | loss: 0.313367635011673\n",
      "Epoch [1/1] | Batch 148 | loss: 0.37429481744766235\n",
      "Epoch [1/1] | Batch 149 | loss: 0.2912827134132385\n",
      "Epoch [1/1] | Batch 150 | loss: 0.3852289617061615\n",
      "Epoch [1/1] | Batch 151 | loss: 0.21767652034759521\n",
      "Epoch [1/1] | Batch 152 | loss: 0.3418019115924835\n",
      "Epoch [1/1] | Batch 153 | loss: 0.41697898507118225\n",
      "Epoch [1/1] | Batch 154 | loss: 0.15042686462402344\n",
      "Epoch [1/1] | Batch 155 | loss: 0.20894983410835266\n",
      "Epoch [1/1] | Batch 156 | loss: 0.2136656492948532\n",
      "Epoch [1/1] | Batch 157 | loss: 0.33945581316947937\n",
      "Epoch [1/1] | Batch 158 | loss: 0.17447349429130554\n",
      "Epoch [1/1] | Batch 159 | loss: 0.3091554343700409\n",
      "Epoch [1/1] | Batch 160 | loss: 0.5261946320533752\n",
      "Epoch [1/1] | Batch 161 | loss: 0.29001492261886597\n",
      "Epoch [1/1] | Batch 162 | loss: 0.41519469022750854\n",
      "Epoch [1/1] | Batch 163 | loss: 0.42248305678367615\n",
      "Epoch [1/1] | Batch 164 | loss: 0.22593975067138672\n",
      "Epoch [1/1] | Batch 165 | loss: 0.457060843706131\n",
      "Epoch [1/1] | Batch 166 | loss: 0.22201567888259888\n",
      "Epoch [1/1] | Batch 167 | loss: 0.2898350954055786\n",
      "Epoch [1/1] | Batch 168 | loss: 0.25085148215293884\n",
      "Epoch [1/1] | Batch 169 | loss: 0.18323126435279846\n",
      "Epoch [1/1] | Batch 170 | loss: 0.4064427316188812\n",
      "Epoch [1/1] | Batch 171 | loss: 0.2964531183242798\n",
      "Epoch [1/1] | Batch 172 | loss: 0.2767738401889801\n",
      "Epoch [1/1] | Batch 173 | loss: 0.2795170843601227\n",
      "Epoch [1/1] | Batch 174 | loss: 0.3185018301010132\n",
      "Epoch [1/1] | Batch 175 | loss: 0.21133850514888763\n",
      "Epoch [1/1] | Batch 176 | loss: 0.21335826814174652\n",
      "Epoch [1/1] | Batch 177 | loss: 0.12555687129497528\n",
      "Epoch [1/1] | Batch 178 | loss: 0.2377777397632599\n",
      "Epoch [1/1] | Batch 179 | loss: 0.41176724433898926\n",
      "Epoch [1/1] | Batch 180 | loss: 0.4709671139717102\n",
      "Epoch [1/1] | Batch 181 | loss: 0.36871302127838135\n",
      "Epoch [1/1] | Batch 182 | loss: 0.36565735936164856\n",
      "Epoch [1/1] | Batch 183 | loss: 0.2890608310699463\n",
      "Epoch [1/1] | Batch 184 | loss: 0.3169410824775696\n",
      "Epoch [1/1] | Batch 185 | loss: 0.35270845890045166\n",
      "Epoch [1/1] | Batch 186 | loss: 0.2999560832977295\n",
      "Epoch [1/1] | Batch 187 | loss: 0.3110947906970978\n",
      "Epoch [1/1] | Batch 188 | loss: 0.2571132481098175\n",
      "Epoch [1/1] | Batch 189 | loss: 0.3315032422542572\n",
      "Epoch [1/1] | Batch 190 | loss: 0.18462418019771576\n",
      "Epoch [1/1] | Batch 191 | loss: 0.23638315498828888\n",
      "Epoch [1/1] | Batch 192 | loss: 0.5154170989990234\n",
      "Epoch [1/1] | Batch 193 | loss: 0.46802034974098206\n",
      "Epoch [1/1] | Batch 194 | loss: 0.3245543837547302\n",
      "Epoch [1/1] | Batch 195 | loss: 0.42440658807754517\n",
      "Epoch [1/1] | Batch 196 | loss: 0.22515146434307098\n",
      "Epoch [1/1] | Batch 197 | loss: 0.3677578270435333\n",
      "Epoch [1/1] | Batch 198 | loss: 0.2138213962316513\n",
      "Epoch [1/1] | Batch 199 | loss: 0.12912188470363617\n",
      "Epoch [1/1] | Batch 200 | loss: 0.4101868271827698\n",
      "Epoch [1/1] | Batch 201 | loss: 0.3235476315021515\n",
      "Epoch [1/1] | Batch 202 | loss: 0.2682259678840637\n",
      "Epoch [1/1] | Batch 203 | loss: 0.42827990651130676\n",
      "Epoch [1/1] | Batch 204 | loss: 0.17226849496364594\n",
      "Epoch [1/1] | Batch 205 | loss: 0.25726518034935\n",
      "Epoch [1/1] | Batch 206 | loss: 0.32798945903778076\n",
      "Epoch [1/1] | Batch 207 | loss: 0.23965105414390564\n",
      "Epoch [1/1] | Batch 208 | loss: 0.31511351466178894\n",
      "Epoch [1/1] | Batch 209 | loss: 0.3286939561367035\n",
      "Epoch [1/1] | Batch 210 | loss: 0.20578992366790771\n",
      "Epoch [1/1] | Batch 211 | loss: 0.2187393307685852\n",
      "Epoch [1/1] | Batch 212 | loss: 0.30097222328186035\n",
      "Epoch [1/1] | Batch 213 | loss: 0.26713618636131287\n",
      "Epoch [1/1] | Batch 214 | loss: 0.20917284488677979\n",
      "Epoch [1/1] | Batch 215 | loss: 0.272368848323822\n",
      "Epoch [1/1] | Batch 216 | loss: 0.20838859677314758\n",
      "Epoch [1/1] | Batch 217 | loss: 0.44230157136917114\n",
      "Epoch [1/1] | Batch 218 | loss: 0.2334892898797989\n",
      "Epoch [1/1] | Batch 219 | loss: 0.5678222179412842\n",
      "Epoch [1/1] | Batch 220 | loss: 0.2798328697681427\n",
      "Epoch [1/1] | Batch 221 | loss: 0.3656172454357147\n",
      "Epoch [1/1] | Batch 222 | loss: 0.30308693647384644\n",
      "Epoch [1/1] | Batch 223 | loss: 0.1748914122581482\n",
      "Epoch [1/1] | Batch 224 | loss: 0.4183803200721741\n",
      "Epoch [1/1] | Batch 225 | loss: 0.21814370155334473\n",
      "Epoch [1/1] | Batch 226 | loss: 0.44920632243156433\n",
      "Epoch [1/1] | Batch 227 | loss: 0.5038815140724182\n",
      "Epoch [1/1] | Batch 228 | loss: 0.32226690649986267\n",
      "Epoch [1/1] | Batch 229 | loss: 0.25337642431259155\n",
      "Epoch [1/1] | Batch 230 | loss: 0.16491754353046417\n",
      "Epoch [1/1] | Batch 231 | loss: 0.2616143226623535\n",
      "Epoch [1/1] | Batch 232 | loss: 0.13346202671527863\n",
      "Epoch [1/1] | Batch 233 | loss: 0.33933138847351074\n",
      "Epoch [1/1] | Batch 234 | loss: 0.40237942337989807\n",
      "Epoch [1/1] | Batch 235 | loss: 0.20649383962154388\n",
      "Epoch [1/1] | Batch 236 | loss: 0.34979382157325745\n",
      "Epoch [1/1] | Batch 237 | loss: 0.4753916561603546\n",
      "Epoch [1/1] | Batch 238 | loss: 0.22369232773780823\n",
      "Epoch [1/1] | Batch 239 | loss: 0.3071039021015167\n",
      "Epoch [1/1] | Batch 240 | loss: 0.18296055495738983\n",
      "Epoch [1/1] | Batch 241 | loss: 0.4051378667354584\n",
      "Epoch [1/1] | Batch 242 | loss: 0.306564062833786\n",
      "Epoch [1/1] | Batch 243 | loss: 0.5381035208702087\n",
      "Epoch [1/1] | Batch 244 | loss: 0.1379471868276596\n",
      "Epoch [1/1] | Batch 245 | loss: 0.22642654180526733\n",
      "Epoch [1/1] | Batch 246 | loss: 0.21505795419216156\n",
      "Epoch [1/1] | Batch 247 | loss: 0.22710883617401123\n",
      "Epoch [1/1] | Batch 248 | loss: 0.2203000783920288\n",
      "Epoch [1/1] | Batch 249 | loss: 0.3359244167804718\n",
      "Epoch [1/1] | Batch 250 | loss: 0.18423286080360413\n",
      "Epoch [1/1] | Batch 251 | loss: 0.39390504360198975\n",
      "Epoch [1/1] | Batch 252 | loss: 0.19061386585235596\n",
      "Epoch [1/1] | Batch 253 | loss: 0.2042175829410553\n",
      "Epoch [1/1] | Batch 254 | loss: 0.2063170224428177\n",
      "Epoch [1/1] | Batch 255 | loss: 0.31769028306007385\n",
      "Epoch [1/1] | Batch 256 | loss: 0.18487735092639923\n",
      "Epoch [1/1] | Batch 257 | loss: 0.16402482986450195\n",
      "Epoch [1/1] | Batch 258 | loss: 0.18732981383800507\n",
      "Epoch [1/1] | Batch 259 | loss: 0.23720815777778625\n",
      "Epoch [1/1] | Batch 260 | loss: 0.22649918496608734\n",
      "Epoch [1/1] | Batch 261 | loss: 0.22991445660591125\n",
      "Epoch [1/1] | Batch 262 | loss: 0.3090740144252777\n",
      "Epoch [1/1] | Batch 263 | loss: 0.3024013042449951\n",
      "Epoch [1/1] | Batch 264 | loss: 0.15739589929580688\n",
      "Epoch [1/1] | Batch 265 | loss: 0.12652531266212463\n",
      "Epoch [1/1] | Batch 266 | loss: 0.2828719913959503\n",
      "Epoch [1/1] | Batch 267 | loss: 0.30869558453559875\n",
      "Epoch [1/1] | Batch 268 | loss: 0.3252047598361969\n",
      "Epoch [1/1] | Batch 269 | loss: 0.3150853216648102\n",
      "Epoch [1/1] | Batch 270 | loss: 0.10630017518997192\n",
      "Epoch [1/1] | Batch 271 | loss: 0.23347510397434235\n",
      "Epoch [1/1] | Batch 272 | loss: 0.24633467197418213\n",
      "Epoch [1/1] | Batch 273 | loss: 0.23105488717556\n",
      "Epoch [1/1] | Batch 274 | loss: 0.22765320539474487\n",
      "Epoch [1/1] | Batch 275 | loss: 0.2581304609775543\n",
      "Epoch [1/1] | Batch 276 | loss: 0.23106613755226135\n",
      "Epoch [1/1] | Batch 277 | loss: 0.14663493633270264\n",
      "Epoch [1/1] | Batch 278 | loss: 0.6299654245376587\n",
      "Epoch [1/1] | Batch 279 | loss: 0.22666814923286438\n",
      "Epoch [1/1] | Batch 280 | loss: 0.29538241028785706\n",
      "Epoch [1/1] | Batch 281 | loss: 0.25282567739486694\n",
      "Epoch [1/1] | Batch 282 | loss: 0.22597993910312653\n",
      "Epoch [1/1] | Batch 283 | loss: 0.2372569888830185\n",
      "Epoch [1/1] | Batch 284 | loss: 0.13569827377796173\n",
      "Epoch [1/1] | Batch 285 | loss: 0.09437412023544312\n",
      "Epoch [1/1] | Batch 286 | loss: 0.23702722787857056\n",
      "Epoch [1/1] | Batch 287 | loss: 0.16495567560195923\n",
      "Epoch [1/1] | Batch 288 | loss: 0.4476687014102936\n",
      "Epoch [1/1] | Batch 289 | loss: 0.25984203815460205\n",
      "Epoch [1/1] | Batch 290 | loss: 0.1730765998363495\n",
      "Epoch [1/1] | Batch 291 | loss: 0.3663427233695984\n",
      "Epoch [1/1] | Batch 292 | loss: 0.2993069887161255\n",
      "Epoch [1/1] | Batch 293 | loss: 0.2024741768836975\n",
      "Epoch [1/1] | Batch 294 | loss: 0.42667824029922485\n",
      "Epoch [1/1] | Batch 295 | loss: 0.394543319940567\n",
      "Epoch [1/1] | Batch 296 | loss: 0.3340555727481842\n",
      "Epoch [1/1] | Batch 297 | loss: 0.30599164962768555\n",
      "Epoch [1/1] | Batch 298 | loss: 0.21365782618522644\n",
      "Epoch [1/1] | Batch 299 | loss: 0.2375660538673401\n",
      "Epoch [1/1] | Batch 300 | loss: 0.4827539324760437\n",
      "Epoch [1/1] | Batch 301 | loss: 0.15531402826309204\n",
      "Epoch [1/1] | Batch 302 | loss: 0.23154792189598083\n",
      "Epoch [1/1] | Batch 303 | loss: 0.411857008934021\n",
      "Epoch [1/1] | Batch 304 | loss: 0.18010562658309937\n",
      "Epoch [1/1] | Batch 305 | loss: 0.2966373860836029\n",
      "Epoch [1/1] | Batch 306 | loss: 0.2780306041240692\n",
      "Epoch [1/1] | Batch 307 | loss: 0.1480560451745987\n",
      "Epoch [1/1] | Batch 308 | loss: 0.1569291651248932\n",
      "Epoch [1/1] | Batch 309 | loss: 0.4773205816745758\n",
      "Epoch [1/1] | Batch 310 | loss: 0.18202243745326996\n",
      "Epoch [1/1] | Batch 311 | loss: 0.19828739762306213\n",
      "Epoch [1/1] | Batch 312 | loss: 0.24663187563419342\n",
      "Epoch [1/1] | Batch 313 | loss: 0.3032011389732361\n",
      "Epoch [1/1] | Batch 314 | loss: 0.37921497225761414\n",
      "Epoch [1/1] | Batch 315 | loss: 0.16443771123886108\n",
      "Epoch [1/1] | Batch 316 | loss: 0.10773149877786636\n",
      "Epoch [1/1] | Batch 317 | loss: 0.2601317763328552\n",
      "Epoch [1/1] | Batch 318 | loss: 0.13596341013908386\n",
      "Epoch [1/1] | Batch 319 | loss: 0.29089829325675964\n",
      "Epoch [1/1] | Batch 320 | loss: 0.1287325769662857\n",
      "Epoch [1/1] | Batch 321 | loss: 0.15993312001228333\n",
      "Epoch [1/1] | Batch 322 | loss: 0.3672502636909485\n",
      "Epoch [1/1] | Batch 323 | loss: 0.20047643780708313\n",
      "Epoch [1/1] | Batch 324 | loss: 0.26257339119911194\n",
      "Epoch [1/1] | Batch 325 | loss: 0.2890019714832306\n",
      "Epoch [1/1] | Batch 326 | loss: 0.26672959327697754\n",
      "Epoch [1/1] | Batch 327 | loss: 0.37868398427963257\n",
      "Epoch [1/1] | Batch 328 | loss: 0.1415865123271942\n",
      "Epoch [1/1] | Batch 329 | loss: 0.2989424467086792\n",
      "Epoch [1/1] | Batch 330 | loss: 0.1808600276708603\n",
      "Epoch [1/1] | Batch 331 | loss: 0.2975921630859375\n",
      "Epoch [1/1] | Batch 332 | loss: 0.15317517518997192\n",
      "Epoch [1/1] | Batch 333 | loss: 0.41122597455978394\n",
      "Epoch [1/1] | Batch 334 | loss: 0.40733134746551514\n",
      "Epoch [1/1] | Batch 335 | loss: 0.18606270849704742\n",
      "Epoch [1/1] | Batch 336 | loss: 0.09331662952899933\n",
      "Epoch [1/1] | Batch 337 | loss: 0.23538874089717865\n",
      "Epoch [1/1] | Batch 338 | loss: 0.28062885999679565\n",
      "Epoch [1/1] | Batch 339 | loss: 0.18683777749538422\n",
      "Epoch [1/1] | Batch 340 | loss: 0.205372616648674\n",
      "Epoch [1/1] | Batch 341 | loss: 0.39692631363868713\n",
      "Epoch [1/1] | Batch 342 | loss: 0.1470034420490265\n",
      "Epoch [1/1] | Batch 343 | loss: 0.2625884413719177\n",
      "Epoch [1/1] | Batch 344 | loss: 0.2953479290008545\n",
      "Epoch [1/1] | Batch 345 | loss: 0.3658088743686676\n",
      "Epoch [1/1] | Batch 346 | loss: 0.36467495560646057\n",
      "Epoch [1/1] | Batch 347 | loss: 0.39055705070495605\n",
      "Epoch [1/1] | Batch 348 | loss: 0.2503799498081207\n",
      "Epoch [1/1] | Batch 349 | loss: 0.24352025985717773\n",
      "Epoch [1/1] | Batch 350 | loss: 0.3137289881706238\n",
      "Epoch [1/1] | Batch 351 | loss: 0.10200852155685425\n",
      "Epoch [1/1] | Batch 352 | loss: 0.18293574452400208\n",
      "Epoch [1/1] | Batch 353 | loss: 0.14091189205646515\n",
      "Epoch [1/1] | Batch 354 | loss: 0.2549563944339752\n",
      "Epoch [1/1] | Batch 355 | loss: 0.22419393062591553\n",
      "Epoch [1/1] | Batch 356 | loss: 0.1772395819425583\n",
      "Epoch [1/1] | Batch 357 | loss: 0.32539281249046326\n",
      "Epoch [1/1] | Batch 358 | loss: 0.1254972219467163\n",
      "Epoch [1/1] | Batch 359 | loss: 0.3900679349899292\n",
      "Epoch [1/1] | Batch 360 | loss: 0.2375897467136383\n",
      "Epoch [1/1] | Batch 361 | loss: 0.2562936544418335\n",
      "Epoch [1/1] | Batch 362 | loss: 0.20137295126914978\n",
      "Epoch [1/1] | Batch 363 | loss: 0.12906695902347565\n",
      "Epoch [1/1] | Batch 364 | loss: 0.16914622485637665\n",
      "Epoch [1/1] | Batch 365 | loss: 0.16609685122966766\n",
      "Epoch [1/1] | Batch 366 | loss: 0.13412784039974213\n",
      "Epoch [1/1] | Batch 367 | loss: 0.4499695897102356\n",
      "Epoch [1/1] | Batch 368 | loss: 0.32122474908828735\n",
      "Epoch [1/1] | Batch 369 | loss: 0.22274674475193024\n",
      "Epoch [1/1] | Batch 370 | loss: 0.09853042662143707\n",
      "Epoch [1/1] | Batch 371 | loss: 0.22487637400627136\n",
      "Epoch [1/1] | Batch 372 | loss: 0.15732601284980774\n",
      "Epoch [1/1] | Batch 373 | loss: 0.08702174574136734\n",
      "Epoch [1/1] | Batch 374 | loss: 0.1216888427734375\n",
      "Epoch [1/1] | Batch 375 | loss: 0.26259320974349976\n",
      "Epoch [1/1] | Batch 376 | loss: 0.33731919527053833\n",
      "Epoch [1/1] | Batch 377 | loss: 0.25466057658195496\n",
      "Epoch [1/1] | Batch 378 | loss: 0.20705023407936096\n",
      "Epoch [1/1] | Batch 379 | loss: 0.22405081987380981\n",
      "Epoch [1/1] | Batch 380 | loss: 0.2880572974681854\n",
      "Epoch [1/1] | Batch 381 | loss: 0.28270137310028076\n",
      "Epoch [1/1] | Batch 382 | loss: 0.1912006139755249\n",
      "Epoch [1/1] | Batch 383 | loss: 0.22106289863586426\n",
      "Epoch [1/1] | Batch 384 | loss: 0.16911689937114716\n",
      "Epoch [1/1] | Batch 385 | loss: 0.13073861598968506\n",
      "Epoch [1/1] | Batch 386 | loss: 0.09526336193084717\n",
      "Epoch [1/1] | Batch 387 | loss: 0.2462937980890274\n",
      "Epoch [1/1] | Batch 388 | loss: 0.2479221671819687\n",
      "Epoch [1/1] | Batch 389 | loss: 0.1428750604391098\n",
      "Epoch [1/1] | Batch 390 | loss: 0.31392136216163635\n",
      "Epoch [1/1] | Batch 391 | loss: 0.2878319323062897\n",
      "Epoch [1/1] | Batch 392 | loss: 0.299324095249176\n",
      "Epoch [1/1] | Batch 393 | loss: 0.12547683715820312\n",
      "Epoch [1/1] | Batch 394 | loss: 0.2539460062980652\n",
      "Epoch [1/1] | Batch 395 | loss: 0.3065963685512543\n",
      "Epoch [1/1] | Batch 396 | loss: 0.44046738743782043\n",
      "Epoch [1/1] | Batch 397 | loss: 0.14351262152194977\n",
      "Epoch [1/1] | Batch 398 | loss: 0.2268538922071457\n",
      "Epoch [1/1] | Batch 399 | loss: 0.21634221076965332\n",
      "Epoch [1/1] | Batch 400 | loss: 0.18294860422611237\n",
      "Epoch [1/1] | Batch 401 | loss: 0.15595643222332\n",
      "Epoch [1/1] | Batch 402 | loss: 0.21155798435211182\n",
      "Epoch [1/1] | Batch 403 | loss: 0.06991150230169296\n",
      "Epoch [1/1] | Batch 404 | loss: 0.3104383051395416\n",
      "Epoch [1/1] | Batch 405 | loss: 0.2642066180706024\n",
      "Epoch [1/1] | Batch 406 | loss: 0.1680331528186798\n",
      "Epoch [1/1] | Batch 407 | loss: 0.2476288229227066\n",
      "Epoch [1/1] | Batch 408 | loss: 0.1512405127286911\n",
      "Epoch [1/1] | Batch 409 | loss: 0.16515332460403442\n",
      "Epoch [1/1] | Batch 410 | loss: 0.20068474113941193\n",
      "Epoch [1/1] | Batch 411 | loss: 0.13872770965099335\n",
      "Epoch [1/1] | Batch 412 | loss: 0.16549138724803925\n",
      "Epoch [1/1] | Batch 413 | loss: 0.1933860331773758\n",
      "Epoch [1/1] | Batch 414 | loss: 0.3212469816207886\n",
      "Epoch [1/1] | Batch 415 | loss: 0.20648357272148132\n",
      "Epoch [1/1] | Batch 416 | loss: 0.3136802017688751\n",
      "Epoch [1/1] | Batch 417 | loss: 0.2792641520500183\n",
      "Epoch [1/1] | Batch 418 | loss: 0.17886315286159515\n",
      "Epoch [1/1] | Batch 419 | loss: 0.1414189338684082\n",
      "Epoch [1/1] | Batch 420 | loss: 0.23081475496292114\n",
      "Epoch [1/1] | Batch 421 | loss: 0.37010809779167175\n",
      "Epoch [1/1] | Batch 422 | loss: 0.30556535720825195\n",
      "Epoch [1/1] | Batch 423 | loss: 0.3869260549545288\n",
      "Epoch [1/1] | Batch 424 | loss: 0.211715966463089\n",
      "Epoch [1/1] | Batch 425 | loss: 0.29168426990509033\n",
      "Epoch [1/1] | Batch 426 | loss: 0.47123342752456665\n",
      "Epoch [1/1] | Batch 427 | loss: 0.18875569105148315\n",
      "Epoch [1/1] | Batch 428 | loss: 0.23086746037006378\n",
      "Epoch [1/1] | Batch 429 | loss: 0.23342111706733704\n",
      "Epoch [1/1] | Batch 430 | loss: 0.16185344755649567\n",
      "Epoch [1/1] | Batch 431 | loss: 0.17138224840164185\n",
      "Epoch [1/1] | Batch 432 | loss: 0.21173448860645294\n",
      "Epoch [1/1] | Batch 433 | loss: 0.2892758846282959\n",
      "Epoch [1/1] | Batch 434 | loss: 0.12803630530834198\n",
      "Epoch [1/1] | Batch 435 | loss: 0.10431082546710968\n",
      "Epoch [1/1] | Batch 436 | loss: 0.20291534066200256\n",
      "Epoch [1/1] | Batch 437 | loss: 0.23144370317459106\n",
      "Epoch [1/1] | Batch 438 | loss: 0.17416830360889435\n",
      "Epoch [1/1] | Batch 439 | loss: 0.15311865508556366\n",
      "Epoch [1/1] | Batch 440 | loss: 0.11713248491287231\n",
      "Epoch [1/1] | Batch 441 | loss: 0.19855333864688873\n",
      "Epoch [1/1] | Batch 442 | loss: 0.09653393179178238\n",
      "Epoch [1/1] | Batch 443 | loss: 0.2637522220611572\n",
      "Epoch [1/1] | Batch 444 | loss: 0.2958298921585083\n",
      "Epoch [1/1] | Batch 445 | loss: 0.21396733820438385\n",
      "Epoch [1/1] | Batch 446 | loss: 0.31345218420028687\n",
      "Epoch [1/1] | Batch 447 | loss: 0.2908865213394165\n",
      "Epoch [1/1] | Batch 448 | loss: 0.23445075750350952\n",
      "Epoch [1/1] | Batch 449 | loss: 0.30938994884490967\n",
      "Epoch [1/1] | Batch 450 | loss: 0.13850189745426178\n",
      "Epoch [1/1] | Batch 451 | loss: 0.24596387147903442\n",
      "Epoch [1/1] | Batch 452 | loss: 0.24174027144908905\n",
      "Epoch [1/1] | Batch 453 | loss: 0.26876989006996155\n",
      "Epoch [1/1] | Batch 454 | loss: 0.19588619470596313\n",
      "Epoch [1/1] | Batch 455 | loss: 0.18828918039798737\n",
      "Epoch [1/1] | Batch 456 | loss: 0.12636716663837433\n",
      "Epoch [1/1] | Batch 457 | loss: 0.31502124667167664\n",
      "Epoch [1/1] | Batch 458 | loss: 0.25468316674232483\n",
      "Epoch [1/1] | Batch 459 | loss: 0.10109435766935349\n",
      "Epoch [1/1] | Batch 460 | loss: 0.16107845306396484\n",
      "Epoch [1/1] | Batch 461 | loss: 0.08707769960165024\n",
      "Epoch [1/1] | Batch 462 | loss: 0.2674829065799713\n",
      "Epoch [1/1] | Batch 463 | loss: 0.22240422666072845\n",
      "Epoch [1/1] | Batch 464 | loss: 0.25920191407203674\n",
      "Epoch [1/1] | Batch 465 | loss: 0.137110635638237\n",
      "Epoch [1/1] | Batch 466 | loss: 0.18205074965953827\n",
      "Epoch [1/1] | Batch 467 | loss: 0.1833478808403015\n",
      "Epoch [1/1] | Batch 468 | loss: 0.20751337707042694\n",
      "Epoch [1/1] | Batch 469 | loss: 0.1702544093132019\n",
      "Epoch [1/1] | Batch 470 | loss: 0.2701520621776581\n",
      "Epoch [1/1] | Batch 471 | loss: 0.19321122765541077\n",
      "Epoch [1/1] | Batch 472 | loss: 0.19376739859580994\n",
      "Epoch [1/1] | Batch 473 | loss: 0.34903112053871155\n",
      "Epoch [1/1] | Batch 474 | loss: 0.13304364681243896\n",
      "Epoch [1/1] | Batch 475 | loss: 0.2418505996465683\n",
      "Epoch [1/1] | Batch 476 | loss: 0.2140885591506958\n",
      "Epoch [1/1] | Batch 477 | loss: 0.29016244411468506\n",
      "Epoch [1/1] | Batch 478 | loss: 0.05915691703557968\n",
      "Epoch [1/1] | Batch 479 | loss: 0.14810095727443695\n",
      "Epoch [1/1] | Batch 480 | loss: 0.19936683773994446\n",
      "Epoch [1/1] | Batch 481 | loss: 0.08243440836668015\n",
      "Epoch [1/1] | Batch 482 | loss: 0.23328787088394165\n",
      "Epoch [1/1] | Batch 483 | loss: 0.376148521900177\n",
      "Epoch [1/1] | Batch 484 | loss: 0.16780801117420197\n",
      "Epoch [1/1] | Batch 485 | loss: 0.3875615894794464\n",
      "Epoch [1/1] | Batch 486 | loss: 0.23278415203094482\n",
      "Epoch [1/1] | Batch 487 | loss: 0.2049385905265808\n",
      "Epoch [1/1] | Batch 488 | loss: 0.10020986199378967\n",
      "Epoch [1/1] | Batch 489 | loss: 0.20119810104370117\n",
      "Epoch [1/1] | Batch 490 | loss: 0.2244727909564972\n",
      "Epoch [1/1] | Batch 491 | loss: 0.1741277575492859\n",
      "Epoch [1/1] | Batch 492 | loss: 0.1654270738363266\n",
      "Epoch [1/1] | Batch 493 | loss: 0.19055292010307312\n",
      "Epoch [1/1] | Batch 494 | loss: 0.1547943651676178\n",
      "Epoch [1/1] | Batch 495 | loss: 0.16976392269134521\n",
      "Epoch [1/1] | Batch 496 | loss: 0.09303901344537735\n",
      "Epoch [1/1] | Batch 497 | loss: 0.23697614669799805\n",
      "Epoch [1/1] | Batch 498 | loss: 0.19933061301708221\n",
      "Epoch [1/1] | Batch 499 | loss: 0.27268075942993164\n",
      "Epoch [1/1] | Batch 500 | loss: 0.214816153049469\n",
      "Epoch [1/1] | Batch 501 | loss: 0.18297621607780457\n",
      "Epoch [1/1] | Batch 502 | loss: 0.27935662865638733\n",
      "Epoch [1/1] | Batch 503 | loss: 0.1764741986989975\n",
      "Epoch [1/1] | Batch 504 | loss: 0.31903156638145447\n",
      "Epoch [1/1] | Batch 505 | loss: 0.1777939349412918\n",
      "Epoch [1/1] | Batch 506 | loss: 0.16050201654434204\n",
      "Epoch [1/1] | Batch 507 | loss: 0.08991770446300507\n",
      "Epoch [1/1] | Batch 508 | loss: 0.2144736647605896\n",
      "Epoch [1/1] | Batch 509 | loss: 0.25186923146247864\n",
      "Epoch [1/1] | Batch 510 | loss: 0.20167145133018494\n",
      "Epoch [1/1] | Batch 511 | loss: 0.10209835320711136\n",
      "Epoch [1/1] | Batch 512 | loss: 0.4146493971347809\n",
      "Epoch [1/1] | Batch 513 | loss: 0.21487882733345032\n",
      "Epoch [1/1] | Batch 514 | loss: 0.23816217482089996\n",
      "Epoch [1/1] | Batch 515 | loss: 0.16852326691150665\n",
      "Epoch [1/1] | Batch 516 | loss: 0.17250177264213562\n",
      "Epoch [1/1] | Batch 517 | loss: 0.19471722841262817\n",
      "Epoch [1/1] | Batch 518 | loss: 0.12552054226398468\n",
      "Epoch [1/1] | Batch 519 | loss: 0.1434948593378067\n",
      "Epoch [1/1] | Batch 520 | loss: 0.354508638381958\n",
      "Epoch [1/1] | Batch 521 | loss: 0.25003018975257874\n",
      "Epoch [1/1] | Batch 522 | loss: 0.2557627856731415\n",
      "Epoch [1/1] | Batch 523 | loss: 0.3535693883895874\n",
      "Epoch [1/1] | Batch 524 | loss: 0.16417676210403442\n",
      "Epoch [1/1] | Batch 525 | loss: 0.15691229701042175\n",
      "Epoch [1/1] | Batch 526 | loss: 0.18608251214027405\n",
      "Epoch [1/1] | Batch 527 | loss: 0.2546655535697937\n",
      "Epoch [1/1] | Batch 528 | loss: 0.21878774464130402\n",
      "Epoch [1/1] | Batch 529 | loss: 0.2031211405992508\n",
      "Epoch [1/1] | Batch 530 | loss: 0.31272798776626587\n",
      "Epoch [1/1] | Batch 531 | loss: 0.29406601190567017\n",
      "Epoch [1/1] | Batch 532 | loss: 0.08643534034490585\n",
      "Epoch [1/1] | Batch 533 | loss: 0.22754721343517303\n",
      "Epoch [1/1] | Batch 534 | loss: 0.12089893221855164\n",
      "Epoch [1/1] | Batch 535 | loss: 0.12353923171758652\n",
      "Epoch [1/1] | Batch 536 | loss: 0.14168871939182281\n",
      "Epoch [1/1] | Batch 537 | loss: 0.1940157413482666\n",
      "Epoch [1/1] | Batch 538 | loss: 0.0996243804693222\n",
      "Epoch [1/1] | Batch 539 | loss: 0.21489068865776062\n",
      "Epoch [1/1] | Batch 540 | loss: 0.13253197073936462\n",
      "Epoch [1/1] | Batch 541 | loss: 0.26252344250679016\n",
      "Epoch [1/1] | Batch 542 | loss: 0.19410446286201477\n",
      "Epoch [1/1] | Batch 543 | loss: 0.12240087985992432\n",
      "Epoch [1/1] | Batch 544 | loss: 0.2817299962043762\n",
      "Epoch [1/1] | Batch 545 | loss: 0.1848737597465515\n",
      "Epoch [1/1] | Batch 546 | loss: 0.24737906455993652\n",
      "Epoch [1/1] | Batch 547 | loss: 0.23556199669837952\n",
      "Epoch [1/1] | Batch 548 | loss: 0.2760849595069885\n",
      "Epoch [1/1] | Batch 549 | loss: 0.15512703359127045\n",
      "Epoch [1/1] | Batch 550 | loss: 0.1625531017780304\n",
      "Epoch [1/1] | Batch 551 | loss: 0.18081095814704895\n",
      "Epoch [1/1] | Batch 552 | loss: 0.10907967388629913\n",
      "Epoch [1/1] | Batch 553 | loss: 0.21736390888690948\n",
      "Epoch [1/1] | Batch 554 | loss: 0.10851903259754181\n",
      "Epoch [1/1] | Batch 555 | loss: 0.12384830415248871\n",
      "Epoch [1/1] | Batch 556 | loss: 0.17241206765174866\n",
      "Epoch [1/1] | Batch 557 | loss: 0.25258246064186096\n",
      "Epoch [1/1] | Batch 558 | loss: 0.1767272800207138\n",
      "Epoch [1/1] | Batch 559 | loss: 0.082892507314682\n",
      "Epoch [1/1] | Batch 560 | loss: 0.1395529955625534\n",
      "Epoch [1/1] | Batch 561 | loss: 0.09631968289613724\n",
      "Epoch [1/1] | Batch 562 | loss: 0.3397349715232849\n",
      "Epoch [1/1] | Batch 563 | loss: 0.11530490964651108\n",
      "Epoch [1/1] | Batch 564 | loss: 0.15754084289073944\n",
      "Epoch [1/1] | Batch 565 | loss: 0.13146404922008514\n",
      "Epoch [1/1] | Batch 566 | loss: 0.1163608580827713\n",
      "Epoch [1/1] | Batch 567 | loss: 0.13290756940841675\n",
      "Epoch [1/1] | Batch 568 | loss: 0.28083211183547974\n",
      "Epoch [1/1] | Batch 569 | loss: 0.14120697975158691\n",
      "Epoch [1/1] | Batch 570 | loss: 0.27724069356918335\n",
      "Epoch [1/1] | Batch 571 | loss: 0.1373458206653595\n",
      "Epoch [1/1] | Batch 572 | loss: 0.19129957258701324\n",
      "Epoch [1/1] | Batch 573 | loss: 0.06465059518814087\n",
      "Epoch [1/1] | Batch 574 | loss: 0.10236787050962448\n",
      "Epoch [1/1] | Batch 575 | loss: 0.24212005734443665\n",
      "Epoch [1/1] | Batch 576 | loss: 0.1574385017156601\n",
      "Epoch [1/1] | Batch 577 | loss: 0.055341508239507675\n",
      "Epoch [1/1] | Batch 578 | loss: 0.19772076606750488\n",
      "Epoch [1/1] | Batch 579 | loss: 0.30698323249816895\n",
      "Epoch [1/1] | Batch 580 | loss: 0.310107558965683\n",
      "Epoch [1/1] | Batch 581 | loss: 0.11062474548816681\n",
      "Epoch [1/1] | Batch 582 | loss: 0.13428854942321777\n",
      "Epoch [1/1] | Batch 583 | loss: 0.1706148236989975\n",
      "Epoch [1/1] | Batch 584 | loss: 0.13671566545963287\n",
      "Epoch [1/1] | Batch 585 | loss: 0.2333633005619049\n",
      "Epoch [1/1] | Batch 586 | loss: 0.2945716381072998\n",
      "Epoch [1/1] | Batch 587 | loss: 0.1023617759346962\n",
      "Epoch [1/1] | Batch 588 | loss: 0.30770689249038696\n",
      "Epoch [1/1] | Batch 589 | loss: 0.329708456993103\n",
      "Epoch [1/1] | Batch 590 | loss: 0.0846424326300621\n",
      "Epoch [1/1] | Batch 591 | loss: 0.06859710812568665\n",
      "Epoch [1/1] | Batch 592 | loss: 0.2911570072174072\n",
      "Epoch [1/1] | Batch 593 | loss: 0.4008382558822632\n",
      "Epoch [1/1] | Batch 594 | loss: 0.2012207806110382\n",
      "Epoch [1/1] | Batch 595 | loss: 0.10781778395175934\n",
      "Epoch [1/1] | Batch 596 | loss: 0.06060975790023804\n",
      "Epoch [1/1] | Batch 597 | loss: 0.05108090490102768\n",
      "Epoch [1/1] | Batch 598 | loss: 0.26572850346565247\n",
      "Epoch [1/1] | Batch 599 | loss: 0.20926426351070404\n",
      "Epoch [1/1] | Batch 600 | loss: 0.21511872112751007\n",
      "Epoch [1/1] | Batch 601 | loss: 0.26010364294052124\n",
      "Epoch [1/1] | Batch 602 | loss: 0.2978251576423645\n",
      "Epoch [1/1] | Batch 603 | loss: 0.12539750337600708\n",
      "Epoch [1/1] | Batch 604 | loss: 0.06422143429517746\n",
      "Epoch [1/1] | Batch 605 | loss: 0.29449841380119324\n",
      "Epoch [1/1] | Batch 606 | loss: 0.07483938336372375\n",
      "Epoch [1/1] | Batch 607 | loss: 0.22079993784427643\n",
      "Epoch [1/1] | Batch 608 | loss: 0.13277146220207214\n",
      "Epoch [1/1] | Batch 609 | loss: 0.28021812438964844\n",
      "Epoch [1/1] | Batch 610 | loss: 0.3022966980934143\n",
      "Epoch [1/1] | Batch 611 | loss: 0.19209368526935577\n",
      "Epoch [1/1] | Batch 612 | loss: 0.1754768192768097\n",
      "Epoch [1/1] | Batch 613 | loss: 0.24124030768871307\n",
      "Epoch [1/1] | Batch 614 | loss: 0.09030313789844513\n",
      "Epoch [1/1] | Batch 615 | loss: 0.22326518595218658\n",
      "Epoch [1/1] | Batch 616 | loss: 0.11526454240083694\n",
      "Epoch [1/1] | Batch 617 | loss: 0.13905096054077148\n",
      "Epoch [1/1] | Batch 618 | loss: 0.3405727744102478\n",
      "Epoch [1/1] | Batch 619 | loss: 0.2627418041229248\n",
      "Epoch [1/1] | Batch 620 | loss: 0.112342469394207\n",
      "Epoch [1/1] | Batch 621 | loss: 0.1456313580274582\n",
      "Epoch [1/1] | Batch 622 | loss: 0.1988396942615509\n",
      "Epoch [1/1] | Batch 623 | loss: 0.38638898730278015\n",
      "Epoch [1/1] | Batch 624 | loss: 0.14545781910419464\n",
      "Epoch [1/1] | Batch 625 | loss: 0.1431548148393631\n",
      "Epoch [1/1] | Batch 626 | loss: 0.12753547728061676\n",
      "Epoch [1/1] | Batch 627 | loss: 0.15131501853466034\n",
      "Epoch [1/1] | Batch 628 | loss: 0.12127851694822311\n",
      "Epoch [1/1] | Batch 629 | loss: 0.12774348258972168\n",
      "Epoch [1/1] | Batch 630 | loss: 0.16497093439102173\n",
      "Epoch [1/1] | Batch 631 | loss: 0.18435245752334595\n",
      "Epoch [1/1] | Batch 632 | loss: 0.22787989675998688\n",
      "Epoch [1/1] | Batch 633 | loss: 0.34868714213371277\n",
      "Epoch [1/1] | Batch 634 | loss: 0.08216606825590134\n",
      "Epoch [1/1] | Batch 635 | loss: 0.30904343724250793\n",
      "Epoch [1/1] | Batch 636 | loss: 0.10809817165136337\n",
      "Epoch [1/1] | Batch 637 | loss: 0.11924177408218384\n",
      "Epoch [1/1] | Batch 638 | loss: 0.10368016362190247\n",
      "Epoch [1/1] | Batch 639 | loss: 0.18291737139225006\n",
      "Epoch [1/1] | Batch 640 | loss: 0.12395750731229782\n",
      "Epoch [1/1] | Batch 641 | loss: 0.2314199060201645\n",
      "Epoch [1/1] | Batch 642 | loss: 0.17009006440639496\n",
      "Epoch [1/1] | Batch 643 | loss: 0.1149175763130188\n",
      "Epoch [1/1] | Batch 644 | loss: 0.17479006946086884\n",
      "Epoch [1/1] | Batch 645 | loss: 0.2340848445892334\n",
      "Epoch [1/1] | Batch 646 | loss: 0.14724712073802948\n",
      "Epoch [1/1] | Batch 647 | loss: 0.25103670358657837\n",
      "Epoch [1/1] | Batch 648 | loss: 0.15618905425071716\n",
      "Epoch [1/1] | Batch 649 | loss: 0.07316868752241135\n",
      "Epoch [1/1] | Batch 650 | loss: 0.1333535611629486\n",
      "Epoch [1/1] | Batch 651 | loss: 0.08265382051467896\n",
      "Epoch [1/1] | Batch 652 | loss: 0.18028351664543152\n",
      "Epoch [1/1] | Batch 653 | loss: 0.23940587043762207\n",
      "Epoch [1/1] | Batch 654 | loss: 0.10344614833593369\n",
      "Epoch [1/1] | Batch 655 | loss: 0.22593927383422852\n",
      "Epoch [1/1] | Batch 656 | loss: 0.1321781873703003\n",
      "Epoch [1/1] | Batch 657 | loss: 0.2383936196565628\n",
      "Epoch [1/1] | Batch 658 | loss: 0.10881230980157852\n",
      "Epoch [1/1] | Batch 659 | loss: 0.19289539754390717\n",
      "Epoch [1/1] | Batch 660 | loss: 0.24832242727279663\n",
      "Epoch [1/1] | Batch 661 | loss: 0.14629140496253967\n",
      "Epoch [1/1] | Batch 662 | loss: 0.1793757975101471\n",
      "Epoch [1/1] | Batch 663 | loss: 0.18025171756744385\n",
      "Epoch [1/1] | Batch 664 | loss: 0.22791077196598053\n",
      "Epoch [1/1] | Batch 665 | loss: 0.21385449171066284\n",
      "Epoch [1/1] | Batch 666 | loss: 0.13977545499801636\n",
      "Epoch [1/1] | Batch 667 | loss: 0.43049800395965576\n",
      "Epoch [1/1] | Batch 668 | loss: 0.23635512590408325\n",
      "Epoch [1/1] | Batch 669 | loss: 0.18169070780277252\n",
      "Epoch [1/1] | Batch 670 | loss: 0.1566561460494995\n",
      "Epoch [1/1] | Batch 671 | loss: 0.14163672924041748\n",
      "Epoch [1/1] | Batch 672 | loss: 0.1619010865688324\n",
      "Epoch [1/1] | Batch 673 | loss: 0.0993511900305748\n",
      "Epoch [1/1] | Batch 674 | loss: 0.22148430347442627\n",
      "Epoch [1/1] | Batch 675 | loss: 0.11065012216567993\n",
      "Epoch [1/1] | Batch 676 | loss: 0.29263490438461304\n",
      "Epoch [1/1] | Batch 677 | loss: 0.2546676695346832\n",
      "Epoch [1/1] | Batch 678 | loss: 0.12024000287055969\n",
      "Epoch [1/1] | Batch 679 | loss: 0.08326565474271774\n",
      "Epoch [1/1] | Batch 680 | loss: 0.18760733306407928\n",
      "Epoch [1/1] | Batch 681 | loss: 0.1603524535894394\n",
      "Epoch [1/1] | Batch 682 | loss: 0.15821927785873413\n",
      "Epoch [1/1] | Batch 683 | loss: 0.1068599671125412\n",
      "Epoch [1/1] | Batch 684 | loss: 0.1018204614520073\n",
      "Epoch [1/1] | Batch 685 | loss: 0.13129667937755585\n",
      "Epoch [1/1] | Batch 686 | loss: 0.22178895771503448\n",
      "Epoch [1/1] | Batch 687 | loss: 0.12723302841186523\n",
      "Epoch [1/1] | Batch 688 | loss: 0.150718092918396\n",
      "Epoch [1/1] | Batch 689 | loss: 0.10849739611148834\n",
      "Epoch [1/1] | Batch 690 | loss: 0.13345301151275635\n",
      "Epoch [1/1] | Batch 691 | loss: 0.16478703916072845\n",
      "Epoch [1/1] | Batch 692 | loss: 0.25665730237960815\n",
      "Epoch [1/1] | Batch 693 | loss: 0.11097582429647446\n",
      "Epoch [1/1] | Batch 694 | loss: 0.24746957421302795\n",
      "Epoch [1/1] | Batch 695 | loss: 0.10386613756418228\n",
      "Epoch [1/1] | Batch 696 | loss: 0.28891345858573914\n",
      "Epoch [1/1] | Batch 697 | loss: 0.12161551415920258\n",
      "Epoch [1/1] | Batch 698 | loss: 0.14611107110977173\n",
      "Epoch [1/1] | Batch 699 | loss: 0.154866024851799\n",
      "Epoch [1/1] | Batch 700 | loss: 0.1445460319519043\n",
      "Epoch [1/1] | Batch 701 | loss: 0.15948589146137238\n",
      "Epoch [1/1] | Batch 702 | loss: 0.11331763863563538\n",
      "Epoch [1/1] | Batch 703 | loss: 0.24063940346240997\n",
      "Epoch [1/1] | Batch 704 | loss: 0.17921647429466248\n",
      "Epoch [1/1] | Batch 705 | loss: 0.05441036820411682\n",
      "Epoch [1/1] | Batch 706 | loss: 0.22768470644950867\n",
      "Epoch [1/1] | Batch 707 | loss: 0.19283626973628998\n",
      "Epoch [1/1] | Batch 708 | loss: 0.15429309010505676\n",
      "Epoch [1/1] | Batch 709 | loss: 0.13394255936145782\n",
      "Epoch [1/1] | Batch 710 | loss: 0.1277274787425995\n",
      "Epoch [1/1] | Batch 711 | loss: 0.22076116502285004\n",
      "Epoch [1/1] | Batch 712 | loss: 0.13119810819625854\n",
      "Epoch [1/1] | Batch 713 | loss: 0.16088512539863586\n",
      "Epoch [1/1] | Batch 714 | loss: 0.2609422504901886\n",
      "Epoch [1/1] | Batch 715 | loss: 0.1053474023938179\n",
      "Epoch [1/1] | Batch 716 | loss: 0.06282584369182587\n",
      "Epoch [1/1] | Batch 717 | loss: 0.2268470823764801\n",
      "Epoch [1/1] | Batch 718 | loss: 0.22995729744434357\n",
      "Epoch [1/1] | Batch 719 | loss: 0.18684715032577515\n",
      "Epoch [1/1] | Batch 720 | loss: 0.16553182899951935\n",
      "Epoch [1/1] | Batch 721 | loss: 0.16079765558242798\n",
      "Epoch [1/1] | Batch 722 | loss: 0.24502679705619812\n",
      "Epoch [1/1] | Batch 723 | loss: 0.0779271125793457\n",
      "Epoch [1/1] | Batch 724 | loss: 0.22090238332748413\n",
      "Epoch [1/1] | Batch 725 | loss: 0.11327231675386429\n",
      "Epoch [1/1] | Batch 726 | loss: 0.14563368260860443\n",
      "Epoch [1/1] | Batch 727 | loss: 0.17619669437408447\n",
      "Epoch [1/1] | Batch 728 | loss: 0.29137828946113586\n",
      "Epoch [1/1] | Batch 729 | loss: 0.10144282132387161\n",
      "Epoch [1/1] | Batch 730 | loss: 0.08849659562110901\n",
      "Epoch [1/1] | Batch 731 | loss: 0.19612230360507965\n",
      "Epoch [1/1] | Batch 732 | loss: 0.17620442807674408\n",
      "Epoch [1/1] | Batch 733 | loss: 0.13326522707939148\n",
      "Epoch [1/1] | Batch 734 | loss: 0.2023477554321289\n",
      "Epoch [1/1] | Batch 735 | loss: 0.16168873012065887\n",
      "Epoch [1/1] | Batch 736 | loss: 0.23874682188034058\n",
      "Epoch [1/1] | Batch 737 | loss: 0.1464008390903473\n",
      "Epoch [1/1] | Batch 738 | loss: 0.10503154247999191\n",
      "Epoch [1/1] | Batch 739 | loss: 0.17475612461566925\n",
      "Epoch [1/1] | Batch 740 | loss: 0.08667833358049393\n",
      "Epoch [1/1] | Batch 741 | loss: 0.13421590626239777\n",
      "Epoch [1/1] | Batch 742 | loss: 0.09216754138469696\n",
      "Epoch [1/1] | Batch 743 | loss: 0.12723930180072784\n",
      "Epoch [1/1] | Batch 744 | loss: 0.1627446413040161\n",
      "Epoch [1/1] | Batch 745 | loss: 0.23978666961193085\n",
      "Epoch [1/1] | Batch 746 | loss: 0.1730545610189438\n",
      "Epoch [1/1] | Batch 747 | loss: 0.07204136252403259\n",
      "Epoch [1/1] | Batch 748 | loss: 0.0916946604847908\n",
      "Epoch [1/1] | Batch 749 | loss: 0.1702062040567398\n",
      "Epoch [1/1] | Batch 750 | loss: 0.11798905581235886\n",
      "Epoch [1/1] | Batch 751 | loss: 0.11386431753635406\n",
      "Epoch [1/1] | Batch 752 | loss: 0.0975443497300148\n",
      "Epoch [1/1] | Batch 753 | loss: 0.05330502241849899\n",
      "Epoch [1/1] | Batch 754 | loss: 0.08133299648761749\n",
      "Epoch [1/1] | Batch 755 | loss: 0.1162627711892128\n",
      "Epoch [1/1] | Batch 756 | loss: 0.2667344808578491\n",
      "Epoch [1/1] | Batch 757 | loss: 0.16288137435913086\n",
      "Epoch [1/1] | Batch 758 | loss: 0.18106982111930847\n",
      "Epoch [1/1] | Batch 759 | loss: 0.12262304127216339\n",
      "Epoch [1/1] | Batch 760 | loss: 0.31473278999328613\n",
      "Epoch [1/1] | Batch 761 | loss: 0.38408324122428894\n",
      "Epoch [1/1] | Batch 762 | loss: 0.07727911323308945\n",
      "Epoch [1/1] | Batch 763 | loss: 0.09749104827642441\n",
      "Epoch [1/1] | Batch 764 | loss: 0.19407662749290466\n",
      "Epoch [1/1] | Batch 765 | loss: 0.14294762909412384\n",
      "Epoch [1/1] | Batch 766 | loss: 0.2257419228553772\n",
      "Epoch [1/1] | Batch 767 | loss: 0.2061261683702469\n",
      "Epoch [1/1] | Batch 768 | loss: 0.09509032219648361\n",
      "Epoch [1/1] | Batch 769 | loss: 0.21098797023296356\n",
      "Epoch [1/1] | Batch 770 | loss: 0.11165636777877808\n",
      "Epoch [1/1] | Batch 771 | loss: 0.14401429891586304\n",
      "Epoch [1/1] | Batch 772 | loss: 0.2481994926929474\n",
      "Epoch [1/1] | Batch 773 | loss: 0.15128280222415924\n",
      "Epoch [1/1] | Batch 774 | loss: 0.0798182412981987\n",
      "Epoch [1/1] | Batch 775 | loss: 0.07183432579040527\n",
      "Epoch [1/1] | Batch 776 | loss: 0.1473468840122223\n",
      "Epoch [1/1] | Batch 777 | loss: 0.1403479129076004\n",
      "Epoch [1/1] | Batch 778 | loss: 0.07194635272026062\n",
      "Epoch [1/1] | Batch 779 | loss: 0.13728375732898712\n",
      "Epoch [1/1] | Batch 780 | loss: 0.13354061543941498\n",
      "Epoch [1/1] | Batch 781 | loss: 0.05618087574839592\n",
      "Epoch [1/1] | Batch 782 | loss: 0.09179133176803589\n",
      "Epoch [1/1] | Batch 783 | loss: 0.10508093237876892\n",
      "Epoch [1/1] | Batch 784 | loss: 0.2066519409418106\n",
      "Epoch [1/1] | Batch 785 | loss: 0.043754804879426956\n",
      "Epoch [1/1] | Batch 786 | loss: 0.1672275960445404\n",
      "Epoch [1/1] | Batch 787 | loss: 0.0795065239071846\n",
      "Epoch [1/1] | Batch 788 | loss: 0.0962967649102211\n",
      "Epoch [1/1] | Batch 789 | loss: 0.17414726316928864\n",
      "Epoch [1/1] | Batch 790 | loss: 0.07666166871786118\n",
      "Epoch [1/1] | Batch 791 | loss: 0.15731650590896606\n",
      "Epoch [1/1] | Batch 792 | loss: 0.053871672600507736\n",
      "Epoch [1/1] | Batch 793 | loss: 0.3059122562408447\n",
      "Epoch [1/1] | Batch 794 | loss: 0.328887939453125\n",
      "Epoch [1/1] | Batch 795 | loss: 0.20100317895412445\n",
      "Epoch [1/1] | Batch 796 | loss: 0.17528189718723297\n",
      "Epoch [1/1] | Batch 797 | loss: 0.21225476264953613\n",
      "Epoch [1/1] | Batch 798 | loss: 0.2399221956729889\n",
      "Epoch [1/1] | Batch 799 | loss: 0.19871404767036438\n",
      "Epoch [1/1] | Batch 800 | loss: 0.10251561552286148\n",
      "Epoch [1/1] | Batch 801 | loss: 0.08222325146198273\n",
      "Epoch [1/1] | Batch 802 | loss: 0.2650272250175476\n",
      "Epoch [1/1] | Batch 803 | loss: 0.06648311764001846\n",
      "Epoch [1/1] | Batch 804 | loss: 0.17933213710784912\n",
      "Epoch [1/1] | Batch 805 | loss: 0.2453051209449768\n",
      "Epoch [1/1] | Batch 806 | loss: 0.27954837679862976\n",
      "Epoch [1/1] | Batch 807 | loss: 0.28467169404029846\n",
      "Epoch [1/1] | Batch 808 | loss: 0.5678358674049377\n",
      "Epoch [1/1] | Batch 809 | loss: 0.2989905774593353\n",
      "Epoch [1/1] | Batch 810 | loss: 0.1039714440703392\n",
      "Epoch [1/1] | Batch 811 | loss: 0.09758646041154861\n",
      "Epoch [1/1] | Batch 812 | loss: 0.24448218941688538\n",
      "Epoch [1/1] | Batch 813 | loss: 0.24209846556186676\n",
      "Epoch [1/1] | Batch 814 | loss: 0.11675512045621872\n",
      "Epoch [1/1] | Batch 815 | loss: 0.15570154786109924\n",
      "Epoch [1/1] | Batch 816 | loss: 0.14729344844818115\n",
      "Epoch [1/1] | Batch 817 | loss: 0.091493621468544\n",
      "Epoch [1/1] | Batch 818 | loss: 0.15454626083374023\n",
      "Epoch [1/1] | Batch 819 | loss: 0.10046566277742386\n",
      "Epoch [1/1] | Batch 820 | loss: 0.08729293942451477\n",
      "Epoch [1/1] | Batch 821 | loss: 0.13857416808605194\n",
      "Epoch [1/1] | Batch 822 | loss: 0.21684637665748596\n",
      "Epoch [1/1] | Batch 823 | loss: 0.13590039312839508\n",
      "Epoch [1/1] | Batch 824 | loss: 0.22402936220169067\n",
      "Epoch [1/1] | Batch 825 | loss: 0.19509103894233704\n",
      "Epoch [1/1] | Batch 826 | loss: 0.2047133445739746\n",
      "Epoch [1/1] | Batch 827 | loss: 0.13989464938640594\n",
      "Epoch [1/1] | Batch 828 | loss: 0.271479070186615\n",
      "Epoch [1/1] | Batch 829 | loss: 0.30641189217567444\n",
      "Epoch [1/1] | Batch 830 | loss: 0.2177995890378952\n",
      "Epoch [1/1] | Batch 831 | loss: 0.15651868283748627\n",
      "Epoch [1/1] | Batch 832 | loss: 0.18703284859657288\n",
      "Epoch [1/1] | Batch 833 | loss: 0.06011051684617996\n",
      "Epoch [1/1] | Batch 834 | loss: 0.06990186125040054\n",
      "Epoch [1/1] | Batch 835 | loss: 0.2638430595397949\n",
      "Epoch [1/1] | Batch 836 | loss: 0.14630001783370972\n",
      "Epoch [1/1] | Batch 837 | loss: 0.10211475193500519\n",
      "Epoch [1/1] | Batch 838 | loss: 0.08718367666006088\n",
      "Epoch [1/1] | Batch 839 | loss: 0.03115624375641346\n",
      "Epoch [1/1] | Batch 840 | loss: 0.07672091573476791\n",
      "Epoch [1/1] | Batch 841 | loss: 0.08685299009084702\n",
      "Epoch [1/1] | Batch 842 | loss: 0.29314109683036804\n",
      "Epoch [1/1] | Batch 843 | loss: 0.1331464797258377\n",
      "Epoch [1/1] | Batch 844 | loss: 0.1999795287847519\n",
      "Epoch [1/1] | Batch 845 | loss: 0.30720674991607666\n",
      "Epoch [1/1] | Batch 846 | loss: 0.10786404460668564\n",
      "Epoch [1/1] | Batch 847 | loss: 0.2182285189628601\n",
      "Epoch [1/1] | Batch 848 | loss: 0.12898357212543488\n",
      "Epoch [1/1] | Batch 849 | loss: 0.07709621638059616\n",
      "Epoch [1/1] | Batch 850 | loss: 0.22701196372509003\n",
      "Epoch [1/1] | Batch 851 | loss: 0.15294547379016876\n",
      "Epoch [1/1] | Batch 852 | loss: 0.08889198303222656\n",
      "Epoch [1/1] | Batch 853 | loss: 0.11964826285839081\n",
      "Epoch [1/1] | Batch 854 | loss: 0.0794440433382988\n",
      "Epoch [1/1] | Batch 855 | loss: 0.18649837374687195\n",
      "Epoch [1/1] | Batch 856 | loss: 0.12207416445016861\n",
      "Epoch [1/1] | Batch 857 | loss: 0.301939457654953\n",
      "Epoch [1/1] | Batch 858 | loss: 0.34208256006240845\n",
      "Epoch [1/1] | Batch 859 | loss: 0.14851777255535126\n",
      "Epoch [1/1] | Batch 860 | loss: 0.23859481513500214\n",
      "Epoch [1/1] | Batch 861 | loss: 0.2569954991340637\n",
      "Epoch [1/1] | Batch 862 | loss: 0.11274435371160507\n",
      "Epoch [1/1] | Batch 863 | loss: 0.20241840183734894\n",
      "Epoch [1/1] | Batch 864 | loss: 0.20727060735225677\n",
      "Epoch [1/1] | Batch 865 | loss: 0.10595189779996872\n",
      "Epoch [1/1] | Batch 866 | loss: 0.08491343259811401\n",
      "Epoch [1/1] | Batch 867 | loss: 0.13406014442443848\n",
      "Epoch [1/1] | Batch 868 | loss: 0.12251438945531845\n",
      "Epoch [1/1] | Batch 869 | loss: 0.08149974048137665\n",
      "Epoch [1/1] | Batch 870 | loss: 0.10299259424209595\n",
      "Epoch [1/1] | Batch 871 | loss: 0.12056613713502884\n",
      "Epoch [1/1] | Batch 872 | loss: 0.09511136263608932\n",
      "Epoch [1/1] | Batch 873 | loss: 0.1167021244764328\n",
      "Epoch [1/1] | Batch 874 | loss: 0.12468423694372177\n",
      "Epoch [1/1] | Batch 875 | loss: 0.13231360912322998\n",
      "Epoch [1/1] | Batch 876 | loss: 0.2090206891298294\n",
      "Epoch [1/1] | Batch 877 | loss: 0.310348242521286\n",
      "Epoch [1/1] | Batch 878 | loss: 0.17052781581878662\n",
      "Epoch [1/1] | Batch 879 | loss: 0.42723822593688965\n",
      "Epoch [1/1] | Batch 880 | loss: 0.09306929260492325\n",
      "Epoch [1/1] | Batch 881 | loss: 0.0940132737159729\n",
      "Epoch [1/1] | Batch 882 | loss: 0.14219847321510315\n",
      "Epoch [1/1] | Batch 883 | loss: 0.14926669001579285\n",
      "Epoch [1/1] | Batch 884 | loss: 0.10703147202730179\n",
      "Epoch [1/1] | Batch 885 | loss: 0.19416815042495728\n",
      "Epoch [1/1] | Batch 886 | loss: 0.11911877244710922\n",
      "Epoch [1/1] | Batch 887 | loss: 0.20729590952396393\n",
      "Epoch [1/1] | Batch 888 | loss: 0.25430813431739807\n",
      "Epoch [1/1] | Batch 889 | loss: 0.12992404401302338\n",
      "Epoch [1/1] | Batch 890 | loss: 0.10711945593357086\n",
      "Epoch [1/1] | Batch 891 | loss: 0.1777302324771881\n",
      "Epoch [1/1] | Batch 892 | loss: 0.1281667947769165\n",
      "Epoch [1/1] | Batch 893 | loss: 0.15814994275569916\n",
      "Epoch [1/1] | Batch 894 | loss: 0.25224724411964417\n",
      "Epoch [1/1] | Batch 895 | loss: 0.124864362180233\n",
      "Epoch [1/1] | Batch 896 | loss: 0.06597714871168137\n",
      "Epoch [1/1] | Batch 897 | loss: 0.34107425808906555\n",
      "Epoch [1/1] | Batch 898 | loss: 0.1303427517414093\n",
      "Epoch [1/1] | Batch 899 | loss: 0.23070457577705383\n",
      "Epoch [1/1] | Batch 900 | loss: 0.23412972688674927\n",
      "Epoch [1/1] | Batch 901 | loss: 0.19920076429843903\n",
      "Epoch [1/1] | Batch 902 | loss: 0.12552125751972198\n",
      "Epoch [1/1] | Batch 903 | loss: 0.10567551851272583\n",
      "Epoch [1/1] | Batch 904 | loss: 0.1419992297887802\n",
      "Epoch [1/1] | Batch 905 | loss: 0.09908288717269897\n",
      "Epoch [1/1] | Batch 906 | loss: 0.10830667614936829\n",
      "Epoch [1/1] | Batch 907 | loss: 0.12376823276281357\n",
      "Epoch [1/1] | Batch 908 | loss: 0.1577988862991333\n",
      "Epoch [1/1] | Batch 909 | loss: 0.1983572244644165\n",
      "Epoch [1/1] | Batch 910 | loss: 0.2508026361465454\n",
      "Epoch [1/1] | Batch 911 | loss: 0.12804073095321655\n",
      "Epoch [1/1] | Batch 912 | loss: 0.08824107050895691\n",
      "Epoch [1/1] | Batch 913 | loss: 0.06263964623212814\n",
      "Epoch [1/1] | Batch 914 | loss: 0.14081765711307526\n",
      "Epoch [1/1] | Batch 915 | loss: 0.23612377047538757\n",
      "Epoch [1/1] | Batch 916 | loss: 0.15100504457950592\n",
      "Epoch [1/1] | Batch 917 | loss: 0.11443548649549484\n",
      "Epoch [1/1] | Batch 918 | loss: 0.30037030577659607\n",
      "Epoch [1/1] | Batch 919 | loss: 0.1602901816368103\n",
      "Epoch [1/1] | Batch 920 | loss: 0.18429338932037354\n",
      "Epoch [1/1] | Batch 921 | loss: 0.3315325677394867\n",
      "Epoch [1/1] | Batch 922 | loss: 0.12862782180309296\n",
      "Epoch [1/1] | Batch 923 | loss: 0.17114514112472534\n",
      "Epoch [1/1] | Batch 924 | loss: 0.11344697326421738\n",
      "Epoch [1/1] | Batch 925 | loss: 0.1339649260044098\n",
      "Epoch [1/1] | Batch 926 | loss: 0.22767871618270874\n",
      "Epoch [1/1] | Batch 927 | loss: 0.12869882583618164\n",
      "Epoch [1/1] | Batch 928 | loss: 0.11776665598154068\n",
      "Epoch [1/1] | Batch 929 | loss: 0.05606820806860924\n",
      "Epoch [1/1] | Batch 930 | loss: 0.15980185568332672\n",
      "Epoch [1/1] | Batch 931 | loss: 0.11933769285678864\n",
      "Epoch [1/1] | Batch 932 | loss: 0.17871001362800598\n",
      "Epoch [1/1] | Batch 933 | loss: 0.1198471188545227\n",
      "Epoch [1/1] | Batch 934 | loss: 0.1692628264427185\n",
      "Epoch [1/1] | Batch 935 | loss: 0.09126061201095581\n",
      "Epoch [1/1] | Batch 936 | loss: 0.1847619265317917\n",
      "Epoch [1/1] | Batch 937 | loss: 0.06496307998895645\n",
      "Epoch [1/1] | Batch 938 | loss: 0.19804005324840546\n"
     ]
    }
   ],
   "source": [
    "model = Net() # re-initialize net\n",
    "num_epochs = 1\n",
    "lr = 1e-1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# re-define dataloader \n",
    "batch_size = 64\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "\n",
    "        # compute loss    \n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # The three Musketeers!\n",
    "        optimizer.zero_grad() # sets p.grad = 0 for all params\n",
    "        loss.backward() # sets p.grad += dloss/dp\n",
    "        optimizer.step() # performs actual optimization step\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Batch {batch_idx+1} | loss: {loss.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observation: loss surface drastically changes over time and so is the hessian.\n",
    "* Idea: change the learning rate over time.\n",
    "* The most common practice is to reduce the learning rate after few epochs.\n",
    "* Very useful in practice.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.researchgate.net/publication/332709751/figure/fig3/AS:752573329969153@1556438872095/Comparison-of-the-training-curves-The-left-compares-scale-ScaleNet-50-and-ResNet-50-on.png\" width=25%, height=25%>\n",
    "</p>\n",
    "\n",
    "* Schedulers are also supported by torch.optim library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] | Batch 1 | loss: 2.297530174255371 | lr: 0.1\n",
      "Epoch [1/2] | Batch 2 | loss: 2.2116317749023438 | lr: 0.1\n",
      "Epoch [1/2] | Batch 3 | loss: 2.1043801307678223 | lr: 0.1\n",
      "Epoch [1/2] | Batch 4 | loss: 1.9877595901489258 | lr: 0.1\n",
      "Epoch [1/2] | Batch 5 | loss: 1.8876607418060303 | lr: 0.1\n",
      "Epoch [1/2] | Batch 6 | loss: 1.6155986785888672 | lr: 0.1\n",
      "Epoch [1/2] | Batch 7 | loss: 1.7638261318206787 | lr: 0.1\n",
      "Epoch [1/2] | Batch 8 | loss: 1.4920846223831177 | lr: 0.1\n",
      "Epoch [1/2] | Batch 9 | loss: 1.3593358993530273 | lr: 0.1\n",
      "Epoch [1/2] | Batch 10 | loss: 1.3291488885879517 | lr: 0.1\n",
      "Epoch [1/2] | Batch 11 | loss: 1.0863240957260132 | lr: 0.1\n",
      "Epoch [1/2] | Batch 12 | loss: 1.0341304540634155 | lr: 0.1\n",
      "Epoch [1/2] | Batch 13 | loss: 0.969967782497406 | lr: 0.1\n",
      "Epoch [1/2] | Batch 14 | loss: 0.9051041603088379 | lr: 0.1\n",
      "Epoch [1/2] | Batch 15 | loss: 0.8547211289405823 | lr: 0.1\n",
      "Epoch [1/2] | Batch 16 | loss: 0.8923367857933044 | lr: 0.1\n",
      "Epoch [1/2] | Batch 17 | loss: 0.7887994647026062 | lr: 0.1\n",
      "Epoch [1/2] | Batch 18 | loss: 0.9915962219238281 | lr: 0.1\n",
      "Epoch [1/2] | Batch 19 | loss: 0.7675345540046692 | lr: 0.1\n",
      "Epoch [1/2] | Batch 20 | loss: 0.7228323817253113 | lr: 0.1\n",
      "Epoch [1/2] | Batch 21 | loss: 0.594860315322876 | lr: 0.1\n",
      "Epoch [1/2] | Batch 22 | loss: 0.771761417388916 | lr: 0.1\n",
      "Epoch [1/2] | Batch 23 | loss: 0.6713259220123291 | lr: 0.1\n",
      "Epoch [1/2] | Batch 24 | loss: 0.6940869092941284 | lr: 0.1\n",
      "Epoch [1/2] | Batch 25 | loss: 0.8176971673965454 | lr: 0.1\n",
      "Epoch [1/2] | Batch 26 | loss: 0.5824893116950989 | lr: 0.1\n",
      "Epoch [1/2] | Batch 27 | loss: 0.6094369888305664 | lr: 0.1\n",
      "Epoch [1/2] | Batch 28 | loss: 0.7940057516098022 | lr: 0.1\n",
      "Epoch [1/2] | Batch 29 | loss: 0.48337334394454956 | lr: 0.1\n",
      "Epoch [1/2] | Batch 30 | loss: 0.6362990736961365 | lr: 0.1\n",
      "Epoch [1/2] | Batch 31 | loss: 0.6184811592102051 | lr: 0.1\n",
      "Epoch [1/2] | Batch 32 | loss: 0.6017866134643555 | lr: 0.1\n",
      "Epoch [1/2] | Batch 33 | loss: 0.7784098982810974 | lr: 0.1\n",
      "Epoch [1/2] | Batch 34 | loss: 0.6949697136878967 | lr: 0.1\n",
      "Epoch [1/2] | Batch 35 | loss: 0.651839554309845 | lr: 0.1\n",
      "Epoch [1/2] | Batch 36 | loss: 0.6057629585266113 | lr: 0.1\n",
      "Epoch [1/2] | Batch 37 | loss: 0.5049706697463989 | lr: 0.1\n",
      "Epoch [1/2] | Batch 38 | loss: 0.47428640723228455 | lr: 0.1\n",
      "Epoch [1/2] | Batch 39 | loss: 0.41697537899017334 | lr: 0.1\n",
      "Epoch [1/2] | Batch 40 | loss: 0.36738312244415283 | lr: 0.1\n",
      "Epoch [1/2] | Batch 41 | loss: 0.39791080355644226 | lr: 0.1\n",
      "Epoch [1/2] | Batch 42 | loss: 0.44466671347618103 | lr: 0.1\n",
      "Epoch [1/2] | Batch 43 | loss: 0.5754024386405945 | lr: 0.1\n",
      "Epoch [1/2] | Batch 44 | loss: 0.6896306276321411 | lr: 0.1\n",
      "Epoch [1/2] | Batch 45 | loss: 0.6238508820533752 | lr: 0.1\n",
      "Epoch [1/2] | Batch 46 | loss: 0.600509524345398 | lr: 0.1\n",
      "Epoch [1/2] | Batch 47 | loss: 0.4940378963947296 | lr: 0.1\n",
      "Epoch [1/2] | Batch 48 | loss: 0.42026975750923157 | lr: 0.1\n",
      "Epoch [1/2] | Batch 49 | loss: 0.46763187646865845 | lr: 0.1\n",
      "Epoch [1/2] | Batch 50 | loss: 0.37799182534217834 | lr: 0.1\n",
      "Epoch [1/2] | Batch 51 | loss: 0.4680078327655792 | lr: 0.1\n",
      "Epoch [1/2] | Batch 52 | loss: 0.6124770045280457 | lr: 0.1\n",
      "Epoch [1/2] | Batch 53 | loss: 0.3426615297794342 | lr: 0.1\n",
      "Epoch [1/2] | Batch 54 | loss: 0.8299294114112854 | lr: 0.1\n",
      "Epoch [1/2] | Batch 55 | loss: 0.48786914348602295 | lr: 0.1\n",
      "Epoch [1/2] | Batch 56 | loss: 0.5081599950790405 | lr: 0.1\n",
      "Epoch [1/2] | Batch 57 | loss: 0.4472886621952057 | lr: 0.1\n",
      "Epoch [1/2] | Batch 58 | loss: 0.6071528792381287 | lr: 0.1\n",
      "Epoch [1/2] | Batch 59 | loss: 0.391414076089859 | lr: 0.1\n",
      "Epoch [1/2] | Batch 60 | loss: 0.5066626667976379 | lr: 0.1\n",
      "Epoch [1/2] | Batch 61 | loss: 0.4740981459617615 | lr: 0.1\n",
      "Epoch [1/2] | Batch 62 | loss: 0.28495487570762634 | lr: 0.1\n",
      "Epoch [1/2] | Batch 63 | loss: 0.3708883225917816 | lr: 0.1\n",
      "Epoch [1/2] | Batch 64 | loss: 0.41787832975387573 | lr: 0.1\n",
      "Epoch [1/2] | Batch 65 | loss: 0.405691921710968 | lr: 0.1\n",
      "Epoch [1/2] | Batch 66 | loss: 0.4256512522697449 | lr: 0.1\n",
      "Epoch [1/2] | Batch 67 | loss: 0.42282041907310486 | lr: 0.1\n",
      "Epoch [1/2] | Batch 68 | loss: 0.37805843353271484 | lr: 0.1\n",
      "Epoch [1/2] | Batch 69 | loss: 0.43471473455429077 | lr: 0.1\n",
      "Epoch [1/2] | Batch 70 | loss: 0.3405706584453583 | lr: 0.1\n",
      "Epoch [1/2] | Batch 71 | loss: 0.324430912733078 | lr: 0.1\n",
      "Epoch [1/2] | Batch 72 | loss: 0.5026597380638123 | lr: 0.1\n",
      "Epoch [1/2] | Batch 73 | loss: 0.3397291600704193 | lr: 0.1\n",
      "Epoch [1/2] | Batch 74 | loss: 0.3326502740383148 | lr: 0.1\n",
      "Epoch [1/2] | Batch 75 | loss: 0.26177728176116943 | lr: 0.1\n",
      "Epoch [1/2] | Batch 76 | loss: 0.4782896339893341 | lr: 0.1\n",
      "Epoch [1/2] | Batch 77 | loss: 0.49194031953811646 | lr: 0.1\n",
      "Epoch [1/2] | Batch 78 | loss: 0.5058695077896118 | lr: 0.1\n",
      "Epoch [1/2] | Batch 79 | loss: 0.3621116578578949 | lr: 0.1\n",
      "Epoch [1/2] | Batch 80 | loss: 0.5167588591575623 | lr: 0.1\n",
      "Epoch [1/2] | Batch 81 | loss: 0.31888052821159363 | lr: 0.1\n",
      "Epoch [1/2] | Batch 82 | loss: 0.328321248292923 | lr: 0.1\n",
      "Epoch [1/2] | Batch 83 | loss: 0.2261829972267151 | lr: 0.1\n",
      "Epoch [1/2] | Batch 84 | loss: 0.5486069321632385 | lr: 0.1\n",
      "Epoch [1/2] | Batch 85 | loss: 0.29343727231025696 | lr: 0.1\n",
      "Epoch [1/2] | Batch 86 | loss: 0.45180565118789673 | lr: 0.1\n",
      "Epoch [1/2] | Batch 87 | loss: 0.48071080446243286 | lr: 0.1\n",
      "Epoch [1/2] | Batch 88 | loss: 0.42215558886528015 | lr: 0.1\n",
      "Epoch [1/2] | Batch 89 | loss: 0.297222375869751 | lr: 0.1\n",
      "Epoch [1/2] | Batch 90 | loss: 0.5571603775024414 | lr: 0.1\n",
      "Epoch [1/2] | Batch 91 | loss: 0.3466065526008606 | lr: 0.1\n",
      "Epoch [1/2] | Batch 92 | loss: 0.4362310767173767 | lr: 0.1\n",
      "Epoch [1/2] | Batch 93 | loss: 0.3787536025047302 | lr: 0.1\n",
      "Epoch [1/2] | Batch 94 | loss: 0.4602205753326416 | lr: 0.1\n",
      "Epoch [1/2] | Batch 95 | loss: 0.37293174862861633 | lr: 0.1\n",
      "Epoch [1/2] | Batch 96 | loss: 0.29552072286605835 | lr: 0.1\n",
      "Epoch [1/2] | Batch 97 | loss: 0.31454578042030334 | lr: 0.1\n",
      "Epoch [1/2] | Batch 98 | loss: 0.3143596649169922 | lr: 0.1\n",
      "Epoch [1/2] | Batch 99 | loss: 0.33702459931373596 | lr: 0.1\n",
      "Epoch [1/2] | Batch 100 | loss: 0.24136628210544586 | lr: 0.1\n",
      "Epoch [1/2] | Batch 101 | loss: 0.2967135012149811 | lr: 0.1\n",
      "Epoch [1/2] | Batch 102 | loss: 0.3363569974899292 | lr: 0.1\n",
      "Epoch [1/2] | Batch 103 | loss: 0.4393899440765381 | lr: 0.1\n",
      "Epoch [1/2] | Batch 104 | loss: 0.3942558169364929 | lr: 0.1\n",
      "Epoch [1/2] | Batch 105 | loss: 0.2872730791568756 | lr: 0.1\n",
      "Epoch [1/2] | Batch 106 | loss: 0.492059588432312 | lr: 0.1\n",
      "Epoch [1/2] | Batch 107 | loss: 0.7112575769424438 | lr: 0.1\n",
      "Epoch [1/2] | Batch 108 | loss: 0.3069058358669281 | lr: 0.1\n",
      "Epoch [1/2] | Batch 109 | loss: 0.2925041615962982 | lr: 0.1\n",
      "Epoch [1/2] | Batch 110 | loss: 0.23361735045909882 | lr: 0.1\n",
      "Epoch [1/2] | Batch 111 | loss: 0.32212457060813904 | lr: 0.1\n",
      "Epoch [1/2] | Batch 112 | loss: 0.35135215520858765 | lr: 0.1\n",
      "Epoch [1/2] | Batch 113 | loss: 0.20914782583713531 | lr: 0.1\n",
      "Epoch [1/2] | Batch 114 | loss: 0.22869904339313507 | lr: 0.1\n",
      "Epoch [1/2] | Batch 115 | loss: 0.45337530970573425 | lr: 0.1\n",
      "Epoch [1/2] | Batch 116 | loss: 0.35804325342178345 | lr: 0.1\n",
      "Epoch [1/2] | Batch 117 | loss: 0.1910628229379654 | lr: 0.1\n",
      "Epoch [1/2] | Batch 118 | loss: 0.35846251249313354 | lr: 0.1\n",
      "Epoch [1/2] | Batch 119 | loss: 0.20777307450771332 | lr: 0.1\n",
      "Epoch [1/2] | Batch 120 | loss: 0.2868264317512512 | lr: 0.1\n",
      "Epoch [1/2] | Batch 121 | loss: 0.21778757870197296 | lr: 0.1\n",
      "Epoch [1/2] | Batch 122 | loss: 0.2444155514240265 | lr: 0.1\n",
      "Epoch [1/2] | Batch 123 | loss: 0.4709186851978302 | lr: 0.1\n",
      "Epoch [1/2] | Batch 124 | loss: 0.477104127407074 | lr: 0.1\n",
      "Epoch [1/2] | Batch 125 | loss: 0.3926067352294922 | lr: 0.1\n",
      "Epoch [1/2] | Batch 126 | loss: 0.3926621377468109 | lr: 0.1\n",
      "Epoch [1/2] | Batch 127 | loss: 0.3800541162490845 | lr: 0.1\n",
      "Epoch [1/2] | Batch 128 | loss: 0.36596646904945374 | lr: 0.1\n",
      "Epoch [1/2] | Batch 129 | loss: 0.18909884989261627 | lr: 0.1\n",
      "Epoch [1/2] | Batch 130 | loss: 0.3129465878009796 | lr: 0.1\n",
      "Epoch [1/2] | Batch 131 | loss: 0.48150795698165894 | lr: 0.1\n",
      "Epoch [1/2] | Batch 132 | loss: 0.46663516759872437 | lr: 0.1\n",
      "Epoch [1/2] | Batch 133 | loss: 0.315785676240921 | lr: 0.1\n",
      "Epoch [1/2] | Batch 134 | loss: 0.27989163994789124 | lr: 0.1\n",
      "Epoch [1/2] | Batch 135 | loss: 0.33477160334587097 | lr: 0.1\n",
      "Epoch [1/2] | Batch 136 | loss: 0.46115022897720337 | lr: 0.1\n",
      "Epoch [1/2] | Batch 137 | loss: 0.47128185629844666 | lr: 0.1\n",
      "Epoch [1/2] | Batch 138 | loss: 0.34611937403678894 | lr: 0.1\n",
      "Epoch [1/2] | Batch 139 | loss: 0.41820353269577026 | lr: 0.1\n",
      "Epoch [1/2] | Batch 140 | loss: 0.34835660457611084 | lr: 0.1\n",
      "Epoch [1/2] | Batch 141 | loss: 0.522216796875 | lr: 0.1\n",
      "Epoch [1/2] | Batch 142 | loss: 0.471994549036026 | lr: 0.1\n",
      "Epoch [1/2] | Batch 143 | loss: 0.2754603624343872 | lr: 0.1\n",
      "Epoch [1/2] | Batch 144 | loss: 0.3093826174736023 | lr: 0.1\n",
      "Epoch [1/2] | Batch 145 | loss: 0.33765745162963867 | lr: 0.1\n",
      "Epoch [1/2] | Batch 146 | loss: 0.29855865240097046 | lr: 0.1\n",
      "Epoch [1/2] | Batch 147 | loss: 0.3208465576171875 | lr: 0.1\n",
      "Epoch [1/2] | Batch 148 | loss: 0.2548362910747528 | lr: 0.1\n",
      "Epoch [1/2] | Batch 149 | loss: 0.25919219851493835 | lr: 0.1\n",
      "Epoch [1/2] | Batch 150 | loss: 0.3789658546447754 | lr: 0.1\n",
      "Epoch [1/2] | Batch 151 | loss: 0.2986932098865509 | lr: 0.1\n",
      "Epoch [1/2] | Batch 152 | loss: 0.33417823910713196 | lr: 0.1\n",
      "Epoch [1/2] | Batch 153 | loss: 0.37154677510261536 | lr: 0.1\n",
      "Epoch [1/2] | Batch 154 | loss: 0.16619716584682465 | lr: 0.1\n",
      "Epoch [1/2] | Batch 155 | loss: 0.3673740029335022 | lr: 0.1\n",
      "Epoch [1/2] | Batch 156 | loss: 0.2952224016189575 | lr: 0.1\n",
      "Epoch [1/2] | Batch 157 | loss: 0.23403634130954742 | lr: 0.1\n",
      "Epoch [1/2] | Batch 158 | loss: 0.3721553087234497 | lr: 0.1\n",
      "Epoch [1/2] | Batch 159 | loss: 0.1732298582792282 | lr: 0.1\n",
      "Epoch [1/2] | Batch 160 | loss: 0.38743337988853455 | lr: 0.1\n",
      "Epoch [1/2] | Batch 161 | loss: 0.3736106753349304 | lr: 0.1\n",
      "Epoch [1/2] | Batch 162 | loss: 0.2686540484428406 | lr: 0.1\n",
      "Epoch [1/2] | Batch 163 | loss: 0.3798787593841553 | lr: 0.1\n",
      "Epoch [1/2] | Batch 164 | loss: 0.42323389649391174 | lr: 0.1\n",
      "Epoch [1/2] | Batch 165 | loss: 0.15546627342700958 | lr: 0.1\n",
      "Epoch [1/2] | Batch 166 | loss: 0.12831749022006989 | lr: 0.1\n",
      "Epoch [1/2] | Batch 167 | loss: 0.29325568675994873 | lr: 0.1\n",
      "Epoch [1/2] | Batch 168 | loss: 0.5020748376846313 | lr: 0.1\n",
      "Epoch [1/2] | Batch 169 | loss: 0.1871510148048401 | lr: 0.1\n",
      "Epoch [1/2] | Batch 170 | loss: 0.3399633765220642 | lr: 0.1\n",
      "Epoch [1/2] | Batch 171 | loss: 0.32300522923469543 | lr: 0.1\n",
      "Epoch [1/2] | Batch 172 | loss: 0.27866941690444946 | lr: 0.1\n",
      "Epoch [1/2] | Batch 173 | loss: 0.3038128912448883 | lr: 0.1\n",
      "Epoch [1/2] | Batch 174 | loss: 0.30538368225097656 | lr: 0.1\n",
      "Epoch [1/2] | Batch 175 | loss: 0.4319086968898773 | lr: 0.1\n",
      "Epoch [1/2] | Batch 176 | loss: 0.2661411166191101 | lr: 0.1\n",
      "Epoch [1/2] | Batch 177 | loss: 0.28175026178359985 | lr: 0.1\n",
      "Epoch [1/2] | Batch 178 | loss: 0.2911681532859802 | lr: 0.1\n",
      "Epoch [1/2] | Batch 179 | loss: 0.32418885827064514 | lr: 0.1\n",
      "Epoch [1/2] | Batch 180 | loss: 0.329601526260376 | lr: 0.1\n",
      "Epoch [1/2] | Batch 181 | loss: 0.180060476064682 | lr: 0.1\n",
      "Epoch [1/2] | Batch 182 | loss: 0.4129899740219116 | lr: 0.1\n",
      "Epoch [1/2] | Batch 183 | loss: 0.2775634825229645 | lr: 0.1\n",
      "Epoch [1/2] | Batch 184 | loss: 0.18500065803527832 | lr: 0.1\n",
      "Epoch [1/2] | Batch 185 | loss: 0.2109660655260086 | lr: 0.1\n",
      "Epoch [1/2] | Batch 186 | loss: 0.3396366238594055 | lr: 0.1\n",
      "Epoch [1/2] | Batch 187 | loss: 0.1817948818206787 | lr: 0.1\n",
      "Epoch [1/2] | Batch 188 | loss: 0.4198026657104492 | lr: 0.1\n",
      "Epoch [1/2] | Batch 189 | loss: 0.3674495220184326 | lr: 0.1\n",
      "Epoch [1/2] | Batch 190 | loss: 0.24473689496517181 | lr: 0.1\n",
      "Epoch [1/2] | Batch 191 | loss: 0.34616875648498535 | lr: 0.1\n",
      "Epoch [1/2] | Batch 192 | loss: 0.5860119462013245 | lr: 0.1\n",
      "Epoch [1/2] | Batch 193 | loss: 0.3526902496814728 | lr: 0.1\n",
      "Epoch [1/2] | Batch 194 | loss: 0.2906528413295746 | lr: 0.1\n",
      "Epoch [1/2] | Batch 195 | loss: 0.16715580224990845 | lr: 0.1\n",
      "Epoch [1/2] | Batch 196 | loss: 0.2761046290397644 | lr: 0.1\n",
      "Epoch [1/2] | Batch 197 | loss: 0.16515719890594482 | lr: 0.1\n",
      "Epoch [1/2] | Batch 198 | loss: 0.3086521625518799 | lr: 0.1\n",
      "Epoch [1/2] | Batch 199 | loss: 0.15103429555892944 | lr: 0.1\n",
      "Epoch [1/2] | Batch 200 | loss: 0.47708168625831604 | lr: 0.1\n",
      "Epoch [1/2] | Batch 201 | loss: 0.37830790877342224 | lr: 0.1\n",
      "Epoch [1/2] | Batch 202 | loss: 0.22610582411289215 | lr: 0.1\n",
      "Epoch [1/2] | Batch 203 | loss: 0.4341305196285248 | lr: 0.1\n",
      "Epoch [1/2] | Batch 204 | loss: 0.3651101291179657 | lr: 0.1\n",
      "Epoch [1/2] | Batch 205 | loss: 0.33789077401161194 | lr: 0.1\n",
      "Epoch [1/2] | Batch 206 | loss: 0.2380257546901703 | lr: 0.1\n",
      "Epoch [1/2] | Batch 207 | loss: 0.2434658408164978 | lr: 0.1\n",
      "Epoch [1/2] | Batch 208 | loss: 0.3637693524360657 | lr: 0.1\n",
      "Epoch [1/2] | Batch 209 | loss: 0.2960079312324524 | lr: 0.1\n",
      "Epoch [1/2] | Batch 210 | loss: 0.2713044583797455 | lr: 0.1\n",
      "Epoch [1/2] | Batch 211 | loss: 0.302663654088974 | lr: 0.1\n",
      "Epoch [1/2] | Batch 212 | loss: 0.2864249646663666 | lr: 0.1\n",
      "Epoch [1/2] | Batch 213 | loss: 0.25216808915138245 | lr: 0.1\n",
      "Epoch [1/2] | Batch 214 | loss: 0.2798020541667938 | lr: 0.1\n",
      "Epoch [1/2] | Batch 215 | loss: 0.27788498997688293 | lr: 0.1\n",
      "Epoch [1/2] | Batch 216 | loss: 0.2003515660762787 | lr: 0.1\n",
      "Epoch [1/2] | Batch 217 | loss: 0.18737275898456573 | lr: 0.1\n",
      "Epoch [1/2] | Batch 218 | loss: 0.26446229219436646 | lr: 0.1\n",
      "Epoch [1/2] | Batch 219 | loss: 0.3485310673713684 | lr: 0.1\n",
      "Epoch [1/2] | Batch 220 | loss: 0.18976403772830963 | lr: 0.1\n",
      "Epoch [1/2] | Batch 221 | loss: 0.19589927792549133 | lr: 0.1\n",
      "Epoch [1/2] | Batch 222 | loss: 0.2329660803079605 | lr: 0.1\n",
      "Epoch [1/2] | Batch 223 | loss: 0.22042258083820343 | lr: 0.1\n",
      "Epoch [1/2] | Batch 224 | loss: 0.35251709818840027 | lr: 0.1\n",
      "Epoch [1/2] | Batch 225 | loss: 0.2957666516304016 | lr: 0.1\n",
      "Epoch [1/2] | Batch 226 | loss: 0.2454596906900406 | lr: 0.1\n",
      "Epoch [1/2] | Batch 227 | loss: 0.1766628623008728 | lr: 0.1\n",
      "Epoch [1/2] | Batch 228 | loss: 0.2883736193180084 | lr: 0.1\n",
      "Epoch [1/2] | Batch 229 | loss: 0.1341269314289093 | lr: 0.1\n",
      "Epoch [1/2] | Batch 230 | loss: 0.2302079200744629 | lr: 0.1\n",
      "Epoch [1/2] | Batch 231 | loss: 0.34010574221611023 | lr: 0.1\n",
      "Epoch [1/2] | Batch 232 | loss: 0.15794581174850464 | lr: 0.1\n",
      "Epoch [1/2] | Batch 233 | loss: 0.4480244517326355 | lr: 0.1\n",
      "Epoch [1/2] | Batch 234 | loss: 0.23688217997550964 | lr: 0.1\n",
      "Epoch [1/2] | Batch 235 | loss: 0.3376087248325348 | lr: 0.1\n",
      "Epoch [1/2] | Batch 236 | loss: 0.22010080516338348 | lr: 0.1\n",
      "Epoch [1/2] | Batch 237 | loss: 0.26442912220954895 | lr: 0.1\n",
      "Epoch [1/2] | Batch 238 | loss: 0.3355007767677307 | lr: 0.1\n",
      "Epoch [1/2] | Batch 239 | loss: 0.2560855746269226 | lr: 0.1\n",
      "Epoch [1/2] | Batch 240 | loss: 0.5581087470054626 | lr: 0.1\n",
      "Epoch [1/2] | Batch 241 | loss: 0.24225206673145294 | lr: 0.1\n",
      "Epoch [1/2] | Batch 242 | loss: 0.25170111656188965 | lr: 0.1\n",
      "Epoch [1/2] | Batch 243 | loss: 0.2718338966369629 | lr: 0.1\n",
      "Epoch [1/2] | Batch 244 | loss: 0.4095340371131897 | lr: 0.1\n",
      "Epoch [1/2] | Batch 245 | loss: 0.24832119047641754 | lr: 0.1\n",
      "Epoch [1/2] | Batch 246 | loss: 0.1527458280324936 | lr: 0.1\n",
      "Epoch [1/2] | Batch 247 | loss: 0.38871777057647705 | lr: 0.1\n",
      "Epoch [1/2] | Batch 248 | loss: 0.26394087076187134 | lr: 0.1\n",
      "Epoch [1/2] | Batch 249 | loss: 0.1512356549501419 | lr: 0.1\n",
      "Epoch [1/2] | Batch 250 | loss: 0.26402032375335693 | lr: 0.1\n",
      "Epoch [1/2] | Batch 251 | loss: 0.17355084419250488 | lr: 0.1\n",
      "Epoch [1/2] | Batch 252 | loss: 0.1790166199207306 | lr: 0.1\n",
      "Epoch [1/2] | Batch 253 | loss: 0.37342894077301025 | lr: 0.1\n",
      "Epoch [1/2] | Batch 254 | loss: 0.18545670807361603 | lr: 0.1\n",
      "Epoch [1/2] | Batch 255 | loss: 0.20415231585502625 | lr: 0.1\n",
      "Epoch [1/2] | Batch 256 | loss: 0.3005250096321106 | lr: 0.1\n",
      "Epoch [1/2] | Batch 257 | loss: 0.2646923065185547 | lr: 0.1\n",
      "Epoch [1/2] | Batch 258 | loss: 0.3062346875667572 | lr: 0.1\n",
      "Epoch [1/2] | Batch 259 | loss: 0.30585768818855286 | lr: 0.1\n",
      "Epoch [1/2] | Batch 260 | loss: 0.3976427912712097 | lr: 0.1\n",
      "Epoch [1/2] | Batch 261 | loss: 0.3229275941848755 | lr: 0.1\n",
      "Epoch [1/2] | Batch 262 | loss: 0.20165687799453735 | lr: 0.1\n",
      "Epoch [1/2] | Batch 263 | loss: 0.36352238059043884 | lr: 0.1\n",
      "Epoch [1/2] | Batch 264 | loss: 0.1348734050989151 | lr: 0.1\n",
      "Epoch [1/2] | Batch 265 | loss: 0.1248416006565094 | lr: 0.1\n",
      "Epoch [1/2] | Batch 266 | loss: 0.27903303503990173 | lr: 0.1\n",
      "Epoch [1/2] | Batch 267 | loss: 0.36186957359313965 | lr: 0.1\n",
      "Epoch [1/2] | Batch 268 | loss: 0.1407301127910614 | lr: 0.1\n",
      "Epoch [1/2] | Batch 269 | loss: 0.47414928674697876 | lr: 0.1\n",
      "Epoch [1/2] | Batch 270 | loss: 0.2960417568683624 | lr: 0.1\n",
      "Epoch [1/2] | Batch 271 | loss: 0.3184962272644043 | lr: 0.1\n",
      "Epoch [1/2] | Batch 272 | loss: 0.46402716636657715 | lr: 0.1\n",
      "Epoch [1/2] | Batch 273 | loss: 0.3291066586971283 | lr: 0.1\n",
      "Epoch [1/2] | Batch 274 | loss: 0.22648659348487854 | lr: 0.1\n",
      "Epoch [1/2] | Batch 275 | loss: 0.38397881388664246 | lr: 0.1\n",
      "Epoch [1/2] | Batch 276 | loss: 0.17001399397850037 | lr: 0.1\n",
      "Epoch [1/2] | Batch 277 | loss: 0.32979846000671387 | lr: 0.1\n",
      "Epoch [1/2] | Batch 278 | loss: 0.08268057554960251 | lr: 0.1\n",
      "Epoch [1/2] | Batch 279 | loss: 0.21721313893795013 | lr: 0.1\n",
      "Epoch [1/2] | Batch 280 | loss: 0.1578073501586914 | lr: 0.1\n",
      "Epoch [1/2] | Batch 281 | loss: 0.37680909037590027 | lr: 0.1\n",
      "Epoch [1/2] | Batch 282 | loss: 0.5246806144714355 | lr: 0.1\n",
      "Epoch [1/2] | Batch 283 | loss: 0.28535306453704834 | lr: 0.1\n",
      "Epoch [1/2] | Batch 284 | loss: 0.16582711040973663 | lr: 0.1\n",
      "Epoch [1/2] | Batch 285 | loss: 0.20479479432106018 | lr: 0.1\n",
      "Epoch [1/2] | Batch 286 | loss: 0.14207379519939423 | lr: 0.1\n",
      "Epoch [1/2] | Batch 287 | loss: 0.24600312113761902 | lr: 0.1\n",
      "Epoch [1/2] | Batch 288 | loss: 0.2711866497993469 | lr: 0.1\n",
      "Epoch [1/2] | Batch 289 | loss: 0.32480424642562866 | lr: 0.1\n",
      "Epoch [1/2] | Batch 290 | loss: 0.21893613040447235 | lr: 0.1\n",
      "Epoch [1/2] | Batch 291 | loss: 0.13661175966262817 | lr: 0.1\n",
      "Epoch [1/2] | Batch 292 | loss: 0.18862785398960114 | lr: 0.1\n",
      "Epoch [1/2] | Batch 293 | loss: 0.4103611409664154 | lr: 0.1\n",
      "Epoch [1/2] | Batch 294 | loss: 0.15257331728935242 | lr: 0.1\n",
      "Epoch [1/2] | Batch 295 | loss: 0.12925060093402863 | lr: 0.1\n",
      "Epoch [1/2] | Batch 296 | loss: 0.19137734174728394 | lr: 0.1\n",
      "Epoch [1/2] | Batch 297 | loss: 0.26163455843925476 | lr: 0.1\n",
      "Epoch [1/2] | Batch 298 | loss: 0.08400042355060577 | lr: 0.1\n",
      "Epoch [1/2] | Batch 299 | loss: 0.2273174673318863 | lr: 0.1\n",
      "Epoch [1/2] | Batch 300 | loss: 0.31754711270332336 | lr: 0.1\n",
      "Epoch [1/2] | Batch 301 | loss: 0.23882600665092468 | lr: 0.1\n",
      "Epoch [1/2] | Batch 302 | loss: 0.17750737071037292 | lr: 0.1\n",
      "Epoch [1/2] | Batch 303 | loss: 0.19023464620113373 | lr: 0.1\n",
      "Epoch [1/2] | Batch 304 | loss: 0.2439015507698059 | lr: 0.1\n",
      "Epoch [1/2] | Batch 305 | loss: 0.13294725120067596 | lr: 0.1\n",
      "Epoch [1/2] | Batch 306 | loss: 0.263781875371933 | lr: 0.1\n",
      "Epoch [1/2] | Batch 307 | loss: 0.23730003833770752 | lr: 0.1\n",
      "Epoch [1/2] | Batch 308 | loss: 0.28145238757133484 | lr: 0.1\n",
      "Epoch [1/2] | Batch 309 | loss: 0.31303784251213074 | lr: 0.1\n",
      "Epoch [1/2] | Batch 310 | loss: 0.26142919063568115 | lr: 0.1\n",
      "Epoch [1/2] | Batch 311 | loss: 0.35476353764533997 | lr: 0.1\n",
      "Epoch [1/2] | Batch 312 | loss: 0.5320768356323242 | lr: 0.1\n",
      "Epoch [1/2] | Batch 313 | loss: 0.17638777196407318 | lr: 0.1\n",
      "Epoch [1/2] | Batch 314 | loss: 0.33081039786338806 | lr: 0.1\n",
      "Epoch [1/2] | Batch 315 | loss: 0.16069449484348297 | lr: 0.1\n",
      "Epoch [1/2] | Batch 316 | loss: 0.2862561345100403 | lr: 0.1\n",
      "Epoch [1/2] | Batch 317 | loss: 0.27342164516448975 | lr: 0.1\n",
      "Epoch [1/2] | Batch 318 | loss: 0.23677749931812286 | lr: 0.1\n",
      "Epoch [1/2] | Batch 319 | loss: 0.38027194142341614 | lr: 0.1\n",
      "Epoch [1/2] | Batch 320 | loss: 0.17967776954174042 | lr: 0.1\n",
      "Epoch [1/2] | Batch 321 | loss: 0.15905185043811798 | lr: 0.1\n",
      "Epoch [1/2] | Batch 322 | loss: 0.17419888079166412 | lr: 0.1\n",
      "Epoch [1/2] | Batch 323 | loss: 0.38843390345573425 | lr: 0.1\n",
      "Epoch [1/2] | Batch 324 | loss: 0.21685223281383514 | lr: 0.1\n",
      "Epoch [1/2] | Batch 325 | loss: 0.38464871048927307 | lr: 0.1\n",
      "Epoch [1/2] | Batch 326 | loss: 0.33877646923065186 | lr: 0.1\n",
      "Epoch [1/2] | Batch 327 | loss: 0.2140837162733078 | lr: 0.1\n",
      "Epoch [1/2] | Batch 328 | loss: 0.3087720572948456 | lr: 0.1\n",
      "Epoch [1/2] | Batch 329 | loss: 0.24254192411899567 | lr: 0.1\n",
      "Epoch [1/2] | Batch 330 | loss: 0.29870831966400146 | lr: 0.1\n",
      "Epoch [1/2] | Batch 331 | loss: 0.13900503516197205 | lr: 0.1\n",
      "Epoch [1/2] | Batch 332 | loss: 0.2891474664211273 | lr: 0.1\n",
      "Epoch [1/2] | Batch 333 | loss: 0.1626831591129303 | lr: 0.1\n",
      "Epoch [1/2] | Batch 334 | loss: 0.2867553234100342 | lr: 0.1\n",
      "Epoch [1/2] | Batch 335 | loss: 0.38164812326431274 | lr: 0.1\n",
      "Epoch [1/2] | Batch 336 | loss: 0.4889741539955139 | lr: 0.1\n",
      "Epoch [1/2] | Batch 337 | loss: 0.3504737615585327 | lr: 0.1\n",
      "Epoch [1/2] | Batch 338 | loss: 0.1679956316947937 | lr: 0.1\n",
      "Epoch [1/2] | Batch 339 | loss: 0.2119361162185669 | lr: 0.1\n",
      "Epoch [1/2] | Batch 340 | loss: 0.3947770595550537 | lr: 0.1\n",
      "Epoch [1/2] | Batch 341 | loss: 0.3312980830669403 | lr: 0.1\n",
      "Epoch [1/2] | Batch 342 | loss: 0.16828584671020508 | lr: 0.1\n",
      "Epoch [1/2] | Batch 343 | loss: 0.23571300506591797 | lr: 0.1\n",
      "Epoch [1/2] | Batch 344 | loss: 0.08197595179080963 | lr: 0.1\n",
      "Epoch [1/2] | Batch 345 | loss: 0.289054811000824 | lr: 0.1\n",
      "Epoch [1/2] | Batch 346 | loss: 0.3042638599872589 | lr: 0.1\n",
      "Epoch [1/2] | Batch 347 | loss: 0.31973424553871155 | lr: 0.1\n",
      "Epoch [1/2] | Batch 348 | loss: 0.1300528198480606 | lr: 0.1\n",
      "Epoch [1/2] | Batch 349 | loss: 0.28408563137054443 | lr: 0.1\n",
      "Epoch [1/2] | Batch 350 | loss: 0.31230437755584717 | lr: 0.1\n",
      "Epoch [1/2] | Batch 351 | loss: 0.10785453021526337 | lr: 0.1\n",
      "Epoch [1/2] | Batch 352 | loss: 0.1983896791934967 | lr: 0.1\n",
      "Epoch [1/2] | Batch 353 | loss: 0.16944174468517303 | lr: 0.1\n",
      "Epoch [1/2] | Batch 354 | loss: 0.22398093342781067 | lr: 0.1\n",
      "Epoch [1/2] | Batch 355 | loss: 0.1685401052236557 | lr: 0.1\n",
      "Epoch [1/2] | Batch 356 | loss: 0.2725299894809723 | lr: 0.1\n",
      "Epoch [1/2] | Batch 357 | loss: 0.37979668378829956 | lr: 0.1\n",
      "Epoch [1/2] | Batch 358 | loss: 0.18832936882972717 | lr: 0.1\n",
      "Epoch [1/2] | Batch 359 | loss: 0.23898518085479736 | lr: 0.1\n",
      "Epoch [1/2] | Batch 360 | loss: 0.09393806010484695 | lr: 0.1\n",
      "Epoch [1/2] | Batch 361 | loss: 0.36743345856666565 | lr: 0.1\n",
      "Epoch [1/2] | Batch 362 | loss: 0.2314973920583725 | lr: 0.1\n",
      "Epoch [1/2] | Batch 363 | loss: 0.15963920950889587 | lr: 0.1\n",
      "Epoch [1/2] | Batch 364 | loss: 0.42334437370300293 | lr: 0.1\n",
      "Epoch [1/2] | Batch 365 | loss: 0.23054036498069763 | lr: 0.1\n",
      "Epoch [1/2] | Batch 366 | loss: 0.14603163301944733 | lr: 0.1\n",
      "Epoch [1/2] | Batch 367 | loss: 0.2874274253845215 | lr: 0.1\n",
      "Epoch [1/2] | Batch 368 | loss: 0.1409764438867569 | lr: 0.1\n",
      "Epoch [1/2] | Batch 369 | loss: 0.3238486647605896 | lr: 0.1\n",
      "Epoch [1/2] | Batch 370 | loss: 0.2694031000137329 | lr: 0.1\n",
      "Epoch [1/2] | Batch 371 | loss: 0.21054239571094513 | lr: 0.1\n",
      "Epoch [1/2] | Batch 372 | loss: 0.3368094861507416 | lr: 0.1\n",
      "Epoch [1/2] | Batch 373 | loss: 0.2385399341583252 | lr: 0.1\n",
      "Epoch [1/2] | Batch 374 | loss: 0.3482164740562439 | lr: 0.1\n",
      "Epoch [1/2] | Batch 375 | loss: 0.17677398025989532 | lr: 0.1\n",
      "Epoch [1/2] | Batch 376 | loss: 0.21343421936035156 | lr: 0.1\n",
      "Epoch [1/2] | Batch 377 | loss: 0.572363555431366 | lr: 0.1\n",
      "Epoch [1/2] | Batch 378 | loss: 0.2480553388595581 | lr: 0.1\n",
      "Epoch [1/2] | Batch 379 | loss: 0.10828332602977753 | lr: 0.1\n",
      "Epoch [1/2] | Batch 380 | loss: 0.28367117047309875 | lr: 0.1\n",
      "Epoch [1/2] | Batch 381 | loss: 0.09381838142871857 | lr: 0.1\n",
      "Epoch [1/2] | Batch 382 | loss: 0.3012762665748596 | lr: 0.1\n",
      "Epoch [1/2] | Batch 383 | loss: 0.3640955686569214 | lr: 0.1\n",
      "Epoch [1/2] | Batch 384 | loss: 0.29200732707977295 | lr: 0.1\n",
      "Epoch [1/2] | Batch 385 | loss: 0.25617507100105286 | lr: 0.1\n",
      "Epoch [1/2] | Batch 386 | loss: 0.29951611161231995 | lr: 0.1\n",
      "Epoch [1/2] | Batch 387 | loss: 0.3146558403968811 | lr: 0.1\n",
      "Epoch [1/2] | Batch 388 | loss: 0.26793962717056274 | lr: 0.1\n",
      "Epoch [1/2] | Batch 389 | loss: 0.18418335914611816 | lr: 0.1\n",
      "Epoch [1/2] | Batch 390 | loss: 0.2626820206642151 | lr: 0.1\n",
      "Epoch [1/2] | Batch 391 | loss: 0.11620655655860901 | lr: 0.1\n",
      "Epoch [1/2] | Batch 392 | loss: 0.12650741636753082 | lr: 0.1\n",
      "Epoch [1/2] | Batch 393 | loss: 0.29017338156700134 | lr: 0.1\n",
      "Epoch [1/2] | Batch 394 | loss: 0.13099895417690277 | lr: 0.1\n",
      "Epoch [1/2] | Batch 395 | loss: 0.15676605701446533 | lr: 0.1\n",
      "Epoch [1/2] | Batch 396 | loss: 0.3034910559654236 | lr: 0.1\n",
      "Epoch [1/2] | Batch 397 | loss: 0.36302128434181213 | lr: 0.1\n",
      "Epoch [1/2] | Batch 398 | loss: 0.2675427794456482 | lr: 0.1\n",
      "Epoch [1/2] | Batch 399 | loss: 0.21835248172283173 | lr: 0.1\n",
      "Epoch [1/2] | Batch 400 | loss: 0.1438540816307068 | lr: 0.1\n",
      "Epoch [1/2] | Batch 401 | loss: 0.16563405096530914 | lr: 0.1\n",
      "Epoch [1/2] | Batch 402 | loss: 0.1406298130750656 | lr: 0.1\n",
      "Epoch [1/2] | Batch 403 | loss: 0.22339919209480286 | lr: 0.1\n",
      "Epoch [1/2] | Batch 404 | loss: 0.3927954137325287 | lr: 0.1\n",
      "Epoch [1/2] | Batch 405 | loss: 0.4065020680427551 | lr: 0.1\n",
      "Epoch [1/2] | Batch 406 | loss: 0.21289728581905365 | lr: 0.1\n",
      "Epoch [1/2] | Batch 407 | loss: 0.21406109631061554 | lr: 0.1\n",
      "Epoch [1/2] | Batch 408 | loss: 0.18606650829315186 | lr: 0.1\n",
      "Epoch [1/2] | Batch 409 | loss: 0.4892021119594574 | lr: 0.1\n",
      "Epoch [1/2] | Batch 410 | loss: 0.1970404088497162 | lr: 0.1\n",
      "Epoch [1/2] | Batch 411 | loss: 0.22163373231887817 | lr: 0.1\n",
      "Epoch [1/2] | Batch 412 | loss: 0.1454673409461975 | lr: 0.1\n",
      "Epoch [1/2] | Batch 413 | loss: 0.20156656205654144 | lr: 0.1\n",
      "Epoch [1/2] | Batch 414 | loss: 0.17462627589702606 | lr: 0.1\n",
      "Epoch [1/2] | Batch 415 | loss: 0.18469105660915375 | lr: 0.1\n",
      "Epoch [1/2] | Batch 416 | loss: 0.21771012246608734 | lr: 0.1\n",
      "Epoch [1/2] | Batch 417 | loss: 0.22468611598014832 | lr: 0.1\n",
      "Epoch [1/2] | Batch 418 | loss: 0.3905552327632904 | lr: 0.1\n",
      "Epoch [1/2] | Batch 419 | loss: 0.16220371425151825 | lr: 0.1\n",
      "Epoch [1/2] | Batch 420 | loss: 0.2787417471408844 | lr: 0.1\n",
      "Epoch [1/2] | Batch 421 | loss: 0.26501816511154175 | lr: 0.1\n",
      "Epoch [1/2] | Batch 422 | loss: 0.1551390290260315 | lr: 0.1\n",
      "Epoch [1/2] | Batch 423 | loss: 0.1730901151895523 | lr: 0.1\n",
      "Epoch [1/2] | Batch 424 | loss: 0.37678760290145874 | lr: 0.1\n",
      "Epoch [1/2] | Batch 425 | loss: 0.1335427165031433 | lr: 0.1\n",
      "Epoch [1/2] | Batch 426 | loss: 0.36788052320480347 | lr: 0.1\n",
      "Epoch [1/2] | Batch 427 | loss: 0.2570492625236511 | lr: 0.1\n",
      "Epoch [1/2] | Batch 428 | loss: 0.27405330538749695 | lr: 0.1\n",
      "Epoch [1/2] | Batch 429 | loss: 0.21189430356025696 | lr: 0.1\n",
      "Epoch [1/2] | Batch 430 | loss: 0.3106409013271332 | lr: 0.1\n",
      "Epoch [1/2] | Batch 431 | loss: 0.24353504180908203 | lr: 0.1\n",
      "Epoch [1/2] | Batch 432 | loss: 0.24001415073871613 | lr: 0.1\n",
      "Epoch [1/2] | Batch 433 | loss: 0.21649213135242462 | lr: 0.1\n",
      "Epoch [1/2] | Batch 434 | loss: 0.1597708910703659 | lr: 0.1\n",
      "Epoch [1/2] | Batch 435 | loss: 0.19768497347831726 | lr: 0.1\n",
      "Epoch [1/2] | Batch 436 | loss: 0.2384326010942459 | lr: 0.1\n",
      "Epoch [1/2] | Batch 437 | loss: 0.14020808041095734 | lr: 0.1\n",
      "Epoch [1/2] | Batch 438 | loss: 0.20913074910640717 | lr: 0.1\n",
      "Epoch [1/2] | Batch 439 | loss: 0.19163627922534943 | lr: 0.1\n",
      "Epoch [1/2] | Batch 440 | loss: 0.2774791419506073 | lr: 0.1\n",
      "Epoch [1/2] | Batch 441 | loss: 0.23678281903266907 | lr: 0.1\n",
      "Epoch [1/2] | Batch 442 | loss: 0.07686598598957062 | lr: 0.1\n",
      "Epoch [1/2] | Batch 443 | loss: 0.11989258974790573 | lr: 0.1\n",
      "Epoch [1/2] | Batch 444 | loss: 0.14242272078990936 | lr: 0.1\n",
      "Epoch [1/2] | Batch 445 | loss: 0.3187411427497864 | lr: 0.1\n",
      "Epoch [1/2] | Batch 446 | loss: 0.3175654411315918 | lr: 0.1\n",
      "Epoch [1/2] | Batch 447 | loss: 0.255860298871994 | lr: 0.1\n",
      "Epoch [1/2] | Batch 448 | loss: 0.13623766601085663 | lr: 0.1\n",
      "Epoch [1/2] | Batch 449 | loss: 0.1864449828863144 | lr: 0.1\n",
      "Epoch [1/2] | Batch 450 | loss: 0.27558955550193787 | lr: 0.1\n",
      "Epoch [1/2] | Batch 451 | loss: 0.13790945708751678 | lr: 0.1\n",
      "Epoch [1/2] | Batch 452 | loss: 0.2677646577358246 | lr: 0.1\n",
      "Epoch [1/2] | Batch 453 | loss: 0.14853674173355103 | lr: 0.1\n",
      "Epoch [1/2] | Batch 454 | loss: 0.24152809381484985 | lr: 0.1\n",
      "Epoch [1/2] | Batch 455 | loss: 0.15472368896007538 | lr: 0.1\n",
      "Epoch [1/2] | Batch 456 | loss: 0.2080497145652771 | lr: 0.1\n",
      "Epoch [1/2] | Batch 457 | loss: 0.2169477939605713 | lr: 0.1\n",
      "Epoch [1/2] | Batch 458 | loss: 0.2398848533630371 | lr: 0.1\n",
      "Epoch [1/2] | Batch 459 | loss: 0.15213996171951294 | lr: 0.1\n",
      "Epoch [1/2] | Batch 460 | loss: 0.3155871033668518 | lr: 0.1\n",
      "Epoch [1/2] | Batch 461 | loss: 0.39103609323501587 | lr: 0.1\n",
      "Epoch [1/2] | Batch 462 | loss: 0.2995256185531616 | lr: 0.1\n",
      "Epoch [1/2] | Batch 463 | loss: 0.16676649451255798 | lr: 0.1\n",
      "Epoch [1/2] | Batch 464 | loss: 0.3167891204357147 | lr: 0.1\n",
      "Epoch [1/2] | Batch 465 | loss: 0.15073081851005554 | lr: 0.1\n",
      "Epoch [1/2] | Batch 466 | loss: 0.3465206027030945 | lr: 0.1\n",
      "Epoch [1/2] | Batch 467 | loss: 0.13522402942180634 | lr: 0.1\n",
      "Epoch [1/2] | Batch 468 | loss: 0.17786994576454163 | lr: 0.1\n",
      "Epoch [1/2] | Batch 469 | loss: 0.2751702070236206 | lr: 0.1\n",
      "Epoch [1/2] | Batch 470 | loss: 0.2518630623817444 | lr: 0.1\n",
      "Epoch [1/2] | Batch 471 | loss: 0.11005572229623795 | lr: 0.1\n",
      "Epoch [1/2] | Batch 472 | loss: 0.07533470541238785 | lr: 0.1\n",
      "Epoch [1/2] | Batch 473 | loss: 0.25787219405174255 | lr: 0.1\n",
      "Epoch [1/2] | Batch 474 | loss: 0.14742538332939148 | lr: 0.1\n",
      "Epoch [1/2] | Batch 475 | loss: 0.19032888114452362 | lr: 0.1\n",
      "Epoch [1/2] | Batch 476 | loss: 0.12282659858465195 | lr: 0.1\n",
      "Epoch [1/2] | Batch 477 | loss: 0.08284753561019897 | lr: 0.1\n",
      "Epoch [1/2] | Batch 478 | loss: 0.22277377545833588 | lr: 0.1\n",
      "Epoch [1/2] | Batch 479 | loss: 0.21072763204574585 | lr: 0.1\n",
      "Epoch [1/2] | Batch 480 | loss: 0.3883258104324341 | lr: 0.1\n",
      "Epoch [1/2] | Batch 481 | loss: 0.2576412856578827 | lr: 0.1\n",
      "Epoch [1/2] | Batch 482 | loss: 0.06257685273885727 | lr: 0.1\n",
      "Epoch [1/2] | Batch 483 | loss: 0.2105909287929535 | lr: 0.1\n",
      "Epoch [1/2] | Batch 484 | loss: 0.18505237996578217 | lr: 0.1\n",
      "Epoch [1/2] | Batch 485 | loss: 0.3744499087333679 | lr: 0.1\n",
      "Epoch [1/2] | Batch 486 | loss: 0.23610633611679077 | lr: 0.1\n",
      "Epoch [1/2] | Batch 487 | loss: 0.4051540195941925 | lr: 0.1\n",
      "Epoch [1/2] | Batch 488 | loss: 0.16173617541790009 | lr: 0.1\n",
      "Epoch [1/2] | Batch 489 | loss: 0.23448589444160461 | lr: 0.1\n",
      "Epoch [1/2] | Batch 490 | loss: 0.200322687625885 | lr: 0.1\n",
      "Epoch [1/2] | Batch 491 | loss: 0.24508966505527496 | lr: 0.1\n",
      "Epoch [1/2] | Batch 492 | loss: 0.151367649435997 | lr: 0.1\n",
      "Epoch [1/2] | Batch 493 | loss: 0.21688231825828552 | lr: 0.1\n",
      "Epoch [1/2] | Batch 494 | loss: 0.1858917772769928 | lr: 0.1\n",
      "Epoch [1/2] | Batch 495 | loss: 0.23145820200443268 | lr: 0.1\n",
      "Epoch [1/2] | Batch 496 | loss: 0.13444884121418 | lr: 0.1\n",
      "Epoch [1/2] | Batch 497 | loss: 0.2669358551502228 | lr: 0.1\n",
      "Epoch [1/2] | Batch 498 | loss: 0.21434178948402405 | lr: 0.1\n",
      "Epoch [1/2] | Batch 499 | loss: 0.28988152742385864 | lr: 0.1\n",
      "Epoch [1/2] | Batch 500 | loss: 0.20863772928714752 | lr: 0.1\n",
      "Epoch [1/2] | Batch 501 | loss: 0.2712450325489044 | lr: 0.1\n",
      "Epoch [1/2] | Batch 502 | loss: 0.1437184065580368 | lr: 0.1\n",
      "Epoch [1/2] | Batch 503 | loss: 0.13613027334213257 | lr: 0.1\n",
      "Epoch [1/2] | Batch 504 | loss: 0.20166265964508057 | lr: 0.1\n",
      "Epoch [1/2] | Batch 505 | loss: 0.160987988114357 | lr: 0.1\n",
      "Epoch [1/2] | Batch 506 | loss: 0.21195928752422333 | lr: 0.1\n",
      "Epoch [1/2] | Batch 507 | loss: 0.24851562082767487 | lr: 0.1\n",
      "Epoch [1/2] | Batch 508 | loss: 0.17494788765907288 | lr: 0.1\n",
      "Epoch [1/2] | Batch 509 | loss: 0.11060788482427597 | lr: 0.1\n",
      "Epoch [1/2] | Batch 510 | loss: 0.09448587894439697 | lr: 0.1\n",
      "Epoch [1/2] | Batch 511 | loss: 0.17947067320346832 | lr: 0.1\n",
      "Epoch [1/2] | Batch 512 | loss: 0.04725296422839165 | lr: 0.1\n",
      "Epoch [1/2] | Batch 513 | loss: 0.23083491623401642 | lr: 0.1\n",
      "Epoch [1/2] | Batch 514 | loss: 0.29110124707221985 | lr: 0.1\n",
      "Epoch [1/2] | Batch 515 | loss: 0.19967065751552582 | lr: 0.1\n",
      "Epoch [1/2] | Batch 516 | loss: 0.06086697056889534 | lr: 0.1\n",
      "Epoch [1/2] | Batch 517 | loss: 0.12703466415405273 | lr: 0.1\n",
      "Epoch [1/2] | Batch 518 | loss: 0.10729774832725525 | lr: 0.1\n",
      "Epoch [1/2] | Batch 519 | loss: 0.29194357991218567 | lr: 0.1\n",
      "Epoch [1/2] | Batch 520 | loss: 0.1458411067724228 | lr: 0.1\n",
      "Epoch [1/2] | Batch 521 | loss: 0.2214680314064026 | lr: 0.1\n",
      "Epoch [1/2] | Batch 522 | loss: 0.1652986705303192 | lr: 0.1\n",
      "Epoch [1/2] | Batch 523 | loss: 0.12997353076934814 | lr: 0.1\n",
      "Epoch [1/2] | Batch 524 | loss: 0.0825284793972969 | lr: 0.1\n",
      "Epoch [1/2] | Batch 525 | loss: 0.10874815285205841 | lr: 0.1\n",
      "Epoch [1/2] | Batch 526 | loss: 0.20967885851860046 | lr: 0.1\n",
      "Epoch [1/2] | Batch 527 | loss: 0.1311042457818985 | lr: 0.1\n",
      "Epoch [1/2] | Batch 528 | loss: 0.14544835686683655 | lr: 0.1\n",
      "Epoch [1/2] | Batch 529 | loss: 0.11040984094142914 | lr: 0.1\n",
      "Epoch [1/2] | Batch 530 | loss: 0.19357560575008392 | lr: 0.1\n",
      "Epoch [1/2] | Batch 531 | loss: 0.21377861499786377 | lr: 0.1\n",
      "Epoch [1/2] | Batch 532 | loss: 0.2589854598045349 | lr: 0.1\n",
      "Epoch [1/2] | Batch 533 | loss: 0.21082328259944916 | lr: 0.1\n",
      "Epoch [1/2] | Batch 534 | loss: 0.11014774441719055 | lr: 0.1\n",
      "Epoch [1/2] | Batch 535 | loss: 0.5030807852745056 | lr: 0.1\n",
      "Epoch [1/2] | Batch 536 | loss: 0.2043994963169098 | lr: 0.1\n",
      "Epoch [1/2] | Batch 537 | loss: 0.2030438333749771 | lr: 0.1\n",
      "Epoch [1/2] | Batch 538 | loss: 0.16187390685081482 | lr: 0.1\n",
      "Epoch [1/2] | Batch 539 | loss: 0.2354024350643158 | lr: 0.1\n",
      "Epoch [1/2] | Batch 540 | loss: 0.3032446503639221 | lr: 0.1\n",
      "Epoch [1/2] | Batch 541 | loss: 0.1259627640247345 | lr: 0.1\n",
      "Epoch [1/2] | Batch 542 | loss: 0.16647811233997345 | lr: 0.1\n",
      "Epoch [1/2] | Batch 543 | loss: 0.2109392136335373 | lr: 0.1\n",
      "Epoch [1/2] | Batch 544 | loss: 0.13615399599075317 | lr: 0.1\n",
      "Epoch [1/2] | Batch 545 | loss: 0.20112194120883942 | lr: 0.1\n",
      "Epoch [1/2] | Batch 546 | loss: 0.2083449363708496 | lr: 0.1\n",
      "Epoch [1/2] | Batch 547 | loss: 0.13084354996681213 | lr: 0.1\n",
      "Epoch [1/2] | Batch 548 | loss: 0.13701075315475464 | lr: 0.1\n",
      "Epoch [1/2] | Batch 549 | loss: 0.1295287311077118 | lr: 0.1\n",
      "Epoch [1/2] | Batch 550 | loss: 0.1813947558403015 | lr: 0.1\n",
      "Epoch [1/2] | Batch 551 | loss: 0.10709790140390396 | lr: 0.1\n",
      "Epoch [1/2] | Batch 552 | loss: 0.127078577876091 | lr: 0.1\n",
      "Epoch [1/2] | Batch 553 | loss: 0.14378198981285095 | lr: 0.1\n",
      "Epoch [1/2] | Batch 554 | loss: 0.08235859870910645 | lr: 0.1\n",
      "Epoch [1/2] | Batch 555 | loss: 0.16031400859355927 | lr: 0.1\n",
      "Epoch [1/2] | Batch 556 | loss: 0.11128595471382141 | lr: 0.1\n",
      "Epoch [1/2] | Batch 557 | loss: 0.09193618595600128 | lr: 0.1\n",
      "Epoch [1/2] | Batch 558 | loss: 0.10552865266799927 | lr: 0.1\n",
      "Epoch [1/2] | Batch 559 | loss: 0.3360210955142975 | lr: 0.1\n",
      "Epoch [1/2] | Batch 560 | loss: 0.13015204668045044 | lr: 0.1\n",
      "Epoch [1/2] | Batch 561 | loss: 0.234772726893425 | lr: 0.1\n",
      "Epoch [1/2] | Batch 562 | loss: 0.36721089482307434 | lr: 0.1\n",
      "Epoch [1/2] | Batch 563 | loss: 0.25897276401519775 | lr: 0.1\n",
      "Epoch [1/2] | Batch 564 | loss: 0.15916183590888977 | lr: 0.1\n",
      "Epoch [1/2] | Batch 565 | loss: 0.18472489714622498 | lr: 0.1\n",
      "Epoch [1/2] | Batch 566 | loss: 0.2006841003894806 | lr: 0.1\n",
      "Epoch [1/2] | Batch 567 | loss: 0.11145210266113281 | lr: 0.1\n",
      "Epoch [1/2] | Batch 568 | loss: 0.16168123483657837 | lr: 0.1\n",
      "Epoch [1/2] | Batch 569 | loss: 0.3111816644668579 | lr: 0.1\n",
      "Epoch [1/2] | Batch 570 | loss: 0.21404796838760376 | lr: 0.1\n",
      "Epoch [1/2] | Batch 571 | loss: 0.07940836995840073 | lr: 0.1\n",
      "Epoch [1/2] | Batch 572 | loss: 0.242031067609787 | lr: 0.1\n",
      "Epoch [1/2] | Batch 573 | loss: 0.17393149435520172 | lr: 0.1\n",
      "Epoch [1/2] | Batch 574 | loss: 0.11150515824556351 | lr: 0.1\n",
      "Epoch [1/2] | Batch 575 | loss: 0.07864341884851456 | lr: 0.1\n",
      "Epoch [1/2] | Batch 576 | loss: 0.2977435290813446 | lr: 0.1\n",
      "Epoch [1/2] | Batch 577 | loss: 0.26198428869247437 | lr: 0.1\n",
      "Epoch [1/2] | Batch 578 | loss: 0.16909244656562805 | lr: 0.1\n",
      "Epoch [1/2] | Batch 579 | loss: 0.13679452240467072 | lr: 0.1\n",
      "Epoch [1/2] | Batch 580 | loss: 0.2144910842180252 | lr: 0.1\n",
      "Epoch [1/2] | Batch 581 | loss: 0.1096964105963707 | lr: 0.1\n",
      "Epoch [1/2] | Batch 582 | loss: 0.17413072288036346 | lr: 0.1\n",
      "Epoch [1/2] | Batch 583 | loss: 0.19766275584697723 | lr: 0.1\n",
      "Epoch [1/2] | Batch 584 | loss: 0.2819153070449829 | lr: 0.1\n",
      "Epoch [1/2] | Batch 585 | loss: 0.1880183219909668 | lr: 0.1\n",
      "Epoch [1/2] | Batch 586 | loss: 0.18954961001873016 | lr: 0.1\n",
      "Epoch [1/2] | Batch 587 | loss: 0.2541714608669281 | lr: 0.1\n",
      "Epoch [1/2] | Batch 588 | loss: 0.12457007169723511 | lr: 0.1\n",
      "Epoch [1/2] | Batch 589 | loss: 0.190673828125 | lr: 0.1\n",
      "Epoch [1/2] | Batch 590 | loss: 0.35506144165992737 | lr: 0.1\n",
      "Epoch [1/2] | Batch 591 | loss: 0.29053089022636414 | lr: 0.1\n",
      "Epoch [1/2] | Batch 592 | loss: 0.13321492075920105 | lr: 0.1\n",
      "Epoch [1/2] | Batch 593 | loss: 0.22245630621910095 | lr: 0.1\n",
      "Epoch [1/2] | Batch 594 | loss: 0.3058100640773773 | lr: 0.1\n",
      "Epoch [1/2] | Batch 595 | loss: 0.28414857387542725 | lr: 0.1\n",
      "Epoch [1/2] | Batch 596 | loss: 0.15052925050258636 | lr: 0.1\n",
      "Epoch [1/2] | Batch 597 | loss: 0.21834582090377808 | lr: 0.1\n",
      "Epoch [1/2] | Batch 598 | loss: 0.35182496905326843 | lr: 0.1\n",
      "Epoch [1/2] | Batch 599 | loss: 0.24145938456058502 | lr: 0.1\n",
      "Epoch [1/2] | Batch 600 | loss: 0.12579554319381714 | lr: 0.1\n",
      "Epoch [1/2] | Batch 601 | loss: 0.26195213198661804 | lr: 0.1\n",
      "Epoch [1/2] | Batch 602 | loss: 0.2133333832025528 | lr: 0.1\n",
      "Epoch [1/2] | Batch 603 | loss: 0.14501991868019104 | lr: 0.1\n",
      "Epoch [1/2] | Batch 604 | loss: 0.18685154616832733 | lr: 0.1\n",
      "Epoch [1/2] | Batch 605 | loss: 0.2594848573207855 | lr: 0.1\n",
      "Epoch [1/2] | Batch 606 | loss: 0.19945533573627472 | lr: 0.1\n",
      "Epoch [1/2] | Batch 607 | loss: 0.1631731390953064 | lr: 0.1\n",
      "Epoch [1/2] | Batch 608 | loss: 0.19411467015743256 | lr: 0.1\n",
      "Epoch [1/2] | Batch 609 | loss: 0.18159720301628113 | lr: 0.1\n",
      "Epoch [1/2] | Batch 610 | loss: 0.09624578803777695 | lr: 0.1\n",
      "Epoch [1/2] | Batch 611 | loss: 0.11333856731653214 | lr: 0.1\n",
      "Epoch [1/2] | Batch 612 | loss: 0.33105024695396423 | lr: 0.1\n",
      "Epoch [1/2] | Batch 613 | loss: 0.0885637104511261 | lr: 0.1\n",
      "Epoch [1/2] | Batch 614 | loss: 0.26432743668556213 | lr: 0.1\n",
      "Epoch [1/2] | Batch 615 | loss: 0.0966235026717186 | lr: 0.1\n",
      "Epoch [1/2] | Batch 616 | loss: 0.1722143143415451 | lr: 0.1\n",
      "Epoch [1/2] | Batch 617 | loss: 0.13851295411586761 | lr: 0.1\n",
      "Epoch [1/2] | Batch 618 | loss: 0.1548280119895935 | lr: 0.1\n",
      "Epoch [1/2] | Batch 619 | loss: 0.14726421236991882 | lr: 0.1\n",
      "Epoch [1/2] | Batch 620 | loss: 0.20652659237384796 | lr: 0.1\n",
      "Epoch [1/2] | Batch 621 | loss: 0.2799665033817291 | lr: 0.1\n",
      "Epoch [1/2] | Batch 622 | loss: 0.18497295677661896 | lr: 0.1\n",
      "Epoch [1/2] | Batch 623 | loss: 0.19901248812675476 | lr: 0.1\n",
      "Epoch [1/2] | Batch 624 | loss: 0.21157748997211456 | lr: 0.1\n",
      "Epoch [1/2] | Batch 625 | loss: 0.38396701216697693 | lr: 0.1\n",
      "Epoch [1/2] | Batch 626 | loss: 0.2079450637102127 | lr: 0.1\n",
      "Epoch [1/2] | Batch 627 | loss: 0.20441997051239014 | lr: 0.1\n",
      "Epoch [1/2] | Batch 628 | loss: 0.20814645290374756 | lr: 0.1\n",
      "Epoch [1/2] | Batch 629 | loss: 0.20329371094703674 | lr: 0.1\n",
      "Epoch [1/2] | Batch 630 | loss: 0.20408499240875244 | lr: 0.1\n",
      "Epoch [1/2] | Batch 631 | loss: 0.12498887628316879 | lr: 0.1\n",
      "Epoch [1/2] | Batch 632 | loss: 0.1373419463634491 | lr: 0.1\n",
      "Epoch [1/2] | Batch 633 | loss: 0.1664579212665558 | lr: 0.1\n",
      "Epoch [1/2] | Batch 634 | loss: 0.12929324805736542 | lr: 0.1\n",
      "Epoch [1/2] | Batch 635 | loss: 0.1258983463048935 | lr: 0.1\n",
      "Epoch [1/2] | Batch 636 | loss: 0.09714657813310623 | lr: 0.1\n",
      "Epoch [1/2] | Batch 637 | loss: 0.14526402950286865 | lr: 0.1\n",
      "Epoch [1/2] | Batch 638 | loss: 0.102585569024086 | lr: 0.1\n",
      "Epoch [1/2] | Batch 639 | loss: 0.05701186880469322 | lr: 0.1\n",
      "Epoch [1/2] | Batch 640 | loss: 0.16788525879383087 | lr: 0.1\n",
      "Epoch [1/2] | Batch 641 | loss: 0.15070125460624695 | lr: 0.1\n",
      "Epoch [1/2] | Batch 642 | loss: 0.20064043998718262 | lr: 0.1\n",
      "Epoch [1/2] | Batch 643 | loss: 0.24743354320526123 | lr: 0.1\n",
      "Epoch [1/2] | Batch 644 | loss: 0.10982373356819153 | lr: 0.1\n",
      "Epoch [1/2] | Batch 645 | loss: 0.21349087357521057 | lr: 0.1\n",
      "Epoch [1/2] | Batch 646 | loss: 0.217532679438591 | lr: 0.1\n",
      "Epoch [1/2] | Batch 647 | loss: 0.23045752942562103 | lr: 0.1\n",
      "Epoch [1/2] | Batch 648 | loss: 0.1910826414823532 | lr: 0.1\n",
      "Epoch [1/2] | Batch 649 | loss: 0.05826681852340698 | lr: 0.1\n",
      "Epoch [1/2] | Batch 650 | loss: 0.2953384816646576 | lr: 0.1\n",
      "Epoch [1/2] | Batch 651 | loss: 0.17282061278820038 | lr: 0.1\n",
      "Epoch [1/2] | Batch 652 | loss: 0.10641199350357056 | lr: 0.1\n",
      "Epoch [1/2] | Batch 653 | loss: 0.2605569064617157 | lr: 0.1\n",
      "Epoch [1/2] | Batch 654 | loss: 0.17180687189102173 | lr: 0.1\n",
      "Epoch [1/2] | Batch 655 | loss: 0.0830174908041954 | lr: 0.1\n",
      "Epoch [1/2] | Batch 656 | loss: 0.20319582521915436 | lr: 0.1\n",
      "Epoch [1/2] | Batch 657 | loss: 0.11498045176267624 | lr: 0.1\n",
      "Epoch [1/2] | Batch 658 | loss: 0.24124474823474884 | lr: 0.1\n",
      "Epoch [1/2] | Batch 659 | loss: 0.209610715508461 | lr: 0.1\n",
      "Epoch [1/2] | Batch 660 | loss: 0.18017354607582092 | lr: 0.1\n",
      "Epoch [1/2] | Batch 661 | loss: 0.13618826866149902 | lr: 0.1\n",
      "Epoch [1/2] | Batch 662 | loss: 0.09486205875873566 | lr: 0.1\n",
      "Epoch [1/2] | Batch 663 | loss: 0.13205260038375854 | lr: 0.1\n",
      "Epoch [1/2] | Batch 664 | loss: 0.35288113355636597 | lr: 0.1\n",
      "Epoch [1/2] | Batch 665 | loss: 0.4910376965999603 | lr: 0.1\n",
      "Epoch [1/2] | Batch 666 | loss: 0.10556185990571976 | lr: 0.1\n",
      "Epoch [1/2] | Batch 667 | loss: 0.1684364527463913 | lr: 0.1\n",
      "Epoch [1/2] | Batch 668 | loss: 0.1583942472934723 | lr: 0.1\n",
      "Epoch [1/2] | Batch 669 | loss: 0.12999123334884644 | lr: 0.1\n",
      "Epoch [1/2] | Batch 670 | loss: 0.1641291230916977 | lr: 0.1\n",
      "Epoch [1/2] | Batch 671 | loss: 0.17214345932006836 | lr: 0.1\n",
      "Epoch [1/2] | Batch 672 | loss: 0.2674236595630646 | lr: 0.1\n",
      "Epoch [1/2] | Batch 673 | loss: 0.09792472422122955 | lr: 0.1\n",
      "Epoch [1/2] | Batch 674 | loss: 0.2372131198644638 | lr: 0.1\n",
      "Epoch [1/2] | Batch 675 | loss: 0.19801662862300873 | lr: 0.1\n",
      "Epoch [1/2] | Batch 676 | loss: 0.24816051125526428 | lr: 0.1\n",
      "Epoch [1/2] | Batch 677 | loss: 0.2551004886627197 | lr: 0.1\n",
      "Epoch [1/2] | Batch 678 | loss: 0.2074940800666809 | lr: 0.1\n",
      "Epoch [1/2] | Batch 679 | loss: 0.0824495181441307 | lr: 0.1\n",
      "Epoch [1/2] | Batch 680 | loss: 0.12050078064203262 | lr: 0.1\n",
      "Epoch [1/2] | Batch 681 | loss: 0.08956601470708847 | lr: 0.1\n",
      "Epoch [1/2] | Batch 682 | loss: 0.19140592217445374 | lr: 0.1\n",
      "Epoch [1/2] | Batch 683 | loss: 0.1849399209022522 | lr: 0.1\n",
      "Epoch [1/2] | Batch 684 | loss: 0.12252304702997208 | lr: 0.1\n",
      "Epoch [1/2] | Batch 685 | loss: 0.12304479628801346 | lr: 0.1\n",
      "Epoch [1/2] | Batch 686 | loss: 0.19568660855293274 | lr: 0.1\n",
      "Epoch [1/2] | Batch 687 | loss: 0.15118248760700226 | lr: 0.1\n",
      "Epoch [1/2] | Batch 688 | loss: 0.3075742721557617 | lr: 0.1\n",
      "Epoch [1/2] | Batch 689 | loss: 0.11275383085012436 | lr: 0.1\n",
      "Epoch [1/2] | Batch 690 | loss: 0.2257552146911621 | lr: 0.1\n",
      "Epoch [1/2] | Batch 691 | loss: 0.219960018992424 | lr: 0.1\n",
      "Epoch [1/2] | Batch 692 | loss: 0.2028738111257553 | lr: 0.1\n",
      "Epoch [1/2] | Batch 693 | loss: 0.18230994045734406 | lr: 0.1\n",
      "Epoch [1/2] | Batch 694 | loss: 0.1777312457561493 | lr: 0.1\n",
      "Epoch [1/2] | Batch 695 | loss: 0.08868218958377838 | lr: 0.1\n",
      "Epoch [1/2] | Batch 696 | loss: 0.2694297134876251 | lr: 0.1\n",
      "Epoch [1/2] | Batch 697 | loss: 0.10966063290834427 | lr: 0.1\n",
      "Epoch [1/2] | Batch 698 | loss: 0.2275332510471344 | lr: 0.1\n",
      "Epoch [1/2] | Batch 699 | loss: 0.2539322078227997 | lr: 0.1\n",
      "Epoch [1/2] | Batch 700 | loss: 0.18778438866138458 | lr: 0.1\n",
      "Epoch [1/2] | Batch 701 | loss: 0.19323605298995972 | lr: 0.1\n",
      "Epoch [1/2] | Batch 702 | loss: 0.3120711147785187 | lr: 0.1\n",
      "Epoch [1/2] | Batch 703 | loss: 0.21084904670715332 | lr: 0.1\n",
      "Epoch [1/2] | Batch 704 | loss: 0.17072466015815735 | lr: 0.1\n",
      "Epoch [1/2] | Batch 705 | loss: 0.21525420248508453 | lr: 0.1\n",
      "Epoch [1/2] | Batch 706 | loss: 0.06914472579956055 | lr: 0.1\n",
      "Epoch [1/2] | Batch 707 | loss: 0.15234965085983276 | lr: 0.1\n",
      "Epoch [1/2] | Batch 708 | loss: 0.0962238758802414 | lr: 0.1\n",
      "Epoch [1/2] | Batch 709 | loss: 0.14548663794994354 | lr: 0.1\n",
      "Epoch [1/2] | Batch 710 | loss: 0.10437643527984619 | lr: 0.1\n",
      "Epoch [1/2] | Batch 711 | loss: 0.18426616489887238 | lr: 0.1\n",
      "Epoch [1/2] | Batch 712 | loss: 0.1254425346851349 | lr: 0.1\n",
      "Epoch [1/2] | Batch 713 | loss: 0.16246549785137177 | lr: 0.1\n",
      "Epoch [1/2] | Batch 714 | loss: 0.08726444095373154 | lr: 0.1\n",
      "Epoch [1/2] | Batch 715 | loss: 0.09590180218219757 | lr: 0.1\n",
      "Epoch [1/2] | Batch 716 | loss: 0.12891048192977905 | lr: 0.1\n",
      "Epoch [1/2] | Batch 717 | loss: 0.15898025035858154 | lr: 0.1\n",
      "Epoch [1/2] | Batch 718 | loss: 0.3393869996070862 | lr: 0.1\n",
      "Epoch [1/2] | Batch 719 | loss: 0.27351707220077515 | lr: 0.1\n",
      "Epoch [1/2] | Batch 720 | loss: 0.14869672060012817 | lr: 0.1\n",
      "Epoch [1/2] | Batch 721 | loss: 0.17450058460235596 | lr: 0.1\n",
      "Epoch [1/2] | Batch 722 | loss: 0.1761913299560547 | lr: 0.1\n",
      "Epoch [1/2] | Batch 723 | loss: 0.17507092654705048 | lr: 0.1\n",
      "Epoch [1/2] | Batch 724 | loss: 0.17569614946842194 | lr: 0.1\n",
      "Epoch [1/2] | Batch 725 | loss: 0.25261032581329346 | lr: 0.1\n",
      "Epoch [1/2] | Batch 726 | loss: 0.38272154331207275 | lr: 0.1\n",
      "Epoch [1/2] | Batch 727 | loss: 0.15148203074932098 | lr: 0.1\n",
      "Epoch [1/2] | Batch 728 | loss: 0.16743868589401245 | lr: 0.1\n",
      "Epoch [1/2] | Batch 729 | loss: 0.2446269392967224 | lr: 0.1\n",
      "Epoch [1/2] | Batch 730 | loss: 0.16724644601345062 | lr: 0.1\n",
      "Epoch [1/2] | Batch 731 | loss: 0.15224327147006989 | lr: 0.1\n",
      "Epoch [1/2] | Batch 732 | loss: 0.1799003779888153 | lr: 0.1\n",
      "Epoch [1/2] | Batch 733 | loss: 0.13246284425258636 | lr: 0.1\n",
      "Epoch [1/2] | Batch 734 | loss: 0.19564320147037506 | lr: 0.1\n",
      "Epoch [1/2] | Batch 735 | loss: 0.12865173816680908 | lr: 0.1\n",
      "Epoch [1/2] | Batch 736 | loss: 0.14997239410877228 | lr: 0.1\n",
      "Epoch [1/2] | Batch 737 | loss: 0.27066200971603394 | lr: 0.1\n",
      "Epoch [1/2] | Batch 738 | loss: 0.17667646706104279 | lr: 0.1\n",
      "Epoch [1/2] | Batch 739 | loss: 0.1795322597026825 | lr: 0.1\n",
      "Epoch [1/2] | Batch 740 | loss: 0.1611679345369339 | lr: 0.1\n",
      "Epoch [1/2] | Batch 741 | loss: 0.27416345477104187 | lr: 0.1\n",
      "Epoch [1/2] | Batch 742 | loss: 0.10776448249816895 | lr: 0.1\n",
      "Epoch [1/2] | Batch 743 | loss: 0.15946567058563232 | lr: 0.1\n",
      "Epoch [1/2] | Batch 744 | loss: 0.16025447845458984 | lr: 0.1\n",
      "Epoch [1/2] | Batch 745 | loss: 0.31593644618988037 | lr: 0.1\n",
      "Epoch [1/2] | Batch 746 | loss: 0.18706829845905304 | lr: 0.1\n",
      "Epoch [1/2] | Batch 747 | loss: 0.09749124199151993 | lr: 0.1\n",
      "Epoch [1/2] | Batch 748 | loss: 0.11415156722068787 | lr: 0.1\n",
      "Epoch [1/2] | Batch 749 | loss: 0.08948298543691635 | lr: 0.1\n",
      "Epoch [1/2] | Batch 750 | loss: 0.20333978533744812 | lr: 0.1\n",
      "Epoch [1/2] | Batch 751 | loss: 0.35060375928878784 | lr: 0.1\n",
      "Epoch [1/2] | Batch 752 | loss: 0.2268279492855072 | lr: 0.1\n",
      "Epoch [1/2] | Batch 753 | loss: 0.12736408412456512 | lr: 0.1\n",
      "Epoch [1/2] | Batch 754 | loss: 0.17124156653881073 | lr: 0.1\n",
      "Epoch [1/2] | Batch 755 | loss: 0.11774276942014694 | lr: 0.1\n",
      "Epoch [1/2] | Batch 756 | loss: 0.06473226845264435 | lr: 0.1\n",
      "Epoch [1/2] | Batch 757 | loss: 0.2218981236219406 | lr: 0.1\n",
      "Epoch [1/2] | Batch 758 | loss: 0.15135283768177032 | lr: 0.1\n",
      "Epoch [1/2] | Batch 759 | loss: 0.07448709011077881 | lr: 0.1\n",
      "Epoch [1/2] | Batch 760 | loss: 0.29578500986099243 | lr: 0.1\n",
      "Epoch [1/2] | Batch 761 | loss: 0.14818282425403595 | lr: 0.1\n",
      "Epoch [1/2] | Batch 762 | loss: 0.07357478141784668 | lr: 0.1\n",
      "Epoch [1/2] | Batch 763 | loss: 0.19816847145557404 | lr: 0.1\n",
      "Epoch [1/2] | Batch 764 | loss: 0.11949847638607025 | lr: 0.1\n",
      "Epoch [1/2] | Batch 765 | loss: 0.17344729602336884 | lr: 0.1\n",
      "Epoch [1/2] | Batch 766 | loss: 0.11647222936153412 | lr: 0.1\n",
      "Epoch [1/2] | Batch 767 | loss: 0.13531287014484406 | lr: 0.1\n",
      "Epoch [1/2] | Batch 768 | loss: 0.11967243254184723 | lr: 0.1\n",
      "Epoch [1/2] | Batch 769 | loss: 0.062037497758865356 | lr: 0.1\n",
      "Epoch [1/2] | Batch 770 | loss: 0.1700906604528427 | lr: 0.1\n",
      "Epoch [1/2] | Batch 771 | loss: 0.10915041714906693 | lr: 0.1\n",
      "Epoch [1/2] | Batch 772 | loss: 0.3160877823829651 | lr: 0.1\n",
      "Epoch [1/2] | Batch 773 | loss: 0.13477785885334015 | lr: 0.1\n",
      "Epoch [1/2] | Batch 774 | loss: 0.1944398134946823 | lr: 0.1\n",
      "Epoch [1/2] | Batch 775 | loss: 0.12979231774806976 | lr: 0.1\n",
      "Epoch [1/2] | Batch 776 | loss: 0.2246716469526291 | lr: 0.1\n",
      "Epoch [1/2] | Batch 777 | loss: 0.08492366224527359 | lr: 0.1\n",
      "Epoch [1/2] | Batch 778 | loss: 0.08519808202981949 | lr: 0.1\n",
      "Epoch [1/2] | Batch 779 | loss: 0.2842254638671875 | lr: 0.1\n",
      "Epoch [1/2] | Batch 780 | loss: 0.277119904756546 | lr: 0.1\n",
      "Epoch [1/2] | Batch 781 | loss: 0.2053375095129013 | lr: 0.1\n",
      "Epoch [1/2] | Batch 782 | loss: 0.1832352727651596 | lr: 0.1\n",
      "Epoch [1/2] | Batch 783 | loss: 0.24963253736495972 | lr: 0.1\n",
      "Epoch [1/2] | Batch 784 | loss: 0.1404012143611908 | lr: 0.1\n",
      "Epoch [1/2] | Batch 785 | loss: 0.2651519775390625 | lr: 0.1\n",
      "Epoch [1/2] | Batch 786 | loss: 0.1316596269607544 | lr: 0.1\n",
      "Epoch [1/2] | Batch 787 | loss: 0.13907748460769653 | lr: 0.1\n",
      "Epoch [1/2] | Batch 788 | loss: 0.14706504344940186 | lr: 0.1\n",
      "Epoch [1/2] | Batch 789 | loss: 0.2150130718946457 | lr: 0.1\n",
      "Epoch [1/2] | Batch 790 | loss: 0.04802973195910454 | lr: 0.1\n",
      "Epoch [1/2] | Batch 791 | loss: 0.19393934309482574 | lr: 0.1\n",
      "Epoch [1/2] | Batch 792 | loss: 0.0690007284283638 | lr: 0.1\n",
      "Epoch [1/2] | Batch 793 | loss: 0.25559037923812866 | lr: 0.1\n",
      "Epoch [1/2] | Batch 794 | loss: 0.3973361849784851 | lr: 0.1\n",
      "Epoch [1/2] | Batch 795 | loss: 0.10672928392887115 | lr: 0.1\n",
      "Epoch [1/2] | Batch 796 | loss: 0.1137295663356781 | lr: 0.1\n",
      "Epoch [1/2] | Batch 797 | loss: 0.10111310333013535 | lr: 0.1\n",
      "Epoch [1/2] | Batch 798 | loss: 0.12120119482278824 | lr: 0.1\n",
      "Epoch [1/2] | Batch 799 | loss: 0.12442280352115631 | lr: 0.1\n",
      "Epoch [1/2] | Batch 800 | loss: 0.15287292003631592 | lr: 0.1\n",
      "Epoch [1/2] | Batch 801 | loss: 0.158974289894104 | lr: 0.1\n",
      "Epoch [1/2] | Batch 802 | loss: 0.19638510048389435 | lr: 0.1\n",
      "Epoch [1/2] | Batch 803 | loss: 0.12078825384378433 | lr: 0.1\n",
      "Epoch [1/2] | Batch 804 | loss: 0.17454974353313446 | lr: 0.1\n",
      "Epoch [1/2] | Batch 805 | loss: 0.05264284461736679 | lr: 0.1\n",
      "Epoch [1/2] | Batch 806 | loss: 0.1355561763048172 | lr: 0.1\n",
      "Epoch [1/2] | Batch 807 | loss: 0.08368294686079025 | lr: 0.1\n",
      "Epoch [1/2] | Batch 808 | loss: 0.15812073647975922 | lr: 0.1\n",
      "Epoch [1/2] | Batch 809 | loss: 0.16025272011756897 | lr: 0.1\n",
      "Epoch [1/2] | Batch 810 | loss: 0.1562444120645523 | lr: 0.1\n",
      "Epoch [1/2] | Batch 811 | loss: 0.14857566356658936 | lr: 0.1\n",
      "Epoch [1/2] | Batch 812 | loss: 0.17791134119033813 | lr: 0.1\n",
      "Epoch [1/2] | Batch 813 | loss: 0.1589748114347458 | lr: 0.1\n",
      "Epoch [1/2] | Batch 814 | loss: 0.15889392793178558 | lr: 0.1\n",
      "Epoch [1/2] | Batch 815 | loss: 0.2535058259963989 | lr: 0.1\n",
      "Epoch [1/2] | Batch 816 | loss: 0.18484005331993103 | lr: 0.1\n",
      "Epoch [1/2] | Batch 817 | loss: 0.18090566992759705 | lr: 0.1\n",
      "Epoch [1/2] | Batch 818 | loss: 0.05682111531496048 | lr: 0.1\n",
      "Epoch [1/2] | Batch 819 | loss: 0.12792961299419403 | lr: 0.1\n",
      "Epoch [1/2] | Batch 820 | loss: 0.11916960030794144 | lr: 0.1\n",
      "Epoch [1/2] | Batch 821 | loss: 0.08472631126642227 | lr: 0.1\n",
      "Epoch [1/2] | Batch 822 | loss: 0.04386473074555397 | lr: 0.1\n",
      "Epoch [1/2] | Batch 823 | loss: 0.072449691593647 | lr: 0.1\n",
      "Epoch [1/2] | Batch 824 | loss: 0.11956679075956345 | lr: 0.1\n",
      "Epoch [1/2] | Batch 825 | loss: 0.06129859387874603 | lr: 0.1\n",
      "Epoch [1/2] | Batch 826 | loss: 0.1249348446726799 | lr: 0.1\n",
      "Epoch [1/2] | Batch 827 | loss: 0.23627282679080963 | lr: 0.1\n",
      "Epoch [1/2] | Batch 828 | loss: 0.10025372356176376 | lr: 0.1\n",
      "Epoch [1/2] | Batch 829 | loss: 0.1786322295665741 | lr: 0.1\n",
      "Epoch [1/2] | Batch 830 | loss: 0.15000078082084656 | lr: 0.1\n",
      "Epoch [1/2] | Batch 831 | loss: 0.16533640027046204 | lr: 0.1\n",
      "Epoch [1/2] | Batch 832 | loss: 0.20820705592632294 | lr: 0.1\n",
      "Epoch [1/2] | Batch 833 | loss: 0.0732201635837555 | lr: 0.1\n",
      "Epoch [1/2] | Batch 834 | loss: 0.1309186965227127 | lr: 0.1\n",
      "Epoch [1/2] | Batch 835 | loss: 0.06967826932668686 | lr: 0.1\n",
      "Epoch [1/2] | Batch 836 | loss: 0.08249041438102722 | lr: 0.1\n",
      "Epoch [1/2] | Batch 837 | loss: 0.10707196593284607 | lr: 0.1\n",
      "Epoch [1/2] | Batch 838 | loss: 0.07483521103858948 | lr: 0.1\n",
      "Epoch [1/2] | Batch 839 | loss: 0.08480262011289597 | lr: 0.1\n",
      "Epoch [1/2] | Batch 840 | loss: 0.08050569146871567 | lr: 0.1\n",
      "Epoch [1/2] | Batch 841 | loss: 0.06028151139616966 | lr: 0.1\n",
      "Epoch [1/2] | Batch 842 | loss: 0.1094239354133606 | lr: 0.1\n",
      "Epoch [1/2] | Batch 843 | loss: 0.1404685229063034 | lr: 0.1\n",
      "Epoch [1/2] | Batch 844 | loss: 0.1401563584804535 | lr: 0.1\n",
      "Epoch [1/2] | Batch 845 | loss: 0.2898837625980377 | lr: 0.1\n",
      "Epoch [1/2] | Batch 846 | loss: 0.08970807492733002 | lr: 0.1\n",
      "Epoch [1/2] | Batch 847 | loss: 0.17169900238513947 | lr: 0.1\n",
      "Epoch [1/2] | Batch 848 | loss: 0.116570845246315 | lr: 0.1\n",
      "Epoch [1/2] | Batch 849 | loss: 0.1762160062789917 | lr: 0.1\n",
      "Epoch [1/2] | Batch 850 | loss: 0.04390603303909302 | lr: 0.1\n",
      "Epoch [1/2] | Batch 851 | loss: 0.09409931302070618 | lr: 0.1\n",
      "Epoch [1/2] | Batch 852 | loss: 0.15519118309020996 | lr: 0.1\n",
      "Epoch [1/2] | Batch 853 | loss: 0.17442181706428528 | lr: 0.1\n",
      "Epoch [1/2] | Batch 854 | loss: 0.06208927184343338 | lr: 0.1\n",
      "Epoch [1/2] | Batch 855 | loss: 0.16560885310173035 | lr: 0.1\n",
      "Epoch [1/2] | Batch 856 | loss: 0.06993308663368225 | lr: 0.1\n",
      "Epoch [1/2] | Batch 857 | loss: 0.07718522101640701 | lr: 0.1\n",
      "Epoch [1/2] | Batch 858 | loss: 0.08499383926391602 | lr: 0.1\n",
      "Epoch [1/2] | Batch 859 | loss: 0.11601785570383072 | lr: 0.1\n",
      "Epoch [1/2] | Batch 860 | loss: 0.04005098715424538 | lr: 0.1\n",
      "Epoch [1/2] | Batch 861 | loss: 0.2007409781217575 | lr: 0.1\n",
      "Epoch [1/2] | Batch 862 | loss: 0.10176151245832443 | lr: 0.1\n",
      "Epoch [1/2] | Batch 863 | loss: 0.04602258279919624 | lr: 0.1\n",
      "Epoch [1/2] | Batch 864 | loss: 0.1307789832353592 | lr: 0.1\n",
      "Epoch [1/2] | Batch 865 | loss: 0.10493124276399612 | lr: 0.1\n",
      "Epoch [1/2] | Batch 866 | loss: 0.15356063842773438 | lr: 0.1\n",
      "Epoch [1/2] | Batch 867 | loss: 0.09350795298814774 | lr: 0.1\n",
      "Epoch [1/2] | Batch 868 | loss: 0.14318309724330902 | lr: 0.1\n",
      "Epoch [1/2] | Batch 869 | loss: 0.2501964569091797 | lr: 0.1\n",
      "Epoch [1/2] | Batch 870 | loss: 0.1719323843717575 | lr: 0.1\n",
      "Epoch [1/2] | Batch 871 | loss: 0.08581165224313736 | lr: 0.1\n",
      "Epoch [1/2] | Batch 872 | loss: 0.2086031585931778 | lr: 0.1\n",
      "Epoch [1/2] | Batch 873 | loss: 0.1376890391111374 | lr: 0.1\n",
      "Epoch [1/2] | Batch 874 | loss: 0.21775130927562714 | lr: 0.1\n",
      "Epoch [1/2] | Batch 875 | loss: 0.16190756857395172 | lr: 0.1\n",
      "Epoch [1/2] | Batch 876 | loss: 0.2701674997806549 | lr: 0.1\n",
      "Epoch [1/2] | Batch 877 | loss: 0.11365136504173279 | lr: 0.1\n",
      "Epoch [1/2] | Batch 878 | loss: 0.08673539757728577 | lr: 0.1\n",
      "Epoch [1/2] | Batch 879 | loss: 0.14444269239902496 | lr: 0.1\n",
      "Epoch [1/2] | Batch 880 | loss: 0.1346665322780609 | lr: 0.1\n",
      "Epoch [1/2] | Batch 881 | loss: 0.2528342306613922 | lr: 0.1\n",
      "Epoch [1/2] | Batch 882 | loss: 0.23169998824596405 | lr: 0.1\n",
      "Epoch [1/2] | Batch 883 | loss: 0.1590307354927063 | lr: 0.1\n",
      "Epoch [1/2] | Batch 884 | loss: 0.1691044569015503 | lr: 0.1\n",
      "Epoch [1/2] | Batch 885 | loss: 0.28626158833503723 | lr: 0.1\n",
      "Epoch [1/2] | Batch 886 | loss: 0.11108384281396866 | lr: 0.1\n",
      "Epoch [1/2] | Batch 887 | loss: 0.08660697191953659 | lr: 0.1\n",
      "Epoch [1/2] | Batch 888 | loss: 0.18752127885818481 | lr: 0.1\n",
      "Epoch [1/2] | Batch 889 | loss: 0.13332097232341766 | lr: 0.1\n",
      "Epoch [1/2] | Batch 890 | loss: 0.10313761234283447 | lr: 0.1\n",
      "Epoch [1/2] | Batch 891 | loss: 0.2535969316959381 | lr: 0.1\n",
      "Epoch [1/2] | Batch 892 | loss: 0.13548250496387482 | lr: 0.1\n",
      "Epoch [1/2] | Batch 893 | loss: 0.1626342236995697 | lr: 0.1\n",
      "Epoch [1/2] | Batch 894 | loss: 0.32363361120224 | lr: 0.1\n",
      "Epoch [1/2] | Batch 895 | loss: 0.09037114679813385 | lr: 0.1\n",
      "Epoch [1/2] | Batch 896 | loss: 0.060637786984443665 | lr: 0.1\n",
      "Epoch [1/2] | Batch 897 | loss: 0.23010560870170593 | lr: 0.1\n",
      "Epoch [1/2] | Batch 898 | loss: 0.44265785813331604 | lr: 0.1\n",
      "Epoch [1/2] | Batch 899 | loss: 0.06955628097057343 | lr: 0.1\n",
      "Epoch [1/2] | Batch 900 | loss: 0.05773288756608963 | lr: 0.1\n",
      "Epoch [1/2] | Batch 901 | loss: 0.18883243203163147 | lr: 0.1\n",
      "Epoch [1/2] | Batch 902 | loss: 0.043525371700525284 | lr: 0.1\n",
      "Epoch [1/2] | Batch 903 | loss: 0.22013187408447266 | lr: 0.1\n",
      "Epoch [1/2] | Batch 904 | loss: 0.11081074923276901 | lr: 0.1\n",
      "Epoch [1/2] | Batch 905 | loss: 0.21940630674362183 | lr: 0.1\n",
      "Epoch [1/2] | Batch 906 | loss: 0.24485722184181213 | lr: 0.1\n",
      "Epoch [1/2] | Batch 907 | loss: 0.1331053227186203 | lr: 0.1\n",
      "Epoch [1/2] | Batch 908 | loss: 0.1724240779876709 | lr: 0.1\n",
      "Epoch [1/2] | Batch 909 | loss: 0.2451491802930832 | lr: 0.1\n",
      "Epoch [1/2] | Batch 910 | loss: 0.16584104299545288 | lr: 0.1\n",
      "Epoch [1/2] | Batch 911 | loss: 0.11801157891750336 | lr: 0.1\n",
      "Epoch [1/2] | Batch 912 | loss: 0.21135561168193817 | lr: 0.1\n",
      "Epoch [1/2] | Batch 913 | loss: 0.13520266115665436 | lr: 0.1\n",
      "Epoch [1/2] | Batch 914 | loss: 0.1755400002002716 | lr: 0.1\n",
      "Epoch [1/2] | Batch 915 | loss: 0.17467176914215088 | lr: 0.1\n",
      "Epoch [1/2] | Batch 916 | loss: 0.13798004388809204 | lr: 0.1\n",
      "Epoch [1/2] | Batch 917 | loss: 0.09143789857625961 | lr: 0.1\n",
      "Epoch [1/2] | Batch 918 | loss: 0.037850964814424515 | lr: 0.1\n",
      "Epoch [1/2] | Batch 919 | loss: 0.14026343822479248 | lr: 0.1\n",
      "Epoch [1/2] | Batch 920 | loss: 0.06729510426521301 | lr: 0.1\n",
      "Epoch [1/2] | Batch 921 | loss: 0.0708073154091835 | lr: 0.1\n",
      "Epoch [1/2] | Batch 922 | loss: 0.19389708340168 | lr: 0.1\n",
      "Epoch [1/2] | Batch 923 | loss: 0.08576894551515579 | lr: 0.1\n",
      "Epoch [1/2] | Batch 924 | loss: 0.06415504962205887 | lr: 0.1\n",
      "Epoch [1/2] | Batch 925 | loss: 0.07640434056520462 | lr: 0.1\n",
      "Epoch [1/2] | Batch 926 | loss: 0.1588081419467926 | lr: 0.1\n",
      "Epoch [1/2] | Batch 927 | loss: 0.13748708367347717 | lr: 0.1\n",
      "Epoch [1/2] | Batch 928 | loss: 0.13360998034477234 | lr: 0.1\n",
      "Epoch [1/2] | Batch 929 | loss: 0.1274697333574295 | lr: 0.1\n",
      "Epoch [1/2] | Batch 930 | loss: 0.1699613481760025 | lr: 0.1\n",
      "Epoch [1/2] | Batch 931 | loss: 0.06925411522388458 | lr: 0.1\n",
      "Epoch [1/2] | Batch 932 | loss: 0.07163459062576294 | lr: 0.1\n",
      "Epoch [1/2] | Batch 933 | loss: 0.0805458053946495 | lr: 0.1\n",
      "Epoch [1/2] | Batch 934 | loss: 0.13185222446918488 | lr: 0.1\n",
      "Epoch [1/2] | Batch 935 | loss: 0.1624201536178589 | lr: 0.1\n",
      "Epoch [1/2] | Batch 936 | loss: 0.14197658002376556 | lr: 0.1\n",
      "Epoch [1/2] | Batch 937 | loss: 0.10675311833620071 | lr: 0.1\n",
      "Epoch [1/2] | Batch 938 | loss: 0.059506285935640335 | lr: 0.1\n",
      "Epoch [2/2] | Batch 1 | loss: 0.2134583592414856 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 2 | loss: 0.14186370372772217 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 3 | loss: 0.15085087716579437 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 4 | loss: 0.3192889392375946 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 5 | loss: 0.1040719747543335 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 6 | loss: 0.08894293755292892 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 7 | loss: 0.056481946259737015 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 8 | loss: 0.1669980138540268 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 9 | loss: 0.0505748949944973 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 10 | loss: 0.13194164633750916 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 11 | loss: 0.07693508267402649 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 12 | loss: 0.10471317917108536 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 13 | loss: 0.21458669006824493 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 14 | loss: 0.06860339641571045 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 15 | loss: 0.09518413990736008 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 16 | loss: 0.1172148585319519 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 17 | loss: 0.136238694190979 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 18 | loss: 0.20301374793052673 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 19 | loss: 0.054489392787218094 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 20 | loss: 0.06203538179397583 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 21 | loss: 0.07341700047254562 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 22 | loss: 0.1458621472120285 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 23 | loss: 0.048245422542095184 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 24 | loss: 0.08060155808925629 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 25 | loss: 0.12244666367769241 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 26 | loss: 0.13458563387393951 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 27 | loss: 0.19212833046913147 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 28 | loss: 0.14259164035320282 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 29 | loss: 0.06910902261734009 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 30 | loss: 0.11246932297945023 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 31 | loss: 0.16841039061546326 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 32 | loss: 0.16073063015937805 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 33 | loss: 0.10416301339864731 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 34 | loss: 0.059544846415519714 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 35 | loss: 0.04006153345108032 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 36 | loss: 0.12399095296859741 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 37 | loss: 0.07011068612337112 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 38 | loss: 0.12577803432941437 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 39 | loss: 0.13162238895893097 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 40 | loss: 0.10137996077537537 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 41 | loss: 0.14364418387413025 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 42 | loss: 0.0545390285551548 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 43 | loss: 0.1363527923822403 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 44 | loss: 0.14303722977638245 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 45 | loss: 0.13638165593147278 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 46 | loss: 0.13914266228675842 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 47 | loss: 0.08876096457242966 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 48 | loss: 0.17643117904663086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 49 | loss: 0.3088702857494354 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 50 | loss: 0.24615873396396637 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 51 | loss: 0.12545758485794067 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 52 | loss: 0.013902305625379086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 53 | loss: 0.15906554460525513 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 54 | loss: 0.07797224074602127 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 55 | loss: 0.16035068035125732 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 56 | loss: 0.14128953218460083 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 57 | loss: 0.14861823618412018 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 58 | loss: 0.10533483326435089 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 59 | loss: 0.06240952014923096 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 60 | loss: 0.12018859386444092 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 61 | loss: 0.07047160714864731 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 62 | loss: 0.07492847740650177 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 63 | loss: 0.13118475675582886 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 64 | loss: 0.061961978673934937 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 65 | loss: 0.045081980526447296 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 66 | loss: 0.1649991124868393 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 67 | loss: 0.16021987795829773 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 68 | loss: 0.14418748021125793 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 69 | loss: 0.05739574506878853 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 70 | loss: 0.056925322860479355 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 71 | loss: 0.10748033225536346 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 72 | loss: 0.06230034679174423 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 73 | loss: 0.15810450911521912 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 74 | loss: 0.13979975879192352 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 75 | loss: 0.09322343021631241 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 76 | loss: 0.035136692225933075 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 77 | loss: 0.07396408915519714 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 78 | loss: 0.10726207494735718 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 79 | loss: 0.06292063742876053 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 80 | loss: 0.12581057846546173 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 81 | loss: 0.029211273416876793 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 82 | loss: 0.10949781537055969 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 83 | loss: 0.029187951236963272 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 84 | loss: 0.12442003935575485 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 85 | loss: 0.052413854748010635 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 86 | loss: 0.15364378690719604 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 87 | loss: 0.08858563005924225 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 88 | loss: 0.1079038456082344 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 89 | loss: 0.0630219429731369 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 90 | loss: 0.16976617276668549 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 91 | loss: 0.1418525129556656 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 92 | loss: 0.2574179172515869 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 93 | loss: 0.1464342176914215 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 94 | loss: 0.12453444302082062 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 95 | loss: 0.2630091905593872 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 96 | loss: 0.11968640238046646 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 97 | loss: 0.29622167348861694 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 98 | loss: 0.08589392900466919 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 99 | loss: 0.14303836226463318 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 100 | loss: 0.12162695825099945 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 101 | loss: 0.1432247906923294 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 102 | loss: 0.08524087816476822 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 103 | loss: 0.16105018556118011 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 104 | loss: 0.04491030424833298 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 105 | loss: 0.13755707442760468 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 106 | loss: 0.11533632129430771 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 107 | loss: 0.15336540341377258 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 108 | loss: 0.061533667147159576 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 109 | loss: 0.11973731219768524 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 110 | loss: 0.08192570507526398 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 111 | loss: 0.15738624334335327 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 112 | loss: 0.08268002420663834 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 113 | loss: 0.07851863652467728 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 114 | loss: 0.24706634879112244 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 115 | loss: 0.09193409234285355 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 116 | loss: 0.07252863049507141 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 117 | loss: 0.0994948148727417 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 118 | loss: 0.12820671498775482 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 119 | loss: 0.09705788642168045 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 120 | loss: 0.05998649448156357 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 121 | loss: 0.052762046456336975 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 122 | loss: 0.11525426805019379 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 123 | loss: 0.0643085241317749 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 124 | loss: 0.11532097309827805 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 125 | loss: 0.1511092633008957 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 126 | loss: 0.016429506242275238 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 127 | loss: 0.19820892810821533 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 128 | loss: 0.10864603519439697 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 129 | loss: 0.08526376634836197 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 130 | loss: 0.09966212511062622 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 131 | loss: 0.09151848405599594 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 132 | loss: 0.15152046084403992 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 133 | loss: 0.08539587259292603 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 134 | loss: 0.19524554908275604 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 135 | loss: 0.24656365811824799 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 136 | loss: 0.34834206104278564 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 137 | loss: 0.25393858551979065 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 138 | loss: 0.0535023994743824 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 139 | loss: 0.061311181634664536 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 140 | loss: 0.17617350816726685 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 141 | loss: 0.09060485661029816 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 142 | loss: 0.06376899033784866 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 143 | loss: 0.0912298932671547 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 144 | loss: 0.08926693350076675 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 145 | loss: 0.157649427652359 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 146 | loss: 0.12172961235046387 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 147 | loss: 0.09300059825181961 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 148 | loss: 0.09178519248962402 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 149 | loss: 0.23118115961551666 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 150 | loss: 0.09391213953495026 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 151 | loss: 0.2341456562280655 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 152 | loss: 0.0845881775021553 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 153 | loss: 0.28140342235565186 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 154 | loss: 0.10511419922113419 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 155 | loss: 0.06009623408317566 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 156 | loss: 0.12852928042411804 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 157 | loss: 0.02830827794969082 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 158 | loss: 0.08236143738031387 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 159 | loss: 0.04336553066968918 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 160 | loss: 0.08741071820259094 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 161 | loss: 0.2501036524772644 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 162 | loss: 0.054229121655225754 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 163 | loss: 0.06564923375844955 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 164 | loss: 0.11073653399944305 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 165 | loss: 0.024876978248357773 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 166 | loss: 0.03167317062616348 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 167 | loss: 0.06105716899037361 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 168 | loss: 0.050497520714998245 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 169 | loss: 0.16836662590503693 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 170 | loss: 0.0641469657421112 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 171 | loss: 0.09593929350376129 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 172 | loss: 0.0708259716629982 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 173 | loss: 0.046547915786504745 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 174 | loss: 0.23806673288345337 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 175 | loss: 0.1305437833070755 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 176 | loss: 0.13858716189861298 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 177 | loss: 0.10509907454252243 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 178 | loss: 0.12388774007558823 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 179 | loss: 0.06556719541549683 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 180 | loss: 0.06900414824485779 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 181 | loss: 0.12344398349523544 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 182 | loss: 0.03785869851708412 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 183 | loss: 0.13570918142795563 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 184 | loss: 0.058068808168172836 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 185 | loss: 0.15947453677654266 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 186 | loss: 0.1255822777748108 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 187 | loss: 0.054345160722732544 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 188 | loss: 0.08639755845069885 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 189 | loss: 0.31936803460121155 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 190 | loss: 0.06728078424930573 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 191 | loss: 0.0375913567841053 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 192 | loss: 0.04722211882472038 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 193 | loss: 0.3391684889793396 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 194 | loss: 0.12022873759269714 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 195 | loss: 0.20952217280864716 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 196 | loss: 0.2522518038749695 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 197 | loss: 0.17683884501457214 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 198 | loss: 0.15210528671741486 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 199 | loss: 0.14970803260803223 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 200 | loss: 0.39303603768348694 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 201 | loss: 0.1424046903848648 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 202 | loss: 0.09733064472675323 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 203 | loss: 0.17407561838626862 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 204 | loss: 0.02951907552778721 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 205 | loss: 0.10778715461492538 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 206 | loss: 0.23429538309574127 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 207 | loss: 0.1501232087612152 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 208 | loss: 0.04765750840306282 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 209 | loss: 0.17580680549144745 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 210 | loss: 0.18560610711574554 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 211 | loss: 0.13001035153865814 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 212 | loss: 0.06335987150669098 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 213 | loss: 0.1163066104054451 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 214 | loss: 0.06197627633810043 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 215 | loss: 0.049512531608343124 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 216 | loss: 0.11547605693340302 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 217 | loss: 0.14656858146190643 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 218 | loss: 0.09986018389463425 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 219 | loss: 0.12519164383411407 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 220 | loss: 0.05663697421550751 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 221 | loss: 0.07664377987384796 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 222 | loss: 0.06327497214078903 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 223 | loss: 0.22386886179447174 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 224 | loss: 0.047561898827552795 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 225 | loss: 0.17183825373649597 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 226 | loss: 0.1493997573852539 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 227 | loss: 0.1543320268392563 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 228 | loss: 0.2633265554904938 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 229 | loss: 0.07191036641597748 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 230 | loss: 0.0594683475792408 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 231 | loss: 0.09214787930250168 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 232 | loss: 0.19840946793556213 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 233 | loss: 0.1203775703907013 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 234 | loss: 0.07821127772331238 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 235 | loss: 0.10876654833555222 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 236 | loss: 0.040239013731479645 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 237 | loss: 0.0928490161895752 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 238 | loss: 0.07097392529249191 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 239 | loss: 0.0348685123026371 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 240 | loss: 0.07558641582727432 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 241 | loss: 0.09028353542089462 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 242 | loss: 0.03794199228286743 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 243 | loss: 0.11293305456638336 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 244 | loss: 0.0451197512447834 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 245 | loss: 0.1843949854373932 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 246 | loss: 0.2262686938047409 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 247 | loss: 0.16538669168949127 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 248 | loss: 0.1724829524755478 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 249 | loss: 0.1041676253080368 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 250 | loss: 0.08656642585992813 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 251 | loss: 0.058343593031167984 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 252 | loss: 0.1390131562948227 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 253 | loss: 0.20080171525478363 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 254 | loss: 0.1814776211977005 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 255 | loss: 0.2834925353527069 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 256 | loss: 0.02256612852215767 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 257 | loss: 0.1379231959581375 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 258 | loss: 0.056617338210344315 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 259 | loss: 0.08768071234226227 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 260 | loss: 0.08396746963262558 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 261 | loss: 0.12306259572505951 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 262 | loss: 0.30401936173439026 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 263 | loss: 0.1281065195798874 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 264 | loss: 0.14531783759593964 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 265 | loss: 0.07284397631883621 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 266 | loss: 0.15724843740463257 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 267 | loss: 0.03799450397491455 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 268 | loss: 0.11876806616783142 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 269 | loss: 0.08882736414670944 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 270 | loss: 0.16311930119991302 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 271 | loss: 0.07035363465547562 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 272 | loss: 0.37224429845809937 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 273 | loss: 0.222825288772583 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 274 | loss: 0.01569836027920246 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 275 | loss: 0.22022315859794617 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 276 | loss: 0.22866946458816528 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 277 | loss: 0.1230524331331253 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 278 | loss: 0.08127409964799881 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 279 | loss: 0.07758724689483643 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 280 | loss: 0.1588195264339447 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 281 | loss: 0.1089707463979721 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 282 | loss: 0.13648903369903564 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 283 | loss: 0.08671633899211884 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 284 | loss: 0.14131616055965424 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 285 | loss: 0.17362579703330994 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 286 | loss: 0.05414070561528206 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 287 | loss: 0.11661212891340256 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 288 | loss: 0.11639723181724548 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 289 | loss: 0.11531978845596313 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 290 | loss: 0.07427763938903809 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 291 | loss: 0.013432606123387814 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 292 | loss: 0.07298128306865692 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 293 | loss: 0.19612090289592743 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 294 | loss: 0.1434219479560852 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 295 | loss: 0.1459241360425949 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 296 | loss: 0.17530560493469238 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 297 | loss: 0.07227686047554016 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 298 | loss: 0.041012249886989594 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 299 | loss: 0.05106399580836296 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 300 | loss: 0.0473090223968029 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 301 | loss: 0.12841953337192535 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 302 | loss: 0.33333995938301086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 303 | loss: 0.09580439329147339 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 304 | loss: 0.08481403440237045 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 305 | loss: 0.09178995341062546 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 306 | loss: 0.11814188957214355 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 307 | loss: 0.16305194795131683 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 308 | loss: 0.05365214869379997 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 309 | loss: 0.1608991026878357 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 310 | loss: 0.1993032544851303 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 311 | loss: 0.09513363987207413 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 312 | loss: 0.1299254596233368 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 313 | loss: 0.053255870938301086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 314 | loss: 0.060695141553878784 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 315 | loss: 0.0930447205901146 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 316 | loss: 0.054705116897821426 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 317 | loss: 0.1232038214802742 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 318 | loss: 0.045670222491025925 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 319 | loss: 0.041078221052885056 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 320 | loss: 0.09252046048641205 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 321 | loss: 0.11499681323766708 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 322 | loss: 0.04802741855382919 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 323 | loss: 0.02086225524544716 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 324 | loss: 0.05041985958814621 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 325 | loss: 0.0539713054895401 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 326 | loss: 0.08136574178934097 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 327 | loss: 0.08974305540323257 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 328 | loss: 0.10038406401872635 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 329 | loss: 0.12718060612678528 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 330 | loss: 0.0975911095738411 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 331 | loss: 0.23452159762382507 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 332 | loss: 0.1987367421388626 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 333 | loss: 0.19620656967163086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 334 | loss: 0.1770041584968567 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 335 | loss: 0.06225598603487015 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 336 | loss: 0.12155462056398392 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 337 | loss: 0.1495101898908615 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 338 | loss: 0.06824613362550735 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 339 | loss: 0.06446217000484467 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 340 | loss: 0.1022266298532486 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 341 | loss: 0.1910429745912552 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 342 | loss: 0.041595615446567535 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 343 | loss: 0.12797904014587402 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 344 | loss: 0.0747300460934639 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 345 | loss: 0.09142258763313293 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 346 | loss: 0.16659680008888245 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 347 | loss: 0.16581706702709198 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 348 | loss: 0.12403449416160583 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 349 | loss: 0.1201007291674614 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 350 | loss: 0.14356055855751038 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 351 | loss: 0.19424763321876526 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 352 | loss: 0.2323085218667984 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 353 | loss: 0.13677681982517242 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 354 | loss: 0.20116430521011353 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 355 | loss: 0.08959133177995682 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 356 | loss: 0.07644091546535492 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 357 | loss: 0.044906068593263626 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 358 | loss: 0.13206250965595245 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 359 | loss: 0.11499498039484024 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 360 | loss: 0.23084907233715057 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 361 | loss: 0.1438133418560028 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 362 | loss: 0.1587313860654831 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 363 | loss: 0.12991566956043243 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 364 | loss: 0.05626218020915985 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 365 | loss: 0.06346169114112854 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 366 | loss: 0.11108184605836868 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 367 | loss: 0.12152750790119171 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 368 | loss: 0.09704555571079254 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 369 | loss: 0.11572213470935822 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 370 | loss: 0.08998434245586395 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 371 | loss: 0.11714795231819153 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 372 | loss: 0.10673613846302032 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 373 | loss: 0.053720247000455856 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 374 | loss: 0.08981038630008698 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 375 | loss: 0.11082903295755386 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 376 | loss: 0.1039280965924263 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 377 | loss: 0.035763151943683624 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 378 | loss: 0.11203806847333908 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 379 | loss: 0.024570709094405174 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 380 | loss: 0.06285394728183746 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 381 | loss: 0.23008769750595093 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 382 | loss: 0.07773652672767639 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 383 | loss: 0.2155713438987732 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 384 | loss: 0.1813986599445343 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 385 | loss: 0.17003868520259857 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 386 | loss: 0.08106660097837448 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 387 | loss: 0.23662395775318146 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 388 | loss: 0.10951193422079086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 389 | loss: 0.04561126232147217 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 390 | loss: 0.04908956587314606 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 391 | loss: 0.06023202836513519 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 392 | loss: 0.08601851016283035 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 393 | loss: 0.1221151351928711 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 394 | loss: 0.1385142207145691 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 395 | loss: 0.09613913297653198 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 396 | loss: 0.09770254790782928 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 397 | loss: 0.33611562848091125 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 398 | loss: 0.17976893484592438 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 399 | loss: 0.201343834400177 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 400 | loss: 0.03721341863274574 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 401 | loss: 0.14199700951576233 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 402 | loss: 0.1208372414112091 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 403 | loss: 0.21783402562141418 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 404 | loss: 0.14099153876304626 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 405 | loss: 0.04044850170612335 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 406 | loss: 0.1396249532699585 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 407 | loss: 0.15943825244903564 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 408 | loss: 0.08891546726226807 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 409 | loss: 0.09237444400787354 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 410 | loss: 0.09372195601463318 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 411 | loss: 0.0894520953297615 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 412 | loss: 0.02893204614520073 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 413 | loss: 0.1598195880651474 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 414 | loss: 0.08725249767303467 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 415 | loss: 0.09621093422174454 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 416 | loss: 0.11964010447263718 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 417 | loss: 0.273684561252594 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 418 | loss: 0.05776619166135788 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 419 | loss: 0.08584436774253845 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 420 | loss: 0.11567652970552444 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 421 | loss: 0.10148170590400696 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 422 | loss: 0.10733156651258469 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 423 | loss: 0.03604482114315033 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 424 | loss: 0.22002671658992767 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 425 | loss: 0.17604589462280273 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 426 | loss: 0.0640583410859108 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 427 | loss: 0.14234434068202972 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 428 | loss: 0.1584050953388214 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 429 | loss: 0.025064213201403618 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 430 | loss: 0.1837022751569748 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 431 | loss: 0.049552857875823975 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 432 | loss: 0.05354209244251251 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 433 | loss: 0.046809691935777664 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 434 | loss: 0.22542206943035126 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 435 | loss: 0.15941457450389862 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 436 | loss: 0.05168994516134262 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 437 | loss: 0.08093870431184769 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 438 | loss: 0.08319204300642014 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 439 | loss: 0.12031702697277069 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 440 | loss: 0.051898397505283356 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 441 | loss: 0.27465400099754333 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 442 | loss: 0.18790781497955322 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 443 | loss: 0.06262087821960449 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 444 | loss: 0.07605899125337601 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 445 | loss: 0.07445991784334183 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 446 | loss: 0.06999488919973373 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 447 | loss: 0.07704639434814453 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 448 | loss: 0.08824899047613144 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 449 | loss: 0.12686119973659515 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 450 | loss: 0.05944392457604408 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 451 | loss: 0.11315957456827164 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 452 | loss: 0.09305872023105621 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 453 | loss: 0.05096925422549248 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 454 | loss: 0.10324657708406448 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 455 | loss: 0.20478640496730804 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 456 | loss: 0.12497749924659729 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 457 | loss: 0.1475559026002884 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 458 | loss: 0.05628698691725731 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 459 | loss: 0.06312206387519836 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 460 | loss: 0.07579682767391205 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 461 | loss: 0.11550183594226837 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 462 | loss: 0.06461960822343826 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 463 | loss: 0.14575213193893433 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 464 | loss: 0.09561115503311157 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 465 | loss: 0.054876670241355896 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 466 | loss: 0.09183572977781296 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 467 | loss: 0.08599505573511124 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 468 | loss: 0.2282613068819046 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 469 | loss: 0.10302292555570602 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 470 | loss: 0.24554386734962463 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 471 | loss: 0.0901803970336914 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 472 | loss: 0.05801216885447502 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 473 | loss: 0.06875061243772507 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 474 | loss: 0.08089805394411087 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 475 | loss: 0.19041897356510162 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 476 | loss: 0.09291942417621613 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 477 | loss: 0.07354121655225754 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 478 | loss: 0.13446594774723053 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 479 | loss: 0.09848632663488388 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 480 | loss: 0.05477046221494675 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 481 | loss: 0.1852944940328598 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 482 | loss: 0.10866274684667587 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 483 | loss: 0.12196757644414902 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 484 | loss: 0.10866860300302505 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 485 | loss: 0.12115096300840378 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 486 | loss: 0.21035878360271454 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 487 | loss: 0.06518832594156265 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 488 | loss: 0.09119908511638641 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 489 | loss: 0.06426356732845306 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 490 | loss: 0.04308011010289192 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 491 | loss: 0.12411203235387802 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 492 | loss: 0.06888820230960846 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 493 | loss: 0.09375680983066559 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 494 | loss: 0.042081162333488464 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 495 | loss: 0.08817188441753387 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 496 | loss: 0.099774070084095 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 497 | loss: 0.049138620495796204 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 498 | loss: 0.13003243505954742 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 499 | loss: 0.12501972913742065 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 500 | loss: 0.12271272391080856 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 501 | loss: 0.09432993829250336 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 502 | loss: 0.17715081572532654 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 503 | loss: 0.08236242085695267 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 504 | loss: 0.06325561553239822 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 505 | loss: 0.2694018483161926 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 506 | loss: 0.1351267397403717 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 507 | loss: 0.10948195308446884 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 508 | loss: 0.05629643425345421 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 509 | loss: 0.1618974804878235 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 510 | loss: 0.08890187740325928 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 511 | loss: 0.03558632731437683 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 512 | loss: 0.09622050821781158 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 513 | loss: 0.08503509312868118 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 514 | loss: 0.1430341601371765 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 515 | loss: 0.12718498706817627 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 516 | loss: 0.15874454379081726 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 517 | loss: 0.07487217336893082 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 518 | loss: 0.19049817323684692 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 519 | loss: 0.27407851815223694 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 520 | loss: 0.09972385317087173 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 521 | loss: 0.11326204985380173 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 522 | loss: 0.10245886445045471 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 523 | loss: 0.027096878737211227 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 524 | loss: 0.07379817217588425 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 525 | loss: 0.0803542211651802 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 526 | loss: 0.08623434603214264 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 527 | loss: 0.1869206577539444 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 528 | loss: 0.09981115907430649 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 529 | loss: 0.1279526799917221 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 530 | loss: 0.20934434235095978 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 531 | loss: 0.11467193067073822 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 532 | loss: 0.05398218333721161 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 533 | loss: 0.042440272867679596 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 534 | loss: 0.13164746761322021 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 535 | loss: 0.10725834965705872 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 536 | loss: 0.16322815418243408 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 537 | loss: 0.12547506392002106 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 538 | loss: 0.193723663687706 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 539 | loss: 0.0972089171409607 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 540 | loss: 0.026074815541505814 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 541 | loss: 0.06322254985570908 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 542 | loss: 0.0546419695019722 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 543 | loss: 0.08765150606632233 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 544 | loss: 0.05307302996516228 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 545 | loss: 0.094205342233181 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 546 | loss: 0.043353237211704254 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 547 | loss: 0.1798751950263977 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 548 | loss: 0.08153285831212997 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 549 | loss: 0.10451434552669525 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 550 | loss: 0.11952827870845795 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 551 | loss: 0.06362776458263397 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 552 | loss: 0.11609760671854019 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 553 | loss: 0.042703766375780106 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 554 | loss: 0.1541663408279419 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 555 | loss: 0.20575109124183655 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 556 | loss: 0.10734918713569641 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 557 | loss: 0.1840454787015915 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 558 | loss: 0.04392143711447716 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 559 | loss: 0.11852375417947769 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 560 | loss: 0.11950445920228958 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 561 | loss: 0.14865365624427795 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 562 | loss: 0.1778825968503952 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 563 | loss: 0.13558140397071838 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 564 | loss: 0.07988259196281433 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 565 | loss: 0.1994549185037613 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 566 | loss: 0.08910361677408218 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 567 | loss: 0.10718552768230438 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 568 | loss: 0.0961674302816391 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 569 | loss: 0.19848930835723877 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 570 | loss: 0.2806870937347412 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 571 | loss: 0.19052079319953918 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 572 | loss: 0.09441977739334106 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 573 | loss: 0.17842848598957062 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 574 | loss: 0.25300833582878113 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 575 | loss: 0.06768499314785004 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 576 | loss: 0.11527346074581146 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 577 | loss: 0.059837404638528824 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 578 | loss: 0.06766900420188904 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 579 | loss: 0.12163878977298737 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 580 | loss: 0.043567098677158356 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 581 | loss: 0.10770580172538757 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 582 | loss: 0.12787091732025146 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 583 | loss: 0.1513739675283432 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 584 | loss: 0.1739269644021988 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 585 | loss: 0.11526034772396088 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 586 | loss: 0.07894005626440048 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 587 | loss: 0.19074800610542297 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 588 | loss: 0.034012533724308014 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 589 | loss: 0.07250045984983444 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 590 | loss: 0.058141134679317474 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 591 | loss: 0.20432148873806 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 592 | loss: 0.10791831463575363 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 593 | loss: 0.11061523109674454 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 594 | loss: 0.10870910435914993 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 595 | loss: 0.17398685216903687 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 596 | loss: 0.1920452117919922 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 597 | loss: 0.05461538955569267 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 598 | loss: 0.084295853972435 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 599 | loss: 0.043282005935907364 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 600 | loss: 0.26023077964782715 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 601 | loss: 0.10710707306861877 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 602 | loss: 0.10079390555620193 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 603 | loss: 0.13460160791873932 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 604 | loss: 0.1507003754377365 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 605 | loss: 0.14193002879619598 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 606 | loss: 0.16191789507865906 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 607 | loss: 0.08626245707273483 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 608 | loss: 0.23151613771915436 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 609 | loss: 0.05017838999629021 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 610 | loss: 0.17857684195041656 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 611 | loss: 0.2182120680809021 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 612 | loss: 0.04928150773048401 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 613 | loss: 0.056316595524549484 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 614 | loss: 0.07785555720329285 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 615 | loss: 0.19568969309329987 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 616 | loss: 0.09645240008831024 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 617 | loss: 0.114075668156147 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 618 | loss: 0.10151411592960358 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 619 | loss: 0.1355499029159546 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 620 | loss: 0.08123829215765 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 621 | loss: 0.051017001271247864 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 622 | loss: 0.10106293112039566 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 623 | loss: 0.15033255517482758 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 624 | loss: 0.07174119353294373 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 625 | loss: 0.11636418104171753 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 626 | loss: 0.2172258049249649 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 627 | loss: 0.051403969526290894 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 628 | loss: 0.15307362377643585 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 629 | loss: 0.057672109454870224 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 630 | loss: 0.13492418825626373 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 631 | loss: 0.07616580277681351 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 632 | loss: 0.22410181164741516 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 633 | loss: 0.1356661170721054 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 634 | loss: 0.07641074806451797 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 635 | loss: 0.11167063564062119 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 636 | loss: 0.18249957263469696 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 637 | loss: 0.17540843784809113 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 638 | loss: 0.09453464299440384 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 639 | loss: 0.0677206888794899 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 640 | loss: 0.09444296360015869 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 641 | loss: 0.06263755261898041 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 642 | loss: 0.1366509348154068 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 643 | loss: 0.06639497727155685 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 644 | loss: 0.1071430891752243 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 645 | loss: 0.1893606334924698 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 646 | loss: 0.02644975855946541 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 647 | loss: 0.13918934762477875 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 648 | loss: 0.24288372695446014 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 649 | loss: 0.04031658545136452 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 650 | loss: 0.08279263228178024 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 651 | loss: 0.08517613261938095 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 652 | loss: 0.04699905216693878 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 653 | loss: 0.10835642367601395 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 654 | loss: 0.11108508706092834 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 655 | loss: 0.05060680955648422 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 656 | loss: 0.03551719710230827 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 657 | loss: 0.07177771627902985 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 658 | loss: 0.04850185662508011 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 659 | loss: 0.09811961650848389 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 660 | loss: 0.14210250973701477 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 661 | loss: 0.239003524184227 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 662 | loss: 0.1350925862789154 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 663 | loss: 0.09348875284194946 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 664 | loss: 0.22356781363487244 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 665 | loss: 0.06460821628570557 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 666 | loss: 0.06780896335840225 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 667 | loss: 0.04581334441900253 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 668 | loss: 0.06193142384290695 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 669 | loss: 0.04320234805345535 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 670 | loss: 0.06774640828371048 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 671 | loss: 0.1389188915491104 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 672 | loss: 0.07293093204498291 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 673 | loss: 0.044303473085165024 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 674 | loss: 0.18529024720191956 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 675 | loss: 0.07179966568946838 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 676 | loss: 0.13038666546344757 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 677 | loss: 0.07508877664804459 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 678 | loss: 0.09463025629520416 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 679 | loss: 0.1518271118402481 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 680 | loss: 0.08674493432044983 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 681 | loss: 0.11070676147937775 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 682 | loss: 0.18200846016407013 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 683 | loss: 0.17389936745166779 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 684 | loss: 0.11630050092935562 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 685 | loss: 0.1517239212989807 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 686 | loss: 0.18784616887569427 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 687 | loss: 0.13817916810512543 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 688 | loss: 0.20616380870342255 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 689 | loss: 0.07286615669727325 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 690 | loss: 0.08787117898464203 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 691 | loss: 0.16216500103473663 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 692 | loss: 0.07363277673721313 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 693 | loss: 0.05068604275584221 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 694 | loss: 0.1445346176624298 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 695 | loss: 0.1602751761674881 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 696 | loss: 0.05902707949280739 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 697 | loss: 0.10275913774967194 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 698 | loss: 0.12013766914606094 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 699 | loss: 0.06137106567621231 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 700 | loss: 0.11072050034999847 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 701 | loss: 0.08574382960796356 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 702 | loss: 0.14994178712368011 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 703 | loss: 0.22281691431999207 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 704 | loss: 0.09101790189743042 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 705 | loss: 0.11469701677560806 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 706 | loss: 0.07493171840906143 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 707 | loss: 0.15129660069942474 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 708 | loss: 0.039200201630592346 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 709 | loss: 0.24464131891727448 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 710 | loss: 0.10183296352624893 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 711 | loss: 0.12082353979349136 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 712 | loss: 0.13580816984176636 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 713 | loss: 0.16213612258434296 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 714 | loss: 0.08621153980493546 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 715 | loss: 0.10058633238077164 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 716 | loss: 0.060821495950222015 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 717 | loss: 0.10562799870967865 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 718 | loss: 0.08327444642782211 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 719 | loss: 0.10621508955955505 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 720 | loss: 0.074561707675457 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 721 | loss: 0.024731257930397987 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 722 | loss: 0.1693519949913025 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 723 | loss: 0.04146704077720642 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 724 | loss: 0.16449908912181854 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 725 | loss: 0.12226656079292297 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 726 | loss: 0.09298564493656158 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 727 | loss: 0.0429934561252594 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 728 | loss: 0.08998440206050873 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 729 | loss: 0.10323362797498703 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 730 | loss: 0.09716969728469849 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 731 | loss: 0.06018529832363129 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 732 | loss: 0.12874500453472137 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 733 | loss: 0.12901699542999268 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 734 | loss: 0.1741539090871811 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 735 | loss: 0.14060930907726288 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 736 | loss: 0.1205725148320198 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 737 | loss: 0.10122109204530716 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 738 | loss: 0.1364389806985855 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 739 | loss: 0.049743250012397766 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 740 | loss: 0.21956810355186462 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 741 | loss: 0.06213902309536934 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 742 | loss: 0.11766643077135086 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 743 | loss: 0.059193603694438934 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 744 | loss: 0.07671576738357544 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 745 | loss: 0.1574247181415558 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 746 | loss: 0.051476895809173584 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 747 | loss: 0.06352299451828003 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 748 | loss: 0.09862645715475082 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 749 | loss: 0.1537351906299591 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 750 | loss: 0.05164418742060661 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 751 | loss: 0.07030639052391052 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 752 | loss: 0.05662401393055916 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 753 | loss: 0.04069915786385536 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 754 | loss: 0.12231115251779556 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 755 | loss: 0.07734813541173935 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 756 | loss: 0.08322975784540176 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 757 | loss: 0.20931710302829742 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 758 | loss: 0.07401552051305771 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 759 | loss: 0.07826082408428192 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 760 | loss: 0.17930202186107635 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 761 | loss: 0.10818960517644882 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 762 | loss: 0.10067953914403915 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 763 | loss: 0.1369308978319168 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 764 | loss: 0.1531633734703064 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 765 | loss: 0.0338805727660656 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 766 | loss: 0.21606284379959106 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 767 | loss: 0.08802426606416702 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 768 | loss: 0.2373719960451126 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 769 | loss: 0.22010891139507294 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 770 | loss: 0.07266651839017868 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 771 | loss: 0.06597499549388885 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 772 | loss: 0.12558871507644653 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 773 | loss: 0.1917717158794403 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 774 | loss: 0.1358521580696106 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 775 | loss: 0.06458044052124023 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 776 | loss: 0.1044420376420021 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 777 | loss: 0.12248142063617706 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 778 | loss: 0.12307072430849075 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 779 | loss: 0.12470466643571854 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 780 | loss: 0.28962430357933044 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 781 | loss: 0.17685623466968536 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 782 | loss: 0.07114362716674805 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 783 | loss: 0.057644616812467575 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 784 | loss: 0.07401703298091888 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 785 | loss: 0.12705858051776886 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 786 | loss: 0.0749279037117958 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 787 | loss: 0.0852723941206932 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 788 | loss: 0.10368101298809052 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 789 | loss: 0.05995103716850281 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 790 | loss: 0.013873713091015816 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 791 | loss: 0.13612134754657745 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 792 | loss: 0.06949823349714279 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 793 | loss: 0.029866855591535568 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 794 | loss: 0.06980760395526886 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 795 | loss: 0.12825429439544678 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 796 | loss: 0.053602200001478195 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 797 | loss: 0.11912678182125092 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 798 | loss: 0.09761049598455429 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 799 | loss: 0.12218477576971054 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 800 | loss: 0.10175300389528275 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 801 | loss: 0.059547536075115204 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 802 | loss: 0.15171997249126434 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 803 | loss: 0.11335500329732895 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 804 | loss: 0.13668321073055267 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 805 | loss: 0.06729023903608322 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 806 | loss: 0.12043388187885284 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 807 | loss: 0.20858825743198395 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 808 | loss: 0.07118697464466095 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 809 | loss: 0.13523957133293152 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 810 | loss: 0.11644518375396729 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 811 | loss: 0.18532288074493408 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 812 | loss: 0.2370034158229828 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 813 | loss: 0.11030934751033783 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 814 | loss: 0.11994602531194687 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 815 | loss: 0.1161414086818695 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 816 | loss: 0.12387920171022415 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 817 | loss: 0.1166229322552681 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 818 | loss: 0.11700966954231262 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 819 | loss: 0.07527632266283035 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 820 | loss: 0.08916473388671875 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 821 | loss: 0.14671030640602112 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 822 | loss: 0.0999402329325676 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 823 | loss: 0.12788791954517365 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 824 | loss: 0.10429003089666367 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 825 | loss: 0.08810673654079437 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 826 | loss: 0.1955820769071579 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 827 | loss: 0.11896637082099915 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 828 | loss: 0.15164373815059662 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 829 | loss: 0.0734310895204544 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 830 | loss: 0.20004555583000183 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 831 | loss: 0.27717119455337524 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 832 | loss: 0.1403086930513382 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 833 | loss: 0.09929972887039185 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 834 | loss: 0.03261231631040573 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 835 | loss: 0.09465351700782776 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 836 | loss: 0.06494484841823578 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 837 | loss: 0.1526319682598114 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 838 | loss: 0.11997757852077484 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 839 | loss: 0.15433481335639954 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 840 | loss: 0.0883847251534462 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 841 | loss: 0.07696071267127991 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 842 | loss: 0.14351126551628113 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 843 | loss: 0.11240936070680618 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 844 | loss: 0.03840773552656174 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 845 | loss: 0.0425308458507061 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 846 | loss: 0.10307935625314713 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 847 | loss: 0.07226616144180298 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 848 | loss: 0.19741113483905792 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 849 | loss: 0.116460882127285 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 850 | loss: 0.03963492438197136 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 851 | loss: 0.0204311553388834 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 852 | loss: 0.12163642048835754 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 853 | loss: 0.04301341623067856 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 854 | loss: 0.07104415446519852 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 855 | loss: 0.09308303147554398 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 856 | loss: 0.16132454574108124 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 857 | loss: 0.11565786600112915 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 858 | loss: 0.06292951852083206 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 859 | loss: 0.09522931277751923 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 860 | loss: 0.07020732760429382 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 861 | loss: 0.06688414514064789 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 862 | loss: 0.09402451664209366 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 863 | loss: 0.24919156730175018 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 864 | loss: 0.05859661102294922 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 865 | loss: 0.05342254042625427 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 866 | loss: 0.07006843388080597 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 867 | loss: 0.048749327659606934 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 868 | loss: 0.029202302917838097 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 869 | loss: 0.1216641291975975 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 870 | loss: 0.18146111071109772 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 871 | loss: 0.20074418187141418 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 872 | loss: 0.2206539511680603 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 873 | loss: 0.036578088998794556 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 874 | loss: 0.20062975585460663 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 875 | loss: 0.14682163298130035 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 876 | loss: 0.0266365148127079 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 877 | loss: 0.0589679554104805 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 878 | loss: 0.038763247430324554 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 879 | loss: 0.06445567309856415 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 880 | loss: 0.0839865580201149 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 881 | loss: 0.1343136429786682 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 882 | loss: 0.1946871429681778 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 883 | loss: 0.1719442903995514 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 884 | loss: 0.02748742885887623 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 885 | loss: 0.09810109436511993 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 886 | loss: 0.09900563955307007 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 887 | loss: 0.03207889944314957 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 888 | loss: 0.05738136172294617 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 889 | loss: 0.11760495603084564 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 890 | loss: 0.07335357367992401 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 891 | loss: 0.16209080815315247 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 892 | loss: 0.14606837928295135 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 893 | loss: 0.06571130454540253 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 894 | loss: 0.034244243055582047 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 895 | loss: 0.20767875015735626 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 896 | loss: 0.09663897752761841 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 897 | loss: 0.14169420301914215 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 898 | loss: 0.17217770218849182 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 899 | loss: 0.0570150688290596 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 900 | loss: 0.16433501243591309 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 901 | loss: 0.05602505803108215 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 902 | loss: 0.03440751135349274 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 903 | loss: 0.12509457767009735 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 904 | loss: 0.2509130537509918 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 905 | loss: 0.15463228523731232 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 906 | loss: 0.050255924463272095 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 907 | loss: 0.10822918266057968 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 908 | loss: 0.0388544537127018 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 909 | loss: 0.03926611691713333 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 910 | loss: 0.10677599161863327 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 911 | loss: 0.08912359923124313 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 912 | loss: 0.07010088115930557 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 913 | loss: 0.15534581243991852 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 914 | loss: 0.108116015791893 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 915 | loss: 0.11120204627513885 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 916 | loss: 0.15852248668670654 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 917 | loss: 0.06058806926012039 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 918 | loss: 0.09345211833715439 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 919 | loss: 0.1567286103963852 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 920 | loss: 0.015939578413963318 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 921 | loss: 0.14607955515384674 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 922 | loss: 0.1562015265226364 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 923 | loss: 0.08548525720834732 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 924 | loss: 0.11371184140443802 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 925 | loss: 0.03305868059396744 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 926 | loss: 0.09775213897228241 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 927 | loss: 0.07889022678136826 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 928 | loss: 0.020362142473459244 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 929 | loss: 0.1466524302959442 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 930 | loss: 0.10730503499507904 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 931 | loss: 0.15557856857776642 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 932 | loss: 0.1341664344072342 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 933 | loss: 0.02367396466434002 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 934 | loss: 0.24392159283161163 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 935 | loss: 0.03230602294206619 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 936 | loss: 0.05960438400506973 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 937 | loss: 0.058791954070329666 | lr: 0.010000000000000002\n",
      "Epoch [2/2] | Batch 938 | loss: 0.11078988760709763 | lr: 0.010000000000000002\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "num_epochs = 2\n",
    "lr = 1e-1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer and the scheduler\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[1], gamma=0.1) # reduce lr by 0.1 after 1 epoch\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "\n",
    "        # compute loss    \n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # The three Musketeers!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Batch {batch_idx+1} | loss: {loss.item()} | lr: {optimizer.param_groups[0][\"lr\"]}')\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    scheduler.step() # Inform the scheduler an epoch was done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional learning rate scheduling strategies include:\n",
    "* Cosine annealing:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*9hiywbEuoVXz4iX2-BDGZQ.png\" width=20%, height=20%>\n",
    "</p>\n",
    "\n",
    "* Learning rate warmup:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected Gradient Descent (PGD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So far, we have been concerned with **unconstrained** optimization problems.\n",
    "* However, all of the above optimization algorithms may be generalized to **constrained** optimization problem of the following form:\n",
    "\n",
    "$$ \\min_x f(x)\n",
    "\\text{ subject to } x \\in \\mathcal{K} \\\\\n",
    "f: \\mathbb{R}^d \\rightarrow \\mathbb{R} \\text{, } \\mathcal{K} \\subseteq \\mathbb{R}^d \\text{ is closed and convex} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is done by a simple-greedy agorithm named PGD.\n",
    "* The idea is to project $x$ onto $\\mathcal{K}$ after each iteration:\n",
    "$$ \\tilde{x}_{k+1} = x_k - \\eta \\nabla_x f (x_k) \\\\\n",
    "x_{k+1} = \\Pi_\\mathcal{K}(\\tilde{x}_{k+1})$$\n",
    "* The algorithm can be proved to converge under the same conditions required for GD to converge!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://qph.cf2.quoracdn.net/main-qimg-8311c35651e61b7b91e8918388946469\" width=25%, height=25%>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mathematically, the projection of a point onto a set is defined as the closest point to the original point within the set:\n",
    "$$ \\Pi_{\\mathcal{K}}(x) := \\arg \\min_y \\| y-x \\| \\text{ subject to } y \\in \\mathcal{K}$$\n",
    "* Common projections:\n",
    "    + A canonical sphere with radius $R$:\n",
    "    $$ \\Pi_{\\mathcal{B}(R)}(x) = \\min\\{\\frac{R}{\\| x \\|}, 1\\} \\cdot x $$\n",
    "    <p align=\"center\">\n",
    "    <img src=\"http://pi.math.cornell.edu/~dwh/books/eg99/Ch16/37850c60.jpg\" width=25%, height=25%>\n",
    "    </p>\n",
    "    \n",
    "    + A linear subspace $W$:\n",
    "    $$ \\Pi_{W}(x) = \\sum_{i=1}^m \\langle x , \\; w_i \\rangle w_i $$\n",
    "    where $\\{ w_1, ..., w_m\\}$ is an orthonormal basis for $W$.\n",
    "\n",
    "    <p align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Linalg_projection_onto_plane.png/223px-Linalg_projection_onto_plane.png\">\n",
    "    </p>\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: adversarial attacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The goal is to find a small perturbation on a certain input, in a way which would cause the model to generate a wrong prediction.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://i.stack.imgur.com/5rfe9.png\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's carry a PGD attack on a sample from the test dataset with respect to our trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test dataset\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(), # Convert to tensor\n",
    "            transforms.Normalize((0.1307,), (0.3081,)) # Subtract from values 0.13 then divide by 0.31\n",
    "            ])\n",
    "\n",
    "dataset = datasets.MNIST('./data', train=False, download=True, transform=transform) # MNIST test set\n",
    "sample_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "sample, true_y = next(iter(sample_loader))\n",
    "sample = sample.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth: 7\\nPrediction: 7, confidence: 99.95%')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the sample\n",
    "with torch.no_grad():\n",
    "    logit = model(sample)[0]\n",
    "    proba = torch.softmax(logit, dim=0)\n",
    "    pred = torch.argmax(proba)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(sample.reshape(28,28), cmap='gray', interpolation='none')\n",
    "plt.title(\"Ground Truth: {}\\nPrediction: {}, confidence: {:.2f}%\".format(true_y.item(), pred, proba[pred]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0] | loss: 0.0005261705373413861\n",
      "Iteration [1] | loss: 0.0005261705373413861\n",
      "Iteration [2] | loss: 0.0005261705373413861\n",
      "Iteration [3] | loss: 0.0005261705373413861\n",
      "Iteration [4] | loss: 0.0005264088395051658\n",
      "Iteration [5] | loss: 0.0005265279905870557\n",
      "Iteration [6] | loss: 0.0005265279905870557\n",
      "Iteration [7] | loss: 0.0005266471416689456\n",
      "Iteration [8] | loss: 0.0005266471416689456\n",
      "Iteration [9] | loss: 0.0005267662927508354\n",
      "Iteration [10] | loss: 0.0005268854438327253\n",
      "Iteration [11] | loss: 0.0005268854438327253\n",
      "Iteration [12] | loss: 0.0005268854438327253\n",
      "Iteration [13] | loss: 0.0005270045949146152\n",
      "Iteration [14] | loss: 0.000527123745996505\n",
      "Iteration [15] | loss: 0.000527123745996505\n",
      "Iteration [16] | loss: 0.000527123745996505\n",
      "Iteration [17] | loss: 0.0005272428970783949\n",
      "Iteration [18] | loss: 0.0005273620481602848\n",
      "Iteration [19] | loss: 0.0005273620481602848\n",
      "Iteration [20] | loss: 0.0005273620481602848\n",
      "Iteration [21] | loss: 0.0005274811992421746\n",
      "Iteration [22] | loss: 0.0005277194431982934\n",
      "Iteration [23] | loss: 0.0005277194431982934\n",
      "Iteration [24] | loss: 0.0005278385942801833\n",
      "Iteration [25] | loss: 0.0005278385942801833\n",
      "Iteration [26] | loss: 0.0005278385942801833\n",
      "Iteration [27] | loss: 0.000528076896443963\n",
      "Iteration [28] | loss: 0.000528076896443963\n",
      "Iteration [29] | loss: 0.000528076896443963\n",
      "Iteration [30] | loss: 0.000528076896443963\n",
      "Iteration [31] | loss: 0.0005283151986077428\n",
      "Iteration [32] | loss: 0.0005283151986077428\n",
      "Iteration [33] | loss: 0.0005283151986077428\n",
      "Iteration [34] | loss: 0.0005284343496896327\n",
      "Iteration [35] | loss: 0.0005285535007715225\n",
      "Iteration [36] | loss: 0.0005285535007715225\n",
      "Iteration [37] | loss: 0.0005286726518534124\n",
      "Iteration [38] | loss: 0.0005286726518534124\n",
      "Iteration [39] | loss: 0.0005287918029353023\n",
      "Iteration [40] | loss: 0.0005289109540171921\n",
      "Iteration [41] | loss: 0.0005290300468914211\n",
      "Iteration [42] | loss: 0.0005290300468914211\n",
      "Iteration [43] | loss: 0.0005290300468914211\n",
      "Iteration [44] | loss: 0.0005292683490552008\n",
      "Iteration [45] | loss: 0.0005292683490552008\n",
      "Iteration [46] | loss: 0.0005292683490552008\n",
      "Iteration [47] | loss: 0.0005293875001370907\n",
      "Iteration [48] | loss: 0.0005293875001370907\n",
      "Iteration [49] | loss: 0.0005295066512189806\n",
      "Iteration [50] | loss: 0.0005295066512189806\n",
      "Iteration [51] | loss: 0.0005296258023008704\n",
      "Iteration [52] | loss: 0.0005296258023008704\n",
      "Iteration [53] | loss: 0.0005297449533827603\n",
      "Iteration [54] | loss: 0.0005298641044646502\n",
      "Iteration [55] | loss: 0.0005298641044646502\n",
      "Iteration [56] | loss: 0.00052998325554654\n",
      "Iteration [57] | loss: 0.0005301024066284299\n",
      "Iteration [58] | loss: 0.0005302215577103198\n",
      "Iteration [59] | loss: 0.0005302215577103198\n",
      "Iteration [60] | loss: 0.0005302215577103198\n",
      "Iteration [61] | loss: 0.0005303407087922096\n",
      "Iteration [62] | loss: 0.0005304598016664386\n",
      "Iteration [63] | loss: 0.0005304598016664386\n",
      "Iteration [64] | loss: 0.0005305789527483284\n",
      "Iteration [65] | loss: 0.0005305789527483284\n",
      "Iteration [66] | loss: 0.0005306981038302183\n",
      "Iteration [67] | loss: 0.0005308172549121082\n",
      "Iteration [68] | loss: 0.0005308172549121082\n",
      "Iteration [69] | loss: 0.0005308172549121082\n",
      "Iteration [70] | loss: 0.0005308172549121082\n",
      "Iteration [71] | loss: 0.0005310555570758879\n",
      "Iteration [72] | loss: 0.0005310555570758879\n",
      "Iteration [73] | loss: 0.0005311747081577778\n",
      "Iteration [74] | loss: 0.0005312938592396677\n",
      "Iteration [75] | loss: 0.0005314130103215575\n",
      "Iteration [76] | loss: 0.0005314130103215575\n",
      "Iteration [77] | loss: 0.0005315321614034474\n",
      "Iteration [78] | loss: 0.0005315321614034474\n",
      "Iteration [79] | loss: 0.0005315321614034474\n",
      "Iteration [80] | loss: 0.0005316513124853373\n",
      "Iteration [81] | loss: 0.0005317704635672271\n",
      "Iteration [82] | loss: 0.0005317704635672271\n",
      "Iteration [83] | loss: 0.0005317704635672271\n",
      "Iteration [84] | loss: 0.000532008707523346\n",
      "Iteration [85] | loss: 0.000532008707523346\n",
      "Iteration [86] | loss: 0.000532008707523346\n",
      "Iteration [87] | loss: 0.0005321278586052358\n",
      "Iteration [88] | loss: 0.0005322470096871257\n",
      "Iteration [89] | loss: 0.0005323661607690156\n",
      "Iteration [90] | loss: 0.0005323661607690156\n",
      "Iteration [91] | loss: 0.0005324853118509054\n",
      "Iteration [92] | loss: 0.0005324853118509054\n",
      "Iteration [93] | loss: 0.0005326044629327953\n",
      "Iteration [94] | loss: 0.0005327236140146852\n",
      "Iteration [95] | loss: 0.0005327236140146852\n",
      "Iteration [96] | loss: 0.0005327236140146852\n",
      "Iteration [97] | loss: 0.0005329619161784649\n",
      "Iteration [98] | loss: 0.0005329619161784649\n",
      "Iteration [99] | loss: 0.0005329619161784649\n",
      "Iteration [100] | loss: 0.0005330810672603548\n",
      "Iteration [101] | loss: 0.0005330810672603548\n",
      "Iteration [102] | loss: 0.0005332001601345837\n",
      "Iteration [103] | loss: 0.0005332001601345837\n",
      "Iteration [104] | loss: 0.0005333193112164736\n",
      "Iteration [105] | loss: 0.0005333193112164736\n",
      "Iteration [106] | loss: 0.0005335576133802533\n",
      "Iteration [107] | loss: 0.0005336767644621432\n",
      "Iteration [108] | loss: 0.0005336767644621432\n",
      "Iteration [109] | loss: 0.0005336767644621432\n",
      "Iteration [110] | loss: 0.0005339150666259229\n",
      "Iteration [111] | loss: 0.0005339150666259229\n",
      "Iteration [112] | loss: 0.0005339150666259229\n",
      "Iteration [113] | loss: 0.0005339150666259229\n",
      "Iteration [114] | loss: 0.0005340342177078128\n",
      "Iteration [115] | loss: 0.0005341533687897027\n",
      "Iteration [116] | loss: 0.0005341533687897027\n",
      "Iteration [117] | loss: 0.0005342725198715925\n",
      "Iteration [118] | loss: 0.0005342725198715925\n",
      "Iteration [119] | loss: 0.0005343916127458215\n",
      "Iteration [120] | loss: 0.0005345107638277113\n",
      "Iteration [121] | loss: 0.0005345107638277113\n",
      "Iteration [122] | loss: 0.0005346299149096012\n",
      "Iteration [123] | loss: 0.000534868217073381\n",
      "Iteration [124] | loss: 0.000534868217073381\n",
      "Iteration [125] | loss: 0.000534868217073381\n",
      "Iteration [126] | loss: 0.000534868217073381\n",
      "Iteration [127] | loss: 0.0005349873681552708\n",
      "Iteration [128] | loss: 0.0005351065192371607\n",
      "Iteration [129] | loss: 0.0005351065192371607\n",
      "Iteration [130] | loss: 0.0005352256703190506\n",
      "Iteration [131] | loss: 0.0005352256703190506\n",
      "Iteration [132] | loss: 0.0005353448214009404\n",
      "Iteration [133] | loss: 0.0005354639724828303\n",
      "Iteration [134] | loss: 0.0005354639724828303\n",
      "Iteration [135] | loss: 0.0005354639724828303\n",
      "Iteration [136] | loss: 0.0005357022164389491\n",
      "Iteration [137] | loss: 0.0005357022164389491\n",
      "Iteration [138] | loss: 0.0005357022164389491\n",
      "Iteration [139] | loss: 0.000535821367520839\n",
      "Iteration [140] | loss: 0.0005359405186027288\n",
      "Iteration [141] | loss: 0.0005360596696846187\n",
      "Iteration [142] | loss: 0.0005360596696846187\n",
      "Iteration [143] | loss: 0.0005361788207665086\n",
      "Iteration [144] | loss: 0.0005361788207665086\n",
      "Iteration [145] | loss: 0.0005362979718483984\n",
      "Iteration [146] | loss: 0.0005364171229302883\n",
      "Iteration [147] | loss: 0.0005364171229302883\n",
      "Iteration [148] | loss: 0.0005364171229302883\n",
      "Iteration [149] | loss: 0.000536655425094068\n",
      "Iteration [150] | loss: 0.000536655425094068\n",
      "Iteration [151] | loss: 0.000536655425094068\n",
      "Iteration [152] | loss: 0.000536655425094068\n",
      "Iteration [153] | loss: 0.0005367745761759579\n",
      "Iteration [154] | loss: 0.0005368936690501869\n",
      "Iteration [155] | loss: 0.0005370128201320767\n",
      "Iteration [156] | loss: 0.0005371319712139666\n",
      "Iteration [157] | loss: 0.0005371319712139666\n",
      "Iteration [158] | loss: 0.0005372511222958565\n",
      "Iteration [159] | loss: 0.0005373702733777463\n",
      "Iteration [160] | loss: 0.0005373702733777463\n",
      "Iteration [161] | loss: 0.0005373702733777463\n",
      "Iteration [162] | loss: 0.0005376085755415261\n",
      "Iteration [163] | loss: 0.0005376085755415261\n",
      "Iteration [164] | loss: 0.0005376085755415261\n",
      "Iteration [165] | loss: 0.0005376085755415261\n",
      "Iteration [166] | loss: 0.000537727726623416\n",
      "Iteration [167] | loss: 0.0005378468777053058\n",
      "Iteration [168] | loss: 0.0005378468777053058\n",
      "Iteration [169] | loss: 0.0005379660287871957\n",
      "Iteration [170] | loss: 0.0005379660287871957\n",
      "Iteration [171] | loss: 0.0005380851216614246\n",
      "Iteration [172] | loss: 0.0005383234238252044\n",
      "Iteration [173] | loss: 0.0005383234238252044\n",
      "Iteration [174] | loss: 0.0005383234238252044\n",
      "Iteration [175] | loss: 0.0005385617259889841\n",
      "Iteration [176] | loss: 0.0005385617259889841\n",
      "Iteration [177] | loss: 0.0005385617259889841\n",
      "Iteration [178] | loss: 0.000538680877070874\n",
      "Iteration [179] | loss: 0.0005388000281527638\n",
      "Iteration [180] | loss: 0.0005388000281527638\n",
      "Iteration [181] | loss: 0.0005388000281527638\n",
      "Iteration [182] | loss: 0.0005389191792346537\n",
      "Iteration [183] | loss: 0.0005389191792346537\n",
      "Iteration [184] | loss: 0.0005390383303165436\n",
      "Iteration [185] | loss: 0.0005391574813984334\n",
      "Iteration [186] | loss: 0.0005391574813984334\n",
      "Iteration [187] | loss: 0.0005391574813984334\n",
      "Iteration [188] | loss: 0.0005395148764364421\n",
      "Iteration [189] | loss: 0.0005395148764364421\n",
      "Iteration [190] | loss: 0.0005395148764364421\n",
      "Iteration [191] | loss: 0.000539634027518332\n",
      "Iteration [192] | loss: 0.0005397531786002219\n",
      "Iteration [193] | loss: 0.0005397531786002219\n",
      "Iteration [194] | loss: 0.0005398723296821117\n",
      "Iteration [195] | loss: 0.0005398723296821117\n",
      "Iteration [196] | loss: 0.0005398723296821117\n",
      "Iteration [197] | loss: 0.0005399914807640016\n",
      "Iteration [198] | loss: 0.0005401106318458915\n",
      "Iteration [199] | loss: 0.0005401106318458915\n",
      "Iteration [200] | loss: 0.0005401106318458915\n",
      "Iteration [201] | loss: 0.0005403488758020103\n",
      "Iteration [202] | loss: 0.0005403488758020103\n",
      "Iteration [203] | loss: 0.0005403488758020103\n",
      "Iteration [204] | loss: 0.00054058717796579\n",
      "Iteration [205] | loss: 0.0005407063290476799\n",
      "Iteration [206] | loss: 0.0005407063290476799\n",
      "Iteration [207] | loss: 0.0005408254801295698\n",
      "Iteration [208] | loss: 0.0005408254801295698\n",
      "Iteration [209] | loss: 0.0005409446312114596\n",
      "Iteration [210] | loss: 0.0005410637822933495\n",
      "Iteration [211] | loss: 0.0005410637822933495\n",
      "Iteration [212] | loss: 0.0005410637822933495\n",
      "Iteration [213] | loss: 0.0005410637822933495\n",
      "Iteration [214] | loss: 0.0005413020844571292\n",
      "Iteration [215] | loss: 0.0005413020844571292\n",
      "Iteration [216] | loss: 0.0005413020844571292\n",
      "Iteration [217] | loss: 0.0005414212355390191\n",
      "Iteration [218] | loss: 0.0005415403284132481\n",
      "Iteration [219] | loss: 0.0005415403284132481\n",
      "Iteration [220] | loss: 0.0005417786305770278\n",
      "Iteration [221] | loss: 0.0005417786305770278\n",
      "Iteration [222] | loss: 0.0005418977816589177\n",
      "Iteration [223] | loss: 0.0005420169327408075\n",
      "Iteration [224] | loss: 0.0005420169327408075\n",
      "Iteration [225] | loss: 0.0005420169327408075\n",
      "Iteration [226] | loss: 0.0005422552349045873\n",
      "Iteration [227] | loss: 0.0005422552349045873\n",
      "Iteration [228] | loss: 0.0005422552349045873\n",
      "Iteration [229] | loss: 0.0005423743859864771\n",
      "Iteration [230] | loss: 0.000542493537068367\n",
      "Iteration [231] | loss: 0.000542493537068367\n",
      "Iteration [232] | loss: 0.000542493537068367\n",
      "Iteration [233] | loss: 0.000542612629942596\n",
      "Iteration [234] | loss: 0.000542612629942596\n",
      "Iteration [235] | loss: 0.0005427317810244858\n",
      "Iteration [236] | loss: 0.0005428509321063757\n",
      "Iteration [237] | loss: 0.0005429700831882656\n",
      "Iteration [238] | loss: 0.0005429700831882656\n",
      "Iteration [239] | loss: 0.0005432083853520453\n",
      "Iteration [240] | loss: 0.0005432083853520453\n",
      "Iteration [241] | loss: 0.0005432083853520453\n",
      "Iteration [242] | loss: 0.0005433275364339352\n",
      "Iteration [243] | loss: 0.000543446687515825\n",
      "Iteration [244] | loss: 0.000543446687515825\n",
      "Iteration [245] | loss: 0.0005435658385977149\n",
      "Iteration [246] | loss: 0.0005435658385977149\n",
      "Iteration [247] | loss: 0.0005436849314719439\n",
      "Iteration [248] | loss: 0.0005438040825538337\n",
      "Iteration [249] | loss: 0.0005438040825538337\n",
      "Iteration [250] | loss: 0.0005438040825538337\n",
      "Iteration [251] | loss: 0.0005438040825538337\n",
      "Iteration [252] | loss: 0.0005440423847176135\n",
      "Iteration [253] | loss: 0.0005441615357995033\n",
      "Iteration [254] | loss: 0.0005441615357995033\n",
      "Iteration [255] | loss: 0.0005442806868813932\n",
      "Iteration [256] | loss: 0.0005443998379632831\n",
      "Iteration [257] | loss: 0.0005443998379632831\n",
      "Iteration [258] | loss: 0.0005445189890451729\n",
      "Iteration [259] | loss: 0.0005445189890451729\n",
      "Iteration [260] | loss: 0.0005446380819194019\n",
      "Iteration [261] | loss: 0.0005447572330012918\n",
      "Iteration [262] | loss: 0.0005447572330012918\n",
      "Iteration [263] | loss: 0.0005447572330012918\n",
      "Iteration [264] | loss: 0.0005449955351650715\n",
      "Iteration [265] | loss: 0.0005449955351650715\n",
      "Iteration [266] | loss: 0.0005449955351650715\n",
      "Iteration [267] | loss: 0.0005451146862469614\n",
      "Iteration [268] | loss: 0.0005452338373288512\n",
      "Iteration [269] | loss: 0.0005453529884107411\n",
      "Iteration [270] | loss: 0.000545472139492631\n",
      "Iteration [271] | loss: 0.000545472139492631\n",
      "Iteration [272] | loss: 0.000545472139492631\n",
      "Iteration [273] | loss: 0.0005455912905745208\n",
      "Iteration [274] | loss: 0.0005457103834487498\n",
      "Iteration [275] | loss: 0.0005457103834487498\n",
      "Iteration [276] | loss: 0.0005457103834487498\n",
      "Iteration [277] | loss: 0.0005459486856125295\n",
      "Iteration [278] | loss: 0.0005459486856125295\n",
      "Iteration [279] | loss: 0.0005459486856125295\n",
      "Iteration [280] | loss: 0.0005460678366944194\n",
      "Iteration [281] | loss: 0.0005461869877763093\n",
      "Iteration [282] | loss: 0.0005461869877763093\n",
      "Iteration [283] | loss: 0.0005463061388581991\n",
      "Iteration [284] | loss: 0.0005463061388581991\n",
      "Iteration [285] | loss: 0.0005465444410219789\n",
      "Iteration [286] | loss: 0.0005466635921038687\n",
      "Iteration [287] | loss: 0.0005466635921038687\n",
      "Iteration [288] | loss: 0.0005466635921038687\n",
      "Iteration [289] | loss: 0.0005469018360599875\n",
      "Iteration [290] | loss: 0.0005469018360599875\n",
      "Iteration [291] | loss: 0.0005469018360599875\n",
      "Iteration [292] | loss: 0.0005470209871418774\n",
      "Iteration [293] | loss: 0.0005471401382237673\n",
      "Iteration [294] | loss: 0.0005471401382237673\n",
      "Iteration [295] | loss: 0.0005472592893056571\n",
      "Iteration [296] | loss: 0.0005472592893056571\n",
      "Iteration [297] | loss: 0.000547378440387547\n",
      "Iteration [298] | loss: 0.0005474975914694369\n",
      "Iteration [299] | loss: 0.0005474975914694369\n",
      "Iteration [300] | loss: 0.0005474975914694369\n",
      "Iteration [301] | loss: 0.0005476167425513268\n",
      "Iteration [302] | loss: 0.0005478549865074456\n",
      "Iteration [303] | loss: 0.0005478549865074456\n",
      "Iteration [304] | loss: 0.0005478549865074456\n",
      "Iteration [305] | loss: 0.0005479741375893354\n",
      "Iteration [306] | loss: 0.0005480932886712253\n",
      "Iteration [307] | loss: 0.0005480932886712253\n",
      "Iteration [308] | loss: 0.0005482124397531152\n",
      "Iteration [309] | loss: 0.0005482124397531152\n",
      "Iteration [310] | loss: 0.000548331590835005\n",
      "Iteration [311] | loss: 0.0005484507419168949\n",
      "Iteration [312] | loss: 0.0005484507419168949\n",
      "Iteration [313] | loss: 0.0005484507419168949\n",
      "Iteration [314] | loss: 0.0005486889858730137\n",
      "Iteration [315] | loss: 0.0005486889858730137\n",
      "Iteration [316] | loss: 0.0005488081369549036\n",
      "Iteration [317] | loss: 0.0005489272880367935\n",
      "Iteration [318] | loss: 0.0005490464391186833\n",
      "Iteration [319] | loss: 0.0005490464391186833\n",
      "Iteration [320] | loss: 0.0005491655902005732\n",
      "Iteration [321] | loss: 0.0005491655902005732\n",
      "Iteration [322] | loss: 0.0005492847412824631\n",
      "Iteration [323] | loss: 0.0005494038923643529\n",
      "Iteration [324] | loss: 0.0005494038923643529\n",
      "Iteration [325] | loss: 0.0005494038923643529\n",
      "Iteration [326] | loss: 0.0005496421363204718\n",
      "Iteration [327] | loss: 0.0005496421363204718\n",
      "Iteration [328] | loss: 0.0005496421363204718\n",
      "Iteration [329] | loss: 0.0005496421363204718\n",
      "Iteration [330] | loss: 0.0005497612874023616\n",
      "Iteration [331] | loss: 0.0005498804384842515\n",
      "Iteration [332] | loss: 0.0005498804384842515\n",
      "Iteration [333] | loss: 0.0005501187406480312\n",
      "Iteration [334] | loss: 0.0005501187406480312\n",
      "Iteration [335] | loss: 0.0005502378917299211\n",
      "Iteration [336] | loss: 0.000550357042811811\n",
      "Iteration [337] | loss: 0.000550357042811811\n",
      "Iteration [338] | loss: 0.000550357042811811\n",
      "Iteration [339] | loss: 0.0005505952867679298\n",
      "Iteration [340] | loss: 0.0005505952867679298\n",
      "Iteration [341] | loss: 0.0005505952867679298\n",
      "Iteration [342] | loss: 0.0005507144378498197\n",
      "Iteration [343] | loss: 0.0005507144378498197\n",
      "Iteration [344] | loss: 0.0005508335889317095\n",
      "Iteration [345] | loss: 0.0005508335889317095\n",
      "Iteration [346] | loss: 0.0005509527400135994\n",
      "Iteration [347] | loss: 0.0005509527400135994\n",
      "Iteration [348] | loss: 0.0005510718910954893\n",
      "Iteration [349] | loss: 0.0005511910421773791\n",
      "Iteration [350] | loss: 0.0005511910421773791\n",
      "Iteration [351] | loss: 0.000551310193259269\n",
      "Iteration [352] | loss: 0.0005515484372153878\n",
      "Iteration [353] | loss: 0.0005515484372153878\n",
      "Iteration [354] | loss: 0.0005515484372153878\n",
      "Iteration [355] | loss: 0.0005516675882972777\n",
      "Iteration [356] | loss: 0.0005516675882972777\n",
      "Iteration [357] | loss: 0.0005517867393791676\n",
      "Iteration [358] | loss: 0.0005519058904610574\n",
      "Iteration [359] | loss: 0.0005519058904610574\n",
      "Iteration [360] | loss: 0.0005519058904610574\n",
      "Iteration [361] | loss: 0.0005521441926248372\n",
      "Iteration [362] | loss: 0.0005521441926248372\n",
      "Iteration [363] | loss: 0.0005521441926248372\n",
      "Iteration [364] | loss: 0.0005521441926248372\n",
      "Iteration [365] | loss: 0.0005523824947886169\n",
      "Iteration [366] | loss: 0.0005523824947886169\n",
      "Iteration [367] | loss: 0.0005523824947886169\n",
      "Iteration [368] | loss: 0.0005526207387447357\n",
      "Iteration [369] | loss: 0.0005526207387447357\n",
      "Iteration [370] | loss: 0.0005527398898266256\n",
      "Iteration [371] | loss: 0.0005528590409085155\n",
      "Iteration [372] | loss: 0.0005528590409085155\n",
      "Iteration [373] | loss: 0.0005528590409085155\n",
      "Iteration [374] | loss: 0.0005530973430722952\n",
      "Iteration [375] | loss: 0.0005530973430722952\n",
      "Iteration [376] | loss: 0.0005530973430722952\n",
      "Iteration [377] | loss: 0.0005532164941541851\n",
      "Iteration [378] | loss: 0.000553335587028414\n",
      "Iteration [379] | loss: 0.000553335587028414\n",
      "Iteration [380] | loss: 0.0005534547381103039\n",
      "Iteration [381] | loss: 0.0005534547381103039\n",
      "Iteration [382] | loss: 0.0005535738891921937\n",
      "Iteration [383] | loss: 0.0005535738891921937\n",
      "Iteration [384] | loss: 0.0005536930402740836\n",
      "Iteration [385] | loss: 0.0005538121913559735\n",
      "Iteration [386] | loss: 0.0005538121913559735\n",
      "Iteration [387] | loss: 0.0005540504935197532\n",
      "Iteration [388] | loss: 0.0005540504935197532\n",
      "Iteration [389] | loss: 0.0005540504935197532\n",
      "Iteration [390] | loss: 0.0005541696446016431\n",
      "Iteration [391] | loss: 0.000554288737475872\n",
      "Iteration [392] | loss: 0.000554288737475872\n",
      "Iteration [393] | loss: 0.0005544078885577619\n",
      "Iteration [394] | loss: 0.0005544078885577619\n",
      "Iteration [395] | loss: 0.0005545270396396518\n",
      "Iteration [396] | loss: 0.0005546461907215416\n",
      "Iteration [397] | loss: 0.0005546461907215416\n",
      "Iteration [398] | loss: 0.0005546461907215416\n",
      "Iteration [399] | loss: 0.0005547653418034315\n",
      "Iteration [400] | loss: 0.0005548844928853214\n",
      "Iteration [401] | loss: 0.0005548844928853214\n",
      "Iteration [402] | loss: 0.0005550036439672112\n",
      "Iteration [403] | loss: 0.0005551227368414402\n",
      "Iteration [404] | loss: 0.0005552418879233301\n",
      "Iteration [405] | loss: 0.0005552418879233301\n",
      "Iteration [406] | loss: 0.0005553610390052199\n",
      "Iteration [407] | loss: 0.0005553610390052199\n",
      "Iteration [408] | loss: 0.0005554801900871098\n",
      "Iteration [409] | loss: 0.0005555993411689997\n",
      "Iteration [410] | loss: 0.0005555993411689997\n",
      "Iteration [411] | loss: 0.0005555993411689997\n",
      "Iteration [412] | loss: 0.0005558376433327794\n",
      "Iteration [413] | loss: 0.0005558376433327794\n",
      "Iteration [414] | loss: 0.0005558376433327794\n",
      "Iteration [415] | loss: 0.0005559567362070084\n",
      "Iteration [416] | loss: 0.0005559567362070084\n",
      "Iteration [417] | loss: 0.0005560758872888982\n",
      "Iteration [418] | loss: 0.0005561950383707881\n",
      "Iteration [419] | loss: 0.0005561950383707881\n",
      "Iteration [420] | loss: 0.000556314189452678\n",
      "Iteration [421] | loss: 0.0005565524916164577\n",
      "Iteration [422] | loss: 0.0005565524916164577\n",
      "Iteration [423] | loss: 0.0005565524916164577\n",
      "Iteration [424] | loss: 0.0005566716426983476\n",
      "Iteration [425] | loss: 0.0005567907355725765\n",
      "Iteration [426] | loss: 0.0005567907355725765\n",
      "Iteration [427] | loss: 0.0005567907355725765\n",
      "Iteration [428] | loss: 0.0005569098866544664\n",
      "Iteration [429] | loss: 0.0005570290377363563\n",
      "Iteration [430] | loss: 0.0005570290377363563\n",
      "Iteration [431] | loss: 0.0005571481888182461\n",
      "Iteration [432] | loss: 0.0005571481888182461\n",
      "Iteration [433] | loss: 0.000557267339900136\n",
      "Iteration [434] | loss: 0.0005573864909820259\n",
      "Iteration [435] | loss: 0.0005573864909820259\n",
      "Iteration [436] | loss: 0.0005573864909820259\n",
      "Iteration [437] | loss: 0.0005576247931458056\n",
      "Iteration [438] | loss: 0.0005577438860200346\n",
      "Iteration [439] | loss: 0.0005577438860200346\n",
      "Iteration [440] | loss: 0.0005578630371019244\n",
      "Iteration [441] | loss: 0.0005578630371019244\n",
      "Iteration [442] | loss: 0.0005579821881838143\n",
      "Iteration [443] | loss: 0.0005581013392657042\n",
      "Iteration [444] | loss: 0.0005581013392657042\n",
      "Iteration [445] | loss: 0.0005581013392657042\n",
      "Iteration [446] | loss: 0.0005583396414294839\n",
      "Iteration [447] | loss: 0.0005583396414294839\n",
      "Iteration [448] | loss: 0.0005583396414294839\n",
      "Iteration [449] | loss: 0.0005584587925113738\n",
      "Iteration [450] | loss: 0.0005585778853856027\n",
      "Iteration [451] | loss: 0.0005585778853856027\n",
      "Iteration [452] | loss: 0.0005586970364674926\n",
      "Iteration [453] | loss: 0.0005586970364674926\n",
      "Iteration [454] | loss: 0.0005588161875493824\n",
      "Iteration [455] | loss: 0.0005590544897131622\n",
      "Iteration [456] | loss: 0.0005590544897131622\n",
      "Iteration [457] | loss: 0.0005590544897131622\n",
      "Iteration [458] | loss: 0.0005590544897131622\n",
      "Iteration [459] | loss: 0.000559292733669281\n",
      "Iteration [460] | loss: 0.000559292733669281\n",
      "Iteration [461] | loss: 0.000559292733669281\n",
      "Iteration [462] | loss: 0.0005594118847511709\n",
      "Iteration [463] | loss: 0.0005595310358330607\n",
      "Iteration [464] | loss: 0.0005595310358330607\n",
      "Iteration [465] | loss: 0.0005596501869149506\n",
      "Iteration [466] | loss: 0.0005596501869149506\n",
      "Iteration [467] | loss: 0.0005597693379968405\n",
      "Iteration [468] | loss: 0.0005598884890787303\n",
      "Iteration [469] | loss: 0.0005598884890787303\n",
      "Iteration [470] | loss: 0.0005598884890787303\n",
      "Iteration [471] | loss: 0.000560245884116739\n",
      "Iteration [472] | loss: 0.000560245884116739\n",
      "Iteration [473] | loss: 0.000560245884116739\n",
      "Iteration [474] | loss: 0.0005603650351986289\n",
      "Iteration [475] | loss: 0.0005604841862805188\n",
      "Iteration [476] | loss: 0.0005604841862805188\n",
      "Iteration [477] | loss: 0.0005606033373624086\n",
      "Iteration [478] | loss: 0.0005606033373624086\n",
      "Iteration [479] | loss: 0.0005606033373624086\n",
      "Iteration [480] | loss: 0.0005608416395261884\n",
      "Iteration [481] | loss: 0.0005608416395261884\n",
      "Iteration [482] | loss: 0.0005608416395261884\n",
      "Iteration [483] | loss: 0.0005609607324004173\n",
      "Iteration [484] | loss: 0.0005610798834823072\n",
      "Iteration [485] | loss: 0.0005610798834823072\n",
      "Iteration [486] | loss: 0.0005611990345641971\n",
      "Iteration [487] | loss: 0.0005611990345641971\n",
      "Iteration [488] | loss: 0.0005614373367279768\n",
      "Iteration [489] | loss: 0.0005615564878098667\n",
      "Iteration [490] | loss: 0.0005615564878098667\n",
      "Iteration [491] | loss: 0.0005615564878098667\n",
      "Iteration [492] | loss: 0.0005617947317659855\n",
      "Iteration [493] | loss: 0.0005617947317659855\n",
      "Iteration [494] | loss: 0.0005617947317659855\n",
      "Iteration [495] | loss: 0.0005619138828478754\n",
      "Iteration [496] | loss: 0.0005620330339297652\n",
      "Iteration [497] | loss: 0.0005620330339297652\n",
      "Iteration [498] | loss: 0.0005621521850116551\n",
      "Iteration [499] | loss: 0.0005621521850116551\n",
      "Iteration [500] | loss: 0.000562271336093545\n",
      "Iteration [501] | loss: 0.0005623904871754348\n",
      "Iteration [502] | loss: 0.0005623904871754348\n",
      "Iteration [503] | loss: 0.0005623904871754348\n",
      "Iteration [504] | loss: 0.0005623904871754348\n",
      "Iteration [505] | loss: 0.0005627478822134435\n",
      "Iteration [506] | loss: 0.0005627478822134435\n",
      "Iteration [507] | loss: 0.0005627478822134435\n",
      "Iteration [508] | loss: 0.0005628670332953334\n",
      "Iteration [509] | loss: 0.0005629861843772233\n",
      "Iteration [510] | loss: 0.0005629861843772233\n",
      "Iteration [511] | loss: 0.0005631053354591131\n",
      "Iteration [512] | loss: 0.0005631053354591131\n",
      "Iteration [513] | loss: 0.000563224486541003\n",
      "Iteration [514] | loss: 0.0005633435794152319\n",
      "Iteration [515] | loss: 0.0005633435794152319\n",
      "Iteration [516] | loss: 0.0005633435794152319\n",
      "Iteration [517] | loss: 0.0005635818815790117\n",
      "Iteration [518] | loss: 0.0005635818815790117\n",
      "Iteration [519] | loss: 0.0005635818815790117\n",
      "Iteration [520] | loss: 0.0005637010326609015\n",
      "Iteration [521] | loss: 0.0005638201837427914\n",
      "Iteration [522] | loss: 0.0005639393348246813\n",
      "Iteration [523] | loss: 0.0005640584276989102\n",
      "Iteration [524] | loss: 0.0005640584276989102\n",
      "Iteration [525] | loss: 0.0005641775787808001\n",
      "Iteration [526] | loss: 0.00056429672986269\n",
      "Iteration [527] | loss: 0.00056429672986269\n",
      "Iteration [528] | loss: 0.00056429672986269\n",
      "Iteration [529] | loss: 0.0005645350320264697\n",
      "Iteration [530] | loss: 0.0005645350320264697\n",
      "Iteration [531] | loss: 0.0005645350320264697\n",
      "Iteration [532] | loss: 0.0005646541831083596\n",
      "Iteration [533] | loss: 0.0005647733341902494\n",
      "Iteration [534] | loss: 0.0005647733341902494\n",
      "Iteration [535] | loss: 0.0005648924270644784\n",
      "Iteration [536] | loss: 0.0005648924270644784\n",
      "Iteration [537] | loss: 0.0005648924270644784\n",
      "Iteration [538] | loss: 0.0005651307292282581\n",
      "Iteration [539] | loss: 0.000565249880310148\n",
      "Iteration [540] | loss: 0.000565249880310148\n",
      "Iteration [541] | loss: 0.0005653690313920379\n",
      "Iteration [542] | loss: 0.0005654881824739277\n",
      "Iteration [543] | loss: 0.0005654881824739277\n",
      "Iteration [544] | loss: 0.0005656072753481567\n",
      "Iteration [545] | loss: 0.0005656072753481567\n",
      "Iteration [546] | loss: 0.0005657264264300466\n",
      "Iteration [547] | loss: 0.0005658455775119364\n",
      "Iteration [548] | loss: 0.0005658455775119364\n",
      "Iteration [549] | loss: 0.0005658455775119364\n",
      "Iteration [550] | loss: 0.0005660838796757162\n",
      "Iteration [551] | loss: 0.0005660838796757162\n",
      "Iteration [552] | loss: 0.0005660838796757162\n",
      "Iteration [553] | loss: 0.000566203030757606\n",
      "Iteration [554] | loss: 0.0005663221818394959\n",
      "Iteration [555] | loss: 0.0005663221818394959\n",
      "Iteration [556] | loss: 0.0005665604257956147\n",
      "Iteration [557] | loss: 0.0005665604257956147\n",
      "Iteration [558] | loss: 0.0005666795768775046\n",
      "Iteration [559] | loss: 0.0005667987279593945\n",
      "Iteration [560] | loss: 0.0005667987279593945\n",
      "Iteration [561] | loss: 0.0005667987279593945\n",
      "Iteration [562] | loss: 0.0005670370301231742\n",
      "Iteration [563] | loss: 0.0005670370301231742\n",
      "Iteration [564] | loss: 0.0005670370301231742\n",
      "Iteration [565] | loss: 0.0005671561229974031\n",
      "Iteration [566] | loss: 0.000567275274079293\n",
      "Iteration [567] | loss: 0.000567275274079293\n",
      "Iteration [568] | loss: 0.0005673944251611829\n",
      "Iteration [569] | loss: 0.0005673944251611829\n",
      "Iteration [570] | loss: 0.0005675135762430727\n",
      "Iteration [571] | loss: 0.0005676327273249626\n",
      "Iteration [572] | loss: 0.0005677518784068525\n",
      "Iteration [573] | loss: 0.0005677518784068525\n",
      "Iteration [574] | loss: 0.0005679901223629713\n",
      "Iteration [575] | loss: 0.0005679901223629713\n",
      "Iteration [576] | loss: 0.0005679901223629713\n",
      "Iteration [577] | loss: 0.0005681092734448612\n",
      "Iteration [578] | loss: 0.000568228424526751\n",
      "Iteration [579] | loss: 0.000568228424526751\n",
      "Iteration [580] | loss: 0.0005683475756086409\n",
      "Iteration [581] | loss: 0.0005683475756086409\n",
      "Iteration [582] | loss: 0.0005684667266905308\n",
      "Iteration [583] | loss: 0.0005685858195647597\n",
      "Iteration [584] | loss: 0.0005685858195647597\n",
      "Iteration [585] | loss: 0.0005685858195647597\n",
      "Iteration [586] | loss: 0.0005688241217285395\n",
      "Iteration [587] | loss: 0.0005688241217285395\n",
      "Iteration [588] | loss: 0.0005688241217285395\n",
      "Iteration [589] | loss: 0.0005690624238923192\n",
      "Iteration [590] | loss: 0.0005691815749742091\n",
      "Iteration [591] | loss: 0.0005691815749742091\n",
      "Iteration [592] | loss: 0.000569300667848438\n",
      "Iteration [593] | loss: 0.000569300667848438\n",
      "Iteration [594] | loss: 0.000569300667848438\n",
      "Iteration [595] | loss: 0.0005695389700122178\n",
      "Iteration [596] | loss: 0.0005695389700122178\n",
      "Iteration [597] | loss: 0.0005695389700122178\n",
      "Iteration [598] | loss: 0.0005696581210941076\n",
      "Iteration [599] | loss: 0.0005697772721759975\n",
      "Iteration [600] | loss: 0.0005697772721759975\n",
      "Iteration [601] | loss: 0.0005698964232578874\n",
      "Iteration [602] | loss: 0.0005698964232578874\n",
      "Iteration [603] | loss: 0.0005700155161321163\n",
      "Iteration [604] | loss: 0.0005701346672140062\n",
      "Iteration [605] | loss: 0.0005701346672140062\n",
      "Iteration [606] | loss: 0.000570253818295896\n",
      "Iteration [607] | loss: 0.0005704921204596758\n",
      "Iteration [608] | loss: 0.0005704921204596758\n",
      "Iteration [609] | loss: 0.0005706112715415657\n",
      "Iteration [610] | loss: 0.0005706112715415657\n",
      "Iteration [611] | loss: 0.0005707303644157946\n",
      "Iteration [612] | loss: 0.0005708495154976845\n",
      "Iteration [613] | loss: 0.0005708495154976845\n",
      "Iteration [614] | loss: 0.0005708495154976845\n",
      "Iteration [615] | loss: 0.0005710878176614642\n",
      "Iteration [616] | loss: 0.0005710878176614642\n",
      "Iteration [617] | loss: 0.0005710878176614642\n",
      "Iteration [618] | loss: 0.0005712069687433541\n",
      "Iteration [619] | loss: 0.000571326119825244\n",
      "Iteration [620] | loss: 0.000571326119825244\n",
      "Iteration [621] | loss: 0.0005714452126994729\n",
      "Iteration [622] | loss: 0.0005715643637813628\n",
      "Iteration [623] | loss: 0.0005716835148632526\n",
      "Iteration [624] | loss: 0.0005718026659451425\n",
      "Iteration [625] | loss: 0.0005718026659451425\n",
      "Iteration [626] | loss: 0.0005718026659451425\n",
      "Iteration [627] | loss: 0.0005720409681089222\n",
      "Iteration [628] | loss: 0.0005720409681089222\n",
      "Iteration [629] | loss: 0.0005720409681089222\n",
      "Iteration [630] | loss: 0.0005721600609831512\n",
      "Iteration [631] | loss: 0.0005722792120650411\n",
      "Iteration [632] | loss: 0.0005722792120650411\n",
      "Iteration [633] | loss: 0.0005723983631469309\n",
      "Iteration [634] | loss: 0.0005723983631469309\n",
      "Iteration [635] | loss: 0.0005725175142288208\n",
      "Iteration [636] | loss: 0.0005726366653107107\n",
      "Iteration [637] | loss: 0.0005726366653107107\n",
      "Iteration [638] | loss: 0.0005726366653107107\n",
      "Iteration [639] | loss: 0.0005729940603487194\n",
      "Iteration [640] | loss: 0.0005729940603487194\n",
      "Iteration [641] | loss: 0.0005729940603487194\n",
      "Iteration [642] | loss: 0.0005731132114306092\n",
      "Iteration [643] | loss: 0.0005732323625124991\n",
      "Iteration [644] | loss: 0.0005732323625124991\n",
      "Iteration [645] | loss: 0.000573351513594389\n",
      "Iteration [646] | loss: 0.000573351513594389\n",
      "Iteration [647] | loss: 0.0005734706646762788\n",
      "Iteration [648] | loss: 0.0005735897575505078\n",
      "Iteration [649] | loss: 0.0005735897575505078\n",
      "Iteration [650] | loss: 0.0005735897575505078\n",
      "Iteration [651] | loss: 0.0005738280597142875\n",
      "Iteration [652] | loss: 0.0005738280597142875\n",
      "Iteration [653] | loss: 0.0005739472107961774\n",
      "Iteration [654] | loss: 0.0005739472107961774\n",
      "Iteration [655] | loss: 0.0005741854547522962\n",
      "Iteration [656] | loss: 0.0005743046058341861\n",
      "Iteration [657] | loss: 0.0005743046058341861\n",
      "Iteration [658] | loss: 0.0005743046058341861\n",
      "Iteration [659] | loss: 0.0005745429079979658\n",
      "Iteration [660] | loss: 0.0005745429079979658\n",
      "Iteration [661] | loss: 0.0005745429079979658\n",
      "Iteration [662] | loss: 0.0005746620590798557\n",
      "Iteration [663] | loss: 0.0005747812101617455\n",
      "Iteration [664] | loss: 0.0005747812101617455\n",
      "Iteration [665] | loss: 0.0005749003030359745\n",
      "Iteration [666] | loss: 0.0005749003030359745\n",
      "Iteration [667] | loss: 0.0005750194541178644\n",
      "Iteration [668] | loss: 0.0005751386051997542\n",
      "Iteration [669] | loss: 0.0005751386051997542\n",
      "Iteration [670] | loss: 0.0005751386051997542\n",
      "Iteration [671] | loss: 0.0005754960584454238\n",
      "Iteration [672] | loss: 0.0005754960584454238\n",
      "Iteration [673] | loss: 0.0005754960584454238\n",
      "Iteration [674] | loss: 0.0005757343024015427\n",
      "Iteration [675] | loss: 0.0005757343024015427\n",
      "Iteration [676] | loss: 0.0005757343024015427\n",
      "Iteration [677] | loss: 0.0005758534534834325\n",
      "Iteration [678] | loss: 0.0005759726045653224\n",
      "Iteration [679] | loss: 0.0005759726045653224\n",
      "Iteration [680] | loss: 0.0005760917556472123\n",
      "Iteration [681] | loss: 0.0005760917556472123\n",
      "Iteration [682] | loss: 0.0005763299996033311\n",
      "Iteration [683] | loss: 0.0005763299996033311\n",
      "Iteration [684] | loss: 0.0005763299996033311\n",
      "Iteration [685] | loss: 0.000576449150685221\n",
      "Iteration [686] | loss: 0.0005765683017671108\n",
      "Iteration [687] | loss: 0.0005765683017671108\n",
      "Iteration [688] | loss: 0.0005768066039308906\n",
      "Iteration [689] | loss: 0.0005768066039308906\n",
      "Iteration [690] | loss: 0.0005769256968051195\n",
      "Iteration [691] | loss: 0.0005770448478870094\n",
      "Iteration [692] | loss: 0.0005770448478870094\n",
      "Iteration [693] | loss: 0.0005770448478870094\n",
      "Iteration [694] | loss: 0.0005772831500507891\n",
      "Iteration [695] | loss: 0.0005772831500507891\n",
      "Iteration [696] | loss: 0.0005772831500507891\n",
      "Iteration [697] | loss: 0.000577402301132679\n",
      "Iteration [698] | loss: 0.0005775213940069079\n",
      "Iteration [699] | loss: 0.0005775213940069079\n",
      "Iteration [700] | loss: 0.0005776405450887978\n",
      "Iteration [701] | loss: 0.0005776405450887978\n",
      "Iteration [702] | loss: 0.0005777596961706877\n",
      "Iteration [703] | loss: 0.0005778788472525775\n",
      "Iteration [704] | loss: 0.0005779979983344674\n",
      "Iteration [705] | loss: 0.0005779979983344674\n",
      "Iteration [706] | loss: 0.0005782362422905862\n",
      "Iteration [707] | loss: 0.0005782362422905862\n",
      "Iteration [708] | loss: 0.0005783553933724761\n",
      "Iteration [709] | loss: 0.0005783553933724761\n",
      "Iteration [710] | loss: 0.000578474544454366\n",
      "Iteration [711] | loss: 0.0005785936955362558\n",
      "Iteration [712] | loss: 0.0005785936955362558\n",
      "Iteration [713] | loss: 0.0005785936955362558\n",
      "Iteration [714] | loss: 0.0005788319394923747\n",
      "Iteration [715] | loss: 0.0005788319394923747\n",
      "Iteration [716] | loss: 0.0005788319394923747\n",
      "Iteration [717] | loss: 0.0005789510905742645\n",
      "Iteration [718] | loss: 0.0005790702416561544\n",
      "Iteration [719] | loss: 0.0005790702416561544\n",
      "Iteration [720] | loss: 0.0005793085438199341\n",
      "Iteration [721] | loss: 0.0005793085438199341\n",
      "Iteration [722] | loss: 0.0005794276366941631\n",
      "Iteration [723] | loss: 0.000579546787776053\n",
      "Iteration [724] | loss: 0.000579546787776053\n",
      "Iteration [725] | loss: 0.000579546787776053\n",
      "Iteration [726] | loss: 0.0005797850899398327\n",
      "Iteration [727] | loss: 0.0005797850899398327\n",
      "Iteration [728] | loss: 0.0005799042410217226\n",
      "Iteration [729] | loss: 0.0005800233921036124\n",
      "Iteration [730] | loss: 0.0005800233921036124\n",
      "Iteration [731] | loss: 0.0005801424849778414\n",
      "Iteration [732] | loss: 0.0005801424849778414\n",
      "Iteration [733] | loss: 0.0005802616360597312\n",
      "Iteration [734] | loss: 0.0005803807871416211\n",
      "Iteration [735] | loss: 0.0005803807871416211\n",
      "Iteration [736] | loss: 0.000580499938223511\n",
      "Iteration [737] | loss: 0.0005807381821796298\n",
      "Iteration [738] | loss: 0.0005807381821796298\n",
      "Iteration [739] | loss: 0.0005807381821796298\n",
      "Iteration [740] | loss: 0.0005808573332615197\n",
      "Iteration [741] | loss: 0.0005809764843434095\n",
      "Iteration [742] | loss: 0.0005809764843434095\n",
      "Iteration [743] | loss: 0.0005810956354252994\n",
      "Iteration [744] | loss: 0.0005810956354252994\n",
      "Iteration [745] | loss: 0.0005813338793814182\n",
      "Iteration [746] | loss: 0.0005813338793814182\n",
      "Iteration [747] | loss: 0.0005813338793814182\n",
      "Iteration [748] | loss: 0.0005814530304633081\n",
      "Iteration [749] | loss: 0.000581572181545198\n",
      "Iteration [750] | loss: 0.000581572181545198\n",
      "Iteration [751] | loss: 0.0005816913326270878\n",
      "Iteration [752] | loss: 0.0005818104837089777\n",
      "Iteration [753] | loss: 0.0005819296347908676\n",
      "Iteration [754] | loss: 0.0005820487276650965\n",
      "Iteration [755] | loss: 0.0005820487276650965\n",
      "Iteration [756] | loss: 0.0005820487276650965\n",
      "Iteration [757] | loss: 0.0005822870298288763\n",
      "Iteration [758] | loss: 0.0005822870298288763\n",
      "Iteration [759] | loss: 0.0005822870298288763\n",
      "Iteration [760] | loss: 0.0005824061809107661\n",
      "Iteration [761] | loss: 0.000582525331992656\n",
      "Iteration [762] | loss: 0.000582644424866885\n",
      "Iteration [763] | loss: 0.000582644424866885\n",
      "Iteration [764] | loss: 0.0005827635759487748\n",
      "Iteration [765] | loss: 0.0005828827270306647\n",
      "Iteration [766] | loss: 0.0005828827270306647\n",
      "Iteration [767] | loss: 0.0005828827270306647\n",
      "Iteration [768] | loss: 0.0005831210291944444\n",
      "Iteration [769] | loss: 0.0005832401220686734\n",
      "Iteration [770] | loss: 0.0005832401220686734\n",
      "Iteration [771] | loss: 0.0005833592731505632\n",
      "Iteration [772] | loss: 0.0005834784242324531\n",
      "Iteration [773] | loss: 0.0005834784242324531\n",
      "Iteration [774] | loss: 0.000583597575314343\n",
      "Iteration [775] | loss: 0.000583597575314343\n",
      "Iteration [776] | loss: 0.0005837167263962328\n",
      "Iteration [777] | loss: 0.0005838358192704618\n",
      "Iteration [778] | loss: 0.0005838358192704618\n",
      "Iteration [779] | loss: 0.0005839549703523517\n",
      "Iteration [780] | loss: 0.0005840741214342415\n",
      "Iteration [781] | loss: 0.0005840741214342415\n",
      "Iteration [782] | loss: 0.0005841932725161314\n",
      "Iteration [783] | loss: 0.0005841932725161314\n",
      "Iteration [784] | loss: 0.0005844315164722502\n",
      "Iteration [785] | loss: 0.0005845506675541401\n",
      "Iteration [786] | loss: 0.0005845506675541401\n",
      "Iteration [787] | loss: 0.00058466981863603\n",
      "Iteration [788] | loss: 0.0005847889697179198\n",
      "Iteration [789] | loss: 0.0005847889697179198\n",
      "Iteration [790] | loss: 0.0005847889697179198\n",
      "Iteration [791] | loss: 0.0005850272136740386\n",
      "Iteration [792] | loss: 0.0005850272136740386\n",
      "Iteration [793] | loss: 0.0005851463647559285\n",
      "Iteration [794] | loss: 0.0005851463647559285\n",
      "Iteration [795] | loss: 0.0005852655158378184\n",
      "Iteration [796] | loss: 0.0005853846669197083\n",
      "Iteration [797] | loss: 0.0005853846669197083\n",
      "Iteration [798] | loss: 0.0005853846669197083\n",
      "Iteration [799] | loss: 0.0005856229108758271\n",
      "Iteration [800] | loss: 0.0005857420619577169\n",
      "Iteration [801] | loss: 0.0005857420619577169\n",
      "Iteration [802] | loss: 0.0005858612130396068\n",
      "Iteration [803] | loss: 0.0005859803641214967\n",
      "Iteration [804] | loss: 0.0005860995152033865\n",
      "Iteration [805] | loss: 0.0005860995152033865\n",
      "Iteration [806] | loss: 0.0005860995152033865\n",
      "Iteration [807] | loss: 0.0005863377591595054\n",
      "Iteration [808] | loss: 0.0005863377591595054\n",
      "Iteration [809] | loss: 0.0005863377591595054\n",
      "Iteration [810] | loss: 0.0005865760613232851\n",
      "Iteration [811] | loss: 0.0005865760613232851\n",
      "Iteration [812] | loss: 0.0005865760613232851\n",
      "Iteration [813] | loss: 0.000586695212405175\n",
      "Iteration [814] | loss: 0.0005868143052794039\n",
      "Iteration [815] | loss: 0.0005868143052794039\n",
      "Iteration [816] | loss: 0.0005870526074431837\n",
      "Iteration [817] | loss: 0.0005870526074431837\n",
      "Iteration [818] | loss: 0.0005872909096069634\n",
      "Iteration [819] | loss: 0.0005872909096069634\n",
      "Iteration [820] | loss: 0.0005872909096069634\n",
      "Iteration [821] | loss: 0.0005874100024811924\n",
      "Iteration [822] | loss: 0.0005875291535630822\n",
      "Iteration [823] | loss: 0.0005875291535630822\n",
      "Iteration [824] | loss: 0.0005876483046449721\n",
      "Iteration [825] | loss: 0.0005876483046449721\n",
      "Iteration [826] | loss: 0.000587767455726862\n",
      "Iteration [827] | loss: 0.0005878866068087518\n",
      "Iteration [828] | loss: 0.0005878866068087518\n",
      "Iteration [829] | loss: 0.0005881248507648706\n",
      "Iteration [830] | loss: 0.0005881248507648706\n",
      "Iteration [831] | loss: 0.0005881248507648706\n",
      "Iteration [832] | loss: 0.0005883631529286504\n",
      "Iteration [833] | loss: 0.0005884823040105402\n",
      "Iteration [834] | loss: 0.0005884823040105402\n",
      "Iteration [835] | loss: 0.0005886013968847692\n",
      "Iteration [836] | loss: 0.0005886013968847692\n",
      "Iteration [837] | loss: 0.0005887205479666591\n",
      "Iteration [838] | loss: 0.0005888396990485489\n",
      "Iteration [839] | loss: 0.0005888396990485489\n",
      "Iteration [840] | loss: 0.0005889588501304388\n",
      "Iteration [841] | loss: 0.0005890780012123287\n",
      "Iteration [842] | loss: 0.0005890780012123287\n",
      "Iteration [843] | loss: 0.0005891970940865576\n",
      "Iteration [844] | loss: 0.0005891970940865576\n",
      "Iteration [845] | loss: 0.0005893162451684475\n",
      "Iteration [846] | loss: 0.0005894353962503374\n",
      "Iteration [847] | loss: 0.0005894353962503374\n",
      "Iteration [848] | loss: 0.0005896736984141171\n",
      "Iteration [849] | loss: 0.000589792791288346\n",
      "Iteration [850] | loss: 0.000589792791288346\n",
      "Iteration [851] | loss: 0.0005899119423702359\n",
      "Iteration [852] | loss: 0.0005900310934521258\n",
      "Iteration [853] | loss: 0.0005900310934521258\n",
      "Iteration [854] | loss: 0.0005901502445340157\n",
      "Iteration [855] | loss: 0.0005901502445340157\n",
      "Iteration [856] | loss: 0.0005902693956159055\n",
      "Iteration [857] | loss: 0.0005903884884901345\n",
      "Iteration [858] | loss: 0.0005903884884901345\n",
      "Iteration [859] | loss: 0.0005903884884901345\n",
      "Iteration [860] | loss: 0.0005906267906539142\n",
      "Iteration [861] | loss: 0.0005906267906539142\n",
      "Iteration [862] | loss: 0.0005907459417358041\n",
      "Iteration [863] | loss: 0.000590865034610033\n",
      "Iteration [864] | loss: 0.0005909841856919229\n",
      "Iteration [865] | loss: 0.0005911033367738128\n",
      "Iteration [866] | loss: 0.0005911033367738128\n",
      "Iteration [867] | loss: 0.0005912224878557026\n",
      "Iteration [868] | loss: 0.0005913416389375925\n",
      "Iteration [869] | loss: 0.0005913416389375925\n",
      "Iteration [870] | loss: 0.0005913416389375925\n",
      "Iteration [871] | loss: 0.0005915798828937113\n",
      "Iteration [872] | loss: 0.0005915798828937113\n",
      "Iteration [873] | loss: 0.0005916990339756012\n",
      "Iteration [874] | loss: 0.0005916990339756012\n",
      "Iteration [875] | loss: 0.0005918181850574911\n",
      "Iteration [876] | loss: 0.0005919373361393809\n",
      "Iteration [877] | loss: 0.0005919373361393809\n",
      "Iteration [878] | loss: 0.0005920564290136099\n",
      "Iteration [879] | loss: 0.0005922947311773896\n",
      "Iteration [880] | loss: 0.0005922947311773896\n",
      "Iteration [881] | loss: 0.0005924138822592795\n",
      "Iteration [882] | loss: 0.0005925330333411694\n",
      "Iteration [883] | loss: 0.0005925330333411694\n",
      "Iteration [884] | loss: 0.0005926521262153983\n",
      "Iteration [885] | loss: 0.0005926521262153983\n",
      "Iteration [886] | loss: 0.0005927712772972882\n",
      "Iteration [887] | loss: 0.000592890428379178\n",
      "Iteration [888] | loss: 0.000592890428379178\n",
      "Iteration [889] | loss: 0.000592890428379178\n",
      "Iteration [890] | loss: 0.0005931286723352969\n",
      "Iteration [891] | loss: 0.0005931286723352969\n",
      "Iteration [892] | loss: 0.0005932478234171867\n",
      "Iteration [893] | loss: 0.0005933669744990766\n",
      "Iteration [894] | loss: 0.0005933669744990766\n",
      "Iteration [895] | loss: 0.0005936052766628563\n",
      "Iteration [896] | loss: 0.0005936052766628563\n",
      "Iteration [897] | loss: 0.0005937243695370853\n",
      "Iteration [898] | loss: 0.0005938435206189752\n",
      "Iteration [899] | loss: 0.0005938435206189752\n",
      "Iteration [900] | loss: 0.000593962671700865\n",
      "Iteration [901] | loss: 0.0005940818227827549\n",
      "Iteration [902] | loss: 0.0005940818227827549\n",
      "Iteration [903] | loss: 0.0005942009738646448\n",
      "Iteration [904] | loss: 0.0005942009738646448\n",
      "Iteration [905] | loss: 0.0005943200667388737\n",
      "Iteration [906] | loss: 0.0005944392178207636\n",
      "Iteration [907] | loss: 0.0005944392178207636\n",
      "Iteration [908] | loss: 0.0005945583689026535\n",
      "Iteration [909] | loss: 0.0005946775199845433\n",
      "Iteration [910] | loss: 0.0005947966128587723\n",
      "Iteration [911] | loss: 0.0005949157639406621\n",
      "Iteration [912] | loss: 0.000595034915022552\n",
      "Iteration [913] | loss: 0.000595034915022552\n",
      "Iteration [914] | loss: 0.0005951540661044419\n",
      "Iteration [915] | loss: 0.0005951540661044419\n",
      "Iteration [916] | loss: 0.0005952732171863317\n",
      "Iteration [917] | loss: 0.0005953923100605607\n",
      "Iteration [918] | loss: 0.0005953923100605607\n",
      "Iteration [919] | loss: 0.0005956306122243404\n",
      "Iteration [920] | loss: 0.0005956306122243404\n",
      "Iteration [921] | loss: 0.0005956306122243404\n",
      "Iteration [922] | loss: 0.0005957497633062303\n",
      "Iteration [923] | loss: 0.0005958688561804593\n",
      "Iteration [924] | loss: 0.0005958688561804593\n",
      "Iteration [925] | loss: 0.0005959880072623491\n",
      "Iteration [926] | loss: 0.000596107158344239\n",
      "Iteration [927] | loss: 0.0005963454605080187\n",
      "Iteration [928] | loss: 0.0005963454605080187\n",
      "Iteration [929] | loss: 0.0005963454605080187\n",
      "Iteration [930] | loss: 0.0005965837044641376\n",
      "Iteration [931] | loss: 0.0005965837044641376\n",
      "Iteration [932] | loss: 0.0005965837044641376\n",
      "Iteration [933] | loss: 0.0005967028555460274\n",
      "Iteration [934] | loss: 0.0005968220066279173\n",
      "Iteration [935] | loss: 0.0005969410995021462\n",
      "Iteration [936] | loss: 0.0005969410995021462\n",
      "Iteration [937] | loss: 0.0005969410995021462\n",
      "Iteration [938] | loss: 0.000597179401665926\n",
      "Iteration [939] | loss: 0.000597179401665926\n",
      "Iteration [940] | loss: 0.000597179401665926\n",
      "Iteration [941] | loss: 0.0005975367967039347\n",
      "Iteration [942] | loss: 0.0005975367967039347\n",
      "Iteration [943] | loss: 0.0005976559477858245\n",
      "Iteration [944] | loss: 0.0005976559477858245\n",
      "Iteration [945] | loss: 0.0005977750988677144\n",
      "Iteration [946] | loss: 0.0005978942499496043\n",
      "Iteration [947] | loss: 0.0005978942499496043\n",
      "Iteration [948] | loss: 0.0005978942499496043\n",
      "Iteration [949] | loss: 0.0005981324939057231\n",
      "Iteration [950] | loss: 0.0005981324939057231\n",
      "Iteration [951] | loss: 0.000598251644987613\n",
      "Iteration [952] | loss: 0.0005983707960695028\n",
      "Iteration [953] | loss: 0.0005983707960695028\n",
      "Iteration [954] | loss: 0.0005984899471513927\n",
      "Iteration [955] | loss: 0.0005984899471513927\n",
      "Iteration [956] | loss: 0.0005986090400256217\n",
      "Iteration [957] | loss: 0.0005988473421894014\n",
      "Iteration [958] | loss: 0.0005988473421894014\n",
      "Iteration [959] | loss: 0.0005989664932712913\n",
      "Iteration [960] | loss: 0.0005990855861455202\n",
      "Iteration [961] | loss: 0.0005990855861455202\n",
      "Iteration [962] | loss: 0.0005992047372274101\n",
      "Iteration [963] | loss: 0.0005993238883093\n",
      "Iteration [964] | loss: 0.0005993238883093\n",
      "Iteration [965] | loss: 0.0005994430393911898\n",
      "Iteration [966] | loss: 0.0005994430393911898\n",
      "Iteration [967] | loss: 0.0005996812833473086\n",
      "Iteration [968] | loss: 0.0005996812833473086\n",
      "Iteration [969] | loss: 0.0005996812833473086\n",
      "Iteration [970] | loss: 0.0005998004344291985\n",
      "Iteration [971] | loss: 0.0005999195855110884\n",
      "Iteration [972] | loss: 0.0006000387365929782\n",
      "Iteration [973] | loss: 0.0006001578294672072\n",
      "Iteration [974] | loss: 0.0006002769805490971\n",
      "Iteration [975] | loss: 0.0006003961316309869\n",
      "Iteration [976] | loss: 0.0006003961316309869\n",
      "Iteration [977] | loss: 0.0006003961316309869\n",
      "Iteration [978] | loss: 0.0006006343755871058\n",
      "Iteration [979] | loss: 0.0006006343755871058\n",
      "Iteration [980] | loss: 0.0006006343755871058\n",
      "Iteration [981] | loss: 0.0006007535266689956\n",
      "Iteration [982] | loss: 0.0006008726777508855\n",
      "Iteration [983] | loss: 0.0006009918288327754\n",
      "Iteration [984] | loss: 0.0006009918288327754\n",
      "Iteration [985] | loss: 0.0006011109799146652\n",
      "Iteration [986] | loss: 0.0006012300727888942\n",
      "Iteration [987] | loss: 0.000601349223870784\n",
      "Iteration [988] | loss: 0.0006014683749526739\n",
      "Iteration [989] | loss: 0.0006015875260345638\n",
      "Iteration [990] | loss: 0.0006015875260345638\n",
      "Iteration [991] | loss: 0.0006017066189087927\n",
      "Iteration [992] | loss: 0.0006017066189087927\n",
      "Iteration [993] | loss: 0.0006018257699906826\n",
      "Iteration [994] | loss: 0.0006019449210725725\n",
      "Iteration [995] | loss: 0.0006019449210725725\n",
      "Iteration [996] | loss: 0.0006021831650286913\n",
      "Iteration [997] | loss: 0.0006021831650286913\n",
      "Iteration [998] | loss: 0.0006021831650286913\n",
      "Iteration [999] | loss: 0.0006023023161105812\n",
      "Iteration [1000] | loss: 0.000602421467192471\n",
      "Iteration [1001] | loss: 0.000602421467192471\n",
      "Iteration [1002] | loss: 0.0006025406182743609\n",
      "Iteration [1003] | loss: 0.0006027788622304797\n",
      "Iteration [1004] | loss: 0.0006028980133123696\n",
      "Iteration [1005] | loss: 0.0006028980133123696\n",
      "Iteration [1006] | loss: 0.0006028980133123696\n",
      "Iteration [1007] | loss: 0.0006031363154761493\n",
      "Iteration [1008] | loss: 0.0006031363154761493\n",
      "Iteration [1009] | loss: 0.0006032554083503783\n",
      "Iteration [1010] | loss: 0.0006032554083503783\n",
      "Iteration [1011] | loss: 0.0006033745594322681\n",
      "Iteration [1012] | loss: 0.000603493710514158\n",
      "Iteration [1013] | loss: 0.000603493710514158\n",
      "Iteration [1014] | loss: 0.0006036128615960479\n",
      "Iteration [1015] | loss: 0.0006037319544702768\n",
      "Iteration [1016] | loss: 0.0006037319544702768\n",
      "Iteration [1017] | loss: 0.0006038511055521667\n",
      "Iteration [1018] | loss: 0.0006040894077159464\n",
      "Iteration [1019] | loss: 0.0006040894077159464\n",
      "Iteration [1020] | loss: 0.0006042085005901754\n",
      "Iteration [1021] | loss: 0.0006043276516720653\n",
      "Iteration [1022] | loss: 0.0006044468027539551\n",
      "Iteration [1023] | loss: 0.0006044468027539551\n",
      "Iteration [1024] | loss: 0.0006044468027539551\n",
      "Iteration [1025] | loss: 0.0006046851049177349\n",
      "Iteration [1026] | loss: 0.0006046851049177349\n",
      "Iteration [1027] | loss: 0.0006046851049177349\n",
      "Iteration [1028] | loss: 0.0006048041977919638\n",
      "Iteration [1029] | loss: 0.0006049233488738537\n",
      "Iteration [1030] | loss: 0.0006050424999557436\n",
      "Iteration [1031] | loss: 0.0006050424999557436\n",
      "Iteration [1032] | loss: 0.0006051616510376334\n",
      "Iteration [1033] | loss: 0.0006053998949937522\n",
      "Iteration [1034] | loss: 0.0006053998949937522\n",
      "Iteration [1035] | loss: 0.0006055190460756421\n",
      "Iteration [1036] | loss: 0.000605638197157532\n",
      "Iteration [1037] | loss: 0.000605638197157532\n",
      "Iteration [1038] | loss: 0.0006057572900317609\n",
      "Iteration [1039] | loss: 0.0006058764411136508\n",
      "Iteration [1040] | loss: 0.0006058764411136508\n",
      "Iteration [1041] | loss: 0.0006059955921955407\n",
      "Iteration [1042] | loss: 0.0006059955921955407\n",
      "Iteration [1043] | loss: 0.0006062338361516595\n",
      "Iteration [1044] | loss: 0.0006062338361516595\n",
      "Iteration [1045] | loss: 0.0006062338361516595\n",
      "Iteration [1046] | loss: 0.0006064721383154392\n",
      "Iteration [1047] | loss: 0.0006064721383154392\n",
      "Iteration [1048] | loss: 0.000606710382271558\n",
      "Iteration [1049] | loss: 0.000606710382271558\n",
      "Iteration [1050] | loss: 0.0006068295333534479\n",
      "Iteration [1051] | loss: 0.0006069486844353378\n",
      "Iteration [1052] | loss: 0.0006069486844353378\n",
      "Iteration [1053] | loss: 0.0006070678355172276\n",
      "Iteration [1054] | loss: 0.0006071869283914566\n",
      "Iteration [1055] | loss: 0.0006071869283914566\n",
      "Iteration [1056] | loss: 0.0006073060794733465\n",
      "Iteration [1057] | loss: 0.0006074252305552363\n",
      "Iteration [1058] | loss: 0.0006074252305552363\n",
      "Iteration [1059] | loss: 0.0006075443816371262\n",
      "Iteration [1060] | loss: 0.0006075443816371262\n",
      "Iteration [1061] | loss: 0.000607782625593245\n",
      "Iteration [1062] | loss: 0.000607782625593245\n",
      "Iteration [1063] | loss: 0.0006079017766751349\n",
      "Iteration [1064] | loss: 0.0006081400788389146\n",
      "Iteration [1065] | loss: 0.0006081400788389146\n",
      "Iteration [1066] | loss: 0.0006082591717131436\n",
      "Iteration [1067] | loss: 0.0006082591717131436\n",
      "Iteration [1068] | loss: 0.0006083783227950335\n",
      "Iteration [1069] | loss: 0.0006084974738769233\n",
      "Iteration [1070] | loss: 0.0006084974738769233\n",
      "Iteration [1071] | loss: 0.0006087357178330421\n",
      "Iteration [1072] | loss: 0.0006087357178330421\n",
      "Iteration [1073] | loss: 0.0006087357178330421\n",
      "Iteration [1074] | loss: 0.000608854868914932\n",
      "Iteration [1075] | loss: 0.0006089740199968219\n",
      "Iteration [1076] | loss: 0.0006089740199968219\n",
      "Iteration [1077] | loss: 0.0006090931710787117\n",
      "Iteration [1078] | loss: 0.0006093314150348306\n",
      "Iteration [1079] | loss: 0.0006094505661167204\n",
      "Iteration [1080] | loss: 0.0006094505661167204\n",
      "Iteration [1081] | loss: 0.0006094505661167204\n",
      "Iteration [1082] | loss: 0.0006096888100728393\n",
      "Iteration [1083] | loss: 0.0006096888100728393\n",
      "Iteration [1084] | loss: 0.0006098079611547291\n",
      "Iteration [1085] | loss: 0.0006098079611547291\n",
      "Iteration [1086] | loss: 0.000609927112236619\n",
      "Iteration [1087] | loss: 0.0006100462633185089\n",
      "Iteration [1088] | loss: 0.0006100462633185089\n",
      "Iteration [1089] | loss: 0.0006102845072746277\n",
      "Iteration [1090] | loss: 0.0006102845072746277\n",
      "Iteration [1091] | loss: 0.0006102845072746277\n",
      "Iteration [1092] | loss: 0.0006104036583565176\n",
      "Iteration [1093] | loss: 0.0006106419023126364\n",
      "Iteration [1094] | loss: 0.0006107610533945262\n",
      "Iteration [1095] | loss: 0.0006107610533945262\n",
      "Iteration [1096] | loss: 0.0006108802044764161\n",
      "Iteration [1097] | loss: 0.000610999355558306\n",
      "Iteration [1098] | loss: 0.000610999355558306\n",
      "Iteration [1099] | loss: 0.0006111184484325349\n",
      "Iteration [1100] | loss: 0.0006112375995144248\n",
      "Iteration [1101] | loss: 0.0006112375995144248\n",
      "Iteration [1102] | loss: 0.0006113567505963147\n",
      "Iteration [1103] | loss: 0.0006114759016782045\n",
      "Iteration [1104] | loss: 0.0006114759016782045\n",
      "Iteration [1105] | loss: 0.0006115949945524335\n",
      "Iteration [1106] | loss: 0.0006115949945524335\n",
      "Iteration [1107] | loss: 0.0006118332967162132\n",
      "Iteration [1108] | loss: 0.0006119524477981031\n",
      "Iteration [1109] | loss: 0.0006119524477981031\n",
      "Iteration [1110] | loss: 0.0006121906917542219\n",
      "Iteration [1111] | loss: 0.0006121906917542219\n",
      "Iteration [1112] | loss: 0.0006123098428361118\n",
      "Iteration [1113] | loss: 0.0006123098428361118\n",
      "Iteration [1114] | loss: 0.0006124289939180017\n",
      "Iteration [1115] | loss: 0.0006125480867922306\n",
      "Iteration [1116] | loss: 0.0006125480867922306\n",
      "Iteration [1117] | loss: 0.0006127863889560103\n",
      "Iteration [1118] | loss: 0.0006127863889560103\n",
      "Iteration [1119] | loss: 0.0006127863889560103\n",
      "Iteration [1120] | loss: 0.0006129055400379002\n",
      "Iteration [1121] | loss: 0.0006130246329121292\n",
      "Iteration [1122] | loss: 0.000613143783994019\n",
      "Iteration [1123] | loss: 0.0006132629350759089\n",
      "Iteration [1124] | loss: 0.0006133820279501379\n",
      "Iteration [1125] | loss: 0.0006135011790320277\n",
      "Iteration [1126] | loss: 0.0006135011790320277\n",
      "Iteration [1127] | loss: 0.0006136203301139176\n",
      "Iteration [1128] | loss: 0.0006137394811958075\n",
      "Iteration [1129] | loss: 0.0006137394811958075\n",
      "Iteration [1130] | loss: 0.0006138585740700364\n",
      "Iteration [1131] | loss: 0.0006139777251519263\n",
      "Iteration [1132] | loss: 0.0006140968762338161\n",
      "Iteration [1133] | loss: 0.0006140968762338161\n",
      "Iteration [1134] | loss: 0.0006140968762338161\n",
      "Iteration [1135] | loss: 0.000614335120189935\n",
      "Iteration [1136] | loss: 0.000614335120189935\n",
      "Iteration [1137] | loss: 0.0006144542712718248\n",
      "Iteration [1138] | loss: 0.0006146925734356046\n",
      "Iteration [1139] | loss: 0.0006146925734356046\n",
      "Iteration [1140] | loss: 0.0006148116663098335\n",
      "Iteration [1141] | loss: 0.0006148116663098335\n",
      "Iteration [1142] | loss: 0.0006150499684736133\n",
      "Iteration [1143] | loss: 0.0006150499684736133\n",
      "Iteration [1144] | loss: 0.0006150499684736133\n",
      "Iteration [1145] | loss: 0.0006152882124297321\n",
      "Iteration [1146] | loss: 0.0006152882124297321\n",
      "Iteration [1147] | loss: 0.000615407363511622\n",
      "Iteration [1148] | loss: 0.0006155265145935118\n",
      "Iteration [1149] | loss: 0.0006155265145935118\n",
      "Iteration [1150] | loss: 0.0006156456656754017\n",
      "Iteration [1151] | loss: 0.0006156456656754017\n",
      "Iteration [1152] | loss: 0.0006160030607134104\n",
      "Iteration [1153] | loss: 0.0006160030607134104\n",
      "Iteration [1154] | loss: 0.0006160030607134104\n",
      "Iteration [1155] | loss: 0.0006162413046695292\n",
      "Iteration [1156] | loss: 0.0006162413046695292\n",
      "Iteration [1157] | loss: 0.0006163604557514191\n",
      "Iteration [1158] | loss: 0.0006163604557514191\n",
      "Iteration [1159] | loss: 0.0006164796068333089\n",
      "Iteration [1160] | loss: 0.0006165986997075379\n",
      "Iteration [1161] | loss: 0.0006165986997075379\n",
      "Iteration [1162] | loss: 0.0006168370018713176\n",
      "Iteration [1163] | loss: 0.0006168370018713176\n",
      "Iteration [1164] | loss: 0.0006168370018713176\n",
      "Iteration [1165] | loss: 0.0006169561529532075\n",
      "Iteration [1166] | loss: 0.0006170752458274364\n",
      "Iteration [1167] | loss: 0.0006173135479912162\n",
      "Iteration [1168] | loss: 0.0006173135479912162\n",
      "Iteration [1169] | loss: 0.000617432699073106\n",
      "Iteration [1170] | loss: 0.000617551791947335\n",
      "Iteration [1171] | loss: 0.000617551791947335\n",
      "Iteration [1172] | loss: 0.0006176709430292249\n",
      "Iteration [1173] | loss: 0.0006177900941111147\n",
      "Iteration [1174] | loss: 0.0006177900941111147\n",
      "Iteration [1175] | loss: 0.0006179092451930046\n",
      "Iteration [1176] | loss: 0.0006180283380672336\n",
      "Iteration [1177] | loss: 0.0006181474891491234\n",
      "Iteration [1178] | loss: 0.0006181474891491234\n",
      "Iteration [1179] | loss: 0.0006181474891491234\n",
      "Iteration [1180] | loss: 0.0006183857913129032\n",
      "Iteration [1181] | loss: 0.0006183857913129032\n",
      "Iteration [1182] | loss: 0.000618624035269022\n",
      "Iteration [1183] | loss: 0.0006187431863509119\n",
      "Iteration [1184] | loss: 0.0006187431863509119\n",
      "Iteration [1185] | loss: 0.0006188622792251408\n",
      "Iteration [1186] | loss: 0.0006189814303070307\n",
      "Iteration [1187] | loss: 0.0006191005813889205\n",
      "Iteration [1188] | loss: 0.0006191005813889205\n",
      "Iteration [1189] | loss: 0.0006192197324708104\n",
      "Iteration [1190] | loss: 0.0006193388253450394\n",
      "Iteration [1191] | loss: 0.0006193388253450394\n",
      "Iteration [1192] | loss: 0.0006194579764269292\n",
      "Iteration [1193] | loss: 0.0006195771275088191\n",
      "Iteration [1194] | loss: 0.000619696278590709\n",
      "Iteration [1195] | loss: 0.000619696278590709\n",
      "Iteration [1196] | loss: 0.0006198153714649379\n",
      "Iteration [1197] | loss: 0.0006200536736287177\n",
      "Iteration [1198] | loss: 0.0006200536736287177\n",
      "Iteration [1199] | loss: 0.0006201728247106075\n",
      "Iteration [1200] | loss: 0.0006202919175848365\n",
      "Iteration [1201] | loss: 0.0006202919175848365\n",
      "Iteration [1202] | loss: 0.0006204110686667264\n",
      "Iteration [1203] | loss: 0.0006204110686667264\n",
      "Iteration [1204] | loss: 0.0006206493126228452\n",
      "Iteration [1205] | loss: 0.0006206493126228452\n",
      "Iteration [1206] | loss: 0.0006206493126228452\n",
      "Iteration [1207] | loss: 0.0006208876147866249\n",
      "Iteration [1208] | loss: 0.0006208876147866249\n",
      "Iteration [1209] | loss: 0.0006210067658685148\n",
      "Iteration [1210] | loss: 0.0006211258587427437\n",
      "Iteration [1211] | loss: 0.0006212450098246336\n",
      "Iteration [1212] | loss: 0.0006213641609065235\n",
      "Iteration [1213] | loss: 0.0006213641609065235\n",
      "Iteration [1214] | loss: 0.0006216024048626423\n",
      "Iteration [1215] | loss: 0.0006216024048626423\n",
      "Iteration [1216] | loss: 0.0006217215559445322\n",
      "Iteration [1217] | loss: 0.000621840707026422\n",
      "Iteration [1218] | loss: 0.000621840707026422\n",
      "Iteration [1219] | loss: 0.000621959799900651\n",
      "Iteration [1220] | loss: 0.000621959799900651\n",
      "Iteration [1221] | loss: 0.0006221981020644307\n",
      "Iteration [1222] | loss: 0.0006221981020644307\n",
      "Iteration [1223] | loss: 0.0006221981020644307\n",
      "Iteration [1224] | loss: 0.0006224363460205495\n",
      "Iteration [1225] | loss: 0.0006225554971024394\n",
      "Iteration [1226] | loss: 0.0006226746481843293\n",
      "Iteration [1227] | loss: 0.0006227937992662191\n",
      "Iteration [1228] | loss: 0.0006227937992662191\n",
      "Iteration [1229] | loss: 0.0006229128921404481\n",
      "Iteration [1230] | loss: 0.0006229128921404481\n",
      "Iteration [1231] | loss: 0.0006231511943042278\n",
      "Iteration [1232] | loss: 0.0006231511943042278\n",
      "Iteration [1233] | loss: 0.0006231511943042278\n",
      "Iteration [1234] | loss: 0.0006233894382603467\n",
      "Iteration [1235] | loss: 0.0006233894382603467\n",
      "Iteration [1236] | loss: 0.0006235085893422365\n",
      "Iteration [1237] | loss: 0.0006236277404241264\n",
      "Iteration [1238] | loss: 0.0006237468332983553\n",
      "Iteration [1239] | loss: 0.0006237468332983553\n",
      "Iteration [1240] | loss: 0.0006238659843802452\n",
      "Iteration [1241] | loss: 0.000624104228336364\n",
      "Iteration [1242] | loss: 0.000624104228336364\n",
      "Iteration [1243] | loss: 0.0006242233794182539\n",
      "Iteration [1244] | loss: 0.0006243425305001438\n",
      "Iteration [1245] | loss: 0.0006243425305001438\n",
      "Iteration [1246] | loss: 0.0006244616815820336\n",
      "Iteration [1247] | loss: 0.0006244616815820336\n",
      "Iteration [1248] | loss: 0.0006246999255381525\n",
      "Iteration [1249] | loss: 0.0006246999255381525\n",
      "Iteration [1250] | loss: 0.0006248190766200423\n",
      "Iteration [1251] | loss: 0.0006249382277019322\n",
      "Iteration [1252] | loss: 0.0006249382277019322\n",
      "Iteration [1253] | loss: 0.0006250573205761611\n",
      "Iteration [1254] | loss: 0.0006252956227399409\n",
      "Iteration [1255] | loss: 0.0006254147156141698\n",
      "Iteration [1256] | loss: 0.0006254147156141698\n",
      "Iteration [1257] | loss: 0.0006254147156141698\n",
      "Iteration [1258] | loss: 0.0006256530177779496\n",
      "Iteration [1259] | loss: 0.0006256530177779496\n",
      "Iteration [1260] | loss: 0.0006257721688598394\n",
      "Iteration [1261] | loss: 0.0006258912617340684\n",
      "Iteration [1262] | loss: 0.0006258912617340684\n",
      "Iteration [1263] | loss: 0.0006260104128159583\n",
      "Iteration [1264] | loss: 0.0006261295638978481\n",
      "Iteration [1265] | loss: 0.0006262486567720771\n",
      "Iteration [1266] | loss: 0.0006262486567720771\n",
      "Iteration [1267] | loss: 0.000626367807853967\n",
      "Iteration [1268] | loss: 0.0006266061100177467\n",
      "Iteration [1269] | loss: 0.0006266061100177467\n",
      "Iteration [1270] | loss: 0.0006267252028919756\n",
      "Iteration [1271] | loss: 0.0006268443539738655\n",
      "Iteration [1272] | loss: 0.0006269635050557554\n",
      "Iteration [1273] | loss: 0.0006269635050557554\n",
      "Iteration [1274] | loss: 0.0006270825979299843\n",
      "Iteration [1275] | loss: 0.0006272017490118742\n",
      "Iteration [1276] | loss: 0.0006272017490118742\n",
      "Iteration [1277] | loss: 0.0006273209000937641\n",
      "Iteration [1278] | loss: 0.0006274400511756539\n",
      "Iteration [1279] | loss: 0.0006275591440498829\n",
      "Iteration [1280] | loss: 0.0006275591440498829\n",
      "Iteration [1281] | loss: 0.0006276782951317728\n",
      "Iteration [1282] | loss: 0.0006277974462136626\n",
      "Iteration [1283] | loss: 0.0006279165390878916\n",
      "Iteration [1284] | loss: 0.0006281548412516713\n",
      "Iteration [1285] | loss: 0.0006281548412516713\n",
      "Iteration [1286] | loss: 0.0006281548412516713\n",
      "Iteration [1287] | loss: 0.0006282739923335612\n",
      "Iteration [1288] | loss: 0.0006283930852077901\n",
      "Iteration [1289] | loss: 0.00062851223628968\n",
      "Iteration [1290] | loss: 0.00062851223628968\n",
      "Iteration [1291] | loss: 0.0006287504802457988\n",
      "Iteration [1292] | loss: 0.0006287504802457988\n",
      "Iteration [1293] | loss: 0.0006287504802457988\n",
      "Iteration [1294] | loss: 0.0006289887824095786\n",
      "Iteration [1295] | loss: 0.0006289887824095786\n",
      "Iteration [1296] | loss: 0.0006291079334914684\n",
      "Iteration [1297] | loss: 0.0006292270263656974\n",
      "Iteration [1298] | loss: 0.0006294653285294771\n",
      "Iteration [1299] | loss: 0.0006294653285294771\n",
      "Iteration [1300] | loss: 0.0006294653285294771\n",
      "Iteration [1301] | loss: 0.0006297035724855959\n",
      "Iteration [1302] | loss: 0.0006297035724855959\n",
      "Iteration [1303] | loss: 0.0006298227235674858\n",
      "Iteration [1304] | loss: 0.0006299418746493757\n",
      "Iteration [1305] | loss: 0.0006299418746493757\n",
      "Iteration [1306] | loss: 0.0006300609675236046\n",
      "Iteration [1307] | loss: 0.0006300609675236046\n",
      "Iteration [1308] | loss: 0.0006302992696873844\n",
      "Iteration [1309] | loss: 0.0006302992696873844\n",
      "Iteration [1310] | loss: 0.0006304183625616133\n",
      "Iteration [1311] | loss: 0.0006306566647253931\n",
      "Iteration [1312] | loss: 0.0006306566647253931\n",
      "Iteration [1313] | loss: 0.0006307758158072829\n",
      "Iteration [1314] | loss: 0.0006308949086815119\n",
      "Iteration [1315] | loss: 0.0006310140597634017\n",
      "Iteration [1316] | loss: 0.0006310140597634017\n",
      "Iteration [1317] | loss: 0.0006311332108452916\n",
      "Iteration [1318] | loss: 0.0006312523037195206\n",
      "Iteration [1319] | loss: 0.0006312523037195206\n",
      "Iteration [1320] | loss: 0.0006313714548014104\n",
      "Iteration [1321] | loss: 0.0006314906058833003\n",
      "Iteration [1322] | loss: 0.0006316096987575293\n",
      "Iteration [1323] | loss: 0.0006316096987575293\n",
      "Iteration [1324] | loss: 0.000631848000921309\n",
      "Iteration [1325] | loss: 0.0006319671520031989\n",
      "Iteration [1326] | loss: 0.0006319671520031989\n",
      "Iteration [1327] | loss: 0.0006322053959593177\n",
      "Iteration [1328] | loss: 0.0006322053959593177\n",
      "Iteration [1329] | loss: 0.0006323245470412076\n",
      "Iteration [1330] | loss: 0.0006323245470412076\n",
      "Iteration [1331] | loss: 0.0006324436399154365\n",
      "Iteration [1332] | loss: 0.0006325627909973264\n",
      "Iteration [1333] | loss: 0.0006325627909973264\n",
      "Iteration [1334] | loss: 0.0006328010931611061\n",
      "Iteration [1335] | loss: 0.0006328010931611061\n",
      "Iteration [1336] | loss: 0.0006329201860353351\n",
      "Iteration [1337] | loss: 0.0006330393371172249\n",
      "Iteration [1338] | loss: 0.0006330393371172249\n",
      "Iteration [1339] | loss: 0.0006331584881991148\n",
      "Iteration [1340] | loss: 0.0006332775810733438\n",
      "Iteration [1341] | loss: 0.0006335158832371235\n",
      "Iteration [1342] | loss: 0.0006335158832371235\n",
      "Iteration [1343] | loss: 0.0006336349761113524\n",
      "Iteration [1344] | loss: 0.0006337541271932423\n",
      "Iteration [1345] | loss: 0.0006337541271932423\n",
      "Iteration [1346] | loss: 0.0006338732782751322\n",
      "Iteration [1347] | loss: 0.000633992429357022\n",
      "Iteration [1348] | loss: 0.000634111522231251\n",
      "Iteration [1349] | loss: 0.000634111522231251\n",
      "Iteration [1350] | loss: 0.0006343498243950307\n",
      "Iteration [1351] | loss: 0.0006343498243950307\n",
      "Iteration [1352] | loss: 0.0006343498243950307\n",
      "Iteration [1353] | loss: 0.0006344689172692597\n",
      "Iteration [1354] | loss: 0.0006347072194330394\n",
      "Iteration [1355] | loss: 0.0006348263123072684\n",
      "Iteration [1356] | loss: 0.0006348263123072684\n",
      "Iteration [1357] | loss: 0.0006350646144710481\n",
      "Iteration [1358] | loss: 0.0006350646144710481\n",
      "Iteration [1359] | loss: 0.0006350646144710481\n",
      "Iteration [1360] | loss: 0.0006353028584271669\n",
      "Iteration [1361] | loss: 0.0006353028584271669\n",
      "Iteration [1362] | loss: 0.0006354220095090568\n",
      "Iteration [1363] | loss: 0.0006355411605909467\n",
      "Iteration [1364] | loss: 0.0006356602534651756\n",
      "Iteration [1365] | loss: 0.0006356602534651756\n",
      "Iteration [1366] | loss: 0.0006356602534651756\n",
      "Iteration [1367] | loss: 0.0006358985556289554\n",
      "Iteration [1368] | loss: 0.0006360176485031843\n",
      "Iteration [1369] | loss: 0.0006361367995850742\n",
      "Iteration [1370] | loss: 0.000636255950666964\n",
      "Iteration [1371] | loss: 0.0006363751017488539\n",
      "Iteration [1372] | loss: 0.0006363751017488539\n",
      "Iteration [1373] | loss: 0.0006364941946230829\n",
      "Iteration [1374] | loss: 0.0006366133457049727\n",
      "Iteration [1375] | loss: 0.0006366133457049727\n",
      "Iteration [1376] | loss: 0.0006368515896610916\n",
      "Iteration [1377] | loss: 0.0006368515896610916\n",
      "Iteration [1378] | loss: 0.0006369707407429814\n",
      "Iteration [1379] | loss: 0.0006369707407429814\n",
      "Iteration [1380] | loss: 0.0006370898918248713\n",
      "Iteration [1381] | loss: 0.0006372089846991003\n",
      "Iteration [1382] | loss: 0.0006373281357809901\n",
      "Iteration [1383] | loss: 0.0006375664379447699\n",
      "Iteration [1384] | loss: 0.0006375664379447699\n",
      "Iteration [1385] | loss: 0.0006376855308189988\n",
      "Iteration [1386] | loss: 0.0006378046819008887\n",
      "Iteration [1387] | loss: 0.0006379238329827785\n",
      "Iteration [1388] | loss: 0.0006379238329827785\n",
      "Iteration [1389] | loss: 0.0006380429258570075\n",
      "Iteration [1390] | loss: 0.0006381620769388974\n",
      "Iteration [1391] | loss: 0.0006381620769388974\n",
      "Iteration [1392] | loss: 0.0006382812280207872\n",
      "Iteration [1393] | loss: 0.0006384003208950162\n",
      "Iteration [1394] | loss: 0.0006385194719769061\n",
      "Iteration [1395] | loss: 0.0006385194719769061\n",
      "Iteration [1396] | loss: 0.0006387577159330249\n",
      "Iteration [1397] | loss: 0.0006388768670149148\n",
      "Iteration [1398] | loss: 0.0006388768670149148\n",
      "Iteration [1399] | loss: 0.0006391151691786945\n",
      "Iteration [1400] | loss: 0.0006391151691786945\n",
      "Iteration [1401] | loss: 0.0006392342620529234\n",
      "Iteration [1402] | loss: 0.0006393534131348133\n",
      "Iteration [1403] | loss: 0.0006393534131348133\n",
      "Iteration [1404] | loss: 0.0006394725642167032\n",
      "Iteration [1405] | loss: 0.0006395916570909321\n",
      "Iteration [1406] | loss: 0.000639710808172822\n",
      "Iteration [1407] | loss: 0.000639710808172822\n",
      "Iteration [1408] | loss: 0.0006398299592547119\n",
      "Iteration [1409] | loss: 0.0006400682032108307\n",
      "Iteration [1410] | loss: 0.0006401873542927206\n",
      "Iteration [1411] | loss: 0.0006401873542927206\n",
      "Iteration [1412] | loss: 0.0006403064471669495\n",
      "Iteration [1413] | loss: 0.0006404255982488394\n",
      "Iteration [1414] | loss: 0.0006404255982488394\n",
      "Iteration [1415] | loss: 0.0006406639004126191\n",
      "Iteration [1416] | loss: 0.0006406639004126191\n",
      "Iteration [1417] | loss: 0.0006407829932868481\n",
      "Iteration [1418] | loss: 0.0006409021443687379\n",
      "Iteration [1419] | loss: 0.0006409021443687379\n",
      "Iteration [1420] | loss: 0.0006410212954506278\n",
      "Iteration [1421] | loss: 0.0006411403883248568\n",
      "Iteration [1422] | loss: 0.0006412595394067466\n",
      "Iteration [1423] | loss: 0.0006413786904886365\n",
      "Iteration [1424] | loss: 0.0006414977833628654\n",
      "Iteration [1425] | loss: 0.0006416169344447553\n",
      "Iteration [1426] | loss: 0.0006417360855266452\n",
      "Iteration [1427] | loss: 0.0006417360855266452\n",
      "Iteration [1428] | loss: 0.0006418551784008741\n",
      "Iteration [1429] | loss: 0.000641974329482764\n",
      "Iteration [1430] | loss: 0.000641974329482764\n",
      "Iteration [1431] | loss: 0.0006422125734388828\n",
      "Iteration [1432] | loss: 0.0006422125734388828\n",
      "Iteration [1433] | loss: 0.0006423317245207727\n",
      "Iteration [1434] | loss: 0.0006424508756026626\n",
      "Iteration [1435] | loss: 0.0006424508756026626\n",
      "Iteration [1436] | loss: 0.0006425699684768915\n",
      "Iteration [1437] | loss: 0.0006428082706406713\n",
      "Iteration [1438] | loss: 0.0006429274217225611\n",
      "Iteration [1439] | loss: 0.0006429274217225611\n",
      "Iteration [1440] | loss: 0.0006430465145967901\n",
      "Iteration [1441] | loss: 0.0006431656656786799\n",
      "Iteration [1442] | loss: 0.0006432848167605698\n",
      "Iteration [1443] | loss: 0.0006432848167605698\n",
      "Iteration [1444] | loss: 0.0006434039096347988\n",
      "Iteration [1445] | loss: 0.0006435230607166886\n",
      "Iteration [1446] | loss: 0.0006435230607166886\n",
      "Iteration [1447] | loss: 0.0006437613046728075\n",
      "Iteration [1448] | loss: 0.0006437613046728075\n",
      "Iteration [1449] | loss: 0.0006438804557546973\n",
      "Iteration [1450] | loss: 0.0006439996068365872\n",
      "Iteration [1451] | loss: 0.000644237850792706\n",
      "Iteration [1452] | loss: 0.000644237850792706\n",
      "Iteration [1453] | loss: 0.0006443570018745959\n",
      "Iteration [1454] | loss: 0.0006444760947488248\n",
      "Iteration [1455] | loss: 0.0006444760947488248\n",
      "Iteration [1456] | loss: 0.0006447143969126046\n",
      "Iteration [1457] | loss: 0.0006447143969126046\n",
      "Iteration [1458] | loss: 0.0006448334897868335\n",
      "Iteration [1459] | loss: 0.0006448334897868335\n",
      "Iteration [1460] | loss: 0.0006450717919506133\n",
      "Iteration [1461] | loss: 0.0006450717919506133\n",
      "Iteration [1462] | loss: 0.0006450717919506133\n",
      "Iteration [1463] | loss: 0.0006453100359067321\n",
      "Iteration [1464] | loss: 0.0006453100359067321\n",
      "Iteration [1465] | loss: 0.0006455483380705118\n",
      "Iteration [1466] | loss: 0.0006456674309447408\n",
      "Iteration [1467] | loss: 0.0006457865820266306\n",
      "Iteration [1468] | loss: 0.0006457865820266306\n",
      "Iteration [1469] | loss: 0.0006460248259827495\n",
      "Iteration [1470] | loss: 0.0006460248259827495\n",
      "Iteration [1471] | loss: 0.0006460248259827495\n",
      "Iteration [1472] | loss: 0.0006462631281465292\n",
      "Iteration [1473] | loss: 0.0006462631281465292\n",
      "Iteration [1474] | loss: 0.0006463822210207582\n",
      "Iteration [1475] | loss: 0.000646501372102648\n",
      "Iteration [1476] | loss: 0.0006466205231845379\n",
      "Iteration [1477] | loss: 0.0006466205231845379\n",
      "Iteration [1478] | loss: 0.0006469779182225466\n",
      "Iteration [1479] | loss: 0.0006469779182225466\n",
      "Iteration [1480] | loss: 0.0006469779182225466\n",
      "Iteration [1481] | loss: 0.0006470970110967755\n",
      "Iteration [1482] | loss: 0.0006472161621786654\n",
      "Iteration [1483] | loss: 0.0006473353132605553\n",
      "Iteration [1484] | loss: 0.0006473353132605553\n",
      "Iteration [1485] | loss: 0.0006475735572166741\n",
      "Iteration [1486] | loss: 0.0006475735572166741\n",
      "Iteration [1487] | loss: 0.000647692708298564\n",
      "Iteration [1488] | loss: 0.0006478118011727929\n",
      "Iteration [1489] | loss: 0.0006478118011727929\n",
      "Iteration [1490] | loss: 0.0006479309522546828\n",
      "Iteration [1491] | loss: 0.0006480501033365726\n",
      "Iteration [1492] | loss: 0.0006482883472926915\n",
      "Iteration [1493] | loss: 0.0006482883472926915\n",
      "Iteration [1494] | loss: 0.0006485265912488103\n",
      "Iteration [1495] | loss: 0.0006485265912488103\n",
      "Iteration [1496] | loss: 0.0006486457423307002\n",
      "Iteration [1497] | loss: 0.00064876489341259\n",
      "Iteration [1498] | loss: 0.000648883986286819\n",
      "Iteration [1499] | loss: 0.000648883986286819\n",
      "Iteration [1500] | loss: 0.0006490031373687088\n",
      "Iteration [1501] | loss: 0.0006491222884505987\n",
      "Iteration [1502] | loss: 0.0006491222884505987\n",
      "Iteration [1503] | loss: 0.0006493605324067175\n",
      "Iteration [1504] | loss: 0.0006493605324067175\n",
      "Iteration [1505] | loss: 0.0006495987763628364\n",
      "Iteration [1506] | loss: 0.0006497179274447262\n",
      "Iteration [1507] | loss: 0.0006498370785266161\n",
      "Iteration [1508] | loss: 0.0006498370785266161\n",
      "Iteration [1509] | loss: 0.0006500753224827349\n",
      "Iteration [1510] | loss: 0.0006500753224827349\n",
      "Iteration [1511] | loss: 0.0006503135664388537\n",
      "Iteration [1512] | loss: 0.0006503135664388537\n",
      "Iteration [1513] | loss: 0.0006504327175207436\n",
      "Iteration [1514] | loss: 0.0006505518686026335\n",
      "Iteration [1515] | loss: 0.0006506709614768624\n",
      "Iteration [1516] | loss: 0.0006506709614768624\n",
      "Iteration [1517] | loss: 0.0006510283565148711\n",
      "Iteration [1518] | loss: 0.0006510283565148711\n",
      "Iteration [1519] | loss: 0.000651147507596761\n",
      "Iteration [1520] | loss: 0.0006512666586786509\n",
      "Iteration [1521] | loss: 0.0006513857515528798\n",
      "Iteration [1522] | loss: 0.0006513857515528798\n",
      "Iteration [1523] | loss: 0.0006516240537166595\n",
      "Iteration [1524] | loss: 0.0006516240537166595\n",
      "Iteration [1525] | loss: 0.0006518622976727784\n",
      "Iteration [1526] | loss: 0.0006518622976727784\n",
      "Iteration [1527] | loss: 0.0006519814487546682\n",
      "Iteration [1528] | loss: 0.0006521005416288972\n",
      "Iteration [1529] | loss: 0.0006523388437926769\n",
      "Iteration [1530] | loss: 0.0006523388437926769\n",
      "Iteration [1531] | loss: 0.0006525770877487957\n",
      "Iteration [1532] | loss: 0.0006525770877487957\n",
      "Iteration [1533] | loss: 0.0006526962388306856\n",
      "Iteration [1534] | loss: 0.0006528153317049146\n",
      "Iteration [1535] | loss: 0.0006529344827868044\n",
      "Iteration [1536] | loss: 0.0006529344827868044\n",
      "Iteration [1537] | loss: 0.0006531727267429233\n",
      "Iteration [1538] | loss: 0.0006531727267429233\n",
      "Iteration [1539] | loss: 0.000653411028906703\n",
      "Iteration [1540] | loss: 0.000653411028906703\n",
      "Iteration [1541] | loss: 0.0006536492728628218\n",
      "Iteration [1542] | loss: 0.0006537684239447117\n",
      "Iteration [1543] | loss: 0.0006538875168189406\n",
      "Iteration [1544] | loss: 0.0006538875168189406\n",
      "Iteration [1545] | loss: 0.0006541258189827204\n",
      "Iteration [1546] | loss: 0.0006541258189827204\n",
      "Iteration [1547] | loss: 0.0006542449118569493\n",
      "Iteration [1548] | loss: 0.0006543640629388392\n",
      "Iteration [1549] | loss: 0.0006544832140207291\n",
      "Iteration [1550] | loss: 0.0006544832140207291\n",
      "Iteration [1551] | loss: 0.0006547214579768479\n",
      "Iteration [1552] | loss: 0.0006548406090587378\n",
      "Iteration [1553] | loss: 0.0006550788530148566\n",
      "Iteration [1554] | loss: 0.0006550788530148566\n",
      "Iteration [1555] | loss: 0.0006551980040967464\n",
      "Iteration [1556] | loss: 0.0006553170969709754\n",
      "Iteration [1557] | loss: 0.0006553170969709754\n",
      "Iteration [1558] | loss: 0.0006554362480528653\n",
      "Iteration [1559] | loss: 0.0006555553991347551\n",
      "Iteration [1560] | loss: 0.0006556744920089841\n",
      "Iteration [1561] | loss: 0.0006556744920089841\n",
      "Iteration [1562] | loss: 0.0006559127941727638\n",
      "Iteration [1563] | loss: 0.0006559127941727638\n",
      "Iteration [1564] | loss: 0.0006562701892107725\n",
      "Iteration [1565] | loss: 0.0006562701892107725\n",
      "Iteration [1566] | loss: 0.0006563892820850015\n",
      "Iteration [1567] | loss: 0.0006565084331668913\n",
      "Iteration [1568] | loss: 0.0006566275842487812\n",
      "Iteration [1569] | loss: 0.0006566275842487812\n",
      "Iteration [1570] | loss: 0.0006568658282049\n",
      "Iteration [1571] | loss: 0.0006568658282049\n",
      "Iteration [1572] | loss: 0.0006569849792867899\n",
      "Iteration [1573] | loss: 0.0006571040721610188\n",
      "Iteration [1574] | loss: 0.0006572232232429087\n",
      "Iteration [1575] | loss: 0.0006573423161171377\n",
      "Iteration [1576] | loss: 0.0006575806182809174\n",
      "Iteration [1577] | loss: 0.0006575806182809174\n",
      "Iteration [1578] | loss: 0.0006578188622370362\n",
      "Iteration [1579] | loss: 0.0006578188622370362\n",
      "Iteration [1580] | loss: 0.0006579380133189261\n",
      "Iteration [1581] | loss: 0.000658057106193155\n",
      "Iteration [1582] | loss: 0.0006581762572750449\n",
      "Iteration [1583] | loss: 0.0006581762572750449\n",
      "Iteration [1584] | loss: 0.0006584145012311637\n",
      "Iteration [1585] | loss: 0.0006584145012311637\n",
      "Iteration [1586] | loss: 0.0006586528033949435\n",
      "Iteration [1587] | loss: 0.0006586528033949435\n",
      "Iteration [1588] | loss: 0.0006588910473510623\n",
      "Iteration [1589] | loss: 0.0006590101984329522\n",
      "Iteration [1590] | loss: 0.0006591292913071811\n",
      "Iteration [1591] | loss: 0.000659248442389071\n",
      "Iteration [1592] | loss: 0.0006593675934709609\n",
      "Iteration [1593] | loss: 0.0006594866863451898\n",
      "Iteration [1594] | loss: 0.0006596058374270797\n",
      "Iteration [1595] | loss: 0.0006597249885089695\n",
      "Iteration [1596] | loss: 0.0006597249885089695\n",
      "Iteration [1597] | loss: 0.0006599632324650884\n",
      "Iteration [1598] | loss: 0.0006599632324650884\n",
      "Iteration [1599] | loss: 0.0006602014764212072\n",
      "Iteration [1600] | loss: 0.0006603206275030971\n",
      "Iteration [1601] | loss: 0.000660439720377326\n",
      "Iteration [1602] | loss: 0.000660439720377326\n",
      "Iteration [1603] | loss: 0.0006606780225411057\n",
      "Iteration [1604] | loss: 0.0006606780225411057\n",
      "Iteration [1605] | loss: 0.0006609162664972246\n",
      "Iteration [1606] | loss: 0.0006609162664972246\n",
      "Iteration [1607] | loss: 0.0006610354175791144\n",
      "Iteration [1608] | loss: 0.0006611545104533434\n",
      "Iteration [1609] | loss: 0.0006612736615352333\n",
      "Iteration [1610] | loss: 0.0006612736615352333\n",
      "Iteration [1611] | loss: 0.000661631056573242\n",
      "Iteration [1612] | loss: 0.000661631056573242\n",
      "Iteration [1613] | loss: 0.0006618693005293608\n",
      "Iteration [1614] | loss: 0.0006618693005293608\n",
      "Iteration [1615] | loss: 0.0006619884516112506\n",
      "Iteration [1616] | loss: 0.0006621075444854796\n",
      "Iteration [1617] | loss: 0.0006622266955673695\n",
      "Iteration [1618] | loss: 0.0006622266955673695\n",
      "Iteration [1619] | loss: 0.0006624649395234883\n",
      "Iteration [1620] | loss: 0.0006624649395234883\n",
      "Iteration [1621] | loss: 0.0006625840906053782\n",
      "Iteration [1622] | loss: 0.000662703241687268\n",
      "Iteration [1623] | loss: 0.0006629414856433868\n",
      "Iteration [1624] | loss: 0.0006630606367252767\n",
      "Iteration [1625] | loss: 0.0006631797295995057\n",
      "Iteration [1626] | loss: 0.0006631797295995057\n",
      "Iteration [1627] | loss: 0.0006634180317632854\n",
      "Iteration [1628] | loss: 0.0006634180317632854\n",
      "Iteration [1629] | loss: 0.0006635371246375144\n",
      "Iteration [1630] | loss: 0.0006636562757194042\n",
      "Iteration [1631] | loss: 0.0006637753685936332\n",
      "Iteration [1632] | loss: 0.000663894519675523\n",
      "Iteration [1633] | loss: 0.0006640136707574129\n",
      "Iteration [1634] | loss: 0.0006641327636316419\n",
      "Iteration [1635] | loss: 0.0006643710657954216\n",
      "Iteration [1636] | loss: 0.0006643710657954216\n",
      "Iteration [1637] | loss: 0.0006644901586696506\n",
      "Iteration [1638] | loss: 0.0006646093097515404\n",
      "Iteration [1639] | loss: 0.0006647284608334303\n",
      "Iteration [1640] | loss: 0.0006648475537076592\n",
      "Iteration [1641] | loss: 0.0006649667047895491\n",
      "Iteration [1642] | loss: 0.0006649667047895491\n",
      "Iteration [1643] | loss: 0.0006652049487456679\n",
      "Iteration [1644] | loss: 0.0006652049487456679\n",
      "Iteration [1645] | loss: 0.0006653240998275578\n",
      "Iteration [1646] | loss: 0.0006655623437836766\n",
      "Iteration [1647] | loss: 0.0006656814948655665\n",
      "Iteration [1648] | loss: 0.0006658005877397954\n",
      "Iteration [1649] | loss: 0.0006659197388216853\n",
      "Iteration [1650] | loss: 0.0006660388899035752\n",
      "Iteration [1651] | loss: 0.0006661579827778041\n",
      "Iteration [1652] | loss: 0.000666277133859694\n",
      "Iteration [1653] | loss: 0.000666277133859694\n",
      "Iteration [1654] | loss: 0.0006665153778158128\n",
      "Iteration [1655] | loss: 0.0006665153778158128\n",
      "Iteration [1656] | loss: 0.0006666345288977027\n",
      "Iteration [1657] | loss: 0.0006668727728538215\n",
      "Iteration [1658] | loss: 0.0006669919239357114\n",
      "Iteration [1659] | loss: 0.0006671110168099403\n",
      "Iteration [1660] | loss: 0.0006672301678918302\n",
      "Iteration [1661] | loss: 0.0006672301678918302\n",
      "Iteration [1662] | loss: 0.000667468411847949\n",
      "Iteration [1663] | loss: 0.000667468411847949\n",
      "Iteration [1664] | loss: 0.0006675875629298389\n",
      "Iteration [1665] | loss: 0.0006677066558040679\n",
      "Iteration [1666] | loss: 0.0006678258068859577\n",
      "Iteration [1667] | loss: 0.0006679449579678476\n",
      "Iteration [1668] | loss: 0.0006680640508420765\n",
      "Iteration [1669] | loss: 0.0006681832019239664\n",
      "Iteration [1670] | loss: 0.0006684214458800852\n",
      "Iteration [1671] | loss: 0.0006684214458800852\n",
      "Iteration [1672] | loss: 0.000668659748043865\n",
      "Iteration [1673] | loss: 0.000668659748043865\n",
      "Iteration [1674] | loss: 0.0006687788409180939\n",
      "Iteration [1675] | loss: 0.0006688979919999838\n",
      "Iteration [1676] | loss: 0.0006690170848742127\n",
      "Iteration [1677] | loss: 0.0006690170848742127\n",
      "Iteration [1678] | loss: 0.0006692553870379925\n",
      "Iteration [1679] | loss: 0.0006693744799122214\n",
      "Iteration [1680] | loss: 0.0006696127820760012\n",
      "Iteration [1681] | loss: 0.0006697318749502301\n",
      "Iteration [1682] | loss: 0.0006697318749502301\n",
      "Iteration [1683] | loss: 0.0006699701189063489\n",
      "Iteration [1684] | loss: 0.0006699701189063489\n",
      "Iteration [1685] | loss: 0.0006700892699882388\n",
      "Iteration [1686] | loss: 0.0006702084210701287\n",
      "Iteration [1687] | loss: 0.0006703275139443576\n",
      "Iteration [1688] | loss: 0.0006704466650262475\n",
      "Iteration [1689] | loss: 0.0006705658161081374\n",
      "Iteration [1690] | loss: 0.0006705658161081374\n",
      "Iteration [1691] | loss: 0.0006708040600642562\n",
      "Iteration [1692] | loss: 0.0006709231529384851\n",
      "Iteration [1693] | loss: 0.000671042304020375\n",
      "Iteration [1694] | loss: 0.0006711614551022649\n",
      "Iteration [1695] | loss: 0.0006712805479764938\n",
      "Iteration [1696] | loss: 0.0006713996990583837\n",
      "Iteration [1697] | loss: 0.0006715188501402736\n",
      "Iteration [1698] | loss: 0.0006715188501402736\n",
      "Iteration [1699] | loss: 0.0006717570940963924\n",
      "Iteration [1700] | loss: 0.0006717570940963924\n",
      "Iteration [1701] | loss: 0.0006718761869706213\n",
      "Iteration [1702] | loss: 0.0006719953380525112\n",
      "Iteration [1703] | loss: 0.00067223358200863\n",
      "Iteration [1704] | loss: 0.0006724718841724098\n",
      "Iteration [1705] | loss: 0.0006724718841724098\n",
      "Iteration [1706] | loss: 0.0006725909770466387\n",
      "Iteration [1707] | loss: 0.0006727101281285286\n",
      "Iteration [1708] | loss: 0.0006728292210027575\n",
      "Iteration [1709] | loss: 0.0006728292210027575\n",
      "Iteration [1710] | loss: 0.0006730675231665373\n",
      "Iteration [1711] | loss: 0.0006730675231665373\n",
      "Iteration [1712] | loss: 0.0006733057671226561\n",
      "Iteration [1713] | loss: 0.0006733057671226561\n",
      "Iteration [1714] | loss: 0.0006735440110787749\n",
      "Iteration [1715] | loss: 0.0006736631621606648\n",
      "Iteration [1716] | loss: 0.0006737822550348938\n",
      "Iteration [1717] | loss: 0.0006739014061167836\n",
      "Iteration [1718] | loss: 0.0006740205571986735\n",
      "Iteration [1719] | loss: 0.0006740205571986735\n",
      "Iteration [1720] | loss: 0.0006742588011547923\n",
      "Iteration [1721] | loss: 0.0006742588011547923\n",
      "Iteration [1722] | loss: 0.0006743779522366822\n",
      "Iteration [1723] | loss: 0.0006744970451109111\n",
      "Iteration [1724] | loss: 0.000674616196192801\n",
      "Iteration [1725] | loss: 0.0006748544401489198\n",
      "Iteration [1726] | loss: 0.0006749735912308097\n",
      "Iteration [1727] | loss: 0.0006750926841050386\n",
      "Iteration [1728] | loss: 0.0006752118351869285\n",
      "Iteration [1729] | loss: 0.0006753309280611575\n",
      "Iteration [1730] | loss: 0.0006754500791430473\n",
      "Iteration [1731] | loss: 0.0006755692302249372\n",
      "Iteration [1732] | loss: 0.0006755692302249372\n",
      "Iteration [1733] | loss: 0.000675807474181056\n",
      "Iteration [1734] | loss: 0.000675807474181056\n",
      "Iteration [1735] | loss: 0.0006759266252629459\n",
      "Iteration [1736] | loss: 0.0006760457181371748\n",
      "Iteration [1737] | loss: 0.0006762839620932937\n",
      "Iteration [1738] | loss: 0.0006764031131751835\n",
      "Iteration [1739] | loss: 0.0006765222642570734\n",
      "Iteration [1740] | loss: 0.0006765222642570734\n",
      "Iteration [1741] | loss: 0.0006767605082131922\n",
      "Iteration [1742] | loss: 0.0006768796010874212\n",
      "Iteration [1743] | loss: 0.000676998752169311\n",
      "Iteration [1744] | loss: 0.0006771179032512009\n",
      "Iteration [1745] | loss: 0.0006771179032512009\n",
      "Iteration [1746] | loss: 0.0006773561472073197\n",
      "Iteration [1747] | loss: 0.0006773561472073197\n",
      "Iteration [1748] | loss: 0.0006775943911634386\n",
      "Iteration [1749] | loss: 0.0006777135422453284\n",
      "Iteration [1750] | loss: 0.0006778326351195574\n",
      "Iteration [1751] | loss: 0.0006779517862014472\n",
      "Iteration [1752] | loss: 0.0006780709372833371\n",
      "Iteration [1753] | loss: 0.0006780709372833371\n",
      "Iteration [1754] | loss: 0.0006783091812394559\n",
      "Iteration [1755] | loss: 0.0006783091812394559\n",
      "Iteration [1756] | loss: 0.0006785474251955748\n",
      "Iteration [1757] | loss: 0.0006785474251955748\n",
      "Iteration [1758] | loss: 0.0006786665762774646\n",
      "Iteration [1759] | loss: 0.0006790239713154733\n",
      "Iteration [1760] | loss: 0.0006790239713154733\n",
      "Iteration [1761] | loss: 0.0006791430641897023\n",
      "Iteration [1762] | loss: 0.0006792622152715921\n",
      "Iteration [1763] | loss: 0.0006793813081458211\n",
      "Iteration [1764] | loss: 0.000679500459227711\n",
      "Iteration [1765] | loss: 0.0006796196103096008\n",
      "Iteration [1766] | loss: 0.0006796196103096008\n",
      "Iteration [1767] | loss: 0.0006798578542657197\n",
      "Iteration [1768] | loss: 0.0006798578542657197\n",
      "Iteration [1769] | loss: 0.0006800960982218385\n",
      "Iteration [1770] | loss: 0.0006802152493037283\n",
      "Iteration [1771] | loss: 0.0006803343421779573\n",
      "Iteration [1772] | loss: 0.0006805725861340761\n",
      "Iteration [1773] | loss: 0.0006805725861340761\n",
      "Iteration [1774] | loss: 0.000680691737215966\n",
      "Iteration [1775] | loss: 0.0006808108882978559\n",
      "Iteration [1776] | loss: 0.0006809299811720848\n",
      "Iteration [1777] | loss: 0.0006810491322539747\n",
      "Iteration [1778] | loss: 0.0006811682251282036\n",
      "Iteration [1779] | loss: 0.0006811682251282036\n",
      "Iteration [1780] | loss: 0.0006814065272919834\n",
      "Iteration [1781] | loss: 0.0006814065272919834\n",
      "Iteration [1782] | loss: 0.0006817638641223311\n",
      "Iteration [1783] | loss: 0.0006817638641223311\n",
      "Iteration [1784] | loss: 0.000681883015204221\n",
      "Iteration [1785] | loss: 0.0006820021662861109\n",
      "Iteration [1786] | loss: 0.0006821212591603398\n",
      "Iteration [1787] | loss: 0.0006823595031164587\n",
      "Iteration [1788] | loss: 0.0006823595031164587\n",
      "Iteration [1789] | loss: 0.0006824786541983485\n",
      "Iteration [1790] | loss: 0.0006825978052802384\n",
      "Iteration [1791] | loss: 0.0006827168981544673\n",
      "Iteration [1792] | loss: 0.0006828360492363572\n",
      "Iteration [1793] | loss: 0.000683074293192476\n",
      "Iteration [1794] | loss: 0.000683074293192476\n",
      "Iteration [1795] | loss: 0.0006833125371485949\n",
      "Iteration [1796] | loss: 0.0006833125371485949\n",
      "Iteration [1797] | loss: 0.0006834316882304847\n",
      "Iteration [1798] | loss: 0.0006836699321866035\n",
      "Iteration [1799] | loss: 0.0006836699321866035\n",
      "Iteration [1800] | loss: 0.0006839081761427224\n",
      "Iteration [1801] | loss: 0.0006839081761427224\n",
      "Iteration [1802] | loss: 0.0006840273272246122\n",
      "Iteration [1803] | loss: 0.0006841464783065021\n",
      "Iteration [1804] | loss: 0.0006843847222626209\n",
      "Iteration [1805] | loss: 0.0006845038151368499\n",
      "Iteration [1806] | loss: 0.0006846229662187397\n",
      "Iteration [1807] | loss: 0.0006846229662187397\n",
      "Iteration [1808] | loss: 0.0006848612101748586\n",
      "Iteration [1809] | loss: 0.0006848612101748586\n",
      "Iteration [1810] | loss: 0.0006850994541309774\n",
      "Iteration [1811] | loss: 0.0006852186052128673\n",
      "Iteration [1812] | loss: 0.0006852186052128673\n",
      "Iteration [1813] | loss: 0.0006854568491689861\n",
      "Iteration [1814] | loss: 0.0006854568491689861\n",
      "Iteration [1815] | loss: 0.0006858142442069948\n",
      "Iteration [1816] | loss: 0.0006858142442069948\n",
      "Iteration [1817] | loss: 0.0006859333370812237\n",
      "Iteration [1818] | loss: 0.0006860524881631136\n",
      "Iteration [1819] | loss: 0.0006861716392450035\n",
      "Iteration [1820] | loss: 0.0006862907321192324\n",
      "Iteration [1821] | loss: 0.0006864098832011223\n",
      "Iteration [1822] | loss: 0.0006865289760753512\n",
      "Iteration [1823] | loss: 0.0006866481271572411\n",
      "Iteration [1824] | loss: 0.000686767278239131\n",
      "Iteration [1825] | loss: 0.0006868863711133599\n",
      "Iteration [1826] | loss: 0.0006871246150694788\n",
      "Iteration [1827] | loss: 0.0006871246150694788\n",
      "Iteration [1828] | loss: 0.0006873629172332585\n",
      "Iteration [1829] | loss: 0.0006873629172332585\n",
      "Iteration [1830] | loss: 0.0006876011611893773\n",
      "Iteration [1831] | loss: 0.0006877202540636063\n",
      "Iteration [1832] | loss: 0.0006877202540636063\n",
      "Iteration [1833] | loss: 0.000687958556227386\n",
      "Iteration [1834] | loss: 0.000687958556227386\n",
      "Iteration [1835] | loss: 0.000688077649101615\n",
      "Iteration [1836] | loss: 0.0006881968001835048\n",
      "Iteration [1837] | loss: 0.0006884350441396236\n",
      "Iteration [1838] | loss: 0.0006885541952215135\n",
      "Iteration [1839] | loss: 0.0006886732880957425\n",
      "Iteration [1840] | loss: 0.0006886732880957425\n",
      "Iteration [1841] | loss: 0.0006889115320518613\n",
      "Iteration [1842] | loss: 0.0006890306831337512\n",
      "Iteration [1843] | loss: 0.0006891497760079801\n",
      "Iteration [1844] | loss: 0.00068926892708987\n",
      "Iteration [1845] | loss: 0.00068926892708987\n",
      "Iteration [1846] | loss: 0.0006895071710459888\n",
      "Iteration [1847] | loss: 0.0006896263221278787\n",
      "Iteration [1848] | loss: 0.0006898645660839975\n",
      "Iteration [1849] | loss: 0.0006898645660839975\n",
      "Iteration [1850] | loss: 0.0006899837171658874\n",
      "Iteration [1851] | loss: 0.0006902219611220062\n",
      "Iteration [1852] | loss: 0.0006902219611220062\n",
      "Iteration [1853] | loss: 0.000690460205078125\n",
      "Iteration [1854] | loss: 0.000690460205078125\n",
      "Iteration [1855] | loss: 0.0006905793561600149\n",
      "Iteration [1856] | loss: 0.0006906984490342438\n",
      "Iteration [1857] | loss: 0.0006908176001161337\n",
      "Iteration [1858] | loss: 0.0006910558440722525\n",
      "Iteration [1859] | loss: 0.0006911749369464815\n",
      "Iteration [1860] | loss: 0.0006912940880283713\n",
      "Iteration [1861] | loss: 0.0006914132391102612\n",
      "Iteration [1862] | loss: 0.0006915323319844902\n",
      "Iteration [1863] | loss: 0.00069165148306638\n",
      "Iteration [1864] | loss: 0.000691770575940609\n",
      "Iteration [1865] | loss: 0.000691770575940609\n",
      "Iteration [1866] | loss: 0.0006920088781043887\n",
      "Iteration [1867] | loss: 0.0006920088781043887\n",
      "Iteration [1868] | loss: 0.0006922471220605075\n",
      "Iteration [1869] | loss: 0.0006924853660166264\n",
      "Iteration [1870] | loss: 0.0006924853660166264\n",
      "Iteration [1871] | loss: 0.0006927236099727452\n",
      "Iteration [1872] | loss: 0.0006927236099727452\n",
      "Iteration [1873] | loss: 0.000692961853928864\n",
      "Iteration [1874] | loss: 0.000692961853928864\n",
      "Iteration [1875] | loss: 0.0006930810050107539\n",
      "Iteration [1876] | loss: 0.0006932000978849828\n",
      "Iteration [1877] | loss: 0.0006933192489668727\n",
      "Iteration [1878] | loss: 0.0006935574929229915\n",
      "Iteration [1879] | loss: 0.0006935574929229915\n",
      "Iteration [1880] | loss: 0.0006939148879610002\n",
      "Iteration [1881] | loss: 0.0006939148879610002\n",
      "Iteration [1882] | loss: 0.0006940339808352292\n",
      "Iteration [1883] | loss: 0.000694153131917119\n",
      "Iteration [1884] | loss: 0.0006942722829990089\n",
      "Iteration [1885] | loss: 0.0006943913758732378\n",
      "Iteration [1886] | loss: 0.0006945105269551277\n",
      "Iteration [1887] | loss: 0.0006946296198293567\n",
      "Iteration [1888] | loss: 0.0006947487709112465\n",
      "Iteration [1889] | loss: 0.0006948678637854755\n",
      "Iteration [1890] | loss: 0.0006949870148673654\n",
      "Iteration [1891] | loss: 0.0006952252588234842\n",
      "Iteration [1892] | loss: 0.0006952252588234842\n",
      "Iteration [1893] | loss: 0.000695463502779603\n",
      "Iteration [1894] | loss: 0.0006955826538614929\n",
      "Iteration [1895] | loss: 0.0006957017467357218\n",
      "Iteration [1896] | loss: 0.0006958208978176117\n",
      "Iteration [1897] | loss: 0.0006958208978176117\n",
      "Iteration [1898] | loss: 0.0006960591417737305\n",
      "Iteration [1899] | loss: 0.0006960591417737305\n",
      "Iteration [1900] | loss: 0.0006962973857298493\n",
      "Iteration [1901] | loss: 0.0006964165368117392\n",
      "Iteration [1902] | loss: 0.0006965356878936291\n",
      "Iteration [1903] | loss: 0.0006967739318497479\n",
      "Iteration [1904] | loss: 0.0006967739318497479\n",
      "Iteration [1905] | loss: 0.0006970121758058667\n",
      "Iteration [1906] | loss: 0.0006970121758058667\n",
      "Iteration [1907] | loss: 0.0006971312686800957\n",
      "Iteration [1908] | loss: 0.0006972504197619855\n",
      "Iteration [1909] | loss: 0.0006973695708438754\n",
      "Iteration [1910] | loss: 0.0006976078147999942\n",
      "Iteration [1911] | loss: 0.0006976078147999942\n",
      "Iteration [1912] | loss: 0.000697965151630342\n",
      "Iteration [1913] | loss: 0.000697965151630342\n",
      "Iteration [1914] | loss: 0.0006980843027122319\n",
      "Iteration [1915] | loss: 0.0006982034537941217\n",
      "Iteration [1916] | loss: 0.0006983225466683507\n",
      "Iteration [1917] | loss: 0.0006985607906244695\n",
      "Iteration [1918] | loss: 0.0006985607906244695\n",
      "Iteration [1919] | loss: 0.0006986799417063594\n",
      "Iteration [1920] | loss: 0.0006987990345805883\n",
      "Iteration [1921] | loss: 0.0006989181856624782\n",
      "Iteration [1922] | loss: 0.0006990373367443681\n",
      "Iteration [1923] | loss: 0.0006992755807004869\n",
      "Iteration [1924] | loss: 0.0006993946735747159\n",
      "Iteration [1925] | loss: 0.0006995138246566057\n",
      "Iteration [1926] | loss: 0.0006996329175308347\n",
      "Iteration [1927] | loss: 0.0006997520686127245\n",
      "Iteration [1928] | loss: 0.0006998711614869535\n",
      "Iteration [1929] | loss: 0.0006998711614869535\n",
      "Iteration [1930] | loss: 0.0007001094636507332\n",
      "Iteration [1931] | loss: 0.0007001094636507332\n",
      "Iteration [1932] | loss: 0.0007003477076068521\n",
      "Iteration [1933] | loss: 0.0007005859515629709\n",
      "Iteration [1934] | loss: 0.0007007050444371998\n",
      "Iteration [1935] | loss: 0.0007008241955190897\n",
      "Iteration [1936] | loss: 0.0007008241955190897\n",
      "Iteration [1937] | loss: 0.0007010624394752085\n",
      "Iteration [1938] | loss: 0.0007010624394752085\n",
      "Iteration [1939] | loss: 0.0007013006834313273\n",
      "Iteration [1940] | loss: 0.0007014198345132172\n",
      "Iteration [1941] | loss: 0.0007014198345132172\n",
      "Iteration [1942] | loss: 0.000701658078469336\n",
      "Iteration [1943] | loss: 0.000701658078469336\n",
      "Iteration [1944] | loss: 0.0007020154735073447\n",
      "Iteration [1945] | loss: 0.0007021345663815737\n",
      "Iteration [1946] | loss: 0.0007022537174634635\n",
      "Iteration [1947] | loss: 0.0007023728103376925\n",
      "Iteration [1948] | loss: 0.0007023728103376925\n",
      "Iteration [1949] | loss: 0.0007026110542938113\n",
      "Iteration [1950] | loss: 0.0007026110542938113\n",
      "Iteration [1951] | loss: 0.0007028493564575911\n",
      "Iteration [1952] | loss: 0.00070296844933182\n",
      "Iteration [1953] | loss: 0.00070296844933182\n",
      "Iteration [1954] | loss: 0.0007032066932879388\n",
      "Iteration [1955] | loss: 0.0007033258443698287\n",
      "Iteration [1956] | loss: 0.0007035640883259475\n",
      "Iteration [1957] | loss: 0.0007035640883259475\n",
      "Iteration [1958] | loss: 0.0007036832394078374\n",
      "Iteration [1959] | loss: 0.0007039214833639562\n",
      "Iteration [1960] | loss: 0.0007039214833639562\n",
      "Iteration [1961] | loss: 0.000704159727320075\n",
      "Iteration [1962] | loss: 0.000704159727320075\n",
      "Iteration [1963] | loss: 0.0007043979712761939\n",
      "Iteration [1964] | loss: 0.0007043979712761939\n",
      "Iteration [1965] | loss: 0.0007046362152323127\n",
      "Iteration [1966] | loss: 0.0007048744591884315\n",
      "Iteration [1967] | loss: 0.0007048744591884315\n",
      "Iteration [1968] | loss: 0.0007051127031445503\n",
      "Iteration [1969] | loss: 0.0007051127031445503\n",
      "Iteration [1970] | loss: 0.0007052318542264402\n",
      "Iteration [1971] | loss: 0.000705470098182559\n",
      "Iteration [1972] | loss: 0.000705470098182559\n",
      "Iteration [1973] | loss: 0.0007057083421386778\n",
      "Iteration [1974] | loss: 0.0007057083421386778\n",
      "Iteration [1975] | loss: 0.0007059465860947967\n",
      "Iteration [1976] | loss: 0.0007060657371766865\n",
      "Iteration [1977] | loss: 0.0007061848300509155\n",
      "Iteration [1978] | loss: 0.0007064230740070343\n",
      "Iteration [1979] | loss: 0.0007064230740070343\n",
      "Iteration [1980] | loss: 0.0007066613179631531\n",
      "Iteration [1981] | loss: 0.0007066613179631531\n",
      "Iteration [1982] | loss: 0.0007068996201269329\n",
      "Iteration [1983] | loss: 0.0007068996201269329\n",
      "Iteration [1984] | loss: 0.0007070187130011618\n",
      "Iteration [1985] | loss: 0.0007072569569572806\n",
      "Iteration [1986] | loss: 0.0007073761080391705\n",
      "Iteration [1987] | loss: 0.0007076143519952893\n",
      "Iteration [1988] | loss: 0.0007076143519952893\n",
      "Iteration [1989] | loss: 0.0007077334448695183\n",
      "Iteration [1990] | loss: 0.000707971747033298\n",
      "Iteration [1991] | loss: 0.000707971747033298\n",
      "Iteration [1992] | loss: 0.0007082099909894168\n",
      "Iteration [1993] | loss: 0.0007082099909894168\n",
      "Iteration [1994] | loss: 0.0007084482349455357\n",
      "Iteration [1995] | loss: 0.0007085673278197646\n",
      "Iteration [1996] | loss: 0.0007085673278197646\n",
      "Iteration [1997] | loss: 0.0007089247228577733\n",
      "Iteration [1998] | loss: 0.0007089247228577733\n",
      "Iteration [1999] | loss: 0.0007091629668138921\n",
      "Iteration [2000] | loss: 0.0007091629668138921\n",
      "Iteration [2001] | loss: 0.000709282117895782\n",
      "Iteration [2002] | loss: 0.0007095203618519008\n",
      "Iteration [2003] | loss: 0.0007095203618519008\n",
      "Iteration [2004] | loss: 0.0007097586058080196\n",
      "Iteration [2005] | loss: 0.0007097586058080196\n",
      "Iteration [2006] | loss: 0.0007099968497641385\n",
      "Iteration [2007] | loss: 0.0007102350937202573\n",
      "Iteration [2008] | loss: 0.0007102350937202573\n",
      "Iteration [2009] | loss: 0.0007104733376763761\n",
      "Iteration [2010] | loss: 0.0007104733376763761\n",
      "Iteration [2011] | loss: 0.0007107115816324949\n",
      "Iteration [2012] | loss: 0.0007107115816324949\n",
      "Iteration [2013] | loss: 0.0007109498255886137\n",
      "Iteration [2014] | loss: 0.0007110689766705036\n",
      "Iteration [2015] | loss: 0.0007110689766705036\n",
      "Iteration [2016] | loss: 0.0007113072206266224\n",
      "Iteration [2017] | loss: 0.0007114263135008514\n",
      "Iteration [2018] | loss: 0.0007116645574569702\n",
      "Iteration [2019] | loss: 0.0007117837085388601\n",
      "Iteration [2020] | loss: 0.00071190285962075\n",
      "Iteration [2021] | loss: 0.0007120219524949789\n",
      "Iteration [2022] | loss: 0.0007120219524949789\n",
      "Iteration [2023] | loss: 0.0007122601964510977\n",
      "Iteration [2024] | loss: 0.0007123793475329876\n",
      "Iteration [2025] | loss: 0.0007124984404072165\n",
      "Iteration [2026] | loss: 0.0007126175914891064\n",
      "Iteration [2027] | loss: 0.0007127366843633354\n",
      "Iteration [2028] | loss: 0.0007129749283194542\n",
      "Iteration [2029] | loss: 0.0007130940794013441\n",
      "Iteration [2030] | loss: 0.000713213172275573\n",
      "Iteration [2031] | loss: 0.0007133323233574629\n",
      "Iteration [2032] | loss: 0.0007134514744393528\n",
      "Iteration [2033] | loss: 0.0007135705673135817\n",
      "Iteration [2034] | loss: 0.0007138088112697005\n",
      "Iteration [2035] | loss: 0.0007138088112697005\n",
      "Iteration [2036] | loss: 0.0007139279623515904\n",
      "Iteration [2037] | loss: 0.0007140470552258193\n",
      "Iteration [2038] | loss: 0.0007142852991819382\n",
      "Iteration [2039] | loss: 0.000714523543138057\n",
      "Iteration [2040] | loss: 0.000714523543138057\n",
      "Iteration [2041] | loss: 0.0007147617870941758\n",
      "Iteration [2042] | loss: 0.0007147617870941758\n",
      "Iteration [2043] | loss: 0.0007148809381760657\n",
      "Iteration [2044] | loss: 0.0007150000892579556\n",
      "Iteration [2045] | loss: 0.0007151191821321845\n",
      "Iteration [2046] | loss: 0.0007153574260883033\n",
      "Iteration [2047] | loss: 0.0007153574260883033\n",
      "Iteration [2048] | loss: 0.000715714821126312\n",
      "Iteration [2049] | loss: 0.000715714821126312\n",
      "Iteration [2050] | loss: 0.000715833914000541\n",
      "Iteration [2051] | loss: 0.0007160721579566598\n",
      "Iteration [2052] | loss: 0.0007160721579566598\n",
      "Iteration [2053] | loss: 0.0007163104019127786\n",
      "Iteration [2054] | loss: 0.0007163104019127786\n",
      "Iteration [2055] | loss: 0.0007165487040765584\n",
      "Iteration [2056] | loss: 0.0007166677969507873\n",
      "Iteration [2057] | loss: 0.0007166677969507873\n",
      "Iteration [2058] | loss: 0.0007169060409069061\n",
      "Iteration [2059] | loss: 0.000717025191988796\n",
      "Iteration [2060] | loss: 0.0007172634359449148\n",
      "Iteration [2061] | loss: 0.0007173825288191438\n",
      "Iteration [2062] | loss: 0.0007175016799010336\n",
      "Iteration [2063] | loss: 0.0007176207727752626\n",
      "Iteration [2064] | loss: 0.0007176207727752626\n",
      "Iteration [2065] | loss: 0.0007178590167313814\n",
      "Iteration [2066] | loss: 0.0007179781678132713\n",
      "Iteration [2067] | loss: 0.0007180972606875002\n",
      "Iteration [2068] | loss: 0.0007182164117693901\n",
      "Iteration [2069] | loss: 0.0007185738068073988\n",
      "Iteration [2070] | loss: 0.0007185738068073988\n",
      "Iteration [2071] | loss: 0.0007186928996816278\n",
      "Iteration [2072] | loss: 0.0007188120507635176\n",
      "Iteration [2073] | loss: 0.0007189311436377466\n",
      "Iteration [2074] | loss: 0.0007191693875938654\n",
      "Iteration [2075] | loss: 0.0007191693875938654\n",
      "Iteration [2076] | loss: 0.0007194076315499842\n",
      "Iteration [2077] | loss: 0.0007194076315499842\n",
      "Iteration [2078] | loss: 0.0007195267826318741\n",
      "Iteration [2079] | loss: 0.0007198841194622219\n",
      "Iteration [2080] | loss: 0.0007198841194622219\n",
      "Iteration [2081] | loss: 0.0007201223634183407\n",
      "Iteration [2082] | loss: 0.0007201223634183407\n",
      "Iteration [2083] | loss: 0.0007203606073744595\n",
      "Iteration [2084] | loss: 0.0007204797584563494\n",
      "Iteration [2085] | loss: 0.0007205988513305783\n",
      "Iteration [2086] | loss: 0.0007207180024124682\n",
      "Iteration [2087] | loss: 0.0007207180024124682\n",
      "Iteration [2088] | loss: 0.000720956246368587\n",
      "Iteration [2089] | loss: 0.0007211944903247058\n",
      "Iteration [2090] | loss: 0.0007213136414065957\n",
      "Iteration [2091] | loss: 0.0007214327342808247\n",
      "Iteration [2092] | loss: 0.0007215518853627145\n",
      "Iteration [2093] | loss: 0.0007216709782369435\n",
      "Iteration [2094] | loss: 0.0007217901293188334\n",
      "Iteration [2095] | loss: 0.0007219092221930623\n",
      "Iteration [2096] | loss: 0.0007220283732749522\n",
      "Iteration [2097] | loss: 0.000722266617231071\n",
      "Iteration [2098] | loss: 0.000722266617231071\n",
      "Iteration [2099] | loss: 0.0007226239540614188\n",
      "Iteration [2100] | loss: 0.0007226239540614188\n",
      "Iteration [2101] | loss: 0.0007227431051433086\n",
      "Iteration [2102] | loss: 0.0007229813490994275\n",
      "Iteration [2103] | loss: 0.0007229813490994275\n",
      "Iteration [2104] | loss: 0.0007232195930555463\n",
      "Iteration [2105] | loss: 0.0007232195930555463\n",
      "Iteration [2106] | loss: 0.0007234578370116651\n",
      "Iteration [2107] | loss: 0.000723576988093555\n",
      "Iteration [2108] | loss: 0.0007236960809677839\n",
      "Iteration [2109] | loss: 0.0007239343249239028\n",
      "Iteration [2110] | loss: 0.0007239343249239028\n",
      "Iteration [2111] | loss: 0.0007241725688800216\n",
      "Iteration [2112] | loss: 0.0007242917199619114\n",
      "Iteration [2113] | loss: 0.0007244108128361404\n",
      "Iteration [2114] | loss: 0.0007245299639180303\n",
      "Iteration [2115] | loss: 0.0007247682078741491\n",
      "Iteration [2116] | loss: 0.0007247682078741491\n",
      "Iteration [2117] | loss: 0.0007250064518302679\n",
      "Iteration [2118] | loss: 0.0007250064518302679\n",
      "Iteration [2119] | loss: 0.0007251255447044969\n",
      "Iteration [2120] | loss: 0.0007254829397425056\n",
      "Iteration [2121] | loss: 0.0007254829397425056\n",
      "Iteration [2122] | loss: 0.0007257211836986244\n",
      "Iteration [2123] | loss: 0.0007257211836986244\n",
      "Iteration [2124] | loss: 0.0007259594276547432\n",
      "Iteration [2125] | loss: 0.0007260785205289721\n",
      "Iteration [2126] | loss: 0.000726197671610862\n",
      "Iteration [2127] | loss: 0.000726316764485091\n",
      "Iteration [2128] | loss: 0.0007264359155669808\n",
      "Iteration [2129] | loss: 0.0007265550084412098\n",
      "Iteration [2130] | loss: 0.0007267932523973286\n",
      "Iteration [2131] | loss: 0.0007269124034792185\n",
      "Iteration [2132] | loss: 0.0007270314963534474\n",
      "Iteration [2133] | loss: 0.0007272697403095663\n",
      "Iteration [2134] | loss: 0.0007272697403095663\n",
      "Iteration [2135] | loss: 0.0007273888913914561\n",
      "Iteration [2136] | loss: 0.000727627135347575\n",
      "Iteration [2137] | loss: 0.000727627135347575\n",
      "Iteration [2138] | loss: 0.0007278653793036938\n",
      "Iteration [2139] | loss: 0.0007278653793036938\n",
      "Iteration [2140] | loss: 0.0007282227743417025\n",
      "Iteration [2141] | loss: 0.0007283418672159314\n",
      "Iteration [2142] | loss: 0.0007284610182978213\n",
      "Iteration [2143] | loss: 0.0007285801111720502\n",
      "Iteration [2144] | loss: 0.0007286992622539401\n",
      "Iteration [2145] | loss: 0.0007288183551281691\n",
      "Iteration [2146] | loss: 0.0007289375062100589\n",
      "Iteration [2147] | loss: 0.0007290565990842879\n",
      "Iteration [2148] | loss: 0.0007291757501661777\n",
      "Iteration [2149] | loss: 0.0007294139941222966\n",
      "Iteration [2150] | loss: 0.0007295330869965255\n",
      "Iteration [2151] | loss: 0.0007297713309526443\n",
      "Iteration [2152] | loss: 0.0007297713309526443\n",
      "Iteration [2153] | loss: 0.0007300095749087632\n",
      "Iteration [2154] | loss: 0.000730128725990653\n",
      "Iteration [2155] | loss: 0.000730128725990653\n",
      "Iteration [2156] | loss: 0.0007303669699467719\n",
      "Iteration [2157] | loss: 0.0007304860628210008\n",
      "Iteration [2158] | loss: 0.0007306052139028907\n",
      "Iteration [2159] | loss: 0.0007307243067771196\n",
      "Iteration [2160] | loss: 0.0007310817018151283\n",
      "Iteration [2161] | loss: 0.0007310817018151283\n",
      "Iteration [2162] | loss: 0.0007313199457712471\n",
      "Iteration [2163] | loss: 0.0007313199457712471\n",
      "Iteration [2164] | loss: 0.0007314390386454761\n",
      "Iteration [2165] | loss: 0.0007316772826015949\n",
      "Iteration [2166] | loss: 0.0007316772826015949\n",
      "Iteration [2167] | loss: 0.0007319155265577137\n",
      "Iteration [2168] | loss: 0.0007320346776396036\n",
      "Iteration [2169] | loss: 0.0007322729215957224\n",
      "Iteration [2170] | loss: 0.0007323920144699514\n",
      "Iteration [2171] | loss: 0.0007325111655518413\n",
      "Iteration [2172] | loss: 0.0007326302584260702\n",
      "Iteration [2173] | loss: 0.0007327494095079601\n",
      "Iteration [2174] | loss: 0.000732868502382189\n",
      "Iteration [2175] | loss: 0.0007329876534640789\n",
      "Iteration [2176] | loss: 0.0007332258974201977\n",
      "Iteration [2177] | loss: 0.0007332258974201977\n",
      "Iteration [2178] | loss: 0.0007334641413763165\n",
      "Iteration [2179] | loss: 0.0007335832342505455\n",
      "Iteration [2180] | loss: 0.0007338214782066643\n",
      "Iteration [2181] | loss: 0.0007339406292885542\n",
      "Iteration [2182] | loss: 0.0007340597221627831\n",
      "Iteration [2183] | loss: 0.000734178873244673\n",
      "Iteration [2184] | loss: 0.000734297966118902\n",
      "Iteration [2185] | loss: 0.0007344171172007918\n",
      "Iteration [2186] | loss: 0.0007345362100750208\n",
      "Iteration [2187] | loss: 0.0007347744540311396\n",
      "Iteration [2188] | loss: 0.0007347744540311396\n",
      "Iteration [2189] | loss: 0.0007351318490691483\n",
      "Iteration [2190] | loss: 0.0007351318490691483\n",
      "Iteration [2191] | loss: 0.0007353700930252671\n",
      "Iteration [2192] | loss: 0.0007354891858994961\n",
      "Iteration [2193] | loss: 0.000735608336981386\n",
      "Iteration [2194] | loss: 0.0007357274298556149\n",
      "Iteration [2195] | loss: 0.0007358465809375048\n",
      "Iteration [2196] | loss: 0.0007359656738117337\n",
      "Iteration [2197] | loss: 0.0007360848248936236\n",
      "Iteration [2198] | loss: 0.0007363230688497424\n",
      "Iteration [2199] | loss: 0.0007364421617239714\n",
      "Iteration [2200] | loss: 0.0007366804056800902\n",
      "Iteration [2201] | loss: 0.0007366804056800902\n",
      "Iteration [2202] | loss: 0.000736918649636209\n",
      "Iteration [2203] | loss: 0.0007370378007180989\n",
      "Iteration [2204] | loss: 0.0007371568935923278\n",
      "Iteration [2205] | loss: 0.0007372760446742177\n",
      "Iteration [2206] | loss: 0.0007373951375484467\n",
      "Iteration [2207] | loss: 0.0007375142886303365\n",
      "Iteration [2208] | loss: 0.0007376333815045655\n",
      "Iteration [2209] | loss: 0.0007379907765425742\n",
      "Iteration [2210] | loss: 0.0007379907765425742\n",
      "Iteration [2211] | loss: 0.0007382289622910321\n",
      "Iteration [2212] | loss: 0.0007382289622910321\n",
      "Iteration [2213] | loss: 0.0007384672062471509\n",
      "Iteration [2214] | loss: 0.0007385863573290408\n",
      "Iteration [2215] | loss: 0.0007387054502032697\n",
      "Iteration [2216] | loss: 0.0007388246012851596\n",
      "Iteration [2217] | loss: 0.0007389436941593885\n",
      "Iteration [2218] | loss: 0.0007390628452412784\n",
      "Iteration [2219] | loss: 0.0007393010891973972\n",
      "Iteration [2220] | loss: 0.000739539333153516\n",
      "Iteration [2221] | loss: 0.000739539333153516\n",
      "Iteration [2222] | loss: 0.0007397775771096349\n",
      "Iteration [2223] | loss: 0.0007398966699838638\n",
      "Iteration [2224] | loss: 0.0007400158210657537\n",
      "Iteration [2225] | loss: 0.0007401349139399827\n",
      "Iteration [2226] | loss: 0.0007402540650218725\n",
      "Iteration [2227] | loss: 0.0007403731578961015\n",
      "Iteration [2228] | loss: 0.0007407305529341102\n",
      "Iteration [2229] | loss: 0.0007407305529341102\n",
      "Iteration [2230] | loss: 0.0007408496458083391\n",
      "Iteration [2231] | loss: 0.0007410878897644579\n",
      "Iteration [2232] | loss: 0.0007410878897644579\n",
      "Iteration [2233] | loss: 0.0007413261337205768\n",
      "Iteration [2234] | loss: 0.0007414452848024666\n",
      "Iteration [2235] | loss: 0.0007415643776766956\n",
      "Iteration [2236] | loss: 0.0007416835287585855\n",
      "Iteration [2237] | loss: 0.0007419217727147043\n",
      "Iteration [2238] | loss: 0.0007420408655889332\n",
      "Iteration [2239] | loss: 0.000742279109545052\n",
      "Iteration [2240] | loss: 0.000742398202419281\n",
      "Iteration [2241] | loss: 0.000742398202419281\n",
      "Iteration [2242] | loss: 0.0007426364463753998\n",
      "Iteration [2243] | loss: 0.0007426364463753998\n",
      "Iteration [2244] | loss: 0.0007428746903315187\n",
      "Iteration [2245] | loss: 0.0007429938414134085\n",
      "Iteration [2246] | loss: 0.0007431129342876375\n",
      "Iteration [2247] | loss: 0.0007432320853695273\n",
      "Iteration [2248] | loss: 0.0007435894221998751\n",
      "Iteration [2249] | loss: 0.0007435894221998751\n",
      "Iteration [2250] | loss: 0.0007438276661559939\n",
      "Iteration [2251] | loss: 0.0007439468172378838\n",
      "Iteration [2252] | loss: 0.0007440659101121128\n",
      "Iteration [2253] | loss: 0.0007441850611940026\n",
      "Iteration [2254] | loss: 0.0007444233051501215\n",
      "Iteration [2255] | loss: 0.0007444233051501215\n",
      "Iteration [2256] | loss: 0.0007445423980243504\n",
      "Iteration [2257] | loss: 0.0007448997348546982\n",
      "Iteration [2258] | loss: 0.0007448997348546982\n",
      "Iteration [2259] | loss: 0.000745137978810817\n",
      "Iteration [2260] | loss: 0.0007452571298927069\n",
      "Iteration [2261] | loss: 0.0007453762227669358\n",
      "Iteration [2262] | loss: 0.0007454953738488257\n",
      "Iteration [2263] | loss: 0.0007456144667230546\n",
      "Iteration [2264] | loss: 0.0007457336178049445\n",
      "Iteration [2265] | loss: 0.0007459718617610633\n",
      "Iteration [2266] | loss: 0.0007459718617610633\n",
      "Iteration [2267] | loss: 0.0007463291985914111\n",
      "Iteration [2268] | loss: 0.000746448349673301\n",
      "Iteration [2269] | loss: 0.0007465674425475299\n",
      "Iteration [2270] | loss: 0.0007466865936294198\n",
      "Iteration [2271] | loss: 0.0007468056865036488\n",
      "Iteration [2272] | loss: 0.0007469248375855386\n",
      "Iteration [2273] | loss: 0.0007470439304597676\n",
      "Iteration [2274] | loss: 0.0007472821744158864\n",
      "Iteration [2275] | loss: 0.0007472821744158864\n",
      "Iteration [2276] | loss: 0.0007475204183720052\n",
      "Iteration [2277] | loss: 0.000747758662328124\n",
      "Iteration [2278] | loss: 0.000747877755202353\n",
      "Iteration [2279] | loss: 0.0007479969062842429\n",
      "Iteration [2280] | loss: 0.0007482351502403617\n",
      "Iteration [2281] | loss: 0.0007482351502403617\n",
      "Iteration [2282] | loss: 0.0007484733941964805\n",
      "Iteration [2283] | loss: 0.0007485924870707095\n",
      "Iteration [2284] | loss: 0.0007487116381525993\n",
      "Iteration [2285] | loss: 0.0007488307310268283\n",
      "Iteration [2286] | loss: 0.0007490689749829471\n",
      "Iteration [2287] | loss: 0.0007491880678571761\n",
      "Iteration [2288] | loss: 0.0007493072189390659\n",
      "Iteration [2289] | loss: 0.0007495454628951848\n",
      "Iteration [2290] | loss: 0.0007495454628951848\n",
      "Iteration [2291] | loss: 0.0007497837068513036\n",
      "Iteration [2292] | loss: 0.0007499027997255325\n",
      "Iteration [2293] | loss: 0.0007500219508074224\n",
      "Iteration [2294] | loss: 0.0007501410436816514\n",
      "Iteration [2295] | loss: 0.0007503792876377702\n",
      "Iteration [2296] | loss: 0.00075049843871966\n",
      "Iteration [2297] | loss: 0.000750736624468118\n",
      "Iteration [2298] | loss: 0.0007508557755500078\n",
      "Iteration [2299] | loss: 0.0007509748684242368\n",
      "Iteration [2300] | loss: 0.0007510940195061266\n",
      "Iteration [2301] | loss: 0.0007513322634622455\n",
      "Iteration [2302] | loss: 0.0007513322634622455\n",
      "Iteration [2303] | loss: 0.0007514513563364744\n",
      "Iteration [2304] | loss: 0.0007516896002925932\n",
      "Iteration [2305] | loss: 0.0007518087513744831\n",
      "Iteration [2306] | loss: 0.0007520469953306019\n",
      "Iteration [2307] | loss: 0.0007521660882048309\n",
      "Iteration [2308] | loss: 0.0007522851810790598\n",
      "Iteration [2309] | loss: 0.0007524043321609497\n",
      "Iteration [2310] | loss: 0.0007526425761170685\n",
      "Iteration [2311] | loss: 0.0007526425761170685\n",
      "Iteration [2312] | loss: 0.0007528808200731874\n",
      "Iteration [2313] | loss: 0.0007529999129474163\n",
      "Iteration [2314] | loss: 0.0007531190640293062\n",
      "Iteration [2315] | loss: 0.000753357307985425\n",
      "Iteration [2316] | loss: 0.0007535954937338829\n",
      "Iteration [2317] | loss: 0.0007535954937338829\n",
      "Iteration [2318] | loss: 0.0007538337376900017\n",
      "Iteration [2319] | loss: 0.0007539528887718916\n",
      "Iteration [2320] | loss: 0.0007540719816461205\n",
      "Iteration [2321] | loss: 0.0007541911327280104\n",
      "Iteration [2322] | loss: 0.0007544293766841292\n",
      "Iteration [2323] | loss: 0.0007544293766841292\n",
      "Iteration [2324] | loss: 0.0007546676206402481\n",
      "Iteration [2325] | loss: 0.0007549058645963669\n",
      "Iteration [2326] | loss: 0.0007549058645963669\n",
      "Iteration [2327] | loss: 0.0007551440503448248\n",
      "Iteration [2328] | loss: 0.0007552632014267147\n",
      "Iteration [2329] | loss: 0.0007553822943009436\n",
      "Iteration [2330] | loss: 0.0007555014453828335\n",
      "Iteration [2331] | loss: 0.0007557396893389523\n",
      "Iteration [2332] | loss: 0.0007557396893389523\n",
      "Iteration [2333] | loss: 0.0007559779332950711\n",
      "Iteration [2334] | loss: 0.000756216119043529\n",
      "Iteration [2335] | loss: 0.0007563352701254189\n",
      "Iteration [2336] | loss: 0.0007564543629996479\n",
      "Iteration [2337] | loss: 0.0007566926069557667\n",
      "Iteration [2338] | loss: 0.0007566926069557667\n",
      "Iteration [2339] | loss: 0.0007569308509118855\n",
      "Iteration [2340] | loss: 0.0007570500019937754\n",
      "Iteration [2341] | loss: 0.0007571690948680043\n",
      "Iteration [2342] | loss: 0.0007572882459498942\n",
      "Iteration [2343] | loss: 0.000757645582780242\n",
      "Iteration [2344] | loss: 0.000757645582780242\n",
      "Iteration [2345] | loss: 0.0007578838267363608\n",
      "Iteration [2346] | loss: 0.0007580029196105897\n",
      "Iteration [2347] | loss: 0.0007582411635667086\n",
      "Iteration [2348] | loss: 0.0007582411635667086\n",
      "Iteration [2349] | loss: 0.0007583603146485984\n",
      "Iteration [2350] | loss: 0.0007585985003970563\n",
      "Iteration [2351] | loss: 0.0007585985003970563\n",
      "Iteration [2352] | loss: 0.0007588367443531752\n",
      "Iteration [2353] | loss: 0.000759074988309294\n",
      "Iteration [2354] | loss: 0.0007591941393911839\n",
      "Iteration [2355] | loss: 0.0007593132322654128\n",
      "Iteration [2356] | loss: 0.0007595514762215316\n",
      "Iteration [2357] | loss: 0.0007595514762215316\n",
      "Iteration [2358] | loss: 0.0007597897201776505\n",
      "Iteration [2359] | loss: 0.0007599088130518794\n",
      "Iteration [2360] | loss: 0.0007600279641337693\n",
      "Iteration [2361] | loss: 0.0007601470570079982\n",
      "Iteration [2362] | loss: 0.0007605044520460069\n",
      "Iteration [2363] | loss: 0.0007605044520460069\n",
      "Iteration [2364] | loss: 0.0007607426377944648\n",
      "Iteration [2365] | loss: 0.0007608617888763547\n",
      "Iteration [2366] | loss: 0.0007609808817505836\n",
      "Iteration [2367] | loss: 0.0007611000328324735\n",
      "Iteration [2368] | loss: 0.0007613382767885923\n",
      "Iteration [2369] | loss: 0.0007614573696628213\n",
      "Iteration [2370] | loss: 0.0007615765207447112\n",
      "Iteration [2371] | loss: 0.0007618147064931691\n",
      "Iteration [2372] | loss: 0.0007620529504492879\n",
      "Iteration [2373] | loss: 0.0007620529504492879\n",
      "Iteration [2374] | loss: 0.0007622911944054067\n",
      "Iteration [2375] | loss: 0.0007624103454872966\n",
      "Iteration [2376] | loss: 0.0007625294383615255\n",
      "Iteration [2377] | loss: 0.0007626485894434154\n",
      "Iteration [2378] | loss: 0.0007628867751918733\n",
      "Iteration [2379] | loss: 0.0007628867751918733\n",
      "Iteration [2380] | loss: 0.000763244170229882\n",
      "Iteration [2381] | loss: 0.000763363263104111\n",
      "Iteration [2382] | loss: 0.000763363263104111\n",
      "Iteration [2383] | loss: 0.0007636015070602298\n",
      "Iteration [2384] | loss: 0.0007637205999344587\n",
      "Iteration [2385] | loss: 0.0007639588438905776\n",
      "Iteration [2386] | loss: 0.0007639588438905776\n",
      "Iteration [2387] | loss: 0.0007641970878466964\n",
      "Iteration [2388] | loss: 0.0007643162389285862\n",
      "Iteration [2389] | loss: 0.0007644353318028152\n",
      "Iteration [2390] | loss: 0.000764673575758934\n",
      "Iteration [2391] | loss: 0.0007649118197150528\n",
      "Iteration [2392] | loss: 0.0007649118197150528\n",
      "Iteration [2393] | loss: 0.0007651500636711717\n",
      "Iteration [2394] | loss: 0.0007652691565454006\n",
      "Iteration [2395] | loss: 0.0007653883076272905\n",
      "Iteration [2396] | loss: 0.0007655074005015194\n",
      "Iteration [2397] | loss: 0.0007657456444576383\n",
      "Iteration [2398] | loss: 0.0007658647373318672\n",
      "Iteration [2399] | loss: 0.000766102981287986\n",
      "Iteration [2400] | loss: 0.0007662221323698759\n",
      "Iteration [2401] | loss: 0.0007664603181183338\n",
      "Iteration [2402] | loss: 0.0007664603181183338\n",
      "Iteration [2403] | loss: 0.0007666985620744526\n",
      "Iteration [2404] | loss: 0.0007668177131563425\n",
      "Iteration [2405] | loss: 0.0007669368060305715\n",
      "Iteration [2406] | loss: 0.0007670559571124613\n",
      "Iteration [2407] | loss: 0.0007672941428609192\n",
      "Iteration [2408] | loss: 0.0007674132939428091\n",
      "Iteration [2409] | loss: 0.0007676515378989279\n",
      "Iteration [2410] | loss: 0.0007677706307731569\n",
      "Iteration [2411] | loss: 0.0007680088747292757\n",
      "Iteration [2412] | loss: 0.0007680088747292757\n",
      "Iteration [2413] | loss: 0.0007682471186853945\n",
      "Iteration [2414] | loss: 0.0007683662115596235\n",
      "Iteration [2415] | loss: 0.0007684853626415133\n",
      "Iteration [2416] | loss: 0.0007686044555157423\n",
      "Iteration [2417] | loss: 0.0007689617923460901\n",
      "Iteration [2418] | loss: 0.0007689617923460901\n",
      "Iteration [2419] | loss: 0.0007692000363022089\n",
      "Iteration [2420] | loss: 0.0007693191873840988\n",
      "Iteration [2421] | loss: 0.0007695574313402176\n",
      "Iteration [2422] | loss: 0.0007695574313402176\n",
      "Iteration [2423] | loss: 0.0007697956170886755\n",
      "Iteration [2424] | loss: 0.0007699147681705654\n",
      "Iteration [2425] | loss: 0.0007700338610447943\n",
      "Iteration [2426] | loss: 0.0007701530121266842\n",
      "Iteration [2427] | loss: 0.000770510348957032\n",
      "Iteration [2428] | loss: 0.000770510348957032\n",
      "Iteration [2429] | loss: 0.0007707485929131508\n",
      "Iteration [2430] | loss: 0.0007708676857873797\n",
      "Iteration [2431] | loss: 0.0007711059297434986\n",
      "Iteration [2432] | loss: 0.0007711059297434986\n",
      "Iteration [2433] | loss: 0.0007713441736996174\n",
      "Iteration [2434] | loss: 0.0007714632665738463\n",
      "Iteration [2435] | loss: 0.0007715824176557362\n",
      "Iteration [2436] | loss: 0.000771820661611855\n",
      "Iteration [2437] | loss: 0.0007720588473603129\n",
      "Iteration [2438] | loss: 0.0007721779984422028\n",
      "Iteration [2439] | loss: 0.0007722970913164318\n",
      "Iteration [2440] | loss: 0.0007724162423983216\n",
      "Iteration [2441] | loss: 0.0007726544863544405\n",
      "Iteration [2442] | loss: 0.0007726544863544405\n",
      "Iteration [2443] | loss: 0.0007728926721028984\n",
      "Iteration [2444] | loss: 0.0007730118231847882\n",
      "Iteration [2445] | loss: 0.000773250067140907\n",
      "Iteration [2446] | loss: 0.000773369160015136\n",
      "Iteration [2447] | loss: 0.0007736074039712548\n",
      "Iteration [2448] | loss: 0.0007737264968454838\n",
      "Iteration [2449] | loss: 0.0007738456479273736\n",
      "Iteration [2450] | loss: 0.0007739647408016026\n",
      "Iteration [2451] | loss: 0.0007742029847577214\n",
      "Iteration [2452] | loss: 0.0007742029847577214\n",
      "Iteration [2453] | loss: 0.0007744412287138402\n",
      "Iteration [2454] | loss: 0.0007746794726699591\n",
      "Iteration [2455] | loss: 0.000774917658418417\n",
      "Iteration [2456] | loss: 0.000774917658418417\n",
      "Iteration [2457] | loss: 0.0007751559023745358\n",
      "Iteration [2458] | loss: 0.0007752750534564257\n",
      "Iteration [2459] | loss: 0.0007753941463306546\n",
      "Iteration [2460] | loss: 0.0007755132392048836\n",
      "Iteration [2461] | loss: 0.0007757514831610024\n",
      "Iteration [2462] | loss: 0.0007758706342428923\n",
      "Iteration [2463] | loss: 0.0007761088781990111\n",
      "Iteration [2464] | loss: 0.00077622797107324\n",
      "Iteration [2465] | loss: 0.0007764662150293589\n",
      "Iteration [2466] | loss: 0.0007764662150293589\n",
      "Iteration [2467] | loss: 0.0007767044589854777\n",
      "Iteration [2468] | loss: 0.0007768235518597066\n",
      "Iteration [2469] | loss: 0.0007770617958158255\n",
      "Iteration [2470] | loss: 0.0007770617958158255\n",
      "Iteration [2471] | loss: 0.0007773000397719443\n",
      "Iteration [2472] | loss: 0.0007775382255204022\n",
      "Iteration [2473] | loss: 0.0007776573766022921\n",
      "Iteration [2474] | loss: 0.000777776469476521\n",
      "Iteration [2475] | loss: 0.0007780147134326398\n",
      "Iteration [2476] | loss: 0.0007781338645145297\n",
      "Iteration [2477] | loss: 0.0007782529573887587\n",
      "Iteration [2478] | loss: 0.0007783720502629876\n",
      "Iteration [2479] | loss: 0.0007786102942191064\n",
      "Iteration [2480] | loss: 0.0007786102942191064\n",
      "Iteration [2481] | loss: 0.0007789676310494542\n",
      "Iteration [2482] | loss: 0.0007790867821313441\n",
      "Iteration [2483] | loss: 0.0007793250260874629\n",
      "Iteration [2484] | loss: 0.0007793250260874629\n",
      "Iteration [2485] | loss: 0.0007795632118359208\n",
      "Iteration [2486] | loss: 0.0007796823629178107\n",
      "Iteration [2487] | loss: 0.0007799206068739295\n",
      "Iteration [2488] | loss: 0.0007799206068739295\n",
      "Iteration [2489] | loss: 0.0007801587926223874\n",
      "Iteration [2490] | loss: 0.0007803970365785062\n",
      "Iteration [2491] | loss: 0.0007805161876603961\n",
      "Iteration [2492] | loss: 0.000780635280534625\n",
      "Iteration [2493] | loss: 0.0007808735244907439\n",
      "Iteration [2494] | loss: 0.0007809926173649728\n",
      "Iteration [2495] | loss: 0.0007811117684468627\n",
      "Iteration [2496] | loss: 0.0007812308613210917\n",
      "Iteration [2497] | loss: 0.0007814691052772105\n",
      "Iteration [2498] | loss: 0.0007814691052772105\n",
      "Iteration [2499] | loss: 0.0007818264421075583\n",
      "Iteration [2500] | loss: 0.0007819455349817872\n",
      "Iteration [2501] | loss: 0.000782183778937906\n",
      "Iteration [2502] | loss: 0.0007823029300197959\n",
      "Iteration [2503] | loss: 0.0007824220228940248\n",
      "Iteration [2504] | loss: 0.0007826602668501437\n",
      "Iteration [2505] | loss: 0.0007827793597243726\n",
      "Iteration [2506] | loss: 0.0007828985108062625\n",
      "Iteration [2507] | loss: 0.0007830176036804914\n",
      "Iteration [2508] | loss: 0.0007833749405108392\n",
      "Iteration [2509] | loss: 0.0007833749405108392\n",
      "Iteration [2510] | loss: 0.000783613184466958\n",
      "Iteration [2511] | loss: 0.000783732277341187\n",
      "Iteration [2512] | loss: 0.0007839705212973058\n",
      "Iteration [2513] | loss: 0.0007839705212973058\n",
      "Iteration [2514] | loss: 0.0007842087652534246\n",
      "Iteration [2515] | loss: 0.0007843278581276536\n",
      "Iteration [2516] | loss: 0.0007845661020837724\n",
      "Iteration [2517] | loss: 0.0007846852531656623\n",
      "Iteration [2518] | loss: 0.0007849234389141202\n",
      "Iteration [2519] | loss: 0.0007850425899960101\n",
      "Iteration [2520] | loss: 0.000785280775744468\n",
      "Iteration [2521] | loss: 0.000785280775744468\n",
      "Iteration [2522] | loss: 0.0007855190197005868\n",
      "Iteration [2523] | loss: 0.0007856381707824767\n",
      "Iteration [2524] | loss: 0.0007858763565309346\n",
      "Iteration [2525] | loss: 0.0007858763565309346\n",
      "Iteration [2526] | loss: 0.0007862337515689433\n",
      "Iteration [2527] | loss: 0.0007863528444431722\n",
      "Iteration [2528] | loss: 0.0007864719373174012\n",
      "Iteration [2529] | loss: 0.00078671018127352\n",
      "Iteration [2530] | loss: 0.0007868293323554099\n",
      "Iteration [2531] | loss: 0.0007870675181038678\n",
      "Iteration [2532] | loss: 0.0007870675181038678\n",
      "Iteration [2533] | loss: 0.0007873057620599866\n",
      "Iteration [2534] | loss: 0.0007875440060161054\n",
      "Iteration [2535] | loss: 0.0007877822499722242\n",
      "Iteration [2536] | loss: 0.0007877822499722242\n",
      "Iteration [2537] | loss: 0.0007880204357206821\n",
      "Iteration [2538] | loss: 0.000788139586802572\n",
      "Iteration [2539] | loss: 0.0007883778307586908\n",
      "Iteration [2540] | loss: 0.0007883778307586908\n",
      "Iteration [2541] | loss: 0.0007886160165071487\n",
      "Iteration [2542] | loss: 0.0007887351675890386\n",
      "Iteration [2543] | loss: 0.0007890925044193864\n",
      "Iteration [2544] | loss: 0.0007890925044193864\n",
      "Iteration [2545] | loss: 0.0007893307483755052\n",
      "Iteration [2546] | loss: 0.0007895689341239631\n",
      "Iteration [2547] | loss: 0.000789688085205853\n",
      "Iteration [2548] | loss: 0.0007898071780800819\n",
      "Iteration [2549] | loss: 0.0007899263291619718\n",
      "Iteration [2550] | loss: 0.0007901645149104297\n",
      "Iteration [2551] | loss: 0.0007902836659923196\n",
      "Iteration [2552] | loss: 0.0007905219099484384\n",
      "Iteration [2553] | loss: 0.0007906410028226674\n",
      "Iteration [2554] | loss: 0.0007908792467787862\n",
      "Iteration [2555] | loss: 0.0007909983396530151\n",
      "Iteration [2556] | loss: 0.0007911174325272441\n",
      "Iteration [2557] | loss: 0.000791236583609134\n",
      "Iteration [2558] | loss: 0.0007914748275652528\n",
      "Iteration [2559] | loss: 0.0007915939204394817\n",
      "Iteration [2560] | loss: 0.0007918321643956006\n",
      "Iteration [2561] | loss: 0.0007919512572698295\n",
      "Iteration [2562] | loss: 0.0007921895012259483\n",
      "Iteration [2563] | loss: 0.0007924277451820672\n",
      "Iteration [2564] | loss: 0.0007924277451820672\n",
      "Iteration [2565] | loss: 0.0007926659309305251\n",
      "Iteration [2566] | loss: 0.0007927850820124149\n",
      "Iteration [2567] | loss: 0.0007930232677608728\n",
      "Iteration [2568] | loss: 0.0007930232677608728\n",
      "Iteration [2569] | loss: 0.0007933806627988815\n",
      "Iteration [2570] | loss: 0.0007934997556731105\n",
      "Iteration [2571] | loss: 0.0007937379996292293\n",
      "Iteration [2572] | loss: 0.0007938570925034583\n",
      "Iteration [2573] | loss: 0.0007939761853776872\n",
      "Iteration [2574] | loss: 0.0007940953364595771\n",
      "Iteration [2575] | loss: 0.0007943335804156959\n",
      "Iteration [2576] | loss: 0.0007944526732899249\n",
      "Iteration [2577] | loss: 0.0007945717661641538\n",
      "Iteration [2578] | loss: 0.0007949291029945016\n",
      "Iteration [2579] | loss: 0.0007950482540763915\n",
      "Iteration [2580] | loss: 0.0007952864980325103\n",
      "Iteration [2581] | loss: 0.0007952864980325103\n",
      "Iteration [2582] | loss: 0.0007955246837809682\n",
      "Iteration [2583] | loss: 0.0007956438348628581\n",
      "Iteration [2584] | loss: 0.000795882020611316\n",
      "Iteration [2585] | loss: 0.000795882020611316\n",
      "Iteration [2586] | loss: 0.0007962394156493247\n",
      "Iteration [2587] | loss: 0.0007963585085235536\n",
      "Iteration [2588] | loss: 0.0007965967524796724\n",
      "Iteration [2589] | loss: 0.0007965967524796724\n",
      "Iteration [2590] | loss: 0.0007968349382281303\n",
      "Iteration [2591] | loss: 0.0007970731821842492\n",
      "Iteration [2592] | loss: 0.000797192333266139\n",
      "Iteration [2593] | loss: 0.0007974305190145969\n",
      "Iteration [2594] | loss: 0.0007974305190145969\n",
      "Iteration [2595] | loss: 0.0007977878558449447\n",
      "Iteration [2596] | loss: 0.0007979070069268346\n",
      "Iteration [2597] | loss: 0.0007981451926752925\n",
      "Iteration [2598] | loss: 0.0007981451926752925\n",
      "Iteration [2599] | loss: 0.0007983834366314113\n",
      "Iteration [2600] | loss: 0.0007985025877133012\n",
      "Iteration [2601] | loss: 0.0007987407734617591\n",
      "Iteration [2602] | loss: 0.000798859924543649\n",
      "Iteration [2603] | loss: 0.0007990981102921069\n",
      "Iteration [2604] | loss: 0.0007993363542482257\n",
      "Iteration [2605] | loss: 0.0007994554471224546\n",
      "Iteration [2606] | loss: 0.0007995745982043445\n",
      "Iteration [2607] | loss: 0.0007996936910785735\n",
      "Iteration [2608] | loss: 0.0007999319350346923\n",
      "Iteration [2609] | loss: 0.0008000510279089212\n",
      "Iteration [2610] | loss: 0.0008002892718650401\n",
      "Iteration [2611] | loss: 0.0008002892718650401\n",
      "Iteration [2612] | loss: 0.0008006466086953878\n",
      "Iteration [2613] | loss: 0.0008007657015696168\n",
      "Iteration [2614] | loss: 0.0008010039455257356\n",
      "Iteration [2615] | loss: 0.0008011230966076255\n",
      "Iteration [2616] | loss: 0.0008012421894818544\n",
      "Iteration [2617] | loss: 0.0008014804334379733\n",
      "Iteration [2618] | loss: 0.0008015995263122022\n",
      "Iteration [2619] | loss: 0.000801837770268321\n",
      "Iteration [2620] | loss: 0.000801837770268321\n",
      "Iteration [2621] | loss: 0.0008021951070986688\n",
      "Iteration [2622] | loss: 0.0008023141999728978\n",
      "Iteration [2623] | loss: 0.0008025524439290166\n",
      "Iteration [2624] | loss: 0.0008025524439290166\n",
      "Iteration [2625] | loss: 0.0008027906878851354\n",
      "Iteration [2626] | loss: 0.0008030288736335933\n",
      "Iteration [2627] | loss: 0.0008031480247154832\n",
      "Iteration [2628] | loss: 0.0008033862104639411\n",
      "Iteration [2629] | loss: 0.000803505361545831\n",
      "Iteration [2630] | loss: 0.0008037435472942889\n",
      "Iteration [2631] | loss: 0.0008038626983761787\n",
      "Iteration [2632] | loss: 0.0008041008841246367\n",
      "Iteration [2633] | loss: 0.0008041008841246367\n",
      "Iteration [2634] | loss: 0.0008043391280807555\n",
      "Iteration [2635] | loss: 0.0008045773720368743\n",
      "Iteration [2636] | loss: 0.0008046964649111032\n",
      "Iteration [2637] | loss: 0.000805053801741451\n",
      "Iteration [2638] | loss: 0.000805053801741451\n",
      "Iteration [2639] | loss: 0.0008052920456975698\n",
      "Iteration [2640] | loss: 0.0008054111385717988\n",
      "Iteration [2641] | loss: 0.0008056493825279176\n",
      "Iteration [2642] | loss: 0.0008056493825279176\n",
      "Iteration [2643] | loss: 0.0008058876264840364\n",
      "Iteration [2644] | loss: 0.0008060067193582654\n",
      "Iteration [2645] | loss: 0.0008062449633143842\n",
      "Iteration [2646] | loss: 0.000806602300144732\n",
      "Iteration [2647] | loss: 0.000806602300144732\n",
      "Iteration [2648] | loss: 0.0008068405441008508\n",
      "Iteration [2649] | loss: 0.0008069596369750798\n",
      "Iteration [2650] | loss: 0.0008071978809311986\n",
      "Iteration [2651] | loss: 0.0008073169738054276\n",
      "Iteration [2652] | loss: 0.0008074360666796565\n",
      "Iteration [2653] | loss: 0.0008075552177615464\n",
      "Iteration [2654] | loss: 0.0008079125545918941\n",
      "Iteration [2655] | loss: 0.0008081507403403521\n",
      "Iteration [2656] | loss: 0.0008081507403403521\n",
      "Iteration [2657] | loss: 0.0008083889842964709\n",
      "Iteration [2658] | loss: 0.0008085080771706998\n",
      "Iteration [2659] | loss: 0.0008087463211268187\n",
      "Iteration [2660] | loss: 0.0008088654140010476\n",
      "Iteration [2661] | loss: 0.0008089845650829375\n",
      "Iteration [2662] | loss: 0.0008091036579571664\n",
      "Iteration [2663] | loss: 0.0008094609947875142\n",
      "Iteration [2664] | loss: 0.000809699238743633\n",
      "Iteration [2665] | loss: 0.000809818331617862\n",
      "Iteration [2666] | loss: 0.0008099374244920909\n",
      "Iteration [2667] | loss: 0.0008100565755739808\n",
      "Iteration [2668] | loss: 0.0008102948195300996\n",
      "Iteration [2669] | loss: 0.0008104139124043286\n",
      "Iteration [2670] | loss: 0.0008106521563604474\n",
      "Iteration [2671] | loss: 0.0008107712492346764\n",
      "Iteration [2672] | loss: 0.0008110094931907952\n",
      "Iteration [2673] | loss: 0.0008112476789392531\n",
      "Iteration [2674] | loss: 0.000811366830021143\n",
      "Iteration [2675] | loss: 0.0008114859228953719\n",
      "Iteration [2676] | loss: 0.0008116050157696009\n",
      "Iteration [2677] | loss: 0.0008118432597257197\n",
      "Iteration [2678] | loss: 0.0008119623525999486\n",
      "Iteration [2679] | loss: 0.0008123196894302964\n",
      "Iteration [2680] | loss: 0.0008123196894302964\n",
      "Iteration [2681] | loss: 0.0008125579333864152\n",
      "Iteration [2682] | loss: 0.0008127961773425341\n",
      "Iteration [2683] | loss: 0.000812915270216763\n",
      "Iteration [2684] | loss: 0.0008131535141728818\n",
      "Iteration [2685] | loss: 0.0008131535141728818\n",
      "Iteration [2686] | loss: 0.0008133916999213398\n",
      "Iteration [2687] | loss: 0.0008135108510032296\n",
      "Iteration [2688] | loss: 0.0008138681878335774\n",
      "Iteration [2689] | loss: 0.0008141063735820353\n",
      "Iteration [2690] | loss: 0.0008141063735820353\n",
      "Iteration [2691] | loss: 0.0008143446175381541\n",
      "Iteration [2692] | loss: 0.0008144637104123831\n",
      "Iteration [2693] | loss: 0.0008147019543685019\n",
      "Iteration [2694] | loss: 0.0008148210472427309\n",
      "Iteration [2695] | loss: 0.0008149401983246207\n",
      "Iteration [2696] | loss: 0.0008151783840730786\n",
      "Iteration [2697] | loss: 0.0008154166280291975\n",
      "Iteration [2698] | loss: 0.0008156548719853163\n",
      "Iteration [2699] | loss: 0.0008157739648595452\n",
      "Iteration [2700] | loss: 0.0008158930577337742\n",
      "Iteration [2701] | loss: 0.000816012208815664\n",
      "Iteration [2702] | loss: 0.000816250394564122\n",
      "Iteration [2703] | loss: 0.0008163695456460118\n",
      "Iteration [2704] | loss: 0.0008167268824763596\n",
      "Iteration [2705] | loss: 0.0008169650682248175\n",
      "Iteration [2706] | loss: 0.0008169650682248175\n",
      "Iteration [2707] | loss: 0.0008172033121809363\n",
      "Iteration [2708] | loss: 0.0008173224050551653\n",
      "Iteration [2709] | loss: 0.0008175606490112841\n",
      "Iteration [2710] | loss: 0.0008176797418855131\n",
      "Iteration [2711] | loss: 0.0008177988929674029\n",
      "Iteration [2712] | loss: 0.0008180370787158608\n",
      "Iteration [2713] | loss: 0.0008182753226719797\n",
      "Iteration [2714] | loss: 0.0008185135084204376\n",
      "Iteration [2715] | loss: 0.0008186326595023274\n",
      "Iteration [2716] | loss: 0.0008187517523765564\n",
      "Iteration [2717] | loss: 0.0008188708452507854\n",
      "Iteration [2718] | loss: 0.0008191090892069042\n",
      "Iteration [2719] | loss: 0.000819347333163023\n",
      "Iteration [2720] | loss: 0.000819466426037252\n",
      "Iteration [2721] | loss: 0.0008197046699933708\n",
      "Iteration [2722] | loss: 0.0008198237628675997\n",
      "Iteration [2723] | loss: 0.0008200620068237185\n",
      "Iteration [2724] | loss: 0.0008201810996979475\n",
      "Iteration [2725] | loss: 0.0008204193436540663\n",
      "Iteration [2726] | loss: 0.0008206575294025242\n",
      "Iteration [2727] | loss: 0.0008206575294025242\n",
      "Iteration [2728] | loss: 0.0008208957733586431\n",
      "Iteration [2729] | loss: 0.0008211340173147619\n",
      "Iteration [2730] | loss: 0.0008213722030632198\n",
      "Iteration [2731] | loss: 0.0008214913541451097\n",
      "Iteration [2732] | loss: 0.0008216104470193386\n",
      "Iteration [2733] | loss: 0.0008218486327677965\n",
      "Iteration [2734] | loss: 0.0008219677838496864\n",
      "Iteration [2735] | loss: 0.0008222059695981443\n",
      "Iteration [2736] | loss: 0.0008223251206800342\n",
      "Iteration [2737] | loss: 0.0008226824575103819\n",
      "Iteration [2738] | loss: 0.0008226824575103819\n",
      "Iteration [2739] | loss: 0.0008229206432588398\n",
      "Iteration [2740] | loss: 0.0008231588872149587\n",
      "Iteration [2741] | loss: 0.0008232779800891876\n",
      "Iteration [2742] | loss: 0.0008235162240453064\n",
      "Iteration [2743] | loss: 0.0008235162240453064\n",
      "Iteration [2744] | loss: 0.0008237544680014253\n",
      "Iteration [2745] | loss: 0.0008239926537498832\n",
      "Iteration [2746] | loss: 0.000824230897706002\n",
      "Iteration [2747] | loss: 0.0008244690834544599\n",
      "Iteration [2748] | loss: 0.0008245882345363498\n",
      "Iteration [2749] | loss: 0.0008247073274105787\n",
      "Iteration [2750] | loss: 0.0008248264202848077\n",
      "Iteration [2751] | loss: 0.0008250646642409265\n",
      "Iteration [2752] | loss: 0.0008251837571151555\n",
      "Iteration [2753] | loss: 0.0008255410939455032\n",
      "Iteration [2754] | loss: 0.0008257793379016221\n",
      "Iteration [2755] | loss: 0.0008257793379016221\n",
      "Iteration [2756] | loss: 0.00082601752365008\n",
      "Iteration [2757] | loss: 0.0008261366747319698\n",
      "Iteration [2758] | loss: 0.0008263748604804277\n",
      "Iteration [2759] | loss: 0.0008266131044365466\n",
      "Iteration [2760] | loss: 0.0008266131044365466\n",
      "Iteration [2761] | loss: 0.0008268513483926654\n",
      "Iteration [2762] | loss: 0.0008270895341411233\n",
      "Iteration [2763] | loss: 0.0008273277780972421\n",
      "Iteration [2764] | loss: 0.0008274468709714711\n",
      "Iteration [2765] | loss: 0.0008276851149275899\n",
      "Iteration [2766] | loss: 0.0008278042078018188\n",
      "Iteration [2767] | loss: 0.0008279233006760478\n",
      "Iteration [2768] | loss: 0.0008281615446321666\n",
      "Iteration [2769] | loss: 0.0008282806375063956\n",
      "Iteration [2770] | loss: 0.0008286379743367434\n",
      "Iteration [2771] | loss: 0.0008287570672109723\n",
      "Iteration [2772] | loss: 0.0008288762182928622\n",
      "Iteration [2773] | loss: 0.0008291144040413201\n",
      "Iteration [2774] | loss: 0.00082923355512321\n",
      "Iteration [2775] | loss: 0.0008294717408716679\n",
      "Iteration [2776] | loss: 0.0008295908919535577\n",
      "Iteration [2777] | loss: 0.0008298290777020156\n",
      "Iteration [2778] | loss: 0.0008300673216581345\n",
      "Iteration [2779] | loss: 0.0008301864145323634\n",
      "Iteration [2780] | loss: 0.0008304246584884822\n",
      "Iteration [2781] | loss: 0.0008305437513627112\n",
      "Iteration [2782] | loss: 0.00083078199531883\n",
      "Iteration [2783] | loss: 0.000830901088193059\n",
      "Iteration [2784] | loss: 0.0008310201810672879\n",
      "Iteration [2785] | loss: 0.0008312584250234067\n",
      "Iteration [2786] | loss: 0.0008314966107718647\n",
      "Iteration [2787] | loss: 0.0008317348547279835\n",
      "Iteration [2788] | loss: 0.0008318539476022124\n",
      "Iteration [2789] | loss: 0.0008320921915583313\n",
      "Iteration [2790] | loss: 0.0008322112844325602\n",
      "Iteration [2791] | loss: 0.0008323303773067892\n",
      "Iteration [2792] | loss: 0.000832568621262908\n",
      "Iteration [2793] | loss: 0.0008326877141371369\n",
      "Iteration [2794] | loss: 0.0008330450509674847\n",
      "Iteration [2795] | loss: 0.0008332832949236035\n",
      "Iteration [2796] | loss: 0.0008334023877978325\n",
      "Iteration [2797] | loss: 0.0008335214806720614\n",
      "Iteration [2798] | loss: 0.0008336406317539513\n",
      "Iteration [2799] | loss: 0.0008338788175024092\n",
      "Iteration [2800] | loss: 0.0008339979685842991\n",
      "Iteration [2801] | loss: 0.000834236154332757\n",
      "Iteration [2802] | loss: 0.0008345934911631048\n",
      "Iteration [2803] | loss: 0.0008345934911631048\n",
      "Iteration [2804] | loss: 0.0008348317351192236\n",
      "Iteration [2805] | loss: 0.0008349508279934525\n",
      "Iteration [2806] | loss: 0.0008351890719495714\n",
      "Iteration [2807] | loss: 0.0008354272576980293\n",
      "Iteration [2808] | loss: 0.0008355463505722582\n",
      "Iteration [2809] | loss: 0.0008357845945283771\n",
      "Iteration [2810] | loss: 0.000835903687402606\n",
      "Iteration [2811] | loss: 0.0008361419313587248\n",
      "Iteration [2812] | loss: 0.0008363801171071827\n",
      "Iteration [2813] | loss: 0.0008364992681890726\n",
      "Iteration [2814] | loss: 0.0008367374539375305\n",
      "Iteration [2815] | loss: 0.0008368566050194204\n",
      "Iteration [2816] | loss: 0.0008369756978936493\n",
      "Iteration [2817] | loss: 0.0008372138836421072\n",
      "Iteration [2818] | loss: 0.0008374521275982261\n",
      "Iteration [2819] | loss: 0.0008376903715543449\n",
      "Iteration [2820] | loss: 0.0008378094644285738\n",
      "Iteration [2821] | loss: 0.0008380476501770318\n",
      "Iteration [2822] | loss: 0.0008382858941331506\n",
      "Iteration [2823] | loss: 0.0008382858941331506\n",
      "Iteration [2824] | loss: 0.0008385241380892694\n",
      "Iteration [2825] | loss: 0.0008386432309634984\n",
      "Iteration [2826] | loss: 0.0008390005677938461\n",
      "Iteration [2827] | loss: 0.000839238753542304\n",
      "Iteration [2828] | loss: 0.0008393579046241939\n",
      "Iteration [2829] | loss: 0.0008395960903726518\n",
      "Iteration [2830] | loss: 0.0008395960903726518\n",
      "Iteration [2831] | loss: 0.0008398343343287706\n",
      "Iteration [2832] | loss: 0.0008400725200772285\n",
      "Iteration [2833] | loss: 0.0008401916129514575\n",
      "Iteration [2834] | loss: 0.0008405489497818053\n",
      "Iteration [2835] | loss: 0.0008406681008636951\n",
      "Iteration [2836] | loss: 0.000840906286612153\n",
      "Iteration [2837] | loss: 0.000840906286612153\n",
      "Iteration [2838] | loss: 0.0008411445305682719\n",
      "Iteration [2839] | loss: 0.0008413827163167298\n",
      "Iteration [2840] | loss: 0.0008415018673986197\n",
      "Iteration [2841] | loss: 0.0008418591460213065\n",
      "Iteration [2842] | loss: 0.0008420973899774253\n",
      "Iteration [2843] | loss: 0.0008422164828516543\n",
      "Iteration [2844] | loss: 0.0008423355757258832\n",
      "Iteration [2845] | loss: 0.0008424547268077731\n",
      "Iteration [2846] | loss: 0.000842692912556231\n",
      "Iteration [2847] | loss: 0.0008429311565123498\n",
      "Iteration [2848] | loss: 0.0008430502493865788\n",
      "Iteration [2849] | loss: 0.0008434075862169266\n",
      "Iteration [2850] | loss: 0.0008435266790911555\n",
      "Iteration [2851] | loss: 0.0008437649230472744\n",
      "Iteration [2852] | loss: 0.0008438840159215033\n",
      "Iteration [2853] | loss: 0.0008440031087957323\n",
      "Iteration [2854] | loss: 0.0008442413527518511\n",
      "Iteration [2855] | loss: 0.00084436044562608\n",
      "Iteration [2856] | loss: 0.0008445986895821989\n",
      "Iteration [2857] | loss: 0.0008449559682048857\n",
      "Iteration [2858] | loss: 0.0008450751192867756\n",
      "Iteration [2859] | loss: 0.0008451942121610045\n",
      "Iteration [2860] | loss: 0.0008453133050352335\n",
      "Iteration [2861] | loss: 0.0008455515489913523\n",
      "Iteration [2862] | loss: 0.0008457897347398102\n",
      "Iteration [2863] | loss: 0.0008459088858217001\n",
      "Iteration [2864] | loss: 0.000846147071570158\n",
      "Iteration [2865] | loss: 0.0008463853155262768\n",
      "Iteration [2866] | loss: 0.0008466235012747347\n",
      "Iteration [2867] | loss: 0.0008467426523566246\n",
      "Iteration [2868] | loss: 0.0008468617452308536\n",
      "Iteration [2869] | loss: 0.0008470999309793115\n",
      "Iteration [2870] | loss: 0.0008472190820612013\n",
      "Iteration [2871] | loss: 0.0008474572678096592\n",
      "Iteration [2872] | loss: 0.0008476955117657781\n",
      "Iteration [2873] | loss: 0.000847933697514236\n",
      "Iteration [2874] | loss: 0.0008481719414703548\n",
      "Iteration [2875] | loss: 0.0008481719414703548\n",
      "Iteration [2876] | loss: 0.0008484101272188127\n",
      "Iteration [2877] | loss: 0.0008486483711749315\n",
      "Iteration [2878] | loss: 0.0008487674640491605\n",
      "Iteration [2879] | loss: 0.0008490057080052793\n",
      "Iteration [2880] | loss: 0.0008492438937537372\n",
      "Iteration [2881] | loss: 0.000849482137709856\n",
      "Iteration [2882] | loss: 0.0008497203234583139\n",
      "Iteration [2883] | loss: 0.0008497203234583139\n",
      "Iteration [2884] | loss: 0.0008499585674144328\n",
      "Iteration [2885] | loss: 0.0008501967531628907\n",
      "Iteration [2886] | loss: 0.0008503158460371196\n",
      "Iteration [2887] | loss: 0.0008505540899932384\n",
      "Iteration [2888] | loss: 0.0008507922757416964\n",
      "Iteration [2889] | loss: 0.0008510305196978152\n",
      "Iteration [2890] | loss: 0.0008512687054462731\n",
      "Iteration [2891] | loss: 0.0008512687054462731\n",
      "Iteration [2892] | loss: 0.0008515069494023919\n",
      "Iteration [2893] | loss: 0.0008517451351508498\n",
      "Iteration [2894] | loss: 0.0008518642862327397\n",
      "Iteration [2895] | loss: 0.0008521024719811976\n",
      "Iteration [2896] | loss: 0.0008523407159373164\n",
      "Iteration [2897] | loss: 0.0008525789016857743\n",
      "Iteration [2898] | loss: 0.0008528171456418931\n",
      "Iteration [2899] | loss: 0.0008529362385161221\n",
      "Iteration [2900] | loss: 0.0008530553313903511\n",
      "Iteration [2901] | loss: 0.0008531744824722409\n",
      "Iteration [2902] | loss: 0.0008534126682206988\n",
      "Iteration [2903] | loss: 0.0008536509121768177\n",
      "Iteration [2904] | loss: 0.0008538890979252756\n",
      "Iteration [2905] | loss: 0.0008541273418813944\n",
      "Iteration [2906] | loss: 0.0008543655276298523\n",
      "Iteration [2907] | loss: 0.0008544846205040812\n",
      "Iteration [2908] | loss: 0.0008547228644602001\n",
      "Iteration [2909] | loss: 0.0008547228644602001\n",
      "Iteration [2910] | loss: 0.000854961050208658\n",
      "Iteration [2911] | loss: 0.0008551992941647768\n",
      "Iteration [2912] | loss: 0.0008554374799132347\n",
      "Iteration [2913] | loss: 0.0008556757238693535\n",
      "Iteration [2914] | loss: 0.0008559139096178114\n",
      "Iteration [2915] | loss: 0.0008560330606997013\n",
      "Iteration [2916] | loss: 0.0008562712464481592\n",
      "Iteration [2917] | loss: 0.0008563903393223882\n",
      "Iteration [2918] | loss: 0.000856509490404278\n",
      "Iteration [2919] | loss: 0.0008568667690269649\n",
      "Iteration [2920] | loss: 0.0008569859201088548\n",
      "Iteration [2921] | loss: 0.0008572241058573127\n",
      "Iteration [2922] | loss: 0.0008573431987315416\n",
      "Iteration [2923] | loss: 0.0008575814426876605\n",
      "Iteration [2924] | loss: 0.0008578196284361184\n",
      "Iteration [2925] | loss: 0.0008579387213103473\n",
      "Iteration [2926] | loss: 0.0008581769652664661\n",
      "Iteration [2927] | loss: 0.0008585343020968139\n",
      "Iteration [2928] | loss: 0.0008585343020968139\n",
      "Iteration [2929] | loss: 0.0008587724878452718\n",
      "Iteration [2930] | loss: 0.0008588915807195008\n",
      "Iteration [2931] | loss: 0.0008591298246756196\n",
      "Iteration [2932] | loss: 0.0008593680104240775\n",
      "Iteration [2933] | loss: 0.0008594871615059674\n",
      "Iteration [2934] | loss: 0.0008598444401286542\n",
      "Iteration [2935] | loss: 0.0008600826840847731\n",
      "Iteration [2936] | loss: 0.000860201776959002\n",
      "Iteration [2937] | loss: 0.0008604400209151208\n",
      "Iteration [2938] | loss: 0.0008604400209151208\n",
      "Iteration [2939] | loss: 0.0008606782066635787\n",
      "Iteration [2940] | loss: 0.0008609164506196976\n",
      "Iteration [2941] | loss: 0.0008610355434939265\n",
      "Iteration [2942] | loss: 0.0008613928221166134\n",
      "Iteration [2943] | loss: 0.0008616310660727322\n",
      "Iteration [2944] | loss: 0.0008617501589469612\n",
      "Iteration [2945] | loss: 0.00086198840290308\n",
      "Iteration [2946] | loss: 0.0008622265886515379\n",
      "Iteration [2947] | loss: 0.0008623456815257668\n",
      "Iteration [2948] | loss: 0.0008625839254818857\n",
      "Iteration [2949] | loss: 0.0008625839254818857\n",
      "Iteration [2950] | loss: 0.0008629412623122334\n",
      "Iteration [2951] | loss: 0.0008631794480606914\n",
      "Iteration [2952] | loss: 0.0008632985409349203\n",
      "Iteration [2953] | loss: 0.0008635367848910391\n",
      "Iteration [2954] | loss: 0.000863774970639497\n",
      "Iteration [2955] | loss: 0.000863894063513726\n",
      "Iteration [2956] | loss: 0.0008641323074698448\n",
      "Iteration [2957] | loss: 0.0008643704932183027\n",
      "Iteration [2958] | loss: 0.0008646087371744215\n",
      "Iteration [2959] | loss: 0.0008648469229228795\n",
      "Iteration [2960] | loss: 0.0008648469229228795\n",
      "Iteration [2961] | loss: 0.0008650851668789983\n",
      "Iteration [2962] | loss: 0.0008653233526274562\n",
      "Iteration [2963] | loss: 0.0008654424455016851\n",
      "Iteration [2964] | loss: 0.000865680689457804\n",
      "Iteration [2965] | loss: 0.0008660380262881517\n",
      "Iteration [2966] | loss: 0.0008661571191623807\n",
      "Iteration [2967] | loss: 0.0008663953049108386\n",
      "Iteration [2968] | loss: 0.0008665143977850676\n",
      "Iteration [2969] | loss: 0.0008667526417411864\n",
      "Iteration [2970] | loss: 0.0008669908274896443\n",
      "Iteration [2971] | loss: 0.0008671099785715342\n",
      "Iteration [2972] | loss: 0.0008673481643199921\n",
      "Iteration [2973] | loss: 0.00086758635006845\n",
      "Iteration [2974] | loss: 0.0008677055011503398\n",
      "Iteration [2975] | loss: 0.0008679436868987978\n",
      "Iteration [2976] | loss: 0.0008681819308549166\n",
      "Iteration [2977] | loss: 0.0008683010237291455\n",
      "Iteration [2978] | loss: 0.0008685392094776034\n",
      "Iteration [2979] | loss: 0.0008686583023518324\n",
      "Iteration [2980] | loss: 0.0008690156391821802\n",
      "Iteration [2981] | loss: 0.000869253883138299\n",
      "Iteration [2982] | loss: 0.0008693729760125279\n",
      "Iteration [2983] | loss: 0.0008696111617609859\n",
      "Iteration [2984] | loss: 0.0008698494057171047\n",
      "Iteration [2985] | loss: 0.0008698494057171047\n",
      "Iteration [2986] | loss: 0.0008700875914655626\n",
      "Iteration [2987] | loss: 0.0008704449282959104\n",
      "Iteration [2988] | loss: 0.0008705640211701393\n",
      "Iteration [2989] | loss: 0.0008708022069185972\n",
      "Iteration [2990] | loss: 0.000871040450874716\n",
      "Iteration [2991] | loss: 0.000871159543748945\n",
      "Iteration [2992] | loss: 0.0008713977294974029\n",
      "Iteration [2993] | loss: 0.0008716359734535217\n",
      "Iteration [2994] | loss: 0.0008717550663277507\n",
      "Iteration [2995] | loss: 0.0008721124031580985\n",
      "Iteration [2996] | loss: 0.0008722314960323274\n",
      "Iteration [2997] | loss: 0.0008724696817807853\n",
      "Iteration [2998] | loss: 0.0008727079257369041\n",
      "Iteration [2999] | loss: 0.0008727079257369041\n",
      "Iteration [3000] | loss: 0.000872946111485362\n",
      "Iteration [3001] | loss: 0.0008731843554414809\n",
      "Iteration [3002] | loss: 0.0008734225411899388\n",
      "Iteration [3003] | loss: 0.0008736607851460576\n",
      "Iteration [3004] | loss: 0.0008738989708945155\n",
      "Iteration [3005] | loss: 0.0008740180637687445\n",
      "Iteration [3006] | loss: 0.0008742563077248633\n",
      "Iteration [3007] | loss: 0.0008744944934733212\n",
      "Iteration [3008] | loss: 0.0008746135863475502\n",
      "Iteration [3009] | loss: 0.000874851830303669\n",
      "Iteration [3010] | loss: 0.0008752091089263558\n",
      "Iteration [3011] | loss: 0.0008753282018005848\n",
      "Iteration [3012] | loss: 0.0008755664457567036\n",
      "Iteration [3013] | loss: 0.0008758046315051615\n",
      "Iteration [3014] | loss: 0.0008759237825870514\n",
      "Iteration [3015] | loss: 0.0008760428754612803\n",
      "Iteration [3016] | loss: 0.0008761619683355093\n",
      "Iteration [3017] | loss: 0.0008765193051658571\n",
      "Iteration [3018] | loss: 0.000876757490914315\n",
      "Iteration [3019] | loss: 0.0008768765837885439\n",
      "Iteration [3020] | loss: 0.0008771148277446628\n",
      "Iteration [3021] | loss: 0.0008773530134931207\n",
      "Iteration [3022] | loss: 0.0008774721063673496\n",
      "Iteration [3023] | loss: 0.0008777103503234684\n",
      "Iteration [3024] | loss: 0.0008780676289461553\n",
      "Iteration [3025] | loss: 0.0008781867218203843\n",
      "Iteration [3026] | loss: 0.0008784249657765031\n",
      "Iteration [3027] | loss: 0.000878663151524961\n",
      "Iteration [3028] | loss: 0.00087878224439919\n",
      "Iteration [3029] | loss: 0.0008790204883553088\n",
      "Iteration [3030] | loss: 0.0008792586741037667\n",
      "Iteration [3031] | loss: 0.0008793777669779956\n",
      "Iteration [3032] | loss: 0.0008797351038083434\n",
      "Iteration [3033] | loss: 0.0008798541966825724\n",
      "Iteration [3034] | loss: 0.0008799732895568013\n",
      "Iteration [3035] | loss: 0.0008802115335129201\n",
      "Iteration [3036] | loss: 0.000880449719261378\n",
      "Iteration [3037] | loss: 0.000880568812135607\n",
      "Iteration [3038] | loss: 0.0008808070560917258\n",
      "Iteration [3039] | loss: 0.0008811643347144127\n",
      "Iteration [3040] | loss: 0.0008812834857963026\n",
      "Iteration [3041] | loss: 0.0008815216715447605\n",
      "Iteration [3042] | loss: 0.0008817598572932184\n",
      "Iteration [3043] | loss: 0.0008818790083751082\n",
      "Iteration [3044] | loss: 0.0008821171941235662\n",
      "Iteration [3045] | loss: 0.0008823553798720241\n",
      "Iteration [3046] | loss: 0.0008824745309539139\n",
      "Iteration [3047] | loss: 0.0008828318095766008\n",
      "Iteration [3048] | loss: 0.0008830699953250587\n",
      "Iteration [3049] | loss: 0.0008831891464069486\n",
      "Iteration [3050] | loss: 0.0008834273321554065\n",
      "Iteration [3051] | loss: 0.0008836655179038644\n",
      "Iteration [3052] | loss: 0.0008837846689857543\n",
      "Iteration [3053] | loss: 0.0008840228547342122\n",
      "Iteration [3054] | loss: 0.0008843801915645599\n",
      "Iteration [3055] | loss: 0.0008843801915645599\n",
      "Iteration [3056] | loss: 0.0008846183773130178\n",
      "Iteration [3057] | loss: 0.0008848565630614758\n",
      "Iteration [3058] | loss: 0.0008849757141433656\n",
      "Iteration [3059] | loss: 0.0008852138998918235\n",
      "Iteration [3060] | loss: 0.0008854520856402814\n",
      "Iteration [3061] | loss: 0.0008856903295964003\n",
      "Iteration [3062] | loss: 0.0008859285153448582\n",
      "Iteration [3063] | loss: 0.0008861667010933161\n",
      "Iteration [3064] | loss: 0.000886285852175206\n",
      "Iteration [3065] | loss: 0.0008865240379236639\n",
      "Iteration [3066] | loss: 0.0008867622236721218\n",
      "Iteration [3067] | loss: 0.0008868813747540116\n",
      "Iteration [3068] | loss: 0.0008872386533766985\n",
      "Iteration [3069] | loss: 0.0008874768391251564\n",
      "Iteration [3070] | loss: 0.0008875959902070463\n",
      "Iteration [3071] | loss: 0.0008878341759555042\n",
      "Iteration [3072] | loss: 0.0008880723617039621\n",
      "Iteration [3073] | loss: 0.000888191512785852\n",
      "Iteration [3074] | loss: 0.0008884296985343099\n",
      "Iteration [3075] | loss: 0.0008886678842827678\n",
      "Iteration [3076] | loss: 0.0008889061282388866\n",
      "Iteration [3077] | loss: 0.0008891443139873445\n",
      "Iteration [3078] | loss: 0.0008893824997358024\n",
      "Iteration [3079] | loss: 0.0008895016508176923\n",
      "Iteration [3080] | loss: 0.0008897398365661502\n",
      "Iteration [3081] | loss: 0.0008899780223146081\n",
      "Iteration [3082] | loss: 0.000890097115188837\n",
      "Iteration [3083] | loss: 0.0008904544520191848\n",
      "Iteration [3084] | loss: 0.0008906926377676427\n",
      "Iteration [3085] | loss: 0.0008908117306418717\n",
      "Iteration [3086] | loss: 0.0008909308817237616\n",
      "Iteration [3087] | loss: 0.0008911690674722195\n",
      "Iteration [3088] | loss: 0.0008912881603464484\n",
      "Iteration [3089] | loss: 0.0008915264043025672\n",
      "Iteration [3090] | loss: 0.0008918836829252541\n",
      "Iteration [3091] | loss: 0.0008920027757994831\n",
      "Iteration [3092] | loss: 0.0008922410197556019\n",
      "Iteration [3093] | loss: 0.0008924792055040598\n",
      "Iteration [3094] | loss: 0.0008925982983782887\n",
      "Iteration [3095] | loss: 0.0008928364841267467\n",
      "Iteration [3096] | loss: 0.0008930747280828655\n",
      "Iteration [3097] | loss: 0.0008933129138313234\n",
      "Iteration [3098] | loss: 0.0008935510995797813\n",
      "Iteration [3099] | loss: 0.0008937893435359001\n",
      "Iteration [3100] | loss: 0.000894027529284358\n",
      "Iteration [3101] | loss: 0.000894146622158587\n",
      "Iteration [3102] | loss: 0.0008943848661147058\n",
      "Iteration [3103] | loss: 0.0008946230518631637\n",
      "Iteration [3104] | loss: 0.0008948612376116216\n",
      "Iteration [3105] | loss: 0.0008950994815677404\n",
      "Iteration [3106] | loss: 0.0008953376673161983\n",
      "Iteration [3107] | loss: 0.0008954567601904273\n",
      "Iteration [3108] | loss: 0.0008956949459388852\n",
      "Iteration [3109] | loss: 0.000895933189895004\n",
      "Iteration [3110] | loss: 0.000896052282769233\n",
      "Iteration [3111] | loss: 0.0008962904685176909\n",
      "Iteration [3112] | loss: 0.0008966478053480387\n",
      "Iteration [3113] | loss: 0.0008967668982222676\n",
      "Iteration [3114] | loss: 0.0008970050839707255\n",
      "Iteration [3115] | loss: 0.0008972433279268444\n",
      "Iteration [3116] | loss: 0.0008973624208010733\n",
      "Iteration [3117] | loss: 0.0008976006065495312\n",
      "Iteration [3118] | loss: 0.0008978387922979891\n",
      "Iteration [3119] | loss: 0.0008981961291283369\n",
      "Iteration [3120] | loss: 0.0008983152220025659\n",
      "Iteration [3121] | loss: 0.0008985534077510238\n",
      "Iteration [3122] | loss: 0.0008987916517071426\n",
      "Iteration [3123] | loss: 0.0008989107445813715\n",
      "Iteration [3124] | loss: 0.0008991489303298295\n",
      "Iteration [3125] | loss: 0.0008993871742859483\n",
      "Iteration [3126] | loss: 0.0008996253600344062\n",
      "Iteration [3127] | loss: 0.0008998635457828641\n",
      "Iteration [3128] | loss: 0.000900101731531322\n",
      "Iteration [3129] | loss: 0.0009002208826132119\n",
      "Iteration [3130] | loss: 0.0009004590683616698\n",
      "Iteration [3131] | loss: 0.0009006972541101277\n",
      "Iteration [3132] | loss: 0.0009008163469843566\n",
      "Iteration [3133] | loss: 0.0009011736838147044\n",
      "Iteration [3134] | loss: 0.0009014118695631623\n",
      "Iteration [3135] | loss: 0.0009016501135192811\n",
      "Iteration [3136] | loss: 0.0009017692063935101\n",
      "Iteration [3137] | loss: 0.000902007392141968\n",
      "Iteration [3138] | loss: 0.0009022455778904259\n",
      "Iteration [3139] | loss: 0.0009023646707646549\n",
      "Iteration [3140] | loss: 0.0009027220075950027\n",
      "Iteration [3141] | loss: 0.0009029601933434606\n",
      "Iteration [3142] | loss: 0.0009030792862176895\n",
      "Iteration [3143] | loss: 0.0009033175301738083\n",
      "Iteration [3144] | loss: 0.0009035557159222662\n",
      "Iteration [3145] | loss: 0.0009036748087964952\n",
      "Iteration [3146] | loss: 0.0009039129945449531\n",
      "Iteration [3147] | loss: 0.0009042703313753009\n",
      "Iteration [3148] | loss: 0.0009045085171237588\n",
      "Iteration [3149] | loss: 0.0009046276099979877\n",
      "Iteration [3150] | loss: 0.0009048658539541066\n",
      "Iteration [3151] | loss: 0.0009051040397025645\n",
      "Iteration [3152] | loss: 0.0009052231325767934\n",
      "Iteration [3153] | loss: 0.0009054613183252513\n",
      "Iteration [3154] | loss: 0.0009058186551555991\n",
      "Iteration [3155] | loss: 0.0009059377480298281\n",
      "Iteration [3156] | loss: 0.000906175933778286\n",
      "Iteration [3157] | loss: 0.0009064141195267439\n",
      "Iteration [3158] | loss: 0.0009066523634828627\n",
      "Iteration [3159] | loss: 0.0009067714563570917\n",
      "Iteration [3160] | loss: 0.0009070096421055496\n",
      "Iteration [3161] | loss: 0.0009073669789358974\n",
      "Iteration [3162] | loss: 0.0009074860718101263\n",
      "Iteration [3163] | loss: 0.0009077242575585842\n",
      "Iteration [3164] | loss: 0.0009079624433070421\n",
      "Iteration [3165] | loss: 0.0009080815361812711\n",
      "Iteration [3166] | loss: 0.0009083197801373899\n",
      "Iteration [3167] | loss: 0.0009085579658858478\n",
      "Iteration [3168] | loss: 0.0009089152445085347\n",
      "Iteration [3169] | loss: 0.0009090343955904245\n",
      "Iteration [3170] | loss: 0.0009092725813388824\n",
      "Iteration [3171] | loss: 0.0009095107670873404\n",
      "Iteration [3172] | loss: 0.0009096298599615693\n",
      "Iteration [3173] | loss: 0.0009098681039176881\n",
      "Iteration [3174] | loss: 0.000910106289666146\n",
      "Iteration [3175] | loss: 0.0009104635682888329\n",
      "Iteration [3176] | loss: 0.0009105826611630619\n",
      "Iteration [3177] | loss: 0.0009108209051191807\n",
      "Iteration [3178] | loss: 0.0009110590908676386\n",
      "Iteration [3179] | loss: 0.0009111781837418675\n",
      "Iteration [3180] | loss: 0.0009115354623645544\n",
      "Iteration [3181] | loss: 0.0009117737063206732\n",
      "Iteration [3182] | loss: 0.0009120118920691311\n",
      "Iteration [3183] | loss: 0.000912250077817589\n",
      "Iteration [3184] | loss: 0.0009124883217737079\n",
      "Iteration [3185] | loss: 0.0009127265075221658\n",
      "Iteration [3186] | loss: 0.0009128456003963947\n",
      "Iteration [3187] | loss: 0.0009130837861448526\n",
      "Iteration [3188] | loss: 0.0009133219718933105\n",
      "Iteration [3189] | loss: 0.0009135602158494294\n",
      "Iteration [3190] | loss: 0.0009137984015978873\n",
      "Iteration [3191] | loss: 0.0009140365873463452\n",
      "Iteration [3192] | loss: 0.0009142747730948031\n",
      "Iteration [3193] | loss: 0.000914393924176693\n",
      "Iteration [3194] | loss: 0.0009146321099251509\n",
      "Iteration [3195] | loss: 0.0009148702956736088\n",
      "Iteration [3196] | loss: 0.0009151084814220667\n",
      "Iteration [3197] | loss: 0.0009153467253781855\n",
      "Iteration [3198] | loss: 0.0009155849111266434\n",
      "Iteration [3199] | loss: 0.0009158230968751013\n",
      "Iteration [3200] | loss: 0.0009159421897493303\n",
      "Iteration [3201] | loss: 0.0009161804337054491\n",
      "Iteration [3202] | loss: 0.000916418619453907\n",
      "Iteration [3203] | loss: 0.0009166568052023649\n",
      "Iteration [3204] | loss: 0.0009168949909508228\n",
      "Iteration [3205] | loss: 0.0009171332349069417\n",
      "Iteration [3206] | loss: 0.0009173714206553996\n",
      "Iteration [3207] | loss: 0.0009174905135296285\n",
      "Iteration [3208] | loss: 0.0009177286992780864\n",
      "Iteration [3209] | loss: 0.0009179668850265443\n",
      "Iteration [3210] | loss: 0.0009183242218568921\n",
      "Iteration [3211] | loss: 0.00091856240760535\n",
      "Iteration [3212] | loss: 0.0009188005933538079\n",
      "Iteration [3213] | loss: 0.0009190387791022658\n",
      "Iteration [3214] | loss: 0.0009191579301841557\n",
      "Iteration [3215] | loss: 0.0009193961159326136\n",
      "Iteration [3216] | loss: 0.0009196343016810715\n",
      "Iteration [3217] | loss: 0.0009198724874295294\n",
      "Iteration [3218] | loss: 0.0009201106731779873\n",
      "Iteration [3219] | loss: 0.0009203489171341062\n",
      "Iteration [3220] | loss: 0.0009205871028825641\n",
      "Iteration [3221] | loss: 0.000920706195756793\n",
      "Iteration [3222] | loss: 0.0009209443815052509\n",
      "Iteration [3223] | loss: 0.0009213017183355987\n",
      "Iteration [3224] | loss: 0.0009214208112098277\n",
      "Iteration [3225] | loss: 0.0009216589969582856\n",
      "Iteration [3226] | loss: 0.0009218971827067435\n",
      "Iteration [3227] | loss: 0.0009221353684552014\n",
      "Iteration [3228] | loss: 0.0009222545195370913\n",
      "Iteration [3229] | loss: 0.0009224927052855492\n",
      "Iteration [3230] | loss: 0.000922849983908236\n",
      "Iteration [3231] | loss: 0.0009232072625309229\n",
      "Iteration [3232] | loss: 0.0009233264136128128\n",
      "Iteration [3233] | loss: 0.0009235645993612707\n",
      "Iteration [3234] | loss: 0.0009238027851097286\n",
      "Iteration [3235] | loss: 0.0009239218779839575\n",
      "Iteration [3236] | loss: 0.0009241600637324154\n",
      "Iteration [3237] | loss: 0.0009245174005627632\n",
      "Iteration [3238] | loss: 0.0009247555863112211\n",
      "Iteration [3239] | loss: 0.0009248746791854501\n",
      "Iteration [3240] | loss: 0.000925112864933908\n",
      "Iteration [3241] | loss: 0.0009253510506823659\n",
      "Iteration [3242] | loss: 0.0009255892946384847\n",
      "Iteration [3243] | loss: 0.0009257083875127137\n",
      "Iteration [3244] | loss: 0.0009260656661354005\n",
      "Iteration [3245] | loss: 0.0009263038518838584\n",
      "Iteration [3246] | loss: 0.0009264229447580874\n",
      "Iteration [3247] | loss: 0.0009266611887142062\n",
      "Iteration [3248] | loss: 0.0009270184673368931\n",
      "Iteration [3249] | loss: 0.000927256653085351\n",
      "Iteration [3250] | loss: 0.0009273757459595799\n",
      "Iteration [3251] | loss: 0.0009277330245822668\n",
      "Iteration [3252] | loss: 0.0009279712685383856\n",
      "Iteration [3253] | loss: 0.0009282094542868435\n",
      "Iteration [3254] | loss: 0.0009283285471610725\n",
      "Iteration [3255] | loss: 0.0009285667329095304\n",
      "Iteration [3256] | loss: 0.0009288049186579883\n",
      "Iteration [3257] | loss: 0.0009291622554883361\n",
      "Iteration [3258] | loss: 0.000929281348362565\n",
      "Iteration [3259] | loss: 0.000929519534111023\n",
      "Iteration [3260] | loss: 0.0009297577198594809\n",
      "Iteration [3261] | loss: 0.0009298768127337098\n",
      "Iteration [3262] | loss: 0.0009301149984821677\n",
      "Iteration [3263] | loss: 0.0009304723353125155\n",
      "Iteration [3264] | loss: 0.0009308296139352024\n",
      "Iteration [3265] | loss: 0.0009309487068094313\n",
      "Iteration [3266] | loss: 0.0009311868925578892\n",
      "Iteration [3267] | loss: 0.0009314250783063471\n",
      "Iteration [3268] | loss: 0.000931663322262466\n",
      "Iteration [3269] | loss: 0.0009317824151366949\n",
      "Iteration [3270] | loss: 0.0009320206008851528\n",
      "Iteration [3271] | loss: 0.0009323778795078397\n",
      "Iteration [3272] | loss: 0.0009326160652562976\n",
      "Iteration [3273] | loss: 0.0009327351581305265\n",
      "Iteration [3274] | loss: 0.0009329734020866454\n",
      "Iteration [3275] | loss: 0.0009332115878351033\n",
      "Iteration [3276] | loss: 0.0009335688664577901\n",
      "Iteration [3277] | loss: 0.0009336879593320191\n",
      "Iteration [3278] | loss: 0.000934045237954706\n",
      "Iteration [3279] | loss: 0.0009342834819108248\n",
      "Iteration [3280] | loss: 0.0009345216676592827\n",
      "Iteration [3281] | loss: 0.0009346407605335116\n",
      "Iteration [3282] | loss: 0.0009348789462819695\n",
      "Iteration [3283] | loss: 0.0009351171320304275\n",
      "Iteration [3284] | loss: 0.0009353553177788854\n",
      "Iteration [3285] | loss: 0.0009355935617350042\n",
      "Iteration [3286] | loss: 0.0009358317474834621\n",
      "Iteration [3287] | loss: 0.00093606993323192\n",
      "Iteration [3288] | loss: 0.0009363081189803779\n",
      "Iteration [3289] | loss: 0.0009365463047288358\n",
      "Iteration [3290] | loss: 0.0009367844904772937\n",
      "Iteration [3291] | loss: 0.0009371418273076415\n",
      "Iteration [3292] | loss: 0.0009372609201818705\n",
      "Iteration [3293] | loss: 0.0009374991059303284\n",
      "Iteration [3294] | loss: 0.0009377372916787863\n",
      "Iteration [3295] | loss: 0.0009379754774272442\n",
      "Iteration [3296] | loss: 0.0009380945703014731\n",
      "Iteration [3297] | loss: 0.000938332756049931\n",
      "Iteration [3298] | loss: 0.0009386900928802788\n",
      "Iteration [3299] | loss: 0.0009390473715029657\n",
      "Iteration [3300] | loss: 0.0009391664643771946\n",
      "Iteration [3301] | loss: 0.0009394046501256526\n",
      "Iteration [3302] | loss: 0.0009396428358741105\n",
      "Iteration [3303] | loss: 0.0009398810216225684\n",
      "Iteration [3304] | loss: 0.0009401192655786872\n",
      "Iteration [3305] | loss: 0.0009403574513271451\n",
      "Iteration [3306] | loss: 0.000940595637075603\n",
      "Iteration [3307] | loss: 0.0009408338228240609\n",
      "Iteration [3308] | loss: 0.0009409529156982899\n",
      "Iteration [3309] | loss: 0.0009411911014467478\n",
      "Iteration [3310] | loss: 0.0009415484382770956\n",
      "Iteration [3311] | loss: 0.0009419057168997824\n",
      "Iteration [3312] | loss: 0.0009420248097740114\n",
      "Iteration [3313] | loss: 0.0009422629955224693\n",
      "Iteration [3314] | loss: 0.0009425011812709272\n",
      "Iteration [3315] | loss: 0.0009427393670193851\n",
      "Iteration [3316] | loss: 0.000942977552767843\n",
      "Iteration [3317] | loss: 0.0009432157967239618\n",
      "Iteration [3318] | loss: 0.0009434539824724197\n",
      "Iteration [3319] | loss: 0.0009438112610951066\n",
      "Iteration [3320] | loss: 0.0009440494468435645\n",
      "Iteration [3321] | loss: 0.0009441685397177935\n",
      "Iteration [3322] | loss: 0.0009444067254662514\n",
      "Iteration [3323] | loss: 0.0009446449112147093\n",
      "Iteration [3324] | loss: 0.0009450022480450571\n",
      "Iteration [3325] | loss: 0.000945121340919286\n",
      "Iteration [3326] | loss: 0.0009453595266677439\n",
      "Iteration [3327] | loss: 0.0009455977124162018\n",
      "Iteration [3328] | loss: 0.0009458358981646597\n",
      "Iteration [3329] | loss: 0.0009460740839131176\n",
      "Iteration [3330] | loss: 0.0009463122696615756\n",
      "Iteration [3331] | loss: 0.0009466695482842624\n",
      "Iteration [3332] | loss: 0.0009469077922403812\n",
      "Iteration [3333] | loss: 0.0009470268851146102\n",
      "Iteration [3334] | loss: 0.0009472650708630681\n",
      "Iteration [3335] | loss: 0.000947503256611526\n",
      "Iteration [3336] | loss: 0.0009477414423599839\n",
      "Iteration [3337] | loss: 0.0009480987209826708\n",
      "Iteration [3338] | loss: 0.0009483369067311287\n",
      "Iteration [3339] | loss: 0.0009485750924795866\n",
      "Iteration [3340] | loss: 0.0009488132782280445\n",
      "Iteration [3341] | loss: 0.0009490515221841633\n",
      "Iteration [3342] | loss: 0.0009491706150583923\n",
      "Iteration [3343] | loss: 0.0009494088008068502\n",
      "Iteration [3344] | loss: 0.0009497660794295371\n",
      "Iteration [3345] | loss: 0.000950004265177995\n",
      "Iteration [3346] | loss: 0.0009502424509264529\n",
      "Iteration [3347] | loss: 0.0009504806366749108\n",
      "Iteration [3348] | loss: 0.0009507188224233687\n",
      "Iteration [3349] | loss: 0.0009509570081718266\n",
      "Iteration [3350] | loss: 0.0009511952521279454\n",
      "Iteration [3351] | loss: 0.0009514334378764033\n",
      "Iteration [3352] | loss: 0.0009516716236248612\n",
      "Iteration [3353] | loss: 0.0009519098093733191\n",
      "Iteration [3354] | loss: 0.000952267087996006\n",
      "Iteration [3355] | loss: 0.000952386180870235\n",
      "Iteration [3356] | loss: 0.0009526243666186929\n",
      "Iteration [3357] | loss: 0.0009529816452413797\n",
      "Iteration [3358] | loss: 0.0009532198309898376\n",
      "Iteration [3359] | loss: 0.0009533389820717275\n",
      "Iteration [3360] | loss: 0.0009535771678201854\n",
      "Iteration [3361] | loss: 0.0009539344464428723\n",
      "Iteration [3362] | loss: 0.0009541726321913302\n",
      "Iteration [3363] | loss: 0.0009544108179397881\n",
      "Iteration [3364] | loss: 0.000954649003688246\n",
      "Iteration [3365] | loss: 0.0009548871894367039\n",
      "Iteration [3366] | loss: 0.0009551253751851618\n",
      "Iteration [3367] | loss: 0.0009553635609336197\n",
      "Iteration [3368] | loss: 0.0009554826538078487\n",
      "Iteration [3369] | loss: 0.0009558399324305356\n",
      "Iteration [3370] | loss: 0.0009561972692608833\n",
      "Iteration [3371] | loss: 0.0009564354550093412\n",
      "Iteration [3372] | loss: 0.0009565545478835702\n",
      "Iteration [3373] | loss: 0.0009567927336320281\n",
      "Iteration [3374] | loss: 0.000957030919380486\n",
      "Iteration [3375] | loss: 0.0009572691051289439\n",
      "Iteration [3376] | loss: 0.0009577454766258597\n",
      "Iteration [3377] | loss: 0.0009578645695000887\n",
      "Iteration [3378] | loss: 0.0009581027552485466\n",
      "Iteration [3379] | loss: 0.0009583409409970045\n",
      "Iteration [3380] | loss: 0.0009585791267454624\n",
      "Iteration [3381] | loss: 0.0009586982196196914\n",
      "Iteration [3382] | loss: 0.0009589364635758102\n",
      "Iteration [3383] | loss: 0.000959412835072726\n",
      "Iteration [3384] | loss: 0.0009596510208211839\n",
      "Iteration [3385] | loss: 0.0009598892065696418\n",
      "Iteration [3386] | loss: 0.0009600082994438708\n",
      "Iteration [3387] | loss: 0.0009602464851923287\n",
      "Iteration [3388] | loss: 0.0009604846709407866\n",
      "Iteration [3389] | loss: 0.0009608419495634735\n",
      "Iteration [3390] | loss: 0.0009611992281861603\n",
      "Iteration [3391] | loss: 0.0009613183210603893\n",
      "Iteration [3392] | loss: 0.0009615565068088472\n",
      "Iteration [3393] | loss: 0.0009617946925573051\n",
      "Iteration [3394] | loss: 0.000962032878305763\n",
      "Iteration [3395] | loss: 0.0009622710640542209\n",
      "Iteration [3396] | loss: 0.0009626284008845687\n",
      "Iteration [3397] | loss: 0.0009628665866330266\n",
      "Iteration [3398] | loss: 0.0009631047723814845\n",
      "Iteration [3399] | loss: 0.0009633429581299424\n",
      "Iteration [3400] | loss: 0.0009634620510041714\n",
      "Iteration [3401] | loss: 0.0009637002367526293\n",
      "Iteration [3402] | loss: 0.0009640575153753161\n",
      "Iteration [3403] | loss: 0.000964414793998003\n",
      "Iteration [3404] | loss: 0.0009646529797464609\n",
      "Iteration [3405] | loss: 0.0009647720726206899\n",
      "Iteration [3406] | loss: 0.0009650102583691478\n",
      "Iteration [3407] | loss: 0.0009652484441176057\n",
      "Iteration [3408] | loss: 0.0009656057227402925\n",
      "Iteration [3409] | loss: 0.0009659630013629794\n",
      "Iteration [3410] | loss: 0.0009660820942372084\n",
      "Iteration [3411] | loss: 0.0009663202799856663\n",
      "Iteration [3412] | loss: 0.0009665584657341242\n",
      "Iteration [3413] | loss: 0.0009667966514825821\n",
      "Iteration [3414] | loss: 0.00096703483723104\n",
      "Iteration [3415] | loss: 0.0009673921740613878\n",
      "Iteration [3416] | loss: 0.0009676303598098457\n",
      "Iteration [3417] | loss: 0.0009678685455583036\n",
      "Iteration [3418] | loss: 0.0009681067313067615\n",
      "Iteration [3419] | loss: 0.0009683449170552194\n",
      "Iteration [3420] | loss: 0.0009687021956779063\n",
      "Iteration [3421] | loss: 0.0009689403814263642\n",
      "Iteration [3422] | loss: 0.0009691785671748221\n",
      "Iteration [3423] | loss: 0.00096941675292328\n",
      "Iteration [3424] | loss: 0.0009696549386717379\n",
      "Iteration [3425] | loss: 0.0009700122172944248\n",
      "Iteration [3426] | loss: 0.0009702504030428827\n",
      "Iteration [3427] | loss: 0.0009706076816655695\n",
      "Iteration [3428] | loss: 0.0009708458674140275\n",
      "Iteration [3429] | loss: 0.0009712031460367143\n",
      "Iteration [3430] | loss: 0.0009714413317851722\n",
      "Iteration [3431] | loss: 0.0009715604246594012\n",
      "Iteration [3432] | loss: 0.0009717986104078591\n",
      "Iteration [3433] | loss: 0.0009722749819047749\n",
      "Iteration [3434] | loss: 0.0009725131676532328\n",
      "Iteration [3435] | loss: 0.0009727513534016907\n",
      "Iteration [3436] | loss: 0.0009729895391501486\n",
      "Iteration [3437] | loss: 0.0009733468177728355\n",
      "Iteration [3438] | loss: 0.0009737040963955224\n",
      "Iteration [3439] | loss: 0.0009739422821439803\n",
      "Iteration [3440] | loss: 0.0009741804678924382\n",
      "Iteration [3441] | loss: 0.000974537746515125\n",
      "Iteration [3442] | loss: 0.000974656839389354\n",
      "Iteration [3443] | loss: 0.0009748950251378119\n",
      "Iteration [3444] | loss: 0.0009752523037604988\n",
      "Iteration [3445] | loss: 0.0009756095823831856\n",
      "Iteration [3446] | loss: 0.0009758477681316435\n",
      "Iteration [3447] | loss: 0.0009760859538801014\n",
      "Iteration [3448] | loss: 0.0009763241396285594\n",
      "Iteration [3449] | loss: 0.0009766814764589071\n",
      "Iteration [3450] | loss: 0.000977038755081594\n",
      "Iteration [3451] | loss: 0.000977276940830052\n",
      "Iteration [3452] | loss: 0.0009775151265785098\n",
      "Iteration [3453] | loss: 0.0009778724052011967\n",
      "Iteration [3454] | loss: 0.0009779914980754256\n",
      "Iteration [3455] | loss: 0.0009782296838238835\n",
      "Iteration [3456] | loss: 0.0009785869624465704\n",
      "Iteration [3457] | loss: 0.0009789442410692573\n",
      "Iteration [3458] | loss: 0.0009791824268177152\n",
      "Iteration [3459] | loss: 0.000979420612566173\n",
      "Iteration [3460] | loss: 0.00097977789118886\n",
      "Iteration [3461] | loss: 0.0009800160769373178\n",
      "Iteration [3462] | loss: 0.0009803733555600047\n",
      "Iteration [3463] | loss: 0.0009806115413084626\n",
      "Iteration [3464] | loss: 0.0009809688199311495\n",
      "Iteration [3465] | loss: 0.0009812070056796074\n",
      "Iteration [3466] | loss: 0.0009814451914280653\n",
      "Iteration [3467] | loss: 0.0009815642843022943\n",
      "Iteration [3468] | loss: 0.00098204065579921\n",
      "Iteration [3469] | loss: 0.000982278841547668\n",
      "Iteration [3470] | loss: 0.0009825170272961259\n",
      "Iteration [3471] | loss: 0.0009828743059188128\n",
      "Iteration [3472] | loss: 0.0009831124916672707\n",
      "Iteration [3473] | loss: 0.0009833505610004067\n",
      "Iteration [3474] | loss: 0.0009837078396230936\n",
      "Iteration [3475] | loss: 0.0009840651182457805\n",
      "Iteration [3476] | loss: 0.0009843033039942384\n",
      "Iteration [3477] | loss: 0.0009845414897426963\n",
      "Iteration [3478] | loss: 0.0009847796754911542\n",
      "Iteration [3479] | loss: 0.000985136954113841\n",
      "Iteration [3480] | loss: 0.000985494232736528\n",
      "Iteration [3481] | loss: 0.0009856133256107569\n",
      "Iteration [3482] | loss: 0.0009859706042334437\n",
      "Iteration [3483] | loss: 0.0009862087899819016\n",
      "Iteration [3484] | loss: 0.0009864469757303596\n",
      "Iteration [3485] | loss: 0.0009868042543530464\n",
      "Iteration [3486] | loss: 0.0009871615329757333\n",
      "Iteration [3487] | loss: 0.0009873997187241912\n",
      "Iteration [3488] | loss: 0.000987637904472649\n",
      "Iteration [3489] | loss: 0.000987995183095336\n",
      "Iteration [3490] | loss: 0.0009882333688437939\n",
      "Iteration [3491] | loss: 0.0009885906474664807\n",
      "Iteration [3492] | loss: 0.0009888288332149386\n",
      "Iteration [3493] | loss: 0.0009891861118376255\n",
      "Iteration [3494] | loss: 0.0009894242975860834\n",
      "Iteration [3495] | loss: 0.0009896624833345413\n",
      "Iteration [3496] | loss: 0.0009900197619572282\n",
      "Iteration [3497] | loss: 0.000990257947705686\n",
      "Iteration [3498] | loss: 0.000990496133454144\n",
      "Iteration [3499] | loss: 0.000990734319202602\n",
      "Iteration [3500] | loss: 0.0009910915978252888\n",
      "Iteration [3501] | loss: 0.0009913297835737467\n",
      "Iteration [3502] | loss: 0.0009915679693222046\n",
      "Iteration [3503] | loss: 0.0009920443408191204\n",
      "Iteration [3504] | loss: 0.0009922825265675783\n",
      "Iteration [3505] | loss: 0.0009925207123160362\n",
      "Iteration [3506] | loss: 0.000992877990938723\n",
      "Iteration [3507] | loss: 0.000993116176687181\n",
      "Iteration [3508] | loss: 0.0009934734553098679\n",
      "Iteration [3509] | loss: 0.000993711524643004\n",
      "Iteration [3510] | loss: 0.0009940688032656908\n",
      "Iteration [3511] | loss: 0.0009943069890141487\n",
      "Iteration [3512] | loss: 0.0009945451747626066\n",
      "Iteration [3513] | loss: 0.0009949024533852935\n",
      "Iteration [3514] | loss: 0.0009952597320079803\n",
      "Iteration [3515] | loss: 0.0009954979177564383\n",
      "Iteration [3516] | loss: 0.0009957361035048962\n",
      "Iteration [3517] | loss: 0.000995974289253354\n",
      "Iteration [3518] | loss: 0.000996212475001812\n",
      "Iteration [3519] | loss: 0.0009965697536244988\n",
      "Iteration [3520] | loss: 0.0009969270322471857\n",
      "Iteration [3521] | loss: 0.0009971652179956436\n",
      "Iteration [3522] | loss: 0.0009974034037441015\n",
      "Iteration [3523] | loss: 0.0009977606823667884\n",
      "Iteration [3524] | loss: 0.0009979988681152463\n",
      "Iteration [3525] | loss: 0.0009982370538637042\n",
      "Iteration [3526] | loss: 0.00099871342536062\n",
      "Iteration [3527] | loss: 0.000998951611109078\n",
      "Iteration [3528] | loss: 0.0009991897968575358\n",
      "Iteration [3529] | loss: 0.0009995469590649009\n",
      "Iteration [3530] | loss: 0.0009997851448133588\n",
      "Iteration [3531] | loss: 0.0010001424234360456\n",
      "Iteration [3532] | loss: 0.0010004997020587325\n",
      "Iteration [3533] | loss: 0.0010007378878071904\n",
      "Iteration [3534] | loss: 0.0010009760735556483\n",
      "Iteration [3535] | loss: 0.0010013333521783352\n",
      "Iteration [3536] | loss: 0.001001571537926793\n",
      "Iteration [3537] | loss: 0.00100192881654948\n",
      "Iteration [3538] | loss: 0.0010022860951721668\n",
      "Iteration [3539] | loss: 0.0010025242809206247\n",
      "Iteration [3540] | loss: 0.0010027624666690826\n",
      "Iteration [3541] | loss: 0.0010030006524175406\n",
      "Iteration [3542] | loss: 0.0010032388381659985\n",
      "Iteration [3543] | loss: 0.0010035961167886853\n",
      "Iteration [3544] | loss: 0.0010039533954113722\n",
      "Iteration [3545] | loss: 0.00100419158115983\n",
      "Iteration [3546] | loss: 0.0010044296504929662\n",
      "Iteration [3547] | loss: 0.001004786929115653\n",
      "Iteration [3548] | loss: 0.00100514420773834\n",
      "Iteration [3549] | loss: 0.0010053823934867978\n",
      "Iteration [3550] | loss: 0.0010057396721094847\n",
      "Iteration [3551] | loss: 0.0010059778578579426\n",
      "Iteration [3552] | loss: 0.0010062160436064005\n",
      "Iteration [3553] | loss: 0.0010065733222290874\n",
      "Iteration [3554] | loss: 0.0010069306008517742\n",
      "Iteration [3555] | loss: 0.0010071687866002321\n",
      "Iteration [3556] | loss: 0.001007526065222919\n",
      "Iteration [3557] | loss: 0.001007764250971377\n",
      "Iteration [3558] | loss: 0.0010080024367198348\n",
      "Iteration [3559] | loss: 0.0010083595989271998\n",
      "Iteration [3560] | loss: 0.0010087168775498867\n",
      "Iteration [3561] | loss: 0.0010089550632983446\n",
      "Iteration [3562] | loss: 0.0010093123419210315\n",
      "Iteration [3563] | loss: 0.0010095505276694894\n",
      "Iteration [3564] | loss: 0.0010097887134179473\n",
      "Iteration [3565] | loss: 0.0010102650849148631\n",
      "Iteration [3566] | loss: 0.001010503270663321\n",
      "Iteration [3567] | loss: 0.001010741456411779\n",
      "Iteration [3568] | loss: 0.0010110987350344658\n",
      "Iteration [3569] | loss: 0.0010113369207829237\n",
      "Iteration [3570] | loss: 0.0010116941994056106\n",
      "Iteration [3571] | loss: 0.0010120513616129756\n",
      "Iteration [3572] | loss: 0.0010122895473614335\n",
      "Iteration [3573] | loss: 0.0010126468259841204\n",
      "Iteration [3574] | loss: 0.0010128850117325783\n",
      "Iteration [3575] | loss: 0.0010131231974810362\n",
      "Iteration [3576] | loss: 0.001013599568977952\n",
      "Iteration [3577] | loss: 0.00101383775472641\n",
      "Iteration [3578] | loss: 0.0010140759404748678\n",
      "Iteration [3579] | loss: 0.0010144332190975547\n",
      "Iteration [3580] | loss: 0.0010146714048460126\n",
      "Iteration [3581] | loss: 0.0010150285670533776\n",
      "Iteration [3582] | loss: 0.0010153858456760645\n",
      "Iteration [3583] | loss: 0.0010156240314245224\n",
      "Iteration [3584] | loss: 0.0010159813100472093\n",
      "Iteration [3585] | loss: 0.0010162194957956672\n",
      "Iteration [3586] | loss: 0.001016457681544125\n",
      "Iteration [3587] | loss: 0.001016934053041041\n",
      "Iteration [3588] | loss: 0.0010171722387894988\n",
      "Iteration [3589] | loss: 0.0010174104245379567\n",
      "Iteration [3590] | loss: 0.0010177677031606436\n",
      "Iteration [3591] | loss: 0.0010180057724937797\n",
      "Iteration [3592] | loss: 0.0010183630511164665\n",
      "Iteration [3593] | loss: 0.0010187203297391534\n",
      "Iteration [3594] | loss: 0.0010189585154876113\n",
      "Iteration [3595] | loss: 0.0010193157941102982\n",
      "Iteration [3596] | loss: 0.001019553979858756\n",
      "Iteration [3597] | loss: 0.001019792165607214\n",
      "Iteration [3598] | loss: 0.0010201494442299008\n",
      "Iteration [3599] | loss: 0.0010205067228525877\n",
      "Iteration [3600] | loss: 0.0010208638850599527\n",
      "Iteration [3601] | loss: 0.0010211020708084106\n",
      "Iteration [3602] | loss: 0.0010213402565568686\n",
      "Iteration [3603] | loss: 0.0010216975351795554\n",
      "Iteration [3604] | loss: 0.0010220548138022423\n",
      "Iteration [3605] | loss: 0.0010224120924249291\n",
      "Iteration [3606] | loss: 0.001022650278173387\n",
      "Iteration [3607] | loss: 0.001022888463921845\n",
      "Iteration [3608] | loss: 0.00102324562612921\n",
      "Iteration [3609] | loss: 0.001023483811877668\n",
      "Iteration [3610] | loss: 0.0010238410905003548\n",
      "Iteration [3611] | loss: 0.0010241983691230416\n",
      "Iteration [3612] | loss: 0.0010244365548714995\n",
      "Iteration [3613] | loss: 0.0010247938334941864\n",
      "Iteration [3614] | loss: 0.0010250320192426443\n",
      "Iteration [3615] | loss: 0.0010253892978653312\n",
      "Iteration [3616] | loss: 0.0010257464600726962\n",
      "Iteration [3617] | loss: 0.0010259846458211541\n",
      "Iteration [3618] | loss: 0.001026341924443841\n",
      "Iteration [3619] | loss: 0.0010265801101922989\n",
      "Iteration [3620] | loss: 0.0010269373888149858\n",
      "Iteration [3621] | loss: 0.0010272946674376726\n",
      "Iteration [3622] | loss: 0.0010275328531861305\n",
      "Iteration [3623] | loss: 0.0010278901318088174\n",
      "Iteration [3624] | loss: 0.0010281282011419535\n",
      "Iteration [3625] | loss: 0.0010283663868904114\n",
      "Iteration [3626] | loss: 0.0010288427583873272\n",
      "Iteration [3627] | loss: 0.001029080944135785\n",
      "Iteration [3628] | loss: 0.001029438222758472\n",
      "Iteration [3629] | loss: 0.0010296764085069299\n",
      "Iteration [3630] | loss: 0.0010299145942553878\n",
      "Iteration [3631] | loss: 0.0010303908493369818\n",
      "Iteration [3632] | loss: 0.0010306290350854397\n",
      "Iteration [3633] | loss: 0.0010309863137081265\n",
      "Iteration [3634] | loss: 0.0010312244994565845\n",
      "Iteration [3635] | loss: 0.0010314626852050424\n",
      "Iteration [3636] | loss: 0.0010318199638277292\n",
      "Iteration [3637] | loss: 0.001032177242450416\n",
      "Iteration [3638] | loss: 0.0010325344046577811\n",
      "Iteration [3639] | loss: 0.001032772590406239\n",
      "Iteration [3640] | loss: 0.001033129869028926\n",
      "Iteration [3641] | loss: 0.0010333680547773838\n",
      "Iteration [3642] | loss: 0.0010337253334000707\n",
      "Iteration [3643] | loss: 0.0010340826120227575\n",
      "Iteration [3644] | loss: 0.0010343207977712154\n",
      "Iteration [3645] | loss: 0.0010346779599785805\n",
      "Iteration [3646] | loss: 0.0010349161457270384\n",
      "Iteration [3647] | loss: 0.0010351543314754963\n",
      "Iteration [3648] | loss: 0.0010356307029724121\n",
      "Iteration [3649] | loss: 0.00103586888872087\n",
      "Iteration [3650] | loss: 0.0010362261673435569\n",
      "Iteration [3651] | loss: 0.001036583329550922\n",
      "Iteration [3652] | loss: 0.0010369406081736088\n",
      "Iteration [3653] | loss: 0.0010372978867962956\n",
      "Iteration [3654] | loss: 0.0010375360725447536\n",
      "Iteration [3655] | loss: 0.0010378933511674404\n",
      "Iteration [3656] | loss: 0.0010381315369158983\n",
      "Iteration [3657] | loss: 0.0010384886991232634\n",
      "Iteration [3658] | loss: 0.0010388459777459502\n",
      "Iteration [3659] | loss: 0.001039203256368637\n",
      "Iteration [3660] | loss: 0.001039441442117095\n",
      "Iteration [3661] | loss: 0.0010397987207397819\n",
      "Iteration [3662] | loss: 0.0010400369064882398\n",
      "Iteration [3663] | loss: 0.0010402749758213758\n",
      "Iteration [3664] | loss: 0.0010407513473182917\n",
      "Iteration [3665] | loss: 0.0010409895330667496\n",
      "Iteration [3666] | loss: 0.0010413468116894364\n",
      "Iteration [3667] | loss: 0.0010415849974378943\n",
      "Iteration [3668] | loss: 0.0010419422760605812\n",
      "Iteration [3669] | loss: 0.0010422994382679462\n",
      "Iteration [3670] | loss: 0.0010425376240164042\n",
      "Iteration [3671] | loss: 0.001042894902639091\n",
      "Iteration [3672] | loss: 0.001043133088387549\n",
      "Iteration [3673] | loss: 0.0010434903670102358\n",
      "Iteration [3674] | loss: 0.0010438475292176008\n",
      "Iteration [3675] | loss: 0.0010442048078402877\n",
      "Iteration [3676] | loss: 0.0010445620864629745\n",
      "Iteration [3677] | loss: 0.0010449193650856614\n",
      "Iteration [3678] | loss: 0.0010451575508341193\n",
      "Iteration [3679] | loss: 0.0010455148294568062\n",
      "Iteration [3680] | loss: 0.0010458719916641712\n",
      "Iteration [3681] | loss: 0.0010461101774126291\n",
      "Iteration [3682] | loss: 0.001046467456035316\n",
      "Iteration [3683] | loss: 0.001046705641783774\n",
      "Iteration [3684] | loss: 0.0010470629204064608\n",
      "Iteration [3685] | loss: 0.0010474200826138258\n",
      "Iteration [3686] | loss: 0.0010477773612365127\n",
      "Iteration [3687] | loss: 0.0010480155469849706\n",
      "Iteration [3688] | loss: 0.0010483728256076574\n",
      "Iteration [3689] | loss: 0.0010486110113561153\n",
      "Iteration [3690] | loss: 0.0010490872664377093\n",
      "Iteration [3691] | loss: 0.0010493254521861672\n",
      "Iteration [3692] | loss: 0.001049682730808854\n",
      "Iteration [3693] | loss: 0.001049920916557312\n",
      "Iteration [3694] | loss: 0.0010502781951799989\n",
      "Iteration [3695] | loss: 0.001050635357387364\n",
      "Iteration [3696] | loss: 0.0010509926360100508\n",
      "Iteration [3697] | loss: 0.0010513499146327376\n",
      "Iteration [3698] | loss: 0.0010515881003811955\n",
      "Iteration [3699] | loss: 0.0010519453790038824\n",
      "Iteration [3700] | loss: 0.0010521834483370185\n",
      "Iteration [3701] | loss: 0.0010526598198339343\n",
      "Iteration [3702] | loss: 0.0010528980055823922\n",
      "Iteration [3703] | loss: 0.001053255284205079\n",
      "Iteration [3704] | loss: 0.001053493469953537\n",
      "Iteration [3705] | loss: 0.001053850632160902\n",
      "Iteration [3706] | loss: 0.0010542079107835889\n",
      "Iteration [3707] | loss: 0.0010545651894062757\n",
      "Iteration [3708] | loss: 0.0010548033751547337\n",
      "Iteration [3709] | loss: 0.0010552796302363276\n",
      "Iteration [3710] | loss: 0.0010555178159847856\n",
      "Iteration [3711] | loss: 0.0010559941874817014\n",
      "Iteration [3712] | loss: 0.0010562323732301593\n",
      "Iteration [3713] | loss: 0.0010565895354375243\n",
      "Iteration [3714] | loss: 0.0010568277211859822\n",
      "Iteration [3715] | loss: 0.0010570659069344401\n",
      "Iteration [3716] | loss: 0.001057423185557127\n",
      "Iteration [3717] | loss: 0.0010577804641798139\n",
      "Iteration [3718] | loss: 0.001058137626387179\n",
      "Iteration [3719] | loss: 0.0010583758121356368\n",
      "Iteration [3720] | loss: 0.0010587330907583237\n",
      "Iteration [3721] | loss: 0.0010589712765067816\n",
      "Iteration [3722] | loss: 0.0010595666244626045\n",
      "Iteration [3723] | loss: 0.0010598048102110624\n",
      "Iteration [3724] | loss: 0.0010601620888337493\n",
      "Iteration [3725] | loss: 0.0010604002745822072\n",
      "Iteration [3726] | loss: 0.001060757553204894\n",
      "Iteration [3727] | loss: 0.001061114715412259\n",
      "Iteration [3728] | loss: 0.001061471994034946\n",
      "Iteration [3729] | loss: 0.0010617101797834039\n",
      "Iteration [3730] | loss: 0.0010620674584060907\n",
      "Iteration [3731] | loss: 0.0010623055277392268\n",
      "Iteration [3732] | loss: 0.0010627818992361426\n",
      "Iteration [3733] | loss: 0.0010630200849846005\n",
      "Iteration [3734] | loss: 0.0010634964564815164\n",
      "Iteration [3735] | loss: 0.0010637345258146524\n",
      "Iteration [3736] | loss: 0.0010640918044373393\n",
      "Iteration [3737] | loss: 0.0010644490830600262\n",
      "Iteration [3738] | loss: 0.001064806361682713\n",
      "Iteration [3739] | loss: 0.0010650444310158491\n",
      "Iteration [3740] | loss: 0.001065401709638536\n",
      "Iteration [3741] | loss: 0.0010657589882612228\n",
      "Iteration [3742] | loss: 0.0010661162668839097\n",
      "Iteration [3743] | loss: 0.0010664734290912747\n",
      "Iteration [3744] | loss: 0.0010667116148397326\n",
      "Iteration [3745] | loss: 0.0010671879863366485\n",
      "Iteration [3746] | loss: 0.0010674261720851064\n",
      "Iteration [3747] | loss: 0.0010677833342924714\n",
      "Iteration [3748] | loss: 0.0010681406129151583\n",
      "Iteration [3749] | loss: 0.0010684978915378451\n",
      "Iteration [3750] | loss: 0.001068736077286303\n",
      "Iteration [3751] | loss: 0.001069093239493668\n",
      "Iteration [3752] | loss: 0.001069331425242126\n",
      "Iteration [3753] | loss: 0.0010698077967390418\n",
      "Iteration [3754] | loss: 0.0010700459824874997\n",
      "Iteration [3755] | loss: 0.0010705222375690937\n",
      "Iteration [3756] | loss: 0.0010707604233175516\n",
      "Iteration [3757] | loss: 0.0010711177019402385\n",
      "Iteration [3758] | loss: 0.0010714748641476035\n",
      "Iteration [3759] | loss: 0.0010718321427702904\n",
      "Iteration [3760] | loss: 0.0010720703285187483\n",
      "Iteration [3761] | loss: 0.0010724276071414351\n",
      "Iteration [3762] | loss: 0.0010727847693488002\n",
      "Iteration [3763] | loss: 0.001073142047971487\n",
      "Iteration [3764] | loss: 0.0010736184194684029\n",
      "Iteration [3765] | loss: 0.001073856488801539\n",
      "Iteration [3766] | loss: 0.0010742137674242258\n",
      "Iteration [3767] | loss: 0.0010744519531726837\n",
      "Iteration [3768] | loss: 0.0010749283246695995\n",
      "Iteration [3769] | loss: 0.0010751663940027356\n",
      "Iteration [3770] | loss: 0.0010755236726254225\n",
      "Iteration [3771] | loss: 0.0010757618583738804\n",
      "Iteration [3772] | loss: 0.0010761191369965672\n",
      "Iteration [3773] | loss: 0.0010767144849523902\n",
      "Iteration [3774] | loss: 0.001076952670700848\n",
      "Iteration [3775] | loss: 0.0010773098329082131\n",
      "Iteration [3776] | loss: 0.001077548018656671\n",
      "Iteration [3777] | loss: 0.001077905297279358\n",
      "Iteration [3778] | loss: 0.0010782625759020448\n",
      "Iteration [3779] | loss: 0.0010786197381094098\n",
      "Iteration [3780] | loss: 0.0010788579238578677\n",
      "Iteration [3781] | loss: 0.0010792152024805546\n",
      "Iteration [3782] | loss: 0.0010795724811032414\n",
      "Iteration [3783] | loss: 0.0010800487361848354\n",
      "Iteration [3784] | loss: 0.0010804060148075223\n",
      "Iteration [3785] | loss: 0.0010806442005559802\n",
      "Iteration [3786] | loss: 0.0010810013627633452\n",
      "Iteration [3787] | loss: 0.0010812395485118032\n",
      "Iteration [3788] | loss: 0.001081715920008719\n",
      "Iteration [3789] | loss: 0.001081953989341855\n",
      "Iteration [3790] | loss: 0.0010824303608387709\n",
      "Iteration [3791] | loss: 0.0010827876394614577\n",
      "Iteration [3792] | loss: 0.0010830257087945938\n",
      "Iteration [3793] | loss: 0.0010835020802915096\n",
      "Iteration [3794] | loss: 0.0010837402660399675\n",
      "Iteration [3795] | loss: 0.0010840974282473326\n",
      "Iteration [3796] | loss: 0.0010843356139957905\n",
      "Iteration [3797] | loss: 0.0010848119854927063\n",
      "Iteration [3798] | loss: 0.0010852882405743003\n",
      "Iteration [3799] | loss: 0.0010855264263227582\n",
      "Iteration [3800] | loss: 0.001085883704945445\n",
      "Iteration [3801] | loss: 0.001086121890693903\n",
      "Iteration [3802] | loss: 0.001086479052901268\n",
      "Iteration [3803] | loss: 0.0010868363315239549\n",
      "Iteration [3804] | loss: 0.0010871936101466417\n",
      "Iteration [3805] | loss: 0.0010876698652282357\n",
      "Iteration [3806] | loss: 0.0010879080509766936\n",
      "Iteration [3807] | loss: 0.0010882653295993805\n",
      "Iteration [3808] | loss: 0.0010886224918067455\n",
      "Iteration [3809] | loss: 0.0010889797704294324\n",
      "Iteration [3810] | loss: 0.0010892179561778903\n",
      "Iteration [3811] | loss: 0.0010895751183852553\n",
      "Iteration [3812] | loss: 0.0010900514898821712\n",
      "Iteration [3813] | loss: 0.0010904086520895362\n",
      "Iteration [3814] | loss: 0.001090765930712223\n",
      "Iteration [3815] | loss: 0.001091004116460681\n",
      "Iteration [3816] | loss: 0.0010913613950833678\n",
      "Iteration [3817] | loss: 0.0010917185572907329\n",
      "Iteration [3818] | loss: 0.0010920758359134197\n",
      "Iteration [3819] | loss: 0.0010925520909950137\n",
      "Iteration [3820] | loss: 0.0010927902767434716\n",
      "Iteration [3821] | loss: 0.0010931475553661585\n",
      "Iteration [3822] | loss: 0.0010935047175735235\n",
      "Iteration [3823] | loss: 0.0010938619961962104\n",
      "Iteration [3824] | loss: 0.0010942192748188972\n",
      "Iteration [3825] | loss: 0.0010945764370262623\n",
      "Iteration [3826] | loss: 0.0010949337156489491\n",
      "Iteration [3827] | loss: 0.001095290994271636\n",
      "Iteration [3828] | loss: 0.001095648156479001\n",
      "Iteration [3829] | loss: 0.001096005435101688\n",
      "Iteration [3830] | loss: 0.0010962436208501458\n",
      "Iteration [3831] | loss: 0.0010966007830575109\n",
      "Iteration [3832] | loss: 0.0010970771545544267\n",
      "Iteration [3833] | loss: 0.0010974343167617917\n",
      "Iteration [3834] | loss: 0.0010977915953844786\n",
      "Iteration [3835] | loss: 0.0010981488740071654\n",
      "Iteration [3836] | loss: 0.0010983869433403015\n",
      "Iteration [3837] | loss: 0.0010987442219629884\n",
      "Iteration [3838] | loss: 0.0010992205934599042\n",
      "Iteration [3839] | loss: 0.0010995777556672692\n",
      "Iteration [3840] | loss: 0.001099935034289956\n",
      "Iteration [3841] | loss: 0.001100173220038414\n",
      "Iteration [3842] | loss: 0.001100530382245779\n",
      "Iteration [3843] | loss: 0.001100887660868466\n",
      "Iteration [3844] | loss: 0.0011013639159500599\n",
      "Iteration [3845] | loss: 0.0011017211945727468\n",
      "Iteration [3846] | loss: 0.0011019593803212047\n",
      "Iteration [3847] | loss: 0.0011023165425285697\n",
      "Iteration [3848] | loss: 0.0011027929140254855\n",
      "Iteration [3849] | loss: 0.0011030309833586216\n",
      "Iteration [3850] | loss: 0.0011035073548555374\n",
      "Iteration [3851] | loss: 0.0011037455406039953\n",
      "Iteration [3852] | loss: 0.0011041027028113604\n",
      "Iteration [3853] | loss: 0.0011045790743082762\n",
      "Iteration [3854] | loss: 0.001104817260056734\n",
      "Iteration [3855] | loss: 0.0011051744222640991\n",
      "Iteration [3856] | loss: 0.001105650793761015\n",
      "Iteration [3857] | loss: 0.001105888863094151\n",
      "Iteration [3858] | loss: 0.0011063652345910668\n",
      "Iteration [3859] | loss: 0.0011067223967984319\n",
      "Iteration [3860] | loss: 0.0011069605825468898\n",
      "Iteration [3861] | loss: 0.0011074369540438056\n",
      "Iteration [3862] | loss: 0.0011076750233769417\n",
      "Iteration [3863] | loss: 0.0011081513948738575\n",
      "Iteration [3864] | loss: 0.0011085085570812225\n",
      "Iteration [3865] | loss: 0.0011087467428296804\n",
      "Iteration [3866] | loss: 0.0011091040214523673\n",
      "Iteration [3867] | loss: 0.0011096993694081903\n",
      "Iteration [3868] | loss: 0.0011099375551566482\n",
      "Iteration [3869] | loss: 0.0011102947173640132\n",
      "Iteration [3870] | loss: 0.0011106519959867\n",
      "Iteration [3871] | loss: 0.001110890181735158\n",
      "Iteration [3872] | loss: 0.001111485529690981\n",
      "Iteration [3873] | loss: 0.0011118428083136678\n",
      "Iteration [3874] | loss: 0.0011120808776468039\n",
      "Iteration [3875] | loss: 0.0011124381562694907\n",
      "Iteration [3876] | loss: 0.0011127954348921776\n",
      "Iteration [3877] | loss: 0.0011132716899737716\n",
      "Iteration [3878] | loss: 0.0011136289685964584\n",
      "Iteration [3879] | loss: 0.0011139861308038235\n",
      "Iteration [3880] | loss: 0.0011142243165522814\n",
      "Iteration [3881] | loss: 0.0011145814787596464\n",
      "Iteration [3882] | loss: 0.0011151769431307912\n",
      "Iteration [3883] | loss: 0.0011154150124639273\n",
      "Iteration [3884] | loss: 0.0011157722910866141\n",
      "Iteration [3885] | loss: 0.001116129569709301\n",
      "Iteration [3886] | loss: 0.001116367639042437\n",
      "Iteration [3887] | loss: 0.0011169631034135818\n",
      "Iteration [3888] | loss: 0.0011173202656209469\n",
      "Iteration [3889] | loss: 0.0011175584513694048\n",
      "Iteration [3890] | loss: 0.0011179156135767698\n",
      "Iteration [3891] | loss: 0.0011183919850736856\n",
      "Iteration [3892] | loss: 0.0011187491472810507\n",
      "Iteration [3893] | loss: 0.0011191064259037375\n",
      "Iteration [3894] | loss: 0.0011194637045264244\n",
      "Iteration [3895] | loss: 0.0011197017738595605\n",
      "Iteration [3896] | loss: 0.0011201781453564763\n",
      "Iteration [3897] | loss: 0.0011206544004380703\n",
      "Iteration [3898] | loss: 0.0011208925861865282\n",
      "Iteration [3899] | loss: 0.0011212497483938932\n",
      "Iteration [3900] | loss: 0.00112160702701658\n",
      "Iteration [3901] | loss: 0.001122083282098174\n",
      "Iteration [3902] | loss: 0.001122440560720861\n",
      "Iteration [3903] | loss: 0.001122797722928226\n",
      "Iteration [3904] | loss: 0.0011231550015509129\n",
      "Iteration [3905] | loss: 0.0011233931872993708\n",
      "Iteration [3906] | loss: 0.0011239885352551937\n",
      "Iteration [3907] | loss: 0.0011243456974625587\n",
      "Iteration [3908] | loss: 0.0011245838832110167\n",
      "Iteration [3909] | loss: 0.0011249411618337035\n",
      "Iteration [3910] | loss: 0.0011252983240410686\n",
      "Iteration [3911] | loss: 0.0011258936719968915\n",
      "Iteration [3912] | loss: 0.0011261318577453494\n",
      "Iteration [3913] | loss: 0.0011264891363680363\n",
      "Iteration [3914] | loss: 0.0011268462985754013\n",
      "Iteration [3915] | loss: 0.0011272035771980882\n",
      "Iteration [3916] | loss: 0.0011276798322796822\n",
      "Iteration [3917] | loss: 0.001128037110902369\n",
      "Iteration [3918] | loss: 0.001128275180235505\n",
      "Iteration [3919] | loss: 0.001128632458858192\n",
      "Iteration [3920] | loss: 0.001129227806814015\n",
      "Iteration [3921] | loss: 0.0011295850854367018\n",
      "Iteration [3922] | loss: 0.0011298231547698379\n",
      "Iteration [3923] | loss: 0.0011301804333925247\n",
      "Iteration [3924] | loss: 0.0011306566884741187\n",
      "Iteration [3925] | loss: 0.0011310139670968056\n",
      "Iteration [3926] | loss: 0.0011313711293041706\n",
      "Iteration [3927] | loss: 0.0011317284079268575\n",
      "Iteration [3928] | loss: 0.0011320855701342225\n",
      "Iteration [3929] | loss: 0.0011325619416311383\n",
      "Iteration [3930] | loss: 0.0011329191038385034\n",
      "Iteration [3931] | loss: 0.0011332763824611902\n",
      "Iteration [3932] | loss: 0.0011336335446685553\n",
      "Iteration [3933] | loss: 0.0011339908232912421\n",
      "Iteration [3934] | loss: 0.0011344670783728361\n",
      "Iteration [3935] | loss: 0.001134824356995523\n",
      "Iteration [3936] | loss: 0.001135062426328659\n",
      "Iteration [3937] | loss: 0.0011355387978255749\n",
      "Iteration [3938] | loss: 0.00113589596003294\n",
      "Iteration [3939] | loss: 0.0011363723315298557\n",
      "Iteration [3940] | loss: 0.0011366104008629918\n",
      "Iteration [3941] | loss: 0.0011370867723599076\n",
      "Iteration [3942] | loss: 0.0011374439345672727\n",
      "Iteration [3943] | loss: 0.0011379201896488667\n",
      "Iteration [3944] | loss: 0.0011381583753973246\n",
      "Iteration [3945] | loss: 0.0011386347468942404\n",
      "Iteration [3946] | loss: 0.0011389919091016054\n",
      "Iteration [3947] | loss: 0.0011393491877242923\n",
      "Iteration [3948] | loss: 0.0011397063499316573\n",
      "Iteration [3949] | loss: 0.0011401826050132513\n",
      "Iteration [3950] | loss: 0.0011405398836359382\n",
      "Iteration [3951] | loss: 0.0011408970458433032\n",
      "Iteration [3952] | loss: 0.0011411352315917611\n",
      "Iteration [3953] | loss: 0.001141730579547584\n",
      "Iteration [3954] | loss: 0.001142087858170271\n",
      "Iteration [3955] | loss: 0.001142445020377636\n",
      "Iteration [3956] | loss: 0.0011426832061260939\n",
      "Iteration [3957] | loss: 0.0011432785540819168\n",
      "Iteration [3958] | loss: 0.0011436357162892818\n",
      "Iteration [3959] | loss: 0.0011439929949119687\n",
      "Iteration [3960] | loss: 0.0011442311806604266\n",
      "Iteration [3961] | loss: 0.0011447074357420206\n",
      "Iteration [3962] | loss: 0.0011451836908236146\n",
      "Iteration [3963] | loss: 0.0011455409694463015\n",
      "Iteration [3964] | loss: 0.0011457790387794375\n",
      "Iteration [3965] | loss: 0.0011462554102763534\n",
      "Iteration [3966] | loss: 0.0011466125724837184\n",
      "Iteration [3967] | loss: 0.0011470888275653124\n",
      "Iteration [3968] | loss: 0.0011475651990622282\n",
      "Iteration [3969] | loss: 0.0011478032683953643\n",
      "Iteration [3970] | loss: 0.0011481605470180511\n",
      "Iteration [3971] | loss: 0.0011486368020996451\n",
      "Iteration [3972] | loss: 0.0011491130571812391\n",
      "Iteration [3973] | loss: 0.001149351242929697\n",
      "Iteration [3974] | loss: 0.001149708521552384\n",
      "Iteration [3975] | loss: 0.001150065683759749\n",
      "Iteration [3976] | loss: 0.0011506610317155719\n",
      "Iteration [3977] | loss: 0.0011510183103382587\n",
      "Iteration [3978] | loss: 0.0011512563796713948\n",
      "Iteration [3979] | loss: 0.0011517327511683106\n",
      "Iteration [3980] | loss: 0.0011522090062499046\n",
      "Iteration [3981] | loss: 0.0011525661684572697\n",
      "Iteration [3982] | loss: 0.0011529234470799565\n",
      "Iteration [3983] | loss: 0.0011532806092873216\n",
      "Iteration [3984] | loss: 0.0011536378879100084\n",
      "Iteration [3985] | loss: 0.0011541141429916024\n",
      "Iteration [3986] | loss: 0.0011545903980731964\n",
      "Iteration [3987] | loss: 0.0011548285838216543\n",
      "Iteration [3988] | loss: 0.0011551857460290194\n",
      "Iteration [3989] | loss: 0.0011555430246517062\n",
      "Iteration [3990] | loss: 0.0011561383726075292\n",
      "Iteration [3991] | loss: 0.0011564955348148942\n",
      "Iteration [3992] | loss: 0.001156733720563352\n",
      "Iteration [3993] | loss: 0.001157209975644946\n",
      "Iteration [3994] | loss: 0.001157686347141862\n",
      "Iteration [3995] | loss: 0.001158043509349227\n",
      "Iteration [3996] | loss: 0.001158400671556592\n",
      "Iteration [3997] | loss: 0.0011588770430535078\n",
      "Iteration [3998] | loss: 0.0011591151123866439\n",
      "Iteration [3999] | loss: 0.0011595914838835597\n",
      "Iteration [4000] | loss: 0.0011600677389651537\n",
      "Iteration [4001] | loss: 0.0011604249011725187\n",
      "Iteration [4002] | loss: 0.0011607821797952056\n",
      "Iteration [4003] | loss: 0.0011611393420025706\n",
      "Iteration [4004] | loss: 0.0011616155970841646\n",
      "Iteration [4005] | loss: 0.0011619728757068515\n",
      "Iteration [4006] | loss: 0.0011623300379142165\n",
      "Iteration [4007] | loss: 0.0011628062929958105\n",
      "Iteration [4008] | loss: 0.0011632826644927263\n",
      "Iteration [4009] | loss: 0.0011635207338258624\n",
      "Iteration [4010] | loss: 0.0011639971053227782\n",
      "Iteration [4011] | loss: 0.0011643542675301433\n",
      "Iteration [4012] | loss: 0.0011648305226117373\n",
      "Iteration [4013] | loss: 0.0011653067776933312\n",
      "Iteration [4014] | loss: 0.001165664056316018\n",
      "Iteration [4015] | loss: 0.0011659021256491542\n",
      "Iteration [4016] | loss: 0.001166259404271841\n",
      "Iteration [4017] | loss: 0.001166854752227664\n",
      "Iteration [4018] | loss: 0.001167211914435029\n",
      "Iteration [4019] | loss: 0.001167569193057716\n",
      "Iteration [4020] | loss: 0.0011680454481393099\n",
      "Iteration [4021] | loss: 0.001168402610346675\n",
      "Iteration [4022] | loss: 0.0011687598889693618\n",
      "Iteration [4023] | loss: 0.0011692361440509558\n",
      "Iteration [4024] | loss: 0.0011695933062583208\n",
      "Iteration [4025] | loss: 0.0011699505848810077\n",
      "Iteration [4026] | loss: 0.0011705459328368306\n",
      "Iteration [4027] | loss: 0.0011707840021699667\n",
      "Iteration [4028] | loss: 0.0011711412807926536\n",
      "Iteration [4029] | loss: 0.0011716175358742476\n",
      "Iteration [4030] | loss: 0.0011720937909558415\n",
      "Iteration [4031] | loss: 0.0011724510695785284\n",
      "Iteration [4032] | loss: 0.0011729273246601224\n",
      "Iteration [4033] | loss: 0.0011732844868674874\n",
      "Iteration [4034] | loss: 0.0011735226726159453\n",
      "Iteration [4035] | loss: 0.0011741180205717683\n",
      "Iteration [4036] | loss: 0.0011744751827791333\n",
      "Iteration [4037] | loss: 0.0011748324614018202\n",
      "Iteration [4038] | loss: 0.0011753087164834142\n",
      "Iteration [4039] | loss: 0.0011757849715650082\n",
      "Iteration [4040] | loss: 0.0011761421337723732\n",
      "Iteration [4041] | loss: 0.001176618505269289\n",
      "Iteration [4042] | loss: 0.001176856574602425\n",
      "Iteration [4043] | loss: 0.001177332829684019\n",
      "Iteration [4044] | loss: 0.001177809201180935\n",
      "Iteration [4045] | loss: 0.0011781663633883\n",
      "Iteration [4046] | loss: 0.001178523525595665\n",
      "Iteration [4047] | loss: 0.001178999780677259\n",
      "Iteration [4048] | loss: 0.0011794761521741748\n",
      "Iteration [4049] | loss: 0.0011798333143815398\n",
      "Iteration [4050] | loss: 0.0011801904765889049\n",
      "Iteration [4051] | loss: 0.0011805477552115917\n",
      "Iteration [4052] | loss: 0.0011810240102931857\n",
      "Iteration [4053] | loss: 0.0011815002653747797\n",
      "Iteration [4054] | loss: 0.0011818574275821447\n",
      "Iteration [4055] | loss: 0.0011822147062048316\n",
      "Iteration [4056] | loss: 0.0011826909612864256\n",
      "Iteration [4057] | loss: 0.0011831672163680196\n",
      "Iteration [4058] | loss: 0.0011835244949907064\n",
      "Iteration [4059] | loss: 0.0011838816571980715\n",
      "Iteration [4060] | loss: 0.0011842388194054365\n",
      "Iteration [4061] | loss: 0.0011847150744870305\n",
      "Iteration [4062] | loss: 0.0011851914459839463\n",
      "Iteration [4063] | loss: 0.0011855486081913114\n",
      "Iteration [4064] | loss: 0.0011859057703986764\n",
      "Iteration [4065] | loss: 0.0011865011183544993\n",
      "Iteration [4066] | loss: 0.0011868583969771862\n",
      "Iteration [4067] | loss: 0.0011873346520587802\n",
      "Iteration [4068] | loss: 0.0011876918142661452\n",
      "Iteration [4069] | loss: 0.0011879300000146031\n",
      "Iteration [4070] | loss: 0.001188525347970426\n",
      "Iteration [4071] | loss: 0.0011888825101777911\n",
      "Iteration [4072] | loss: 0.0011892396723851562\n",
      "Iteration [4073] | loss: 0.0011897159274667501\n",
      "Iteration [4074] | loss: 0.001190192298963666\n",
      "Iteration [4075] | loss: 0.001190549461171031\n",
      "Iteration [4076] | loss: 0.001191025716252625\n",
      "Iteration [4077] | loss: 0.00119138287845999\n",
      "Iteration [4078] | loss: 0.001191978226415813\n",
      "Iteration [4079] | loss: 0.0011923355050384998\n",
      "Iteration [4080] | loss: 0.0011926926672458649\n",
      "Iteration [4081] | loss: 0.00119304982945323\n",
      "Iteration [4082] | loss: 0.0011934071080759168\n",
      "Iteration [4083] | loss: 0.0011938833631575108\n",
      "Iteration [4084] | loss: 0.0011943596182391047\n",
      "Iteration [4085] | loss: 0.0011947167804464698\n",
      "Iteration [4086] | loss: 0.0011951930355280638\n",
      "Iteration [4087] | loss: 0.0011956692906096578\n",
      "Iteration [4088] | loss: 0.0011960265692323446\n",
      "Iteration [4089] | loss: 0.0011965028243139386\n",
      "Iteration [4090] | loss: 0.0011968599865213037\n",
      "Iteration [4091] | loss: 0.0011973362416028976\n",
      "Iteration [4092] | loss: 0.0011978124966844916\n",
      "Iteration [4093] | loss: 0.0011981697753071785\n",
      "Iteration [4094] | loss: 0.0011986460303887725\n",
      "Iteration [4095] | loss: 0.0011990031925961375\n",
      "Iteration [4096] | loss: 0.0011994794476777315\n",
      "Iteration [4097] | loss: 0.0011998366098850965\n",
      "Iteration [4098] | loss: 0.0012001938885077834\n",
      "Iteration [4099] | loss: 0.0012006701435893774\n",
      "Iteration [4100] | loss: 0.0012011463986709714\n",
      "Iteration [4101] | loss: 0.0012015035608783364\n",
      "Iteration [4102] | loss: 0.0012019798159599304\n",
      "Iteration [4103] | loss: 0.0012023370945826173\n",
      "Iteration [4104] | loss: 0.0012029323261231184\n",
      "Iteration [4105] | loss: 0.0012032896047458053\n",
      "Iteration [4106] | loss: 0.0012036467669531703\n",
      "Iteration [4107] | loss: 0.0012041230220347643\n",
      "Iteration [4108] | loss: 0.0012044801842421293\n",
      "Iteration [4109] | loss: 0.0012049565557390451\n",
      "Iteration [4110] | loss: 0.0012054328108206391\n",
      "Iteration [4111] | loss: 0.0012057899730280042\n",
      "Iteration [4112] | loss: 0.0012062662281095982\n",
      "Iteration [4113] | loss: 0.0012067424831911922\n",
      "Iteration [4114] | loss: 0.0012072187382727861\n",
      "Iteration [4115] | loss: 0.0012075759004801512\n",
      "Iteration [4116] | loss: 0.001207933179102838\n",
      "Iteration [4117] | loss: 0.0012085284106433392\n",
      "Iteration [4118] | loss: 0.001208885689266026\n",
      "Iteration [4119] | loss: 0.00120936194434762\n",
      "Iteration [4120] | loss: 0.001209719106554985\n",
      "Iteration [4121] | loss: 0.001210195361636579\n",
      "Iteration [4122] | loss: 0.001210671616718173\n",
      "Iteration [4123] | loss: 0.001211028778925538\n",
      "Iteration [4124] | loss: 0.0012115051504224539\n",
      "Iteration [4125] | loss: 0.001211862312629819\n",
      "Iteration [4126] | loss: 0.001212338567711413\n",
      "Iteration [4127] | loss: 0.001212814822793007\n",
      "Iteration [4128] | loss: 0.001213171985000372\n",
      "Iteration [4129] | loss: 0.001213648240081966\n",
      "Iteration [4130] | loss: 0.00121412449516356\n",
      "Iteration [4131] | loss: 0.001214600750245154\n",
      "Iteration [4132] | loss: 0.0012149580288678408\n",
      "Iteration [4133] | loss: 0.0012153151910752058\n",
      "Iteration [4134] | loss: 0.0012159105390310287\n",
      "Iteration [4135] | loss: 0.0012162677012383938\n",
      "Iteration [4136] | loss: 0.0012167439563199878\n",
      "Iteration [4137] | loss: 0.0012171011185273528\n",
      "Iteration [4138] | loss: 0.0012176964664831758\n",
      "Iteration [4139] | loss: 0.0012180536286905408\n",
      "Iteration [4140] | loss: 0.0012184107908979058\n",
      "Iteration [4141] | loss: 0.0012188870459794998\n",
      "Iteration [4142] | loss: 0.0012193633010610938\n",
      "Iteration [4143] | loss: 0.0012198395561426878\n",
      "Iteration [4144] | loss: 0.0012201968347653747\n",
      "Iteration [4145] | loss: 0.0012206730898469687\n",
      "Iteration [4146] | loss: 0.0012210302520543337\n",
      "Iteration [4147] | loss: 0.0012215065071359277\n",
      "Iteration [4148] | loss: 0.0012219827622175217\n",
      "Iteration [4149] | loss: 0.0012223399244248867\n",
      "Iteration [4150] | loss: 0.0012228161795064807\n",
      "Iteration [4151] | loss: 0.0012232924345880747\n",
      "Iteration [4152] | loss: 0.0012237686896696687\n",
      "Iteration [4153] | loss: 0.0012241258518770337\n",
      "Iteration [4154] | loss: 0.0012246021069586277\n",
      "Iteration [4155] | loss: 0.0012250783620402217\n",
      "Iteration [4156] | loss: 0.0012254356406629086\n",
      "Iteration [4157] | loss: 0.0012259118957445025\n",
      "Iteration [4158] | loss: 0.0012262690579518676\n",
      "Iteration [4159] | loss: 0.0012268644059076905\n",
      "Iteration [4160] | loss: 0.0012272215681150556\n",
      "Iteration [4161] | loss: 0.0012276978231966496\n",
      "Iteration [4162] | loss: 0.0012281740782782435\n",
      "Iteration [4163] | loss: 0.0012287693098187447\n",
      "Iteration [4164] | loss: 0.0012291265884414315\n",
      "Iteration [4165] | loss: 0.0012296028435230255\n",
      "Iteration [4166] | loss: 0.0012299600057303905\n",
      "Iteration [4167] | loss: 0.0012304362608119845\n",
      "Iteration [4168] | loss: 0.0012309125158935785\n",
      "Iteration [4169] | loss: 0.0012312696781009436\n",
      "Iteration [4170] | loss: 0.0012317459331825376\n",
      "Iteration [4171] | loss: 0.0012321030953899026\n",
      "Iteration [4172] | loss: 0.0012326984433457255\n",
      "Iteration [4173] | loss: 0.0012330556055530906\n",
      "Iteration [4174] | loss: 0.0012335318606346846\n",
      "Iteration [4175] | loss: 0.0012338890228420496\n",
      "Iteration [4176] | loss: 0.0012344843707978725\n",
      "Iteration [4177] | loss: 0.0012348415330052376\n",
      "Iteration [4178] | loss: 0.0012354368809610605\n",
      "Iteration [4179] | loss: 0.0012357940431684256\n",
      "Iteration [4180] | loss: 0.0012362702982500196\n",
      "Iteration [4181] | loss: 0.0012367465533316135\n",
      "Iteration [4182] | loss: 0.0012371037155389786\n",
      "Iteration [4183] | loss: 0.0012375799706205726\n",
      "Iteration [4184] | loss: 0.0012380562257021666\n",
      "Iteration [4185] | loss: 0.0012385324807837605\n",
      "Iteration [4186] | loss: 0.0012388896429911256\n",
      "Iteration [4187] | loss: 0.0012393658980727196\n",
      "Iteration [4188] | loss: 0.0012398421531543136\n",
      "Iteration [4189] | loss: 0.0012403184082359076\n",
      "Iteration [4190] | loss: 0.0012407946633175015\n",
      "Iteration [4191] | loss: 0.0012412709183990955\n",
      "Iteration [4192] | loss: 0.0012417471734806895\n",
      "Iteration [4193] | loss: 0.0012422234285622835\n",
      "Iteration [4194] | loss: 0.0012425805907696486\n",
      "Iteration [4195] | loss: 0.0012430568458512425\n",
      "Iteration [4196] | loss: 0.0012435331009328365\n",
      "Iteration [4197] | loss: 0.0012440093560144305\n",
      "Iteration [4198] | loss: 0.0012443665182217956\n",
      "Iteration [4199] | loss: 0.0012448427733033895\n",
      "Iteration [4200] | loss: 0.0012454380048438907\n",
      "Iteration [4201] | loss: 0.0012459142599254847\n",
      "Iteration [4202] | loss: 0.0012462714221328497\n",
      "Iteration [4203] | loss: 0.0012467476772144437\n",
      "Iteration [4204] | loss: 0.0012472239322960377\n",
      "Iteration [4205] | loss: 0.0012477001873776317\n",
      "Iteration [4206] | loss: 0.0012480573495849967\n",
      "Iteration [4207] | loss: 0.0012485336046665907\n",
      "Iteration [4208] | loss: 0.0012491289526224136\n",
      "Iteration [4209] | loss: 0.0012496052077040076\n",
      "Iteration [4210] | loss: 0.0012499623699113727\n",
      "Iteration [4211] | loss: 0.0012504386249929667\n",
      "Iteration [4212] | loss: 0.0012509147636592388\n",
      "Iteration [4213] | loss: 0.0012513910187408328\n",
      "Iteration [4214] | loss: 0.0012517482973635197\n",
      "Iteration [4215] | loss: 0.0012522244360297918\n",
      "Iteration [4216] | loss: 0.0012528197839856148\n",
      "Iteration [4217] | loss: 0.0012532960390672088\n",
      "Iteration [4218] | loss: 0.0012536532012745738\n",
      "Iteration [4219] | loss: 0.0012541294563561678\n",
      "Iteration [4220] | loss: 0.0012546057114377618\n",
      "Iteration [4221] | loss: 0.0012550819665193558\n",
      "Iteration [4222] | loss: 0.0012554391287267208\n",
      "Iteration [4223] | loss: 0.001256034360267222\n",
      "Iteration [4224] | loss: 0.001256510615348816\n",
      "Iteration [4225] | loss: 0.00125698687043041\n",
      "Iteration [4226] | loss: 0.001257344032637775\n",
      "Iteration [4227] | loss: 0.001257820287719369\n",
      "Iteration [4228] | loss: 0.0012584156356751919\n",
      "Iteration [4229] | loss: 0.001258891774341464\n",
      "Iteration [4230] | loss: 0.001259368029423058\n",
      "Iteration [4231] | loss: 0.001259725191630423\n",
      "Iteration [4232] | loss: 0.001260320539586246\n",
      "Iteration [4233] | loss: 0.001260677701793611\n",
      "Iteration [4234] | loss: 0.001261153956875205\n",
      "Iteration [4235] | loss: 0.001261630211956799\n",
      "Iteration [4236] | loss: 0.0012622254434973001\n",
      "Iteration [4237] | loss: 0.0012625826057046652\n",
      "Iteration [4238] | loss: 0.0012630588607862592\n",
      "Iteration [4239] | loss: 0.0012634160229936242\n",
      "Iteration [4240] | loss: 0.0012640113709494472\n",
      "Iteration [4241] | loss: 0.0012644876260310411\n",
      "Iteration [4242] | loss: 0.0012649638811126351\n",
      "Iteration [4243] | loss: 0.0012654400197789073\n",
      "Iteration [4244] | loss: 0.0012659162748605013\n",
      "Iteration [4245] | loss: 0.0012663925299420953\n",
      "Iteration [4246] | loss: 0.0012668687850236893\n",
      "Iteration [4247] | loss: 0.0012673450401052833\n",
      "Iteration [4248] | loss: 0.0012678211787715554\n",
      "Iteration [4249] | loss: 0.0012682974338531494\n",
      "Iteration [4250] | loss: 0.0012686545960605145\n",
      "Iteration [4251] | loss: 0.0012692499440163374\n",
      "Iteration [4252] | loss: 0.0012698451755568385\n",
      "Iteration [4253] | loss: 0.0012702024541795254\n",
      "Iteration [4254] | loss: 0.0012706785928457975\n",
      "Iteration [4255] | loss: 0.0012710357550531626\n",
      "Iteration [4256] | loss: 0.0012717501958832145\n",
      "Iteration [4257] | loss: 0.0012721073580905795\n",
      "Iteration [4258] | loss: 0.0012725834967568517\n",
      "Iteration [4259] | loss: 0.0012729407753795385\n",
      "Iteration [4260] | loss: 0.0012735360069200397\n",
      "Iteration [4261] | loss: 0.0012741313548758626\n",
      "Iteration [4262] | loss: 0.0012744885170832276\n",
      "Iteration [4263] | loss: 0.0012749646557494998\n",
      "Iteration [4264] | loss: 0.0012754409108310938\n",
      "Iteration [4265] | loss: 0.0012759171659126878\n",
      "Iteration [4266] | loss: 0.0012763934209942818\n",
      "Iteration [4267] | loss: 0.0012768696760758758\n",
      "Iteration [4268] | loss: 0.0012774649076163769\n",
      "Iteration [4269] | loss: 0.001277822069823742\n",
      "Iteration [4270] | loss: 0.0012784174177795649\n",
      "Iteration [4271] | loss: 0.00127877457998693\n",
      "Iteration [4272] | loss: 0.001279369811527431\n",
      "Iteration [4273] | loss: 0.001279726973734796\n",
      "Iteration [4274] | loss: 0.001280322321690619\n",
      "Iteration [4275] | loss: 0.0012807984603568912\n",
      "Iteration [4276] | loss: 0.0012812747154384851\n",
      "Iteration [4277] | loss: 0.0012817509705200791\n",
      "Iteration [4278] | loss: 0.0012822272256016731\n",
      "Iteration [4279] | loss: 0.0012827033642679453\n",
      "Iteration [4280] | loss: 0.0012832987122237682\n",
      "Iteration [4281] | loss: 0.0012836558744311333\n",
      "Iteration [4282] | loss: 0.0012842511059716344\n",
      "Iteration [4283] | loss: 0.0012847273610532284\n",
      "Iteration [4284] | loss: 0.0012852036161348224\n",
      "Iteration [4285] | loss: 0.0012856797548010945\n",
      "Iteration [4286] | loss: 0.0012861560098826885\n",
      "Iteration [4287] | loss: 0.0012867513578385115\n",
      "Iteration [4288] | loss: 0.0012871085200458765\n",
      "Iteration [4289] | loss: 0.0012875846587121487\n",
      "Iteration [4290] | loss: 0.0012881800066679716\n",
      "Iteration [4291] | loss: 0.0012886562617495656\n",
      "Iteration [4292] | loss: 0.0012891324004158378\n",
      "Iteration [4293] | loss: 0.0012894895626232028\n",
      "Iteration [4294] | loss: 0.0012900849105790257\n",
      "Iteration [4295] | loss: 0.0012906801421195269\n",
      "Iteration [4296] | loss: 0.001291037304326892\n",
      "Iteration [4297] | loss: 0.0012916326522827148\n",
      "Iteration [4298] | loss: 0.0012919898144900799\n",
      "Iteration [4299] | loss: 0.001292585046030581\n",
      "Iteration [4300] | loss: 0.001293061301112175\n",
      "Iteration [4301] | loss: 0.0012935374397784472\n",
      "Iteration [4302] | loss: 0.0012940136948600411\n",
      "Iteration [4303] | loss: 0.0012944899499416351\n",
      "Iteration [4304] | loss: 0.0012950851814821362\n",
      "Iteration [4305] | loss: 0.0012955614365637302\n",
      "Iteration [4306] | loss: 0.0012959185987710953\n",
      "Iteration [4307] | loss: 0.0012966329231858253\n",
      "Iteration [4308] | loss: 0.0012971091782674193\n",
      "Iteration [4309] | loss: 0.0012974663404747844\n",
      "Iteration [4310] | loss: 0.0012980615720152855\n",
      "Iteration [4311] | loss: 0.0012985378270968795\n",
      "Iteration [4312] | loss: 0.0012990139657631516\n",
      "Iteration [4313] | loss: 0.0012994902208447456\n",
      "Iteration [4314] | loss: 0.0013000854523852468\n",
      "Iteration [4315] | loss: 0.0013005617074668407\n",
      "Iteration [4316] | loss: 0.0013010379625484347\n",
      "Iteration [4317] | loss: 0.001301514101214707\n",
      "Iteration [4318] | loss: 0.0013021094491705298\n",
      "Iteration [4319] | loss: 0.0013024666113778949\n",
      "Iteration [4320] | loss: 0.001303061842918396\n",
      "Iteration [4321] | loss: 0.00130353809799999\n",
      "Iteration [4322] | loss: 0.0013040142366662621\n",
      "Iteration [4323] | loss: 0.001304609584622085\n",
      "Iteration [4324] | loss: 0.0013050857232883573\n",
      "Iteration [4325] | loss: 0.0013054428854957223\n",
      "Iteration [4326] | loss: 0.0013061572099104524\n",
      "Iteration [4327] | loss: 0.0013066334649920464\n",
      "Iteration [4328] | loss: 0.0013069906271994114\n",
      "Iteration [4329] | loss: 0.0013077049516141415\n",
      "Iteration [4330] | loss: 0.0013081812066957355\n",
      "Iteration [4331] | loss: 0.0013085383689031005\n",
      "Iteration [4332] | loss: 0.0013091336004436016\n",
      "Iteration [4333] | loss: 0.0013097288319841027\n",
      "Iteration [4334] | loss: 0.0013100859941914678\n",
      "Iteration [4335] | loss: 0.0013106813421472907\n",
      "Iteration [4336] | loss: 0.001311038387939334\n",
      "Iteration [4337] | loss: 0.0013116337358951569\n",
      "Iteration [4338] | loss: 0.001312228967435658\n",
      "Iteration [4339] | loss: 0.001312586129643023\n",
      "Iteration [4340] | loss: 0.0013131813611835241\n",
      "Iteration [4341] | loss: 0.001313776709139347\n",
      "Iteration [4342] | loss: 0.0013141338713467121\n",
      "Iteration [4343] | loss: 0.0013147291028872132\n",
      "Iteration [4344] | loss: 0.0013153243344277143\n",
      "Iteration [4345] | loss: 0.0013156814966350794\n",
      "Iteration [4346] | loss: 0.0013162767281755805\n",
      "Iteration [4347] | loss: 0.0013167529832571745\n",
      "Iteration [4348] | loss: 0.0013173482147976756\n",
      "Iteration [4349] | loss: 0.0013178244698792696\n",
      "Iteration [4350] | loss: 0.0013183006085455418\n",
      "Iteration [4351] | loss: 0.0013188959565013647\n",
      "Iteration [4352] | loss: 0.0013193720951676369\n",
      "Iteration [4353] | loss: 0.0013198483502492309\n",
      "Iteration [4354] | loss: 0.001320443581789732\n",
      "Iteration [4355] | loss: 0.001320800743997097\n",
      "Iteration [4356] | loss: 0.001321515068411827\n",
      "Iteration [4357] | loss: 0.001321991323493421\n",
      "Iteration [4358] | loss: 0.0013223483692854643\n",
      "Iteration [4359] | loss: 0.0013230626937001944\n",
      "Iteration [4360] | loss: 0.0013235389487817883\n",
      "Iteration [4361] | loss: 0.0013240152038633823\n",
      "Iteration [4362] | loss: 0.0013244913425296545\n",
      "Iteration [4363] | loss: 0.0013250865740701556\n",
      "Iteration [4364] | loss: 0.0013255628291517496\n",
      "Iteration [4365] | loss: 0.0013260389678180218\n",
      "Iteration [4366] | loss: 0.0013266343157738447\n",
      "Iteration [4367] | loss: 0.0013272295473143458\n",
      "Iteration [4368] | loss: 0.001327705685980618\n",
      "Iteration [4369] | loss: 0.001328181941062212\n",
      "Iteration [4370] | loss: 0.001328777172602713\n",
      "Iteration [4371] | loss: 0.001329253427684307\n",
      "Iteration [4372] | loss: 0.0013297295663505793\n",
      "Iteration [4373] | loss: 0.0013303249143064022\n",
      "Iteration [4374] | loss: 0.0013308010529726744\n",
      "Iteration [4375] | loss: 0.0013313962845131755\n",
      "Iteration [4376] | loss: 0.0013318725395947695\n",
      "Iteration [4377] | loss: 0.0013323486782610416\n",
      "Iteration [4378] | loss: 0.0013329440262168646\n",
      "Iteration [4379] | loss: 0.0013334201648831367\n",
      "Iteration [4380] | loss: 0.0013340153964236379\n",
      "Iteration [4381] | loss: 0.0013344916515052319\n",
      "Iteration [4382] | loss: 0.001335086883045733\n",
      "Iteration [4383] | loss: 0.001335563138127327\n",
      "Iteration [4384] | loss: 0.001336158369667828\n",
      "Iteration [4385] | loss: 0.0013366345083341002\n",
      "Iteration [4386] | loss: 0.0013371107634156942\n",
      "Iteration [4387] | loss: 0.0013377059949561954\n",
      "Iteration [4388] | loss: 0.0013381821336224675\n",
      "Iteration [4389] | loss: 0.0013387774815782905\n",
      "Iteration [4390] | loss: 0.0013392536202445626\n",
      "Iteration [4391] | loss: 0.0013398488517850637\n",
      "Iteration [4392] | loss: 0.0013403251068666577\n",
      "Iteration [4393] | loss: 0.0013409203384071589\n",
      "Iteration [4394] | loss: 0.001341396477073431\n",
      "Iteration [4395] | loss: 0.001341991825029254\n",
      "Iteration [4396] | loss: 0.0013424679636955261\n",
      "Iteration [4397] | loss: 0.0013430631952360272\n",
      "Iteration [4398] | loss: 0.0013435394503176212\n",
      "Iteration [4399] | loss: 0.0013442536583170295\n",
      "Iteration [4400] | loss: 0.0013446108205243945\n",
      "Iteration [4401] | loss: 0.0013452060520648956\n",
      "Iteration [4402] | loss: 0.0013456823071464896\n",
      "Iteration [4403] | loss: 0.0013463966315612197\n",
      "Iteration [4404] | loss: 0.0013467537937685847\n",
      "Iteration [4405] | loss: 0.0013473490253090858\n",
      "Iteration [4406] | loss: 0.001347825163975358\n",
      "Iteration [4407] | loss: 0.001348539488390088\n",
      "Iteration [4408] | loss: 0.0013488966505974531\n",
      "Iteration [4409] | loss: 0.0013494918821379542\n",
      "Iteration [4410] | loss: 0.0013500871136784554\n",
      "Iteration [4411] | loss: 0.0013506823452189565\n",
      "Iteration [4412] | loss: 0.0013510395074263215\n",
      "Iteration [4413] | loss: 0.0013516347389668226\n",
      "Iteration [4414] | loss: 0.0013522299705073237\n",
      "Iteration [4415] | loss: 0.0013528252020478249\n",
      "Iteration [4416] | loss: 0.0013533014571294188\n",
      "Iteration [4417] | loss: 0.00135389668866992\n",
      "Iteration [4418] | loss: 0.0013543728273361921\n",
      "Iteration [4419] | loss: 0.0013549680588766932\n",
      "Iteration [4420] | loss: 0.0013554443139582872\n",
      "Iteration [4421] | loss: 0.0013560395454987884\n",
      "Iteration [4422] | loss: 0.0013565156841650605\n",
      "Iteration [4423] | loss: 0.0013571109157055616\n",
      "Iteration [4424] | loss: 0.0013575871707871556\n",
      "Iteration [4425] | loss: 0.0013581824023276567\n",
      "Iteration [4426] | loss: 0.0013587776338681579\n",
      "Iteration [4427] | loss: 0.00135925377253443\n",
      "Iteration [4428] | loss: 0.00135996809694916\n",
      "Iteration [4429] | loss: 0.0013604442356154323\n",
      "Iteration [4430] | loss: 0.0013609204906970263\n",
      "Iteration [4431] | loss: 0.0013613966293632984\n",
      "Iteration [4432] | loss: 0.0013621109537780285\n",
      "Iteration [4433] | loss: 0.0013625870924443007\n",
      "Iteration [4434] | loss: 0.0013631823239848018\n",
      "Iteration [4435] | loss: 0.0013637775555253029\n",
      "Iteration [4436] | loss: 0.0013642538106068969\n",
      "Iteration [4437] | loss: 0.001364849042147398\n",
      "Iteration [4438] | loss: 0.0013653251808136702\n",
      "Iteration [4439] | loss: 0.0013660395052284002\n",
      "Iteration [4440] | loss: 0.0013663965510204434\n",
      "Iteration [4441] | loss: 0.0013669917825609446\n",
      "Iteration [4442] | loss: 0.0013677061069756746\n",
      "Iteration [4443] | loss: 0.0013681822456419468\n",
      "Iteration [4444] | loss: 0.001368777477182448\n",
      "Iteration [4445] | loss: 0.001369134639389813\n",
      "Iteration [4446] | loss: 0.001369848963804543\n",
      "Iteration [4447] | loss: 0.0013703251024708152\n",
      "Iteration [4448] | loss: 0.0013709203340113163\n",
      "Iteration [4449] | loss: 0.0013716346584260464\n",
      "Iteration [4450] | loss: 0.0013719918206334114\n",
      "Iteration [4451] | loss: 0.0013725870521739125\n",
      "Iteration [4452] | loss: 0.0013730631908401847\n",
      "Iteration [4453] | loss: 0.0013737775152549148\n",
      "Iteration [4454] | loss: 0.0013743727467954159\n",
      "Iteration [4455] | loss: 0.001374848885461688\n",
      "Iteration [4456] | loss: 0.0013754441170021892\n",
      "Iteration [4457] | loss: 0.0013759202556684613\n",
      "Iteration [4458] | loss: 0.0013765154872089624\n",
      "Iteration [4459] | loss: 0.0013771107187494636\n",
      "Iteration [4460] | loss: 0.0013777059502899647\n",
      "Iteration [4461] | loss: 0.0013783011818304658\n",
      "Iteration [4462] | loss: 0.001378777320496738\n",
      "Iteration [4463] | loss: 0.001379372552037239\n",
      "Iteration [4464] | loss: 0.0013799677835777402\n",
      "Iteration [4465] | loss: 0.0013804440386593342\n",
      "Iteration [4466] | loss: 0.0013810392701998353\n",
      "Iteration [4467] | loss: 0.0013817534781992435\n",
      "Iteration [4468] | loss: 0.0013821106404066086\n",
      "Iteration [4469] | loss: 0.0013827058719471097\n",
      "Iteration [4470] | loss: 0.001383420079946518\n",
      "Iteration [4471] | loss: 0.001383896335028112\n",
      "Iteration [4472] | loss: 0.001384491566568613\n",
      "Iteration [4473] | loss: 0.0013850866816937923\n",
      "Iteration [4474] | loss: 0.0013856819132342935\n",
      "Iteration [4475] | loss: 0.0013861581683158875\n",
      "Iteration [4476] | loss: 0.0013867533998563886\n",
      "Iteration [4477] | loss: 0.0013873485149815679\n",
      "Iteration [4478] | loss: 0.001387943746522069\n",
      "Iteration [4479] | loss: 0.00138853897806257\n",
      "Iteration [4480] | loss: 0.0013891342096030712\n",
      "Iteration [4481] | loss: 0.0013897294411435723\n",
      "Iteration [4482] | loss: 0.0013902055798098445\n",
      "Iteration [4483] | loss: 0.0013906818348914385\n",
      "Iteration [4484] | loss: 0.0013913960428908467\n",
      "Iteration [4485] | loss: 0.0013919912744313478\n",
      "Iteration [4486] | loss: 0.00139246741309762\n",
      "Iteration [4487] | loss: 0.00139318173751235\n",
      "Iteration [4488] | loss: 0.0013937769690528512\n",
      "Iteration [4489] | loss: 0.0013942531077191234\n",
      "Iteration [4490] | loss: 0.0013947292463853955\n",
      "Iteration [4491] | loss: 0.0013954435708001256\n",
      "Iteration [4492] | loss: 0.0013959197094663978\n",
      "Iteration [4493] | loss: 0.0013965149410068989\n",
      "Iteration [4494] | loss: 0.0013972291490063071\n",
      "Iteration [4495] | loss: 0.0013977054040879011\n",
      "Iteration [4496] | loss: 0.0013983005192130804\n",
      "Iteration [4497] | loss: 0.0013988957507535815\n",
      "Iteration [4498] | loss: 0.0013994909822940826\n",
      "Iteration [4499] | loss: 0.0013999671209603548\n",
      "Iteration [4500] | loss: 0.001400562352500856\n",
      "Iteration [4501] | loss: 0.001401276676915586\n",
      "Iteration [4502] | loss: 0.0014017528155818582\n",
      "Iteration [4503] | loss: 0.0014023480471223593\n",
      "Iteration [4504] | loss: 0.0014030622551217675\n",
      "Iteration [4505] | loss: 0.0014036574866622686\n",
      "Iteration [4506] | loss: 0.0014041336253285408\n",
      "Iteration [4507] | loss: 0.001404728856869042\n",
      "Iteration [4508] | loss: 0.001405324088409543\n",
      "Iteration [4509] | loss: 0.0014059193199500442\n",
      "Iteration [4510] | loss: 0.0014063954586163163\n",
      "Iteration [4511] | loss: 0.0014071096666157246\n",
      "Iteration [4512] | loss: 0.0014077048981562257\n",
      "Iteration [4513] | loss: 0.0014081810368224978\n",
      "Iteration [4514] | loss: 0.001408895361237228\n",
      "Iteration [4515] | loss: 0.0014094904763624072\n",
      "Iteration [4516] | loss: 0.0014100857079029083\n",
      "Iteration [4517] | loss: 0.0014105618465691805\n",
      "Iteration [4518] | loss: 0.0014112761709839106\n",
      "Iteration [4519] | loss: 0.0014118712861090899\n",
      "Iteration [4520] | loss: 0.0014123475411906838\n",
      "Iteration [4521] | loss: 0.001413061749190092\n",
      "Iteration [4522] | loss: 0.0014135378878563643\n",
      "Iteration [4523] | loss: 0.0014141331193968654\n",
      "Iteration [4524] | loss: 0.0014147283509373665\n",
      "Iteration [4525] | loss: 0.0014154425589367747\n",
      "Iteration [4526] | loss: 0.001415918697603047\n",
      "Iteration [4527] | loss: 0.001416513929143548\n",
      "Iteration [4528] | loss: 0.0014172281371429563\n",
      "Iteration [4529] | loss: 0.0014178233686834574\n",
      "Iteration [4530] | loss: 0.0014184186002239585\n",
      "Iteration [4531] | loss: 0.0014190138317644596\n",
      "Iteration [4532] | loss: 0.001419608946889639\n",
      "Iteration [4533] | loss: 0.00142020417843014\n",
      "Iteration [4534] | loss: 0.0014207994099706411\n",
      "Iteration [4535] | loss: 0.0014213945250958204\n",
      "Iteration [4536] | loss: 0.0014219897566363215\n",
      "Iteration [4537] | loss: 0.0014224658953025937\n",
      "Iteration [4538] | loss: 0.0014231802197173238\n",
      "Iteration [4539] | loss: 0.001423775334842503\n",
      "Iteration [4540] | loss: 0.0014243705663830042\n",
      "Iteration [4541] | loss: 0.0014249657979235053\n",
      "Iteration [4542] | loss: 0.0014255610294640064\n",
      "Iteration [4543] | loss: 0.0014261561445891857\n",
      "Iteration [4544] | loss: 0.001426870352588594\n",
      "Iteration [4545] | loss: 0.001427465584129095\n",
      "Iteration [4546] | loss: 0.0014279417227953672\n",
      "Iteration [4547] | loss: 0.0014285369543358684\n",
      "Iteration [4548] | loss: 0.0014292511623352766\n",
      "Iteration [4549] | loss: 0.0014298463938757777\n",
      "Iteration [4550] | loss: 0.0014304416254162788\n",
      "Iteration [4551] | loss: 0.001431155833415687\n",
      "Iteration [4552] | loss: 0.0014316319720819592\n",
      "Iteration [4553] | loss: 0.0014322272036224604\n",
      "Iteration [4554] | loss: 0.0014329414116218686\n",
      "Iteration [4555] | loss: 0.0014335366431623697\n",
      "Iteration [4556] | loss: 0.001434131758287549\n",
      "Iteration [4557] | loss: 0.0014347269898280501\n",
      "Iteration [4558] | loss: 0.0014354411978274584\n",
      "Iteration [4559] | loss: 0.0014359173364937305\n",
      "Iteration [4560] | loss: 0.0014365125680342317\n",
      "Iteration [4561] | loss: 0.00143722677603364\n",
      "Iteration [4562] | loss: 0.001437822007574141\n",
      "Iteration [4563] | loss: 0.0014384171226993203\n",
      "Iteration [4564] | loss: 0.0014391313306987286\n",
      "Iteration [4565] | loss: 0.0014397265622392297\n",
      "Iteration [4566] | loss: 0.0014403217937797308\n",
      "Iteration [4567] | loss: 0.00144091690890491\n",
      "Iteration [4568] | loss: 0.0014415121404454112\n",
      "Iteration [4569] | loss: 0.0014421073719859123\n",
      "Iteration [4570] | loss: 0.0014427024871110916\n",
      "Iteration [4571] | loss: 0.0014434166951104999\n",
      "Iteration [4572] | loss: 0.001444011926651001\n",
      "Iteration [4573] | loss: 0.001444607158191502\n",
      "Iteration [4574] | loss: 0.0014453213661909103\n",
      "Iteration [4575] | loss: 0.0014459164813160896\n",
      "Iteration [4576] | loss: 0.0014465117128565907\n",
      "Iteration [4577] | loss: 0.00144710682798177\n",
      "Iteration [4578] | loss: 0.0014477020595222712\n",
      "Iteration [4579] | loss: 0.0014482972910627723\n",
      "Iteration [4580] | loss: 0.0014490114990621805\n",
      "Iteration [4581] | loss: 0.0014496066141873598\n",
      "Iteration [4582] | loss: 0.001450201845727861\n",
      "Iteration [4583] | loss: 0.001450797077268362\n",
      "Iteration [4584] | loss: 0.0014515112852677703\n",
      "Iteration [4585] | loss: 0.0014521064003929496\n",
      "Iteration [4586] | loss: 0.0014527016319334507\n",
      "Iteration [4587] | loss: 0.001453415839932859\n",
      "Iteration [4588] | loss: 0.0014540109550580382\n",
      "Iteration [4589] | loss: 0.0014546061865985394\n",
      "Iteration [4590] | loss: 0.0014552014181390405\n",
      "Iteration [4591] | loss: 0.0014557965332642198\n",
      "Iteration [4592] | loss: 0.001456510741263628\n",
      "Iteration [4593] | loss: 0.0014572249492630363\n",
      "Iteration [4594] | loss: 0.0014578201808035374\n",
      "Iteration [4595] | loss: 0.0014584152959287167\n",
      "Iteration [4596] | loss: 0.001459129503928125\n",
      "Iteration [4597] | loss: 0.001459724735468626\n",
      "Iteration [4598] | loss: 0.0014603198505938053\n",
      "Iteration [4599] | loss: 0.0014609150821343064\n",
      "Iteration [4600] | loss: 0.0014616292901337147\n",
      "Iteration [4601] | loss: 0.001462224405258894\n",
      "Iteration [4602] | loss: 0.001462819636799395\n",
      "Iteration [4603] | loss: 0.0014635338447988033\n",
      "Iteration [4604] | loss: 0.0014641289599239826\n",
      "Iteration [4605] | loss: 0.0014647241914644837\n",
      "Iteration [4606] | loss: 0.001465438399463892\n",
      "Iteration [4607] | loss: 0.001466033631004393\n",
      "Iteration [4608] | loss: 0.0014667478390038013\n",
      "Iteration [4609] | loss: 0.0014674619305878878\n",
      "Iteration [4610] | loss: 0.0014680571621283889\n",
      "Iteration [4611] | loss: 0.0014686522772535682\n",
      "Iteration [4612] | loss: 0.0014693664852529764\n",
      "Iteration [4613] | loss: 0.0014699617167934775\n",
      "Iteration [4614] | loss: 0.0014705568319186568\n",
      "Iteration [4615] | loss: 0.001471271039918065\n",
      "Iteration [4616] | loss: 0.0014718662714585662\n",
      "Iteration [4617] | loss: 0.0014724613865837455\n",
      "Iteration [4618] | loss: 0.0014730566181242466\n",
      "Iteration [4619] | loss: 0.001473889802582562\n",
      "Iteration [4620] | loss: 0.001474485034123063\n",
      "Iteration [4621] | loss: 0.0014750801492482424\n",
      "Iteration [4622] | loss: 0.0014757943572476506\n",
      "Iteration [4623] | loss: 0.0014763895887881517\n",
      "Iteration [4624] | loss: 0.001476984703913331\n",
      "Iteration [4625] | loss: 0.0014776989119127393\n",
      "Iteration [4626] | loss: 0.0014782940270379186\n",
      "Iteration [4627] | loss: 0.0014788892585784197\n",
      "Iteration [4628] | loss: 0.001479603466577828\n",
      "Iteration [4629] | loss: 0.0014803176745772362\n",
      "Iteration [4630] | loss: 0.0014810318825766444\n",
      "Iteration [4631] | loss: 0.0014817459741607308\n",
      "Iteration [4632] | loss: 0.001482341205701232\n",
      "Iteration [4633] | loss: 0.0014829363208264112\n",
      "Iteration [4634] | loss: 0.0014836505288258195\n",
      "Iteration [4635] | loss: 0.0014842457603663206\n",
      "Iteration [4636] | loss: 0.0014848408754915\n",
      "Iteration [4637] | loss: 0.0014856740599498153\n",
      "Iteration [4638] | loss: 0.0014862692914903164\n",
      "Iteration [4639] | loss: 0.0014868644066154957\n",
      "Iteration [4640] | loss: 0.001487578614614904\n",
      "Iteration [4641] | loss: 0.0014881737297400832\n",
      "Iteration [4642] | loss: 0.0014887689612805843\n",
      "Iteration [4643] | loss: 0.0014894831692799926\n",
      "Iteration [4644] | loss: 0.001490316353738308\n",
      "Iteration [4645] | loss: 0.0014909114688634872\n",
      "Iteration [4646] | loss: 0.0014915067004039884\n",
      "Iteration [4647] | loss: 0.0014922209084033966\n",
      "Iteration [4648] | loss: 0.001492816023528576\n",
      "Iteration [4649] | loss: 0.0014934111386537552\n",
      "Iteration [4650] | loss: 0.0014941253466531634\n",
      "Iteration [4651] | loss: 0.0014948395546525717\n",
      "Iteration [4652] | loss: 0.001495434669777751\n",
      "Iteration [4653] | loss: 0.0014962679706513882\n",
      "Iteration [4654] | loss: 0.0014968630857765675\n",
      "Iteration [4655] | loss: 0.0014974582009017467\n",
      "Iteration [4656] | loss: 0.001498172408901155\n",
      "Iteration [4657] | loss: 0.0014988866169005632\n",
      "Iteration [4658] | loss: 0.0014994817320257425\n",
      "Iteration [4659] | loss: 0.0015001959400251508\n",
      "Iteration [4660] | loss: 0.001500910148024559\n",
      "Iteration [4661] | loss: 0.0015015052631497383\n",
      "Iteration [4662] | loss: 0.0015022194711491466\n",
      "Iteration [4663] | loss: 0.0015029336791485548\n",
      "Iteration [4664] | loss: 0.001503528794273734\n",
      "Iteration [4665] | loss: 0.0015042430022731423\n",
      "Iteration [4666] | loss: 0.0015048381173983216\n",
      "Iteration [4667] | loss: 0.0015055523253977299\n",
      "Iteration [4668] | loss: 0.0015063855098560452\n",
      "Iteration [4669] | loss: 0.0015069806249812245\n",
      "Iteration [4670] | loss: 0.0015075758565217257\n",
      "Iteration [4671] | loss: 0.001508289948105812\n",
      "Iteration [4672] | loss: 0.0015090041561052203\n",
      "Iteration [4673] | loss: 0.0015097183641046286\n",
      "Iteration [4674] | loss: 0.001510432455688715\n",
      "Iteration [4675] | loss: 0.001511027687229216\n",
      "Iteration [4676] | loss: 0.0015116228023543954\n",
      "Iteration [4677] | loss: 0.0015123370103538036\n",
      "Iteration [4678] | loss: 0.001513170194812119\n",
      "Iteration [4679] | loss: 0.0015137653099372983\n",
      "Iteration [4680] | loss: 0.0015144795179367065\n",
      "Iteration [4681] | loss: 0.0015150746330618858\n",
      "Iteration [4682] | loss: 0.0015156697481870651\n",
      "Iteration [4683] | loss: 0.0015166220255196095\n",
      "Iteration [4684] | loss: 0.0015172171406447887\n",
      "Iteration [4685] | loss: 0.001517812255769968\n",
      "Iteration [4686] | loss: 0.0015185264637693763\n",
      "Iteration [4687] | loss: 0.0015193596482276917\n",
      "Iteration [4688] | loss: 0.001519954763352871\n",
      "Iteration [4689] | loss: 0.0015206689713522792\n",
      "Iteration [4690] | loss: 0.0015212640864774585\n",
      "Iteration [4691] | loss: 0.0015220972709357738\n",
      "Iteration [4692] | loss: 0.001522811478935182\n",
      "Iteration [4693] | loss: 0.0015234065940603614\n",
      "Iteration [4694] | loss: 0.0015240018256008625\n",
      "Iteration [4695] | loss: 0.0015248350100591779\n",
      "Iteration [4696] | loss: 0.0015255491016432643\n",
      "Iteration [4697] | loss: 0.0015261442167684436\n",
      "Iteration [4698] | loss: 0.0015268584247678518\n",
      "Iteration [4699] | loss: 0.00152757263276726\n",
      "Iteration [4700] | loss: 0.0015282867243513465\n",
      "Iteration [4701] | loss: 0.0015290009323507547\n",
      "Iteration [4702] | loss: 0.001529596047475934\n",
      "Iteration [4703] | loss: 0.0015304292319342494\n",
      "Iteration [4704] | loss: 0.0015311434399336576\n",
      "Iteration [4705] | loss: 0.001531738555058837\n",
      "Iteration [4706] | loss: 0.0015323336701840162\n",
      "Iteration [4707] | loss: 0.0015332859475165606\n",
      "Iteration [4708] | loss: 0.0015338810626417398\n",
      "Iteration [4709] | loss: 0.0015344761777669191\n",
      "Iteration [4710] | loss: 0.0015353093622252345\n",
      "Iteration [4711] | loss: 0.001536023453809321\n",
      "Iteration [4712] | loss: 0.001536618685349822\n",
      "Iteration [4713] | loss: 0.0015374518698081374\n",
      "Iteration [4714] | loss: 0.0015381659613922238\n",
      "Iteration [4715] | loss: 0.0015387610765174031\n",
      "Iteration [4716] | loss: 0.0015395942609757185\n",
      "Iteration [4717] | loss: 0.0015401893761008978\n",
      "Iteration [4718] | loss: 0.001540903584100306\n",
      "Iteration [4719] | loss: 0.0015417367685586214\n",
      "Iteration [4720] | loss: 0.0015423318836838007\n",
      "Iteration [4721] | loss: 0.0015430459752678871\n",
      "Iteration [4722] | loss: 0.0015438791597262025\n",
      "Iteration [4723] | loss: 0.0015444743912667036\n",
      "Iteration [4724] | loss: 0.001545069506391883\n",
      "Iteration [4725] | loss: 0.0015460216673091054\n",
      "Iteration [4726] | loss: 0.0015466167824342847\n",
      "Iteration [4727] | loss: 0.001547330990433693\n",
      "Iteration [4728] | loss: 0.0015481640584766865\n",
      "Iteration [4729] | loss: 0.0015487592900171876\n",
      "Iteration [4730] | loss: 0.001549473381601274\n",
      "Iteration [4731] | loss: 0.0015503065660595894\n",
      "Iteration [4732] | loss: 0.0015509016811847687\n",
      "Iteration [4733] | loss: 0.001551615772768855\n",
      "Iteration [4734] | loss: 0.0015524489572271705\n",
      "Iteration [4735] | loss: 0.0015530440723523498\n",
      "Iteration [4736] | loss: 0.0015538772568106651\n",
      "Iteration [4737] | loss: 0.0015545914648100734\n",
      "Iteration [4738] | loss: 0.0015551865799352527\n",
      "Iteration [4739] | loss: 0.001556019764393568\n",
      "Iteration [4740] | loss: 0.0015567338559776545\n",
      "Iteration [4741] | loss: 0.0015574480639770627\n",
      "Iteration [4742] | loss: 0.0015581621555611491\n",
      "Iteration [4743] | loss: 0.0015588762471452355\n",
      "Iteration [4744] | loss: 0.0015595904551446438\n",
      "Iteration [4745] | loss: 0.0015603045467287302\n",
      "Iteration [4746] | loss: 0.0015611377311870456\n",
      "Iteration [4747] | loss: 0.0015617328463122249\n",
      "Iteration [4748] | loss: 0.0015625660307705402\n",
      "Iteration [4749] | loss: 0.0015632801223546267\n",
      "Iteration [4750] | loss: 0.001563875237479806\n",
      "Iteration [4751] | loss: 0.0015648273983970284\n",
      "Iteration [4752] | loss: 0.0015654225135222077\n",
      "Iteration [4753] | loss: 0.0015662556979805231\n",
      "Iteration [4754] | loss: 0.0015669699059799314\n",
      "Iteration [4755] | loss: 0.0015676839975640178\n",
      "Iteration [4756] | loss: 0.0015683980891481042\n",
      "Iteration [4757] | loss: 0.0015691122971475124\n",
      "Iteration [4758] | loss: 0.0015698263887315989\n",
      "Iteration [4759] | loss: 0.0015706595731899142\n",
      "Iteration [4760] | loss: 0.0015713736647740006\n",
      "Iteration [4761] | loss: 0.00157196877989918\n",
      "Iteration [4762] | loss: 0.0015729209408164024\n",
      "Iteration [4763] | loss: 0.0015735160559415817\n",
      "Iteration [4764] | loss: 0.00157423026394099\n",
      "Iteration [4765] | loss: 0.0015750633319839835\n",
      "Iteration [4766] | loss: 0.0015757775399833918\n",
      "Iteration [4767] | loss: 0.0015764916315674782\n",
      "Iteration [4768] | loss: 0.0015773248160257936\n",
      "Iteration [4769] | loss: 0.0015779199311509728\n",
      "Iteration [4770] | loss: 0.0015787529991939664\n",
      "Iteration [4771] | loss: 0.0015794672071933746\n",
      "Iteration [4772] | loss: 0.0015803002752363682\n",
      "Iteration [4773] | loss: 0.0015810144832357764\n",
      "Iteration [4774] | loss: 0.00158184755127877\n",
      "Iteration [4775] | loss: 0.0015824426664039493\n",
      "Iteration [4776] | loss: 0.0015832758508622646\n",
      "Iteration [4777] | loss: 0.001583989942446351\n",
      "Iteration [4778] | loss: 0.0015847040340304375\n",
      "Iteration [4779] | loss: 0.0015855372184887528\n",
      "Iteration [4780] | loss: 0.0015862513100728393\n",
      "Iteration [4781] | loss: 0.0015870844945311546\n",
      "Iteration [4782] | loss: 0.001587798586115241\n",
      "Iteration [4783] | loss: 0.0015885126776993275\n",
      "Iteration [4784] | loss: 0.0015892268856987357\n",
      "Iteration [4785] | loss: 0.0015900599537417293\n",
      "Iteration [4786] | loss: 0.0015907741617411375\n",
      "Iteration [4787] | loss: 0.001591607229784131\n",
      "Iteration [4788] | loss: 0.0015923213213682175\n",
      "Iteration [4789] | loss: 0.0015930355293676257\n",
      "Iteration [4790] | loss: 0.0015939876902848482\n",
      "Iteration [4791] | loss: 0.0015945826889947057\n",
      "Iteration [4792] | loss: 0.001595296896994114\n",
      "Iteration [4793] | loss: 0.0015961299650371075\n",
      "Iteration [4794] | loss: 0.0015968440566211939\n",
      "Iteration [4795] | loss: 0.0015975582646206021\n",
      "Iteration [4796] | loss: 0.0015983913326635957\n",
      "Iteration [4797] | loss: 0.001599224517121911\n",
      "Iteration [4798] | loss: 0.0015998195158317685\n",
      "Iteration [4799] | loss: 0.001600771676748991\n",
      "Iteration [4800] | loss: 0.0016013667918741703\n",
      "Iteration [4801] | loss: 0.0016023189527913928\n",
      "Iteration [4802] | loss: 0.0016030330443754792\n",
      "Iteration [4803] | loss: 0.0016037471359595656\n",
      "Iteration [4804] | loss: 0.001604580320417881\n",
      "Iteration [4805] | loss: 0.0016054133884608746\n",
      "Iteration [4806] | loss: 0.0016060085035860538\n",
      "Iteration [4807] | loss: 0.0016069606645032763\n",
      "Iteration [4808] | loss: 0.0016075557796284556\n",
      "Iteration [4809] | loss: 0.0016083888476714492\n",
      "Iteration [4810] | loss: 0.0016092220321297646\n",
      "Iteration [4811] | loss: 0.001609936123713851\n",
      "Iteration [4812] | loss: 0.0016107691917568445\n",
      "Iteration [4813] | loss: 0.00161160237621516\n",
      "Iteration [4814] | loss: 0.0016121974913403392\n",
      "Iteration [4815] | loss: 0.0016131495358422399\n",
      "Iteration [4816] | loss: 0.0016138636274263263\n",
      "Iteration [4817] | loss: 0.0016145778354257345\n",
      "Iteration [4818] | loss: 0.001615410903468728\n",
      "Iteration [4819] | loss: 0.0016162439715117216\n",
      "Iteration [4820] | loss: 0.001616839086636901\n",
      "Iteration [4821] | loss: 0.0016177912475541234\n",
      "Iteration [4822] | loss: 0.0016185053391382098\n",
      "Iteration [4823] | loss: 0.0016194575000554323\n",
      "Iteration [4824] | loss: 0.0016200524987652898\n",
      "Iteration [4825] | loss: 0.0016208856832236052\n",
      "Iteration [4826] | loss: 0.0016217187512665987\n",
      "Iteration [4827] | loss: 0.0016224328428506851\n",
      "Iteration [4828] | loss: 0.0016231469344347715\n",
      "Iteration [4829] | loss: 0.001624099095351994\n",
      "Iteration [4830] | loss: 0.0016249321633949876\n",
      "Iteration [4831] | loss: 0.001625646254979074\n",
      "Iteration [4832] | loss: 0.0016264794394373894\n",
      "Iteration [4833] | loss: 0.0016271935310214758\n",
      "Iteration [4834] | loss: 0.0016280265990644693\n",
      "Iteration [4835] | loss: 0.0016287406906485558\n",
      "Iteration [4836] | loss: 0.0016295737586915493\n",
      "Iteration [4837] | loss: 0.0016304069431498647\n",
      "Iteration [4838] | loss: 0.001631121034733951\n",
      "Iteration [4839] | loss: 0.0016320730792358518\n",
      "Iteration [4840] | loss: 0.0016327871708199382\n",
      "Iteration [4841] | loss: 0.0016336203552782536\n",
      "Iteration [4842] | loss: 0.001634453423321247\n",
      "Iteration [4843] | loss: 0.0016351675149053335\n",
      "Iteration [4844] | loss: 0.001636000582948327\n",
      "Iteration [4845] | loss: 0.0016368337674066424\n",
      "Iteration [4846] | loss: 0.001637666835449636\n",
      "Iteration [4847] | loss: 0.0016382618341594934\n",
      "Iteration [4848] | loss: 0.001639213995076716\n",
      "Iteration [4849] | loss: 0.0016399280866608024\n",
      "Iteration [4850] | loss: 0.0016408802475780249\n",
      "Iteration [4851] | loss: 0.0016417133156210184\n",
      "Iteration [4852] | loss: 0.0016423083143308759\n",
      "Iteration [4853] | loss: 0.0016432604752480984\n",
      "Iteration [4854] | loss: 0.001644093543291092\n",
      "Iteration [4855] | loss: 0.0016448076348751783\n",
      "Iteration [4856] | loss: 0.0016457597957924008\n",
      "Iteration [4857] | loss: 0.0016464737709611654\n",
      "Iteration [4858] | loss: 0.0016473069554194808\n",
      "Iteration [4859] | loss: 0.0016481400234624743\n",
      "Iteration [4860] | loss: 0.0016488541150465608\n",
      "Iteration [4861] | loss: 0.0016498061595484614\n",
      "Iteration [4862] | loss: 0.0016505202511325479\n",
      "Iteration [4863] | loss: 0.0016514724120497704\n",
      "Iteration [4864] | loss: 0.001652186387218535\n",
      "Iteration [4865] | loss: 0.0016529004788026214\n",
      "Iteration [4866] | loss: 0.0016538526397198439\n",
      "Iteration [4867] | loss: 0.0016546857077628374\n",
      "Iteration [4868] | loss: 0.0016553997993469238\n",
      "Iteration [4869] | loss: 0.0016563518438488245\n",
      "Iteration [4870] | loss: 0.001657184911891818\n",
      "Iteration [4871] | loss: 0.0016580180963501334\n",
      "Iteration [4872] | loss: 0.001658732071518898\n",
      "Iteration [4873] | loss: 0.0016595651395618916\n",
      "Iteration [4874] | loss: 0.001660398324020207\n",
      "Iteration [4875] | loss: 0.0016612313920632005\n",
      "Iteration [4876] | loss: 0.001662064460106194\n",
      "Iteration [4877] | loss: 0.0016628975281491876\n",
      "Iteration [4878] | loss: 0.0016637305961921811\n",
      "Iteration [4879] | loss: 0.0016646826406940818\n",
      "Iteration [4880] | loss: 0.0016653967322781682\n",
      "Iteration [4881] | loss: 0.0016662298003211617\n",
      "Iteration [4882] | loss: 0.0016671819612383842\n",
      "Iteration [4883] | loss: 0.0016678959364071488\n",
      "Iteration [4884] | loss: 0.0016688480973243713\n",
      "Iteration [4885] | loss: 0.001669562072493136\n",
      "Iteration [4886] | loss: 0.0016702761640772223\n",
      "Iteration [4887] | loss: 0.001671228208579123\n",
      "Iteration [4888] | loss: 0.0016720612766221166\n",
      "Iteration [4889] | loss: 0.001672894461080432\n",
      "Iteration [4890] | loss: 0.0016737275291234255\n",
      "Iteration [4891] | loss: 0.001674560597166419\n",
      "Iteration [4892] | loss: 0.0016755126416683197\n",
      "Iteration [4893] | loss: 0.0016762267332524061\n",
      "Iteration [4894] | loss: 0.0016770598012953997\n",
      "Iteration [4895] | loss: 0.0016780118457973003\n",
      "Iteration [4896] | loss: 0.001678725820966065\n",
      "Iteration [4897] | loss: 0.0016796779818832874\n",
      "Iteration [4898] | loss: 0.001680511049926281\n",
      "Iteration [4899] | loss: 0.0016813441179692745\n",
      "Iteration [4900] | loss: 0.001682177186012268\n",
      "Iteration [4901] | loss: 0.0016830102540552616\n",
      "Iteration [4902] | loss: 0.0016838433220982552\n",
      "Iteration [4903] | loss: 0.0016847953666001558\n",
      "Iteration [4904] | loss: 0.0016855093417689204\n",
      "Iteration [4905] | loss: 0.001686461502686143\n",
      "Iteration [4906] | loss: 0.0016872945707291365\n",
      "Iteration [4907] | loss: 0.0016881275223568082\n",
      "Iteration [4908] | loss: 0.0016889605903998017\n",
      "Iteration [4909] | loss: 0.0016897936584427953\n",
      "Iteration [4910] | loss: 0.001690745702944696\n",
      "Iteration [4911] | loss: 0.0016916978638619184\n",
      "Iteration [4912] | loss: 0.001692411839030683\n",
      "Iteration [4913] | loss: 0.0016933638835325837\n",
      "Iteration [4914] | loss: 0.0016941969515755773\n",
      "Iteration [4915] | loss: 0.001695148996077478\n",
      "Iteration [4916] | loss: 0.0016959820641204715\n",
      "Iteration [4917] | loss: 0.001696696155704558\n",
      "Iteration [4918] | loss: 0.0016976482002064586\n",
      "Iteration [4919] | loss: 0.0016984812682494521\n",
      "Iteration [4920] | loss: 0.0016994333127513528\n",
      "Iteration [4921] | loss: 0.0017001472879201174\n",
      "Iteration [4922] | loss: 0.001700980355963111\n",
      "Iteration [4923] | loss: 0.0017020514933392406\n",
      "Iteration [4924] | loss: 0.0017028844449669123\n",
      "Iteration [4925] | loss: 0.001703836489468813\n",
      "Iteration [4926] | loss: 0.0017045505810528994\n",
      "Iteration [4927] | loss: 0.001705383649095893\n",
      "Iteration [4928] | loss: 0.0017063356935977936\n",
      "Iteration [4929] | loss: 0.0017071687616407871\n",
      "Iteration [4930] | loss: 0.0017081208061426878\n",
      "Iteration [4931] | loss: 0.0017090728506445885\n",
      "Iteration [4932] | loss: 0.001709786825813353\n",
      "Iteration [4933] | loss: 0.0017107388703152537\n",
      "Iteration [4934] | loss: 0.0017115719383582473\n",
      "Iteration [4935] | loss: 0.001712523982860148\n",
      "Iteration [4936] | loss: 0.0017133570509031415\n",
      "Iteration [4937] | loss: 0.0017141900025308132\n",
      "Iteration [4938] | loss: 0.0017151420470327139\n",
      "Iteration [4939] | loss: 0.0017159751150757074\n",
      "Iteration [4940] | loss: 0.0017169271595776081\n",
      "Iteration [4941] | loss: 0.0017177602276206017\n",
      "Iteration [4942] | loss: 0.0017187122721225023\n",
      "Iteration [4943] | loss: 0.0017196642002090812\n",
      "Iteration [4944] | loss: 0.0017203782917931676\n",
      "Iteration [4945] | loss: 0.0017213303362950683\n",
      "Iteration [4946] | loss: 0.00172216328792274\n",
      "Iteration [4947] | loss: 0.0017231153324246407\n",
      "Iteration [4948] | loss: 0.0017240673769265413\n",
      "Iteration [4949] | loss: 0.0017249004449695349\n",
      "Iteration [4950] | loss: 0.0017258524894714355\n",
      "Iteration [4951] | loss: 0.0017268045339733362\n",
      "Iteration [4952] | loss: 0.0017275185091421008\n",
      "Iteration [4953] | loss: 0.0017284705536440015\n",
      "Iteration [4954] | loss: 0.001729303621686995\n",
      "Iteration [4955] | loss: 0.0017303746426478028\n",
      "Iteration [4956] | loss: 0.0017312075942754745\n",
      "Iteration [4957] | loss: 0.001732040662318468\n",
      "Iteration [4958] | loss: 0.0017329927068203688\n",
      "Iteration [4959] | loss: 0.0017339446349069476\n",
      "Iteration [4960] | loss: 0.0017348966794088483\n",
      "Iteration [4961] | loss: 0.0017356107709929347\n",
      "Iteration [4962] | loss: 0.0017364437226206064\n",
      "Iteration [4963] | loss: 0.0017375147435814142\n",
      "Iteration [4964] | loss: 0.0017383478116244078\n",
      "Iteration [4965] | loss: 0.0017392997397109866\n",
      "Iteration [4966] | loss: 0.0017402517842128873\n",
      "Iteration [4967] | loss: 0.0017410848522558808\n",
      "Iteration [4968] | loss: 0.0017420368967577815\n",
      "Iteration [4969] | loss: 0.0017429888248443604\n",
      "Iteration [4970] | loss: 0.001743940869346261\n",
      "Iteration [4971] | loss: 0.0017447738209739327\n",
      "Iteration [4972] | loss: 0.0017458448419347405\n",
      "Iteration [4973] | loss: 0.001746677909977734\n",
      "Iteration [4974] | loss: 0.0017475108616054058\n",
      "Iteration [4975] | loss: 0.0017485818825662136\n",
      "Iteration [4976] | loss: 0.0017492959741503\n",
      "Iteration [4977] | loss: 0.0017502479022368789\n",
      "Iteration [4978] | loss: 0.0017511999467387795\n",
      "Iteration [4979] | loss: 0.0017520328983664513\n",
      "Iteration [4980] | loss: 0.001753103919327259\n",
      "Iteration [4981] | loss: 0.0017539369873702526\n",
      "Iteration [4982] | loss: 0.0017548889154568315\n",
      "Iteration [4983] | loss: 0.0017558409599587321\n",
      "Iteration [4984] | loss: 0.0017567930044606328\n",
      "Iteration [4985] | loss: 0.0017576259560883045\n",
      "Iteration [4986] | loss: 0.0017585780005902052\n",
      "Iteration [4987] | loss: 0.001759529928676784\n",
      "Iteration [4988] | loss: 0.0017604819731786847\n",
      "Iteration [4989] | loss: 0.0017614339012652636\n",
      "Iteration [4990] | loss: 0.0017623859457671642\n",
      "Iteration [4991] | loss: 0.001763218897394836\n",
      "Iteration [4992] | loss: 0.0017641709418967366\n",
      "Iteration [4993] | loss: 0.0017651228699833155\n",
      "Iteration [4994] | loss: 0.0017660749144852161\n",
      "Iteration [4995] | loss: 0.0017670269589871168\n",
      "Iteration [4996] | loss: 0.0017679788870736957\n",
      "Iteration [4997] | loss: 0.0017689309315755963\n",
      "Iteration [4998] | loss: 0.001769763883203268\n",
      "Iteration [4999] | loss: 0.0017708349041640759\n",
      "Iteration [5000] | loss: 0.0017716678557917476\n",
      "Iteration [5001] | loss: 0.0017727388767525554\n",
      "Iteration [5002] | loss: 0.001773571828380227\n",
      "Iteration [5003] | loss: 0.001774523756466806\n",
      "Iteration [5004] | loss: 0.0017754758009687066\n",
      "Iteration [5005] | loss: 0.0017764277290552855\n",
      "Iteration [5006] | loss: 0.0017774987500160933\n",
      "Iteration [5007] | loss: 0.001778450678102672\n",
      "Iteration [5008] | loss: 0.0017794027226045728\n",
      "Iteration [5009] | loss: 0.0017803546506911516\n",
      "Iteration [5010] | loss: 0.0017811877187341452\n",
      "Iteration [5011] | loss: 0.0017822586232796311\n",
      "Iteration [5012] | loss: 0.0017830915749073029\n",
      "Iteration [5013] | loss: 0.0017841625958681107\n",
      "Iteration [5014] | loss: 0.0017849955474957824\n",
      "Iteration [5015] | loss: 0.0017860665684565902\n",
      "Iteration [5016] | loss: 0.001786899520084262\n",
      "Iteration [5017] | loss: 0.0017878514481708407\n",
      "Iteration [5018] | loss: 0.0017889224691316485\n",
      "Iteration [5019] | loss: 0.0017897554207593203\n",
      "Iteration [5020] | loss: 0.0017909454181790352\n",
      "Iteration [5021] | loss: 0.001791778369806707\n",
      "Iteration [5022] | loss: 0.0017928492743521929\n",
      "Iteration [5023] | loss: 0.0017938013188540936\n",
      "Iteration [5024] | loss: 0.0017946342704817653\n",
      "Iteration [5025] | loss: 0.0017957051750272512\n",
      "Iteration [5026] | loss: 0.001796538126654923\n",
      "Iteration [5027] | loss: 0.0017976091476157308\n",
      "Iteration [5028] | loss: 0.0017985610757023096\n",
      "Iteration [5029] | loss: 0.0017996320966631174\n",
      "Iteration [5030] | loss: 0.0018005840247496963\n",
      "Iteration [5031] | loss: 0.001801416976377368\n",
      "Iteration [5032] | loss: 0.001802487880922854\n",
      "Iteration [5033] | loss: 0.0018034399254247546\n",
      "Iteration [5034] | loss: 0.0018043918535113335\n",
      "Iteration [5035] | loss: 0.0018053437815979123\n",
      "Iteration [5036] | loss: 0.0018065337790176272\n",
      "Iteration [5037] | loss: 0.001807366730645299\n",
      "Iteration [5038] | loss: 0.0018083186587318778\n",
      "Iteration [5039] | loss: 0.0018093895632773638\n",
      "Iteration [5040] | loss: 0.0018102225149050355\n",
      "Iteration [5041] | loss: 0.0018112935358658433\n",
      "Iteration [5042] | loss: 0.0018123644404113293\n",
      "Iteration [5043] | loss: 0.0018134353449568152\n",
      "Iteration [5044] | loss: 0.001814268296584487\n",
      "Iteration [5045] | loss: 0.0018152202246710658\n",
      "Iteration [5046] | loss: 0.0018162912456318736\n",
      "Iteration [5047] | loss: 0.0018171241972595453\n",
      "Iteration [5048] | loss: 0.0018183140782639384\n",
      "Iteration [5049] | loss: 0.0018192660063505173\n",
      "Iteration [5050] | loss: 0.0018203369108960032\n",
      "Iteration [5051] | loss: 0.001821169862523675\n",
      "Iteration [5052] | loss: 0.0018222408834844828\n",
      "Iteration [5053] | loss: 0.0018233117880299687\n",
      "Iteration [5054] | loss: 0.0018242637161165476\n",
      "Iteration [5055] | loss: 0.0018252156442031264\n",
      "Iteration [5056] | loss: 0.0018261675722897053\n",
      "Iteration [5057] | loss: 0.0018273574532940984\n",
      "Iteration [5058] | loss: 0.0018283093813806772\n",
      "Iteration [5059] | loss: 0.001829380402341485\n",
      "Iteration [5060] | loss: 0.001830213237553835\n",
      "Iteration [5061] | loss: 0.0018312842585146427\n",
      "Iteration [5062] | loss: 0.0018323551630601287\n",
      "Iteration [5063] | loss: 0.0018333070911467075\n",
      "Iteration [5064] | loss: 0.0018343779956921935\n",
      "Iteration [5065] | loss: 0.0018352109473198652\n",
      "Iteration [5066] | loss: 0.0018364008283242583\n",
      "Iteration [5067] | loss: 0.0018373527564108372\n",
      "Iteration [5068] | loss: 0.0018384236609563231\n",
      "Iteration [5069] | loss: 0.001839375589042902\n",
      "Iteration [5070] | loss: 0.001840565470047295\n",
      "Iteration [5071] | loss: 0.001841517398133874\n",
      "Iteration [5072] | loss: 0.0018423503497615457\n",
      "Iteration [5073] | loss: 0.0018435402307659388\n",
      "Iteration [5074] | loss: 0.0018444921588525176\n",
      "Iteration [5075] | loss: 0.0018455630633980036\n",
      "Iteration [5076] | loss: 0.0018465149914845824\n",
      "Iteration [5077] | loss: 0.0018477048724889755\n",
      "Iteration [5078] | loss: 0.0018486568005755544\n",
      "Iteration [5079] | loss: 0.0018497277051210403\n",
      "Iteration [5080] | loss: 0.0018507986096665263\n",
      "Iteration [5081] | loss: 0.0018516314448788762\n",
      "Iteration [5082] | loss: 0.0018527023494243622\n",
      "Iteration [5083] | loss: 0.0018537732539698482\n",
      "Iteration [5084] | loss: 0.0018548441585153341\n",
      "Iteration [5085] | loss: 0.001855796086601913\n",
      "Iteration [5086] | loss: 0.001856985967606306\n",
      "Iteration [5087] | loss: 0.001857937895692885\n",
      "Iteration [5088] | loss: 0.001859008800238371\n",
      "Iteration [5089] | loss: 0.0018600797047838569\n",
      "Iteration [5090] | loss: 0.0018611506093293428\n",
      "Iteration [5091] | loss: 0.0018621024210005999\n",
      "Iteration [5092] | loss: 0.0018631733255460858\n",
      "Iteration [5093] | loss: 0.0018642442300915718\n",
      "Iteration [5094] | loss: 0.0018651961581781507\n",
      "Iteration [5095] | loss: 0.0018663860391825438\n",
      "Iteration [5096] | loss: 0.0018673378508538008\n",
      "Iteration [5097] | loss: 0.0018684087553992867\n",
      "Iteration [5098] | loss: 0.0018694796599447727\n",
      "Iteration [5099] | loss: 0.0018705505644902587\n",
      "Iteration [5100] | loss: 0.0018716213526204228\n",
      "Iteration [5101] | loss: 0.0018726922571659088\n",
      "Iteration [5102] | loss: 0.0018736441852524877\n",
      "Iteration [5103] | loss: 0.0018747150897979736\n",
      "Iteration [5104] | loss: 0.0018757858779281378\n",
      "Iteration [5105] | loss: 0.0018767378060147166\n",
      "Iteration [5106] | loss: 0.0018779276870191097\n",
      "Iteration [5107] | loss: 0.0018788794986903667\n",
      "Iteration [5108] | loss: 0.001880188356153667\n",
      "Iteration [5109] | loss: 0.0018811402842402458\n",
      "Iteration [5110] | loss: 0.001882330165244639\n",
      "Iteration [5111] | loss: 0.001883281976915896\n",
      "Iteration [5112] | loss: 0.001884352881461382\n",
      "Iteration [5113] | loss: 0.0018854237860068679\n",
      "Iteration [5114] | loss: 0.001886494574137032\n",
      "Iteration [5115] | loss: 0.001887565478682518\n",
      "Iteration [5116] | loss: 0.001888517290353775\n",
      "Iteration [5117] | loss: 0.0018898261478170753\n",
      "Iteration [5118] | loss: 0.001890778075903654\n",
      "Iteration [5119] | loss: 0.0018918488640338182\n",
      "Iteration [5120] | loss: 0.0018929197685793042\n",
      "Iteration [5121] | loss: 0.0018939905567094684\n",
      "Iteration [5122] | loss: 0.0018950614612549543\n",
      "Iteration [5123] | loss: 0.0018962513422593474\n",
      "Iteration [5124] | loss: 0.0018973221303895116\n",
      "Iteration [5125] | loss: 0.0018983930349349976\n",
      "Iteration [5126] | loss: 0.0018994638230651617\n",
      "Iteration [5127] | loss: 0.0019005347276106477\n",
      "Iteration [5128] | loss: 0.0019016055157408118\n",
      "Iteration [5129] | loss: 0.001902795396745205\n",
      "Iteration [5130] | loss: 0.001903866184875369\n",
      "Iteration [5131] | loss: 0.001904818112961948\n",
      "Iteration [5132] | loss: 0.0019060078775510192\n",
      "Iteration [5133] | loss: 0.0019070786656811833\n",
      "Iteration [5134] | loss: 0.0019082685466855764\n",
      "Iteration [5135] | loss: 0.0019092203583568335\n",
      "Iteration [5136] | loss: 0.0019104102393612266\n",
      "Iteration [5137] | loss: 0.0019114810274913907\n",
      "Iteration [5138] | loss: 0.0019126709084957838\n",
      "Iteration [5139] | loss: 0.0019136227201670408\n",
      "Iteration [5140] | loss: 0.001914812484756112\n",
      "Iteration [5141] | loss: 0.001915883389301598\n",
      "Iteration [5142] | loss: 0.0019170731538906693\n",
      "Iteration [5143] | loss: 0.0019181440584361553\n",
      "Iteration [5144] | loss: 0.0019193338230252266\n",
      "Iteration [5145] | loss: 0.0019204046111553907\n",
      "Iteration [5146] | loss: 0.0019214755157008767\n",
      "Iteration [5147] | loss: 0.001922665280289948\n",
      "Iteration [5148] | loss: 0.0019237360684201121\n",
      "Iteration [5149] | loss: 0.0019248068565502763\n",
      "Iteration [5150] | loss: 0.0019259967375546694\n",
      "Iteration [5151] | loss: 0.0019270675256848335\n",
      "Iteration [5152] | loss: 0.0019281383138149977\n",
      "Iteration [5153] | loss: 0.0019293281948193908\n",
      "Iteration [5154] | loss: 0.001930398982949555\n",
      "Iteration [5155] | loss: 0.001931469771079719\n",
      "Iteration [5156] | loss: 0.0019326595356687903\n",
      "Iteration [5157] | loss: 0.0019338493002578616\n",
      "Iteration [5158] | loss: 0.0019349202048033476\n",
      "Iteration [5159] | loss: 0.0019361099693924189\n",
      "Iteration [5160] | loss: 0.001937180757522583\n",
      "Iteration [5161] | loss: 0.0019383705221116543\n",
      "Iteration [5162] | loss: 0.0019394413102418184\n",
      "Iteration [5163] | loss: 0.0019406310748308897\n",
      "Iteration [5164] | loss: 0.0019417019793763757\n",
      "Iteration [5165] | loss: 0.001943010720424354\n",
      "Iteration [5166] | loss: 0.001943962532095611\n",
      "Iteration [5167] | loss: 0.0019452712731435895\n",
      "Iteration [5168] | loss: 0.0019463420612737536\n",
      "Iteration [5169] | loss: 0.001947531825862825\n",
      "Iteration [5170] | loss: 0.001948602613992989\n",
      "Iteration [5171] | loss: 0.0019497923785820603\n",
      "Iteration [5172] | loss: 0.0019508631667122245\n",
      "Iteration [5173] | loss: 0.0019520529313012958\n",
      "Iteration [5174] | loss: 0.001953242812305689\n",
      "Iteration [5175] | loss: 0.00195443257689476\n",
      "Iteration [5176] | loss: 0.0019555033650249243\n",
      "Iteration [5177] | loss: 0.0019568121060729027\n",
      "Iteration [5178] | loss: 0.0019577639177441597\n",
      "Iteration [5179] | loss: 0.001958953682333231\n",
      "Iteration [5180] | loss: 0.0019601434469223022\n",
      "Iteration [5181] | loss: 0.0019613332115113735\n",
      "Iteration [5182] | loss: 0.0019624039996415377\n",
      "Iteration [5183] | loss: 0.001963593764230609\n",
      "Iteration [5184] | loss: 0.0019647832959890366\n",
      "Iteration [5185] | loss: 0.0019658540841192007\n",
      "Iteration [5186] | loss: 0.001967162825167179\n",
      "Iteration [5187] | loss: 0.0019682336132973433\n",
      "Iteration [5188] | loss: 0.0019695423543453217\n",
      "Iteration [5189] | loss: 0.0019704941660165787\n",
      "Iteration [5190] | loss: 0.001971802907064557\n",
      "Iteration [5191] | loss: 0.0019729926716536283\n",
      "Iteration [5192] | loss: 0.0019741824362426996\n",
      "Iteration [5193] | loss: 0.0019752532243728638\n",
      "Iteration [5194] | loss: 0.001976561965420842\n",
      "Iteration [5195] | loss: 0.0019776327535510063\n",
      "Iteration [5196] | loss: 0.001978941261768341\n",
      "Iteration [5197] | loss: 0.0019801310263574123\n",
      "Iteration [5198] | loss: 0.0019812018144875765\n",
      "Iteration [5199] | loss: 0.0019823915790766478\n",
      "Iteration [5200] | loss: 0.001983700320124626\n",
      "Iteration [5201] | loss: 0.0019847711082547903\n",
      "Iteration [5202] | loss: 0.0019860798493027687\n",
      "Iteration [5203] | loss: 0.001987150404602289\n",
      "Iteration [5204] | loss: 0.0019884591456502676\n",
      "Iteration [5205] | loss: 0.0019895299337804317\n",
      "Iteration [5206] | loss: 0.00199083867482841\n",
      "Iteration [5207] | loss: 0.0019920284394174814\n",
      "Iteration [5208] | loss: 0.0019932182040065527\n",
      "Iteration [5209] | loss: 0.001994288759306073\n",
      "Iteration [5210] | loss: 0.0019955975003540516\n",
      "Iteration [5211] | loss: 0.001996787264943123\n",
      "Iteration [5212] | loss: 0.0019980960059911013\n",
      "Iteration [5213] | loss: 0.0019991665612906218\n",
      "Iteration [5214] | loss: 0.0020004753023386\n",
      "Iteration [5215] | loss: 0.0020015460904687643\n",
      "Iteration [5216] | loss: 0.0020028548315167427\n",
      "Iteration [5217] | loss: 0.002004044596105814\n",
      "Iteration [5218] | loss: 0.0020053531043231487\n",
      "Iteration [5219] | loss: 0.002006423892453313\n",
      "Iteration [5220] | loss: 0.0020077326335012913\n",
      "Iteration [5221] | loss: 0.002008922165259719\n",
      "Iteration [5222] | loss: 0.0020102309063076973\n",
      "Iteration [5223] | loss: 0.0020113016944378614\n",
      "Iteration [5224] | loss: 0.00201261043548584\n",
      "Iteration [5225] | loss: 0.0020139189437031746\n",
      "Iteration [5226] | loss: 0.0020149897318333387\n",
      "Iteration [5227] | loss: 0.002016298472881317\n",
      "Iteration [5228] | loss: 0.0020174880046397448\n",
      "Iteration [5229] | loss: 0.002018796745687723\n",
      "Iteration [5230] | loss: 0.0020199865102767944\n",
      "Iteration [5231] | loss: 0.002021295018494129\n",
      "Iteration [5232] | loss: 0.0020224847830832005\n",
      "Iteration [5233] | loss: 0.002023793524131179\n",
      "Iteration [5234] | loss: 0.0020249830558896065\n",
      "Iteration [5235] | loss: 0.002026291796937585\n",
      "Iteration [5236] | loss: 0.0020274813286960125\n",
      "Iteration [5237] | loss: 0.002028790069743991\n",
      "Iteration [5238] | loss: 0.002029979834333062\n",
      "Iteration [5239] | loss: 0.002031288342550397\n",
      "Iteration [5240] | loss: 0.002032478107139468\n",
      "Iteration [5241] | loss: 0.002033786615356803\n",
      "Iteration [5242] | loss: 0.002034976379945874\n",
      "Iteration [5243] | loss: 0.0020362851209938526\n",
      "Iteration [5244] | loss: 0.0020374746527522802\n",
      "Iteration [5245] | loss: 0.0020387833938002586\n",
      "Iteration [5246] | loss: 0.0020399729255586863\n",
      "Iteration [5247] | loss: 0.0020412816666066647\n",
      "Iteration [5248] | loss: 0.0020424711983650923\n",
      "Iteration [5249] | loss: 0.0020437799394130707\n",
      "Iteration [5250] | loss: 0.0020449694711714983\n",
      "Iteration [5251] | loss: 0.0020462782122194767\n",
      "Iteration [5252] | loss: 0.0020474677439779043\n",
      "Iteration [5253] | loss: 0.0020487764850258827\n",
      "Iteration [5254] | loss: 0.0020500849932432175\n",
      "Iteration [5255] | loss: 0.002051393734291196\n",
      "Iteration [5256] | loss: 0.0020527022425085306\n",
      "Iteration [5257] | loss: 0.002053892007097602\n",
      "Iteration [5258] | loss: 0.0020552005153149366\n",
      "Iteration [5259] | loss: 0.002056390279904008\n",
      "Iteration [5260] | loss: 0.0020576987881213427\n",
      "Iteration [5261] | loss: 0.002058888552710414\n",
      "Iteration [5262] | loss: 0.002060316037386656\n",
      "Iteration [5263] | loss: 0.0020616245456039906\n",
      "Iteration [5264] | loss: 0.002062933286651969\n",
      "Iteration [5265] | loss: 0.0020641228184103966\n",
      "Iteration [5266] | loss: 0.002065431559458375\n",
      "Iteration [5267] | loss: 0.0020667400676757097\n",
      "Iteration [5268] | loss: 0.0020680485758930445\n",
      "Iteration [5269] | loss: 0.0020692383404821157\n",
      "Iteration [5270] | loss: 0.0020705468486994505\n",
      "Iteration [5271] | loss: 0.0020719743333756924\n",
      "Iteration [5272] | loss: 0.0020732830744236708\n",
      "Iteration [5273] | loss: 0.0020745915826410055\n",
      "Iteration [5274] | loss: 0.002075781114399433\n",
      "Iteration [5275] | loss: 0.0020772088319063187\n",
      "Iteration [5276] | loss: 0.0020783983636647463\n",
      "Iteration [5277] | loss: 0.002079706871882081\n",
      "Iteration [5278] | loss: 0.0020811345893889666\n",
      "Iteration [5279] | loss: 0.0020824430976063013\n",
      "Iteration [5280] | loss: 0.002083632629364729\n",
      "Iteration [5281] | loss: 0.002085060114040971\n",
      "Iteration [5282] | loss: 0.002086249878630042\n",
      "Iteration [5283] | loss: 0.002087677363306284\n",
      "Iteration [5284] | loss: 0.0020889858715236187\n",
      "Iteration [5285] | loss: 0.0020902943797409534\n",
      "Iteration [5286] | loss: 0.002091603120788932\n",
      "Iteration [5287] | loss: 0.0020929116290062666\n",
      "Iteration [5288] | loss: 0.0020943391136825085\n",
      "Iteration [5289] | loss: 0.002095528645440936\n",
      "Iteration [5290] | loss: 0.002096956130117178\n",
      "Iteration [5291] | loss: 0.0020981458947062492\n",
      "Iteration [5292] | loss: 0.0020996923558413982\n",
      "Iteration [5293] | loss: 0.002100881887599826\n",
      "Iteration [5294] | loss: 0.0021023093722760677\n",
      "Iteration [5295] | loss: 0.0021034989040344954\n",
      "Iteration [5296] | loss: 0.0021050453651696444\n",
      "Iteration [5297] | loss: 0.0021063541062176228\n",
      "Iteration [5298] | loss: 0.0021075436379760504\n",
      "Iteration [5299] | loss: 0.0021089711226522923\n",
      "Iteration [5300] | loss: 0.002110279630869627\n",
      "Iteration [5301] | loss: 0.002111707115545869\n",
      "Iteration [5302] | loss: 0.0021128966473042965\n",
      "Iteration [5303] | loss: 0.0021144431084394455\n",
      "Iteration [5304] | loss: 0.0021157516166567802\n",
      "Iteration [5305] | loss: 0.002117060124874115\n",
      "Iteration [5306] | loss: 0.002118487609550357\n",
      "Iteration [5307] | loss: 0.0021197961177676916\n",
      "Iteration [5308] | loss: 0.0021212236024439335\n",
      "Iteration [5309] | loss: 0.0021225321106612682\n",
      "Iteration [5310] | loss: 0.00212395959533751\n",
      "Iteration [5311] | loss: 0.002125268103554845\n",
      "Iteration [5312] | loss: 0.0021266955882310867\n",
      "Iteration [5313] | loss: 0.0021280040964484215\n",
      "Iteration [5314] | loss: 0.0021293126046657562\n",
      "Iteration [5315] | loss: 0.002130740089341998\n",
      "Iteration [5316] | loss: 0.00213216757401824\n",
      "Iteration [5317] | loss: 0.002133595058694482\n",
      "Iteration [5318] | loss: 0.0021349035669118166\n",
      "Iteration [5319] | loss: 0.0021363310515880585\n",
      "Iteration [5320] | loss: 0.0021376395598053932\n",
      "Iteration [5321] | loss: 0.002139067044481635\n",
      "Iteration [5322] | loss: 0.002140494529157877\n",
      "Iteration [5323] | loss: 0.0021418030373752117\n",
      "Iteration [5324] | loss: 0.0021431115455925465\n",
      "Iteration [5325] | loss: 0.002144657773897052\n",
      "Iteration [5326] | loss: 0.0021460852585732937\n",
      "Iteration [5327] | loss: 0.0021473937667906284\n",
      "Iteration [5328] | loss: 0.0021488212514668703\n",
      "Iteration [5329] | loss: 0.002150248736143112\n",
      "Iteration [5330] | loss: 0.002151676220819354\n",
      "Iteration [5331] | loss: 0.002152984729036689\n",
      "Iteration [5332] | loss: 0.002154411980882287\n",
      "Iteration [5333] | loss: 0.002155958442017436\n",
      "Iteration [5334] | loss: 0.0021572669502347708\n",
      "Iteration [5335] | loss: 0.0021588134113699198\n",
      "Iteration [5336] | loss: 0.0021600027102977037\n",
      "Iteration [5337] | loss: 0.0021615491714328527\n",
      "Iteration [5338] | loss: 0.0021628576796501875\n",
      "Iteration [5339] | loss: 0.0021644041407853365\n",
      "Iteration [5340] | loss: 0.0021658313926309347\n",
      "Iteration [5341] | loss: 0.0021672588773071766\n",
      "Iteration [5342] | loss: 0.0021686863619834185\n",
      "Iteration [5343] | loss: 0.0021699946373701096\n",
      "Iteration [5344] | loss: 0.0021714221220463514\n",
      "Iteration [5345] | loss: 0.0021728496067225933\n",
      "Iteration [5346] | loss: 0.0021742768585681915\n",
      "Iteration [5347] | loss: 0.0021758233197033405\n",
      "Iteration [5348] | loss: 0.0021771318279206753\n",
      "Iteration [5349] | loss: 0.0021786780562251806\n",
      "Iteration [5350] | loss: 0.0021801055409014225\n",
      "Iteration [5351] | loss: 0.0021815330255776644\n",
      "Iteration [5352] | loss: 0.0021829602774232626\n",
      "Iteration [5353] | loss: 0.0021843877620995045\n",
      "Iteration [5354] | loss: 0.00218593399040401\n",
      "Iteration [5355] | loss: 0.0021873614750802517\n",
      "Iteration [5356] | loss: 0.00218878872692585\n",
      "Iteration [5357] | loss: 0.002190216211602092\n",
      "Iteration [5358] | loss: 0.00219164346344769\n",
      "Iteration [5359] | loss: 0.002193070948123932\n",
      "Iteration [5360] | loss: 0.0021946171764284372\n",
      "Iteration [5361] | loss: 0.002196044661104679\n",
      "Iteration [5362] | loss: 0.0021974719129502773\n",
      "Iteration [5363] | loss: 0.0021990183740854263\n",
      "Iteration [5364] | loss: 0.0022003266494721174\n",
      "Iteration [5365] | loss: 0.0022019920870661736\n",
      "Iteration [5366] | loss: 0.002203538315370679\n",
      "Iteration [5367] | loss: 0.0022048468235880136\n",
      "Iteration [5368] | loss: 0.002206393051892519\n",
      "Iteration [5369] | loss: 0.0022078203037381172\n",
      "Iteration [5370] | loss: 0.002209247788414359\n",
      "Iteration [5371] | loss: 0.0022107940167188644\n",
      "Iteration [5372] | loss: 0.00221234024502337\n",
      "Iteration [5373] | loss: 0.002213886706158519\n",
      "Iteration [5374] | loss: 0.00221519498154521\n",
      "Iteration [5375] | loss: 0.002216741442680359\n",
      "Iteration [5376] | loss: 0.0022182876709848642\n",
      "Iteration [5377] | loss: 0.0022198338992893696\n",
      "Iteration [5378] | loss: 0.002221261151134968\n",
      "Iteration [5379] | loss: 0.0022226886358112097\n",
      "Iteration [5380] | loss: 0.002224353840574622\n",
      "Iteration [5381] | loss: 0.0022257810924202204\n",
      "Iteration [5382] | loss: 0.0022272085770964622\n",
      "Iteration [5383] | loss: 0.0022287548054009676\n",
      "Iteration [5384] | loss: 0.002230301033705473\n",
      "Iteration [5385] | loss: 0.0022318472620099783\n",
      "Iteration [5386] | loss: 0.0022332745138555765\n",
      "Iteration [5387] | loss: 0.0022348209749907255\n",
      "Iteration [5388] | loss: 0.002236367203295231\n",
      "Iteration [5389] | loss: 0.002237794455140829\n",
      "Iteration [5390] | loss: 0.0022394596599042416\n",
      "Iteration [5391] | loss: 0.0022408869117498398\n",
      "Iteration [5392] | loss: 0.0022425521165132523\n",
      "Iteration [5393] | loss: 0.0022439793683588505\n",
      "Iteration [5394] | loss: 0.0022454068530350924\n",
      "Iteration [5395] | loss: 0.002247072057798505\n",
      "Iteration [5396] | loss: 0.002248499309644103\n",
      "Iteration [5397] | loss: 0.0022501645144075155\n",
      "Iteration [5398] | loss: 0.002251710742712021\n",
      "Iteration [5399] | loss: 0.0022532569710165262\n",
      "Iteration [5400] | loss: 0.0022548031993210316\n",
      "Iteration [5401] | loss: 0.00225623045116663\n",
      "Iteration [5402] | loss: 0.002257776679471135\n",
      "Iteration [5403] | loss: 0.0022594418842345476\n",
      "Iteration [5404] | loss: 0.002260869136080146\n",
      "Iteration [5405] | loss: 0.0022625343408435583\n",
      "Iteration [5406] | loss: 0.0022639615926891565\n",
      "Iteration [5407] | loss: 0.0022656265646219254\n",
      "Iteration [5408] | loss: 0.0022671727929264307\n",
      "Iteration [5409] | loss: 0.002268719021230936\n",
      "Iteration [5410] | loss: 0.0022702652495354414\n",
      "Iteration [5411] | loss: 0.0022718114778399467\n",
      "Iteration [5412] | loss: 0.0022734766826033592\n",
      "Iteration [5413] | loss: 0.0022750229109078646\n",
      "Iteration [5414] | loss: 0.00227656913921237\n",
      "Iteration [5415] | loss: 0.0022782341111451387\n",
      "Iteration [5416] | loss: 0.002279780339449644\n",
      "Iteration [5417] | loss: 0.0022813265677541494\n",
      "Iteration [5418] | loss: 0.002282991772517562\n",
      "Iteration [5419] | loss: 0.00228441902436316\n",
      "Iteration [5420] | loss: 0.002286083996295929\n",
      "Iteration [5421] | loss: 0.0022877492010593414\n",
      "Iteration [5422] | loss: 0.0022891764529049397\n",
      "Iteration [5423] | loss: 0.0022908414248377085\n",
      "Iteration [5424] | loss: 0.002292506629601121\n",
      "Iteration [5425] | loss: 0.0022940528579056263\n",
      "Iteration [5426] | loss: 0.0022955990862101316\n",
      "Iteration [5427] | loss: 0.0022971450816839933\n",
      "Iteration [5428] | loss: 0.002298810286447406\n",
      "Iteration [5429] | loss: 0.0023004752583801746\n",
      "Iteration [5430] | loss: 0.002302140463143587\n",
      "Iteration [5431] | loss: 0.0023036866914480925\n",
      "Iteration [5432] | loss: 0.0023053516633808613\n",
      "Iteration [5433] | loss: 0.0023068978916853666\n",
      "Iteration [5434] | loss: 0.0023085628636181355\n",
      "Iteration [5435] | loss: 0.002310109091922641\n",
      "Iteration [5436] | loss: 0.0023117740638554096\n",
      "Iteration [5437] | loss: 0.002313439268618822\n",
      "Iteration [5438] | loss: 0.002315104240551591\n",
      "Iteration [5439] | loss: 0.0023166504688560963\n",
      "Iteration [5440] | loss: 0.002318315440788865\n",
      "Iteration [5441] | loss: 0.0023198616690933704\n",
      "Iteration [5442] | loss: 0.0023215266410261393\n",
      "Iteration [5443] | loss: 0.0023231918457895517\n",
      "Iteration [5444] | loss: 0.0023248568177223206\n",
      "Iteration [5445] | loss: 0.0023265217896550894\n",
      "Iteration [5446] | loss: 0.0023280680179595947\n",
      "Iteration [5447] | loss: 0.0023297329898923635\n",
      "Iteration [5448] | loss: 0.0023315169382840395\n",
      "Iteration [5449] | loss: 0.002333063166588545\n",
      "Iteration [5450] | loss: 0.0023347281385213137\n",
      "Iteration [5451] | loss: 0.0023365120869129896\n",
      "Iteration [5452] | loss: 0.0023381770588457584\n",
      "Iteration [5453] | loss: 0.002339842263609171\n",
      "Iteration [5454] | loss: 0.0023413882590830326\n",
      "Iteration [5455] | loss: 0.0023430532310158014\n",
      "Iteration [5456] | loss: 0.0023448371794074774\n",
      "Iteration [5457] | loss: 0.0023463834077119827\n",
      "Iteration [5458] | loss: 0.0023480483796447515\n",
      "Iteration [5459] | loss: 0.0023498323280364275\n",
      "Iteration [5460] | loss: 0.0023514972999691963\n",
      "Iteration [5461] | loss: 0.002353162271901965\n",
      "Iteration [5462] | loss: 0.002354946220293641\n",
      "Iteration [5463] | loss: 0.0023564924485981464\n",
      "Iteration [5464] | loss: 0.0023581574205309153\n",
      "Iteration [5465] | loss: 0.0023600601125508547\n",
      "Iteration [5466] | loss: 0.00236160634085536\n",
      "Iteration [5467] | loss: 0.002363271312788129\n",
      "Iteration [5468] | loss: 0.002365055261179805\n",
      "Iteration [5469] | loss: 0.0023667202331125736\n",
      "Iteration [5470] | loss: 0.0023685041815042496\n",
      "Iteration [5471] | loss: 0.0023700501769781113\n",
      "Iteration [5472] | loss: 0.002371834125369787\n",
      "Iteration [5473] | loss: 0.0023736178409308195\n",
      "Iteration [5474] | loss: 0.002375164069235325\n",
      "Iteration [5475] | loss: 0.0023770667612552643\n",
      "Iteration [5476] | loss: 0.002378731733188033\n",
      "Iteration [5477] | loss: 0.002380515681579709\n",
      "Iteration [5478] | loss: 0.002382180653512478\n",
      "Iteration [5479] | loss: 0.002383964601904154\n",
      "Iteration [5480] | loss: 0.002385629341006279\n",
      "Iteration [5481] | loss: 0.002387294312939048\n",
      "Iteration [5482] | loss: 0.002389197237789631\n",
      "Iteration [5483] | loss: 0.0023907432332634926\n",
      "Iteration [5484] | loss: 0.002392645925283432\n",
      "Iteration [5485] | loss: 0.002394310897216201\n",
      "Iteration [5486] | loss: 0.0023960948456078768\n",
      "Iteration [5487] | loss: 0.002397878561168909\n",
      "Iteration [5488] | loss: 0.002399662509560585\n",
      "Iteration [5489] | loss: 0.002401327481493354\n",
      "Iteration [5490] | loss: 0.002403111197054386\n",
      "Iteration [5491] | loss: 0.002404895145446062\n",
      "Iteration [5492] | loss: 0.002406560117378831\n",
      "Iteration [5493] | loss: 0.0024084628093987703\n",
      "Iteration [5494] | loss: 0.002410127781331539\n",
      "Iteration [5495] | loss: 0.0024119114968925714\n",
      "Iteration [5496] | loss: 0.0024138144217431545\n",
      "Iteration [5497] | loss: 0.0024154791608452797\n",
      "Iteration [5498] | loss: 0.0024172631092369556\n",
      "Iteration [5499] | loss: 0.002419165801256895\n",
      "Iteration [5500] | loss: 0.002420830773189664\n",
      "Iteration [5501] | loss: 0.002422614488750696\n",
      "Iteration [5502] | loss: 0.002424398437142372\n",
      "Iteration [5503] | loss: 0.0024261821527034044\n",
      "Iteration [5504] | loss: 0.0024279658682644367\n",
      "Iteration [5505] | loss: 0.0024297498166561127\n",
      "Iteration [5506] | loss: 0.002431652508676052\n",
      "Iteration [5507] | loss: 0.0024334362242370844\n",
      "Iteration [5508] | loss: 0.0024352199397981167\n",
      "Iteration [5509] | loss: 0.0024370038881897926\n",
      "Iteration [5510] | loss: 0.002438787603750825\n",
      "Iteration [5511] | loss: 0.0024406902957707644\n",
      "Iteration [5512] | loss: 0.0024424740113317966\n",
      "Iteration [5513] | loss: 0.0024442579597234726\n",
      "Iteration [5514] | loss: 0.002446160651743412\n",
      "Iteration [5515] | loss: 0.0024479443673044443\n",
      "Iteration [5516] | loss: 0.0024498470593243837\n",
      "Iteration [5517] | loss: 0.002451511798426509\n",
      "Iteration [5518] | loss: 0.0024534144904464483\n",
      "Iteration [5519] | loss: 0.0024553171824663877\n",
      "Iteration [5520] | loss: 0.00245710089802742\n",
      "Iteration [5521] | loss: 0.0024588846135884523\n",
      "Iteration [5522] | loss: 0.0024607873056083918\n",
      "Iteration [5523] | loss: 0.002462571021169424\n",
      "Iteration [5524] | loss: 0.0024644737131893635\n",
      "Iteration [5525] | loss: 0.002466376405209303\n",
      "Iteration [5526] | loss: 0.002468160120770335\n",
      "Iteration [5527] | loss: 0.0024700628127902746\n",
      "Iteration [5528] | loss: 0.002471965504810214\n",
      "Iteration [5529] | loss: 0.0024738681968301535\n",
      "Iteration [5530] | loss: 0.0024756519123911858\n",
      "Iteration [5531] | loss: 0.0024775543715804815\n",
      "Iteration [5532] | loss: 0.002479457063600421\n",
      "Iteration [5533] | loss: 0.0024812407791614532\n",
      "Iteration [5534] | loss: 0.0024831434711813927\n",
      "Iteration [5535] | loss: 0.0024851649068295956\n",
      "Iteration [5536] | loss: 0.002486948622390628\n",
      "Iteration [5537] | loss: 0.0024888513144105673\n",
      "Iteration [5538] | loss: 0.002490753773599863\n",
      "Iteration [5539] | loss: 0.0024927754420787096\n",
      "Iteration [5540] | loss: 0.0024945589248090982\n",
      "Iteration [5541] | loss: 0.0024964616168290377\n",
      "Iteration [5542] | loss: 0.002498364308848977\n",
      "Iteration [5543] | loss: 0.002500266768038273\n",
      "Iteration [5544] | loss: 0.0025021694600582123\n",
      "Iteration [5545] | loss: 0.002504071919247508\n",
      "Iteration [5546] | loss: 0.0025059746112674475\n",
      "Iteration [5547] | loss: 0.0025078770704567432\n",
      "Iteration [5548] | loss: 0.002509898506104946\n",
      "Iteration [5549] | loss: 0.0025118011981248856\n",
      "Iteration [5550] | loss: 0.002513584913685918\n",
      "Iteration [5551] | loss: 0.0025156063493341208\n",
      "Iteration [5552] | loss: 0.0025175088085234165\n",
      "Iteration [5553] | loss: 0.002519411500543356\n",
      "Iteration [5554] | loss: 0.0025213139597326517\n",
      "Iteration [5555] | loss: 0.0025233353953808546\n",
      "Iteration [5556] | loss: 0.0025253568310290575\n",
      "Iteration [5557] | loss: 0.0025272592902183533\n",
      "Iteration [5558] | loss: 0.0025291619822382927\n",
      "Iteration [5559] | loss: 0.0025311834178864956\n",
      "Iteration [5560] | loss: 0.0025329669006168842\n",
      "Iteration [5561] | loss: 0.002534988336265087\n",
      "Iteration [5562] | loss: 0.002537128748372197\n",
      "Iteration [5563] | loss: 0.002539031207561493\n",
      "Iteration [5564] | loss: 0.0025409336667507887\n",
      "Iteration [5565] | loss: 0.0025429551023989916\n",
      "Iteration [5566] | loss: 0.0025448575615882874\n",
      "Iteration [5567] | loss: 0.0025469979736953974\n",
      "Iteration [5568] | loss: 0.002548900432884693\n",
      "Iteration [5569] | loss: 0.002550802892073989\n",
      "Iteration [5570] | loss: 0.002552824327722192\n",
      "Iteration [5571] | loss: 0.0025548457633703947\n",
      "Iteration [5572] | loss: 0.0025568671990185976\n",
      "Iteration [5573] | loss: 0.0025587696582078934\n",
      "Iteration [5574] | loss: 0.0025609098374843597\n",
      "Iteration [5575] | loss: 0.0025628122966736555\n",
      "Iteration [5576] | loss: 0.0025648337323218584\n",
      "Iteration [5577] | loss: 0.0025668551679700613\n",
      "Iteration [5578] | loss: 0.0025689953472465277\n",
      "Iteration [5579] | loss: 0.0025708978064358234\n",
      "Iteration [5580] | loss: 0.0025729192420840263\n",
      "Iteration [5581] | loss: 0.0025749404449015856\n",
      "Iteration [5582] | loss: 0.0025769618805497885\n",
      "Iteration [5583] | loss: 0.0025789830833673477\n",
      "Iteration [5584] | loss: 0.0025810045190155506\n",
      "Iteration [5585] | loss: 0.0025830259546637535\n",
      "Iteration [5586] | loss: 0.00258516613394022\n",
      "Iteration [5587] | loss: 0.0025870685931295156\n",
      "Iteration [5588] | loss: 0.002589208772405982\n",
      "Iteration [5589] | loss: 0.0025912299752235413\n",
      "Iteration [5590] | loss: 0.002593251410871744\n",
      "Iteration [5591] | loss: 0.0025952726136893034\n",
      "Iteration [5592] | loss: 0.002597531769424677\n",
      "Iteration [5593] | loss: 0.002599552972242236\n",
      "Iteration [5594] | loss: 0.002601574407890439\n",
      "Iteration [5595] | loss: 0.0026035956107079983\n",
      "Iteration [5596] | loss: 0.0026057357899844646\n",
      "Iteration [5597] | loss: 0.0026077572256326675\n",
      "Iteration [5598] | loss: 0.002609778428450227\n",
      "Iteration [5599] | loss: 0.002611918607726693\n",
      "Iteration [5600] | loss: 0.0026140587870031595\n",
      "Iteration [5601] | loss: 0.0026160799898207188\n",
      "Iteration [5602] | loss: 0.002618220169097185\n",
      "Iteration [5603] | loss: 0.0026203603483736515\n",
      "Iteration [5604] | loss: 0.0026223815511912107\n",
      "Iteration [5605] | loss: 0.002624521730467677\n",
      "Iteration [5606] | loss: 0.0026266619097441435\n",
      "Iteration [5607] | loss: 0.0026286831125617027\n",
      "Iteration [5608] | loss: 0.0026309420354664326\n",
      "Iteration [5609] | loss: 0.0026329634711146355\n",
      "Iteration [5610] | loss: 0.002635103417560458\n",
      "Iteration [5611] | loss: 0.0026372435968369246\n",
      "Iteration [5612] | loss: 0.002639264799654484\n",
      "Iteration [5613] | loss: 0.0026415237225592136\n",
      "Iteration [5614] | loss: 0.00264366390183568\n",
      "Iteration [5615] | loss: 0.0026458040811121464\n",
      "Iteration [5616] | loss: 0.0026478252839297056\n",
      "Iteration [5617] | loss: 0.0026500842068344355\n",
      "Iteration [5618] | loss: 0.002652224386110902\n",
      "Iteration [5619] | loss: 0.0026543643325567245\n",
      "Iteration [5620] | loss: 0.002656385535374284\n",
      "Iteration [5621] | loss: 0.0026586444582790136\n",
      "Iteration [5622] | loss: 0.0026609033811837435\n",
      "Iteration [5623] | loss: 0.00266304356046021\n",
      "Iteration [5624] | loss: 0.002665064763277769\n",
      "Iteration [5625] | loss: 0.002667323686182499\n",
      "Iteration [5626] | loss: 0.0026694636326283216\n",
      "Iteration [5627] | loss: 0.0026717225555330515\n",
      "Iteration [5628] | loss: 0.0026739814784377813\n",
      "Iteration [5629] | loss: 0.0026761216577142477\n",
      "Iteration [5630] | loss: 0.0026782616041600704\n",
      "Iteration [5631] | loss: 0.0026805205270648003\n",
      "Iteration [5632] | loss: 0.002682660473510623\n",
      "Iteration [5633] | loss: 0.0026848006527870893\n",
      "Iteration [5634] | loss: 0.002687059575691819\n",
      "Iteration [5635] | loss: 0.002689318498596549\n",
      "Iteration [5636] | loss: 0.0026915771886706352\n",
      "Iteration [5637] | loss: 0.0026937173679471016\n",
      "Iteration [5638] | loss: 0.0026958573143929243\n",
      "Iteration [5639] | loss: 0.002698116237297654\n",
      "Iteration [5640] | loss: 0.0027003749273717403\n",
      "Iteration [5641] | loss: 0.00270263385027647\n",
      "Iteration [5642] | loss: 0.0027048927731812\n",
      "Iteration [5643] | loss: 0.0027070327196270227\n",
      "Iteration [5644] | loss: 0.0027092916425317526\n",
      "Iteration [5645] | loss: 0.002711669309064746\n",
      "Iteration [5646] | loss: 0.0027139282319694757\n",
      "Iteration [5647] | loss: 0.002716186922043562\n",
      "Iteration [5648] | loss: 0.0027183268684893847\n",
      "Iteration [5649] | loss: 0.002720704535022378\n",
      "Iteration [5650] | loss: 0.002722963457927108\n",
      "Iteration [5651] | loss: 0.002725222148001194\n",
      "Iteration [5652] | loss: 0.002727600047364831\n",
      "Iteration [5653] | loss: 0.00272973976098001\n",
      "Iteration [5654] | loss: 0.00273199868388474\n",
      "Iteration [5655] | loss: 0.002734257373958826\n",
      "Iteration [5656] | loss: 0.0027367540169507265\n",
      "Iteration [5657] | loss: 0.0027390127070248127\n",
      "Iteration [5658] | loss: 0.0027411526534706354\n",
      "Iteration [5659] | loss: 0.0027435303200036287\n",
      "Iteration [5660] | loss: 0.0027457892429083586\n",
      "Iteration [5661] | loss: 0.0027481666766107082\n",
      "Iteration [5662] | loss: 0.0027505443431437016\n",
      "Iteration [5663] | loss: 0.0027526842895895243\n",
      "Iteration [5664] | loss: 0.002755180699750781\n",
      "Iteration [5665] | loss: 0.002757439622655511\n",
      "Iteration [5666] | loss: 0.0027598170563578606\n",
      "Iteration [5667] | loss: 0.0027620759792625904\n",
      "Iteration [5668] | loss: 0.002764572389423847\n",
      "Iteration [5669] | loss: 0.0027667121030390263\n",
      "Iteration [5670] | loss: 0.0027690897695720196\n",
      "Iteration [5671] | loss: 0.002771467436105013\n",
      "Iteration [5672] | loss: 0.002773845102638006\n",
      "Iteration [5673] | loss: 0.002776341512799263\n",
      "Iteration [5674] | loss: 0.002778600202873349\n",
      "Iteration [5675] | loss: 0.0027808588929474354\n",
      "Iteration [5676] | loss: 0.002783355303108692\n",
      "Iteration [5677] | loss: 0.0027856139931827784\n",
      "Iteration [5678] | loss: 0.002788110403344035\n",
      "Iteration [5679] | loss: 0.002790487837046385\n",
      "Iteration [5680] | loss: 0.0027929842472076416\n",
      "Iteration [5681] | loss: 0.0027951241936534643\n",
      "Iteration [5682] | loss: 0.002797620603814721\n",
      "Iteration [5683] | loss: 0.0027999980375170708\n",
      "Iteration [5684] | loss: 0.0028024944476783276\n",
      "Iteration [5685] | loss: 0.0028048718813806772\n",
      "Iteration [5686] | loss: 0.002807368291541934\n",
      "Iteration [5687] | loss: 0.00280962698161602\n",
      "Iteration [5688] | loss: 0.002812123391777277\n",
      "Iteration [5689] | loss: 0.00281461956910789\n",
      "Iteration [5690] | loss: 0.0028169972356408834\n",
      "Iteration [5691] | loss: 0.0028194934129714966\n",
      "Iteration [5692] | loss: 0.00282187107950449\n",
      "Iteration [5693] | loss: 0.0028242485132068396\n",
      "Iteration [5694] | loss: 0.0028267446905374527\n",
      "Iteration [5695] | loss: 0.0028291221242398024\n",
      "Iteration [5696] | loss: 0.002831618534401059\n",
      "Iteration [5697] | loss: 0.002834114944562316\n",
      "Iteration [5698] | loss: 0.002836730098351836\n",
      "Iteration [5699] | loss: 0.002839107532054186\n",
      "Iteration [5700] | loss: 0.0028414849657565355\n",
      "Iteration [5701] | loss: 0.0028439811430871487\n",
      "Iteration [5702] | loss: 0.0028463585767894983\n",
      "Iteration [5703] | loss: 0.0028489737305790186\n",
      "Iteration [5704] | loss: 0.0028514699079096317\n",
      "Iteration [5705] | loss: 0.0028539663180708885\n",
      "Iteration [5706] | loss: 0.0028565814718604088\n",
      "Iteration [5707] | loss: 0.002858958672732115\n",
      "Iteration [5708] | loss: 0.0028614550828933716\n",
      "Iteration [5709] | loss: 0.0028639512602239847\n",
      "Iteration [5710] | loss: 0.002866447437554598\n",
      "Iteration [5711] | loss: 0.002869062591344118\n",
      "Iteration [5712] | loss: 0.0028715587686747313\n",
      "Iteration [5713] | loss: 0.0028741739224642515\n",
      "Iteration [5714] | loss: 0.0028766700997948647\n",
      "Iteration [5715] | loss: 0.002879166277125478\n",
      "Iteration [5716] | loss: 0.002881781430914998\n",
      "Iteration [5717] | loss: 0.002884277608245611\n",
      "Iteration [5718] | loss: 0.002886892529204488\n",
      "Iteration [5719] | loss: 0.002889507682994008\n",
      "Iteration [5720] | loss: 0.002892003860324621\n",
      "Iteration [5721] | loss: 0.002894618781283498\n",
      "Iteration [5722] | loss: 0.002897233935073018\n",
      "Iteration [5723] | loss: 0.002899730112403631\n",
      "Iteration [5724] | loss: 0.0029022260569036007\n",
      "Iteration [5725] | loss: 0.002904841210693121\n",
      "Iteration [5726] | loss: 0.0029074561316519976\n",
      "Iteration [5727] | loss: 0.002910071052610874\n",
      "Iteration [5728] | loss: 0.0029126862064003944\n",
      "Iteration [5729] | loss: 0.002915301127359271\n",
      "Iteration [5730] | loss: 0.002918035024777055\n",
      "Iteration [5731] | loss: 0.0029205309692770243\n",
      "Iteration [5732] | loss: 0.0029230271466076374\n",
      "Iteration [5733] | loss: 0.002925642067566514\n",
      "Iteration [5734] | loss: 0.0029283759649842978\n",
      "Iteration [5735] | loss: 0.0029309908859431744\n",
      "Iteration [5736] | loss: 0.0029337245505303144\n",
      "Iteration [5737] | loss: 0.002936339471489191\n",
      "Iteration [5738] | loss: 0.0029389543924480677\n",
      "Iteration [5739] | loss: 0.0029416880570352077\n",
      "Iteration [5740] | loss: 0.0029443029779940844\n",
      "Iteration [5741] | loss: 0.002946917898952961\n",
      "Iteration [5742] | loss: 0.0029495328199118376\n",
      "Iteration [5743] | loss: 0.0029522664844989777\n",
      "Iteration [5744] | loss: 0.0029548814054578543\n",
      "Iteration [5745] | loss: 0.0029577340465039015\n",
      "Iteration [5746] | loss: 0.002960348967462778\n",
      "Iteration [5747] | loss: 0.002962963655591011\n",
      "Iteration [5748] | loss: 0.0029658162966370583\n",
      "Iteration [5749] | loss: 0.002968431217595935\n",
      "Iteration [5750] | loss: 0.002971164882183075\n",
      "Iteration [5751] | loss: 0.002973898546770215\n",
      "Iteration [5752] | loss: 0.002976632211357355\n",
      "Iteration [5753] | loss: 0.002979246899485588\n",
      "Iteration [5754] | loss: 0.0029820995405316353\n",
      "Iteration [5755] | loss: 0.0029847142286598682\n",
      "Iteration [5756] | loss: 0.002987566636875272\n",
      "Iteration [5757] | loss: 0.0029901815578341484\n",
      "Iteration [5758] | loss: 0.002993033966049552\n",
      "Iteration [5759] | loss: 0.002995767630636692\n",
      "Iteration [5760] | loss: 0.002998501295223832\n",
      "Iteration [5761] | loss: 0.0030013537034392357\n",
      "Iteration [5762] | loss: 0.0030040873680263758\n",
      "Iteration [5763] | loss: 0.0030069397762417793\n",
      "Iteration [5764] | loss: 0.0030096732079982758\n",
      "Iteration [5765] | loss: 0.003012406872585416\n",
      "Iteration [5766] | loss: 0.0030151403043419123\n",
      "Iteration [5767] | loss: 0.003017992712557316\n",
      "Iteration [5768] | loss: 0.003020726377144456\n",
      "Iteration [5769] | loss: 0.0030235787853598595\n",
      "Iteration [5770] | loss: 0.003026431193575263\n",
      "Iteration [5771] | loss: 0.0030292836017906666\n",
      "Iteration [5772] | loss: 0.0030321357771754265\n",
      "Iteration [5773] | loss: 0.0030348694417625666\n",
      "Iteration [5774] | loss: 0.0030377216171473265\n",
      "Iteration [5775] | loss: 0.00304057402536273\n",
      "Iteration [5776] | loss: 0.0030434264335781336\n",
      "Iteration [5777] | loss: 0.0030463975854218006\n",
      "Iteration [5778] | loss: 0.003049249993637204\n",
      "Iteration [5779] | loss: 0.003052102169021964\n",
      "Iteration [5780] | loss: 0.0030548356007784605\n",
      "Iteration [5781] | loss: 0.003057688008993864\n",
      "Iteration [5782] | loss: 0.003060659160837531\n",
      "Iteration [5783] | loss: 0.003063511336222291\n",
      "Iteration [5784] | loss: 0.003066363511607051\n",
      "Iteration [5785] | loss: 0.0030692159198224545\n",
      "Iteration [5786] | loss: 0.0030720680952072144\n",
      "Iteration [5787] | loss: 0.0030750392470508814\n",
      "Iteration [5788] | loss: 0.0030778914224356413\n",
      "Iteration [5789] | loss: 0.003080981317907572\n",
      "Iteration [5790] | loss: 0.0030838334932923317\n",
      "Iteration [5791] | loss: 0.0030868046451359987\n",
      "Iteration [5792] | loss: 0.0030896568205207586\n",
      "Iteration [5793] | loss: 0.0030926279723644257\n",
      "Iteration [5794] | loss: 0.003095598891377449\n",
      "Iteration [5795] | loss: 0.003098451066762209\n",
      "Iteration [5796] | loss: 0.0031015409622341394\n",
      "Iteration [5797] | loss: 0.003104511881247163\n",
      "Iteration [5798] | loss: 0.003107482800260186\n",
      "Iteration [5799] | loss: 0.0031104539521038532\n",
      "Iteration [5800] | loss: 0.0031134248711168766\n",
      "Iteration [5801] | loss: 0.0031163957901299\n",
      "Iteration [5802] | loss: 0.0031193667091429234\n",
      "Iteration [5803] | loss: 0.0031223376281559467\n",
      "Iteration [5804] | loss: 0.0031254275236278772\n",
      "Iteration [5805] | loss: 0.0031283984426409006\n",
      "Iteration [5806] | loss: 0.0031314881052821875\n",
      "Iteration [5807] | loss: 0.003134459024295211\n",
      "Iteration [5808] | loss: 0.003137429943308234\n",
      "Iteration [5809] | loss: 0.003140519605949521\n",
      "Iteration [5810] | loss: 0.0031434905249625444\n",
      "Iteration [5811] | loss: 0.0031466991640627384\n",
      "Iteration [5812] | loss: 0.003149670083075762\n",
      "Iteration [5813] | loss: 0.0031527597457170486\n",
      "Iteration [5814] | loss: 0.0031557304318994284\n",
      "Iteration [5815] | loss: 0.0031589390709996223\n",
      "Iteration [5816] | loss: 0.0031619099900126457\n",
      "Iteration [5817] | loss: 0.003164999419823289\n",
      "Iteration [5818] | loss: 0.0031680890824645758\n",
      "Iteration [5819] | loss: 0.0031711787451058626\n",
      "Iteration [5820] | loss: 0.003174387151375413\n",
      "Iteration [5821] | loss: 0.0031773580703884363\n",
      "Iteration [5822] | loss: 0.0031805664766579866\n",
      "Iteration [5823] | loss: 0.0031836561392992735\n",
      "Iteration [5824] | loss: 0.0031867455691099167\n",
      "Iteration [5825] | loss: 0.003189953975379467\n",
      "Iteration [5826] | loss: 0.003193043638020754\n",
      "Iteration [5827] | loss: 0.003196252044290304\n",
      "Iteration [5828] | loss: 0.0031993414741009474\n",
      "Iteration [5829] | loss: 0.0032024311367422342\n",
      "Iteration [5830] | loss: 0.0032056395430117846\n",
      "Iteration [5831] | loss: 0.0032088477164506912\n",
      "Iteration [5832] | loss: 0.0032120561227202415\n",
      "Iteration [5833] | loss: 0.0032151455525308847\n",
      "Iteration [5834] | loss: 0.003218353958800435\n",
      "Iteration [5835] | loss: 0.0032215621322393417\n",
      "Iteration [5836] | loss: 0.003224770538508892\n",
      "Iteration [5837] | loss: 0.0032279787119477987\n",
      "Iteration [5838] | loss: 0.003231187118217349\n",
      "Iteration [5839] | loss: 0.0032343952916562557\n",
      "Iteration [5840] | loss: 0.003237603697925806\n",
      "Iteration [5841] | loss: 0.003240930614992976\n",
      "Iteration [5842] | loss: 0.003244138788431883\n",
      "Iteration [5843] | loss: 0.003247347194701433\n",
      "Iteration [5844] | loss: 0.0032506741117686033\n",
      "Iteration [5845] | loss: 0.00325388228520751\n",
      "Iteration [5846] | loss: 0.003257209435105324\n",
      "Iteration [5847] | loss: 0.003260536352172494\n",
      "Iteration [5848] | loss: 0.0032637445256114006\n",
      "Iteration [5849] | loss: 0.0032670714426785707\n",
      "Iteration [5850] | loss: 0.0032702796161174774\n",
      "Iteration [5851] | loss: 0.0032737255096435547\n",
      "Iteration [5852] | loss: 0.0032769334502518177\n",
      "Iteration [5853] | loss: 0.0032802606001496315\n",
      "Iteration [5854] | loss: 0.0032834685407578945\n",
      "Iteration [5855] | loss: 0.003286914434283972\n",
      "Iteration [5856] | loss: 0.0032902411185204983\n",
      "Iteration [5857] | loss: 0.003293449291959405\n",
      "Iteration [5858] | loss: 0.0032968949526548386\n",
      "Iteration [5859] | loss: 0.003300340613350272\n",
      "Iteration [5860] | loss: 0.003303786274045706\n",
      "Iteration [5861] | loss: 0.003307113191112876\n",
      "Iteration [5862] | loss: 0.003310440108180046\n",
      "Iteration [5863] | loss: 0.0033138857688754797\n",
      "Iteration [5864] | loss: 0.0033170937094837427\n",
      "Iteration [5865] | loss: 0.0033205393701791763\n",
      "Iteration [5866] | loss: 0.0033239847980439663\n",
      "Iteration [5867] | loss: 0.0033274304587394\n",
      "Iteration [5868] | loss: 0.0033308761194348335\n",
      "Iteration [5869] | loss: 0.00333420280367136\n",
      "Iteration [5870] | loss: 0.0033376484643667936\n",
      "Iteration [5871] | loss: 0.00334097514860332\n",
      "Iteration [5872] | loss: 0.0033446582965552807\n",
      "Iteration [5873] | loss: 0.0033481037244200706\n",
      "Iteration [5874] | loss: 0.0033515493851155043\n",
      "Iteration [5875] | loss: 0.0033549948129802942\n",
      "Iteration [5876] | loss: 0.003358440240845084\n",
      "Iteration [5877] | loss: 0.0033620046451687813\n",
      "Iteration [5878] | loss: 0.0033654500730335712\n",
      "Iteration [5879] | loss: 0.003368895500898361\n",
      "Iteration [5880] | loss: 0.003372340928763151\n",
      "Iteration [5881] | loss: 0.0033759051002562046\n",
      "Iteration [5882] | loss: 0.003379588248208165\n",
      "Iteration [5883] | loss: 0.0033830334432423115\n",
      "Iteration [5884] | loss: 0.0033865978475660086\n",
      "Iteration [5885] | loss: 0.003390162019059062\n",
      "Iteration [5886] | loss: 0.0033937261905521154\n",
      "Iteration [5887] | loss: 0.0033971713855862617\n",
      "Iteration [5888] | loss: 0.003400735557079315\n",
      "Iteration [5889] | loss: 0.0034042997285723686\n",
      "Iteration [5890] | loss: 0.003407863900065422\n",
      "Iteration [5891] | loss: 0.003411546815186739\n",
      "Iteration [5892] | loss: 0.0034151107538491488\n",
      "Iteration [5893] | loss: 0.003418674925342202\n",
      "Iteration [5894] | loss: 0.0034224765840917826\n",
      "Iteration [5895] | loss: 0.003426040755584836\n",
      "Iteration [5896] | loss: 0.003429604694247246\n",
      "Iteration [5897] | loss: 0.0034331686329096556\n",
      "Iteration [5898] | loss: 0.003436970291659236\n",
      "Iteration [5899] | loss: 0.003440653206780553\n",
      "Iteration [5900] | loss: 0.0034442171454429626\n",
      "Iteration [5901] | loss: 0.003447899827733636\n",
      "Iteration [5902] | loss: 0.003451582742854953\n",
      "Iteration [5903] | loss: 0.003455265425145626\n",
      "Iteration [5904] | loss: 0.003458829363808036\n",
      "Iteration [5905] | loss: 0.0034627497661858797\n",
      "Iteration [5906] | loss: 0.003466313472017646\n",
      "Iteration [5907] | loss: 0.0034701151307672262\n",
      "Iteration [5908] | loss: 0.0034737978130578995\n",
      "Iteration [5909] | loss: 0.003477599238976836\n",
      "Iteration [5910] | loss: 0.003481163177639246\n",
      "Iteration [5911] | loss: 0.0034849646035581827\n",
      "Iteration [5912] | loss: 0.003488884773105383\n",
      "Iteration [5913] | loss: 0.0034925672225654125\n",
      "Iteration [5914] | loss: 0.0034963686484843493\n",
      "Iteration [5915] | loss: 0.0035000513307750225\n",
      "Iteration [5916] | loss: 0.0035038527566939592\n",
      "Iteration [5917] | loss: 0.0035076539497822523\n",
      "Iteration [5918] | loss: 0.003511692862957716\n",
      "Iteration [5919] | loss: 0.0035153755452483892\n",
      "Iteration [5920] | loss: 0.0035191767383366823\n",
      "Iteration [5921] | loss: 0.0035229779314249754\n",
      "Iteration [5922] | loss: 0.0035268981009721756\n",
      "Iteration [5923] | loss: 0.0035306992940604687\n",
      "Iteration [5924] | loss: 0.003534619463607669\n",
      "Iteration [5925] | loss: 0.0035385394003242254\n",
      "Iteration [5926] | loss: 0.003542221849784255\n",
      "Iteration [5927] | loss: 0.0035461417865008116\n",
      "Iteration [5928] | loss: 0.003550061723217368\n",
      "Iteration [5929] | loss: 0.0035539816599339247\n",
      "Iteration [5930] | loss: 0.0035579015966504812\n",
      "Iteration [5931] | loss: 0.003561940509825945\n",
      "Iteration [5932] | loss: 0.0035657414700835943\n",
      "Iteration [5933] | loss: 0.003569661406800151\n",
      "Iteration [5934] | loss: 0.003573700087144971\n",
      "Iteration [5935] | loss: 0.0035776200238615274\n",
      "Iteration [5936] | loss: 0.0035815397277474403\n",
      "Iteration [5937] | loss: 0.0035855784080922604\n",
      "Iteration [5938] | loss: 0.003589498344808817\n",
      "Iteration [5939] | loss: 0.0035935367923229933\n",
      "Iteration [5940] | loss: 0.00359745672903955\n",
      "Iteration [5941] | loss: 0.0036016139201819897\n",
      "Iteration [5942] | loss: 0.0036055336240679026\n",
      "Iteration [5943] | loss: 0.003609691048040986\n",
      "Iteration [5944] | loss: 0.0036137294955551624\n",
      "Iteration [5945] | loss: 0.0036176491994410753\n",
      "Iteration [5946] | loss: 0.0036216876469552517\n",
      "Iteration [5947] | loss: 0.003625726094469428\n",
      "Iteration [5948] | loss: 0.003629883285611868\n",
      "Iteration [5949] | loss: 0.0036339217331260443\n",
      "Iteration [5950] | loss: 0.003638078924268484\n",
      "Iteration [5951] | loss: 0.0036421173717826605\n",
      "Iteration [5952] | loss: 0.0036462745629251003\n",
      "Iteration [5953] | loss: 0.00365043175406754\n",
      "Iteration [5954] | loss: 0.0036545887123793364\n",
      "Iteration [5955] | loss: 0.0036586271598935127\n",
      "Iteration [5956] | loss: 0.0036629028618335724\n",
      "Iteration [5957] | loss: 0.0036670600529760122\n",
      "Iteration [5958] | loss: 0.0036712170112878084\n",
      "Iteration [5959] | loss: 0.0036753739695996046\n",
      "Iteration [5960] | loss: 0.0036795311607420444\n",
      "Iteration [5961] | loss: 0.0036836881190538406\n",
      "Iteration [5962] | loss: 0.003687845077365637\n",
      "Iteration [5963] | loss: 0.0036921207793056965\n",
      "Iteration [5964] | loss: 0.003696396481245756\n",
      "Iteration [5965] | loss: 0.003700672183185816\n",
      "Iteration [5966] | loss: 0.003704829141497612\n",
      "Iteration [5967] | loss: 0.0037091048434376717\n",
      "Iteration [5968] | loss: 0.003713499056175351\n",
      "Iteration [5969] | loss: 0.003717774758115411\n",
      "Iteration [5970] | loss: 0.0037219314835965633\n",
      "Iteration [5971] | loss: 0.0037263259291648865\n",
      "Iteration [5972] | loss: 0.0037306013982743025\n",
      "Iteration [5973] | loss: 0.003734877100214362\n",
      "Iteration [5974] | loss: 0.0037392713129520416\n",
      "Iteration [5975] | loss: 0.003743665525689721\n",
      "Iteration [5976] | loss: 0.003747940994799137\n",
      "Iteration [5977] | loss: 0.0037523354403674603\n",
      "Iteration [5978] | loss: 0.0037567296531051397\n",
      "Iteration [5979] | loss: 0.0037611236330121756\n",
      "Iteration [5980] | loss: 0.0037653991021215916\n",
      "Iteration [5981] | loss: 0.003769793314859271\n",
      "Iteration [5982] | loss: 0.003774306271225214\n",
      "Iteration [5983] | loss: 0.00377870025113225\n",
      "Iteration [5984] | loss: 0.003782975720241666\n",
      "Iteration [5985] | loss: 0.0037876071874052286\n",
      "Iteration [5986] | loss: 0.003792001400142908\n",
      "Iteration [5987] | loss: 0.003796395380049944\n",
      "Iteration [5988] | loss: 0.0038010268472135067\n",
      "Iteration [5989] | loss: 0.0038054208271205425\n",
      "Iteration [5990] | loss: 0.003809933550655842\n",
      "Iteration [5991] | loss: 0.003814446274191141\n",
      "Iteration [5992] | loss: 0.0038189589977264404\n",
      "Iteration [5993] | loss: 0.0038234717212617397\n",
      "Iteration [5994] | loss: 0.0038281031884253025\n",
      "Iteration [5995] | loss: 0.003832615679129958\n",
      "Iteration [5996] | loss: 0.0038371284026652575\n",
      "Iteration [5997] | loss: 0.0038417596369981766\n",
      "Iteration [5998] | loss: 0.003846272360533476\n",
      "Iteration [5999] | loss: 0.003850903594866395\n",
      "Iteration [6000] | loss: 0.003855534829199314\n",
      "Iteration [6001] | loss: 0.0038601660635322332\n",
      "Iteration [6002] | loss: 0.003864678554236889\n",
      "Iteration [6003] | loss: 0.003869547275826335\n",
      "Iteration [6004] | loss: 0.0038740597665309906\n",
      "Iteration [6005] | loss: 0.0038786910008639097\n",
      "Iteration [6006] | loss: 0.0038834409788250923\n",
      "Iteration [6007] | loss: 0.0038880719803273678\n",
      "Iteration [6008] | loss: 0.0038928219582885504\n",
      "Iteration [6009] | loss: 0.0038975717034190893\n",
      "Iteration [6010] | loss: 0.003902202704921365\n",
      "Iteration [6011] | loss: 0.003907071426510811\n",
      "Iteration [6012] | loss: 0.003911702428013086\n",
      "Iteration [6013] | loss: 0.003916452173143625\n",
      "Iteration [6014] | loss: 0.003921201918274164\n",
      "Iteration [6015] | loss: 0.003926070407032967\n",
      "Iteration [6016] | loss: 0.003930819686502218\n",
      "Iteration [6017] | loss: 0.003935569431632757\n",
      "Iteration [6018] | loss: 0.00394043792039156\n",
      "Iteration [6019] | loss: 0.003945306409150362\n",
      "Iteration [6020] | loss: 0.003950055688619614\n",
      "Iteration [6021] | loss: 0.0039550429210066795\n",
      "Iteration [6022] | loss: 0.0039597926661372185\n",
      "Iteration [6023] | loss: 0.003964660689234734\n",
      "Iteration [6024] | loss: 0.003969647455960512\n",
      "Iteration [6025] | loss: 0.003974515944719315\n",
      "Iteration [6026] | loss: 0.003979502711445093\n",
      "Iteration [6027] | loss: 0.003984370734542608\n",
      "Iteration [6028] | loss: 0.003989239223301411\n",
      "Iteration [6029] | loss: 0.003994225990027189\n",
      "Iteration [6030] | loss: 0.003999094013124704\n",
      "Iteration [6031] | loss: 0.004004199523478746\n",
      "Iteration [6032] | loss: 0.004009186290204525\n",
      "Iteration [6033] | loss: 0.00401405431330204\n",
      "Iteration [6034] | loss: 0.004019159823656082\n",
      "Iteration [6035] | loss: 0.004024265334010124\n",
      "Iteration [6036] | loss: 0.004029252100735903\n",
      "Iteration [6037] | loss: 0.004034238401800394\n",
      "Iteration [6038] | loss: 0.0040394626557827\n",
      "Iteration [6039] | loss: 0.004044448956847191\n",
      "Iteration [6040] | loss: 0.004049554467201233\n",
      "Iteration [6041] | loss: 0.0040545412339270115\n",
      "Iteration [6042] | loss: 0.004059646278619766\n",
      "Iteration [6043] | loss: 0.004064988810569048\n",
      "Iteration [6044] | loss: 0.0040699755772948265\n",
      "Iteration [6045] | loss: 0.004075199365615845\n",
      "Iteration [6046] | loss: 0.0040803044103085995\n",
      "Iteration [6047] | loss: 0.004085646942257881\n",
      "Iteration [6048] | loss: 0.004090751986950636\n",
      "Iteration [6049] | loss: 0.004095975775271654\n",
      "Iteration [6050] | loss: 0.004101080819964409\n",
      "Iteration [6051] | loss: 0.004106423351913691\n",
      "Iteration [6052] | loss: 0.004111647140234709\n",
      "Iteration [6053] | loss: 0.0041169896721839905\n",
      "Iteration [6054] | loss: 0.004122212994843721\n",
      "Iteration [6055] | loss: 0.0041276742704212666\n",
      "Iteration [6056] | loss: 0.0041328975930809975\n",
      "Iteration [6057] | loss: 0.004138240125030279\n",
      "Iteration [6058] | loss: 0.00414346344769001\n",
      "Iteration [6059] | loss: 0.004148805979639292\n",
      "Iteration [6060] | loss: 0.00415426678955555\n",
      "Iteration [6061] | loss: 0.004159608855843544\n",
      "Iteration [6062] | loss: 0.004165069665759802\n",
      "Iteration [6063] | loss: 0.004170411732047796\n",
      "Iteration [6064] | loss: 0.004175872541964054\n",
      "Iteration [6065] | loss: 0.004181333351880312\n",
      "Iteration [6066] | loss: 0.004186675418168306\n",
      "Iteration [6067] | loss: 0.004192136228084564\n",
      "Iteration [6068] | loss: 0.004197596572339535\n",
      "Iteration [6069] | loss: 0.004203176125884056\n",
      "Iteration [6070] | loss: 0.0042085181921720505\n",
      "Iteration [6071] | loss: 0.004214216023683548\n",
      "Iteration [6072] | loss: 0.0042196763679385185\n",
      "Iteration [6073] | loss: 0.00422525592148304\n",
      "Iteration [6074] | loss: 0.004230835009366274\n",
      "Iteration [6075] | loss: 0.004236414097249508\n",
      "Iteration [6076] | loss: 0.004242111928761005\n",
      "Iteration [6077] | loss: 0.004247572273015976\n",
      "Iteration [6078] | loss: 0.0042532701045274734\n",
      "Iteration [6079] | loss: 0.0042588491924107075\n",
      "Iteration [6080] | loss: 0.004264547023922205\n",
      "Iteration [6081] | loss: 0.004270126111805439\n",
      "Iteration [6082] | loss: 0.004275942221283913\n",
      "Iteration [6083] | loss: 0.00428164005279541\n",
      "Iteration [6084] | loss: 0.004287218675017357\n",
      "Iteration [6085] | loss: 0.004293153528124094\n",
      "Iteration [6086] | loss: 0.004298732616007328\n",
      "Iteration [6087] | loss: 0.004304667469114065\n",
      "Iteration [6088] | loss: 0.004310246091336012\n",
      "Iteration [6089] | loss: 0.004316180944442749\n",
      "Iteration [6090] | loss: 0.004322115797549486\n",
      "Iteration [6091] | loss: 0.004327694419771433\n",
      "Iteration [6092] | loss: 0.00433362927287817\n",
      "Iteration [6093] | loss: 0.004339444916695356\n",
      "Iteration [6094] | loss: 0.0043453797698020935\n",
      "Iteration [6095] | loss: 0.004351314157247543\n",
      "Iteration [6096] | loss: 0.004357130266726017\n",
      "Iteration [6097] | loss: 0.004363064654171467\n",
      "Iteration [6098] | loss: 0.004368999041616917\n",
      "Iteration [6099] | loss: 0.00437505217269063\n",
      "Iteration [6100] | loss: 0.004381105303764343\n",
      "Iteration [6101] | loss: 0.004386921413242817\n",
      "Iteration [6102] | loss: 0.004392974078655243\n",
      "Iteration [6103] | loss: 0.004399027209728956\n",
      "Iteration [6104] | loss: 0.0044050803408026695\n",
      "Iteration [6105] | loss: 0.0044111330062150955\n",
      "Iteration [6106] | loss: 0.004417186137288809\n",
      "Iteration [6107] | loss: 0.004423238802701235\n",
      "Iteration [6108] | loss: 0.004429410211741924\n",
      "Iteration [6109] | loss: 0.004435582086443901\n",
      "Iteration [6110] | loss: 0.004441634751856327\n",
      "Iteration [6111] | loss: 0.0044478061608970165\n",
      "Iteration [6112] | loss: 0.004453977569937706\n",
      "Iteration [6113] | loss: 0.004460267256945372\n",
      "Iteration [6114] | loss: 0.004466438665986061\n",
      "Iteration [6115] | loss: 0.004472728352993727\n",
      "Iteration [6116] | loss: 0.004478899762034416\n",
      "Iteration [6117] | loss: 0.004485189449042082\n",
      "Iteration [6118] | loss: 0.004491479601711035\n",
      "Iteration [6119] | loss: 0.0044977692887187\n",
      "Iteration [6120] | loss: 0.004504058975726366\n",
      "Iteration [6121] | loss: 0.004510585684329271\n",
      "Iteration [6122] | loss: 0.0045167566277086735\n",
      "Iteration [6123] | loss: 0.004523165058344603\n",
      "Iteration [6124] | loss: 0.004529573488980532\n",
      "Iteration [6125] | loss: 0.0045359814539551735\n",
      "Iteration [6126] | loss: 0.004542389884591103\n",
      "Iteration [6127] | loss: 0.004548916593194008\n",
      "Iteration [6128] | loss: 0.00455532455816865\n",
      "Iteration [6129] | loss: 0.004561851266771555\n",
      "Iteration [6130] | loss: 0.00456837797537446\n",
      "Iteration [6131] | loss: 0.0045749046839773655\n",
      "Iteration [6132] | loss: 0.0045814309269189835\n",
      "Iteration [6133] | loss: 0.004587957635521889\n",
      "Iteration [6134] | loss: 0.00459460262209177\n",
      "Iteration [6135] | loss: 0.004601247608661652\n",
      "Iteration [6136] | loss: 0.00460789306089282\n",
      "Iteration [6137] | loss: 0.004614419303834438\n",
      "Iteration [6138] | loss: 0.00462106429040432\n",
      "Iteration [6139] | loss: 0.004627827554941177\n",
      "Iteration [6140] | loss: 0.004634472541511059\n",
      "Iteration [6141] | loss: 0.004641236271709204\n",
      "Iteration [6142] | loss: 0.004647880792617798\n",
      "Iteration [6143] | loss: 0.0046546440571546555\n",
      "Iteration [6144] | loss: 0.0046615260653197765\n",
      "Iteration [6145] | loss: 0.004668171051889658\n",
      "Iteration [6146] | loss: 0.004675053060054779\n",
      "Iteration [6147] | loss: 0.004681815858930349\n",
      "Iteration [6148] | loss: 0.004688816610723734\n",
      "Iteration [6149] | loss: 0.004695698153227568\n",
      "Iteration [6150] | loss: 0.004702580161392689\n",
      "Iteration [6151] | loss: 0.004709580447524786\n",
      "Iteration [6152] | loss: 0.004716343246400356\n",
      "Iteration [6153] | loss: 0.0047233435325324535\n",
      "Iteration [6154] | loss: 0.004730462562292814\n",
      "Iteration [6155] | loss: 0.004737462382763624\n",
      "Iteration [6156] | loss: 0.0047444626688957214\n",
      "Iteration [6157] | loss: 0.004751462489366531\n",
      "Iteration [6158] | loss: 0.004758462775498629\n",
      "Iteration [6159] | loss: 0.0047654625959694386\n",
      "Iteration [6160] | loss: 0.0047724624164402485\n",
      "Iteration [6161] | loss: 0.004779343493282795\n",
      "Iteration [6162] | loss: 0.004786462057381868\n",
      "Iteration [6163] | loss: 0.004793580621480942\n",
      "Iteration [6164] | loss: 0.004800579976290464\n",
      "Iteration [6165] | loss: 0.004807698540389538\n",
      "Iteration [6166] | loss: 0.0048146978951990604\n",
      "Iteration [6167] | loss: 0.0048218159936368465\n",
      "Iteration [6168] | loss: 0.004829052835702896\n",
      "Iteration [6169] | loss: 0.004836170934140682\n",
      "Iteration [6170] | loss: 0.004843407776206732\n",
      "Iteration [6171] | loss: 0.004850644152611494\n",
      "Iteration [6172] | loss: 0.004857880994677544\n",
      "Iteration [6173] | loss: 0.0048649986274540424\n",
      "Iteration [6174] | loss: 0.004872353747487068\n",
      "Iteration [6175] | loss: 0.004879708867520094\n",
      "Iteration [6176] | loss: 0.004886945243924856\n",
      "Iteration [6177] | loss: 0.004894418641924858\n",
      "Iteration [6178] | loss: 0.004901773761957884\n",
      "Iteration [6179] | loss: 0.004909128416329622\n",
      "Iteration [6180] | loss: 0.004916483070701361\n",
      "Iteration [6181] | loss: 0.004923956468701363\n",
      "Iteration [6182] | loss: 0.0049314298667013645\n",
      "Iteration [6183] | loss: 0.004939021542668343\n",
      "Iteration [6184] | loss: 0.004946376197040081\n",
      "Iteration [6185] | loss: 0.004953967873007059\n",
      "Iteration [6186] | loss: 0.004961440805345774\n",
      "Iteration [6187] | loss: 0.004969032481312752\n",
      "Iteration [6188] | loss: 0.00497662415727973\n",
      "Iteration [6189] | loss: 0.004984215367585421\n",
      "Iteration [6190] | loss: 0.004991925787180662\n",
      "Iteration [6191] | loss: 0.004999635741114616\n",
      "Iteration [6192] | loss: 0.005007345695048571\n",
      "Iteration [6193] | loss: 0.005014936905354261\n",
      "Iteration [6194] | loss: 0.005022883880883455\n",
      "Iteration [6195] | loss: 0.005030593369156122\n",
      "Iteration [6196] | loss: 0.0050383033230900764\n",
      "Iteration [6197] | loss: 0.00504625029861927\n",
      "Iteration [6198] | loss: 0.005053959786891937\n",
      "Iteration [6199] | loss: 0.005061906296759844\n",
      "Iteration [6200] | loss: 0.00506985280662775\n",
      "Iteration [6201] | loss: 0.005077681038528681\n",
      "Iteration [6202] | loss: 0.005085508804768324\n",
      "Iteration [6203] | loss: 0.0050934553146362305\n",
      "Iteration [6204] | loss: 0.0051015205681324005\n",
      "Iteration [6205] | loss: 0.00510946661233902\n",
      "Iteration [6206] | loss: 0.00511753186583519\n",
      "Iteration [6207] | loss: 0.0051255966536700726\n",
      "Iteration [6208] | loss: 0.0051337797194719315\n",
      "Iteration [6209] | loss: 0.005141844507306814\n",
      "Iteration [6210] | loss: 0.005149909295141697\n",
      "Iteration [6211] | loss: 0.005158211104571819\n",
      "Iteration [6212] | loss: 0.005166394170373678\n",
      "Iteration [6213] | loss: 0.005174577236175537\n",
      "Iteration [6214] | loss: 0.005182878579944372\n",
      "Iteration [6215] | loss: 0.005191179923713207\n",
      "Iteration [6216] | loss: 0.005199481267482042\n",
      "Iteration [6217] | loss: 0.005207901354879141\n",
      "Iteration [6218] | loss: 0.005216321442276239\n",
      "Iteration [6219] | loss: 0.005224622320383787\n",
      "Iteration [6220] | loss: 0.005233042407780886\n",
      "Iteration [6221] | loss: 0.005241462029516697\n",
      "Iteration [6222] | loss: 0.005249881185591221\n",
      "Iteration [6223] | loss: 0.0052584195509552956\n",
      "Iteration [6224] | loss: 0.0052668387070298195\n",
      "Iteration [6225] | loss: 0.00527549535036087\n",
      "Iteration [6226] | loss: 0.005284151993691921\n",
      "Iteration [6227] | loss: 0.005292571149766445\n",
      "Iteration [6228] | loss: 0.005301227327436209\n",
      "Iteration [6229] | loss: 0.005309883505105972\n",
      "Iteration [6230] | loss: 0.005318658426403999\n",
      "Iteration [6231] | loss: 0.005327432882040739\n",
      "Iteration [6232] | loss: 0.005336207803338766\n",
      "Iteration [6233] | loss: 0.005344982258975506\n",
      "Iteration [6234] | loss: 0.0053537567146122456\n",
      "Iteration [6235] | loss: 0.005362530704587698\n",
      "Iteration [6236] | loss: 0.005371542181819677\n",
      "Iteration [6237] | loss: 0.005380434915423393\n",
      "Iteration [6238] | loss: 0.005389209371060133\n",
      "Iteration [6239] | loss: 0.0053983391262590885\n",
      "Iteration [6240] | loss: 0.005407350137829781\n",
      "Iteration [6241] | loss: 0.005416361149400473\n",
      "Iteration [6242] | loss: 0.005425372160971165\n",
      "Iteration [6243] | loss: 0.005434501450508833\n",
      "Iteration [6244] | loss: 0.005443511996418238\n",
      "Iteration [6245] | loss: 0.0054528783075511456\n",
      "Iteration [6246] | loss: 0.005462007597088814\n",
      "Iteration [6247] | loss: 0.005471136886626482\n",
      "Iteration [6248] | loss: 0.005480384454131126\n",
      "Iteration [6249] | loss: 0.0054896315559744835\n",
      "Iteration [6250] | loss: 0.005498879123479128\n",
      "Iteration [6251] | loss: 0.005508363712579012\n",
      "Iteration [6252] | loss: 0.005517610814422369\n",
      "Iteration [6253] | loss: 0.005527094937860966\n",
      "Iteration [6254] | loss: 0.005536460317671299\n",
      "Iteration [6255] | loss: 0.005545944441109896\n",
      "Iteration [6256] | loss: 0.005555546842515469\n",
      "Iteration [6257] | loss: 0.005565030965954065\n",
      "Iteration [6258] | loss: 0.005574633367359638\n",
      "Iteration [6259] | loss: 0.005584235303103924\n",
      "Iteration [6260] | loss: 0.005593837704509497\n",
      "Iteration [6261] | loss: 0.005603558383882046\n",
      "Iteration [6262] | loss: 0.005613160319626331\n",
      "Iteration [6263] | loss: 0.0056229992769658566\n",
      "Iteration [6264] | loss: 0.005632719490677118\n",
      "Iteration [6265] | loss: 0.005642557982355356\n",
      "Iteration [6266] | loss: 0.005652278196066618\n",
      "Iteration [6267] | loss: 0.005662235431373119\n",
      "Iteration [6268] | loss: 0.0056721922010183334\n",
      "Iteration [6269] | loss: 0.005682149436324835\n",
      "Iteration [6270] | loss: 0.005692106205970049\n",
      "Iteration [6271] | loss: 0.005702062509953976\n",
      "Iteration [6272] | loss: 0.005712138023227453\n",
      "Iteration [6273] | loss: 0.00572233134880662\n",
      "Iteration [6274] | loss: 0.00573240639641881\n",
      "Iteration [6275] | loss: 0.005742480978369713\n",
      "Iteration [6276] | loss: 0.005752674303948879\n",
      "Iteration [6277] | loss: 0.005762985907495022\n",
      "Iteration [6278] | loss: 0.005773179233074188\n",
      "Iteration [6279] | loss: 0.005783135071396828\n",
      "Iteration [6280] | loss: 0.0057928538881242275\n",
      "Iteration [6281] | loss: 0.00580280926078558\n",
      "Iteration [6282] | loss: 0.005812765099108219\n",
      "Iteration [6283] | loss: 0.005822720471769571\n",
      "Iteration [6284] | loss: 0.005832794588059187\n",
      "Iteration [6285] | loss: 0.005842749495059252\n",
      "Iteration [6286] | loss: 0.005852823611348867\n",
      "Iteration [6287] | loss: 0.005862778518348932\n",
      "Iteration [6288] | loss: 0.005872970446944237\n",
      "Iteration [6289] | loss: 0.005883162375539541\n",
      "Iteration [6290] | loss: 0.005893472582101822\n",
      "Iteration [6291] | loss: 0.005903545767068863\n",
      "Iteration [6292] | loss: 0.005913855973631144\n",
      "Iteration [6293] | loss: 0.0059242844581604\n",
      "Iteration [6294] | loss: 0.005934594664722681\n",
      "Iteration [6295] | loss: 0.005944904405623674\n",
      "Iteration [6296] | loss: 0.005955332424491644\n",
      "Iteration [6297] | loss: 0.005965997464954853\n",
      "Iteration [6298] | loss: 0.005976306740194559\n",
      "Iteration [6299] | loss: 0.005986971780657768\n",
      "Iteration [6300] | loss: 0.005997518077492714\n",
      "Iteration [6301] | loss: 0.006008182652294636\n",
      "Iteration [6302] | loss: 0.006018728483468294\n",
      "Iteration [6303] | loss: 0.006029393058270216\n",
      "Iteration [6304] | loss: 0.00604029418900609\n",
      "Iteration [6305] | loss: 0.006050958298146725\n",
      "Iteration [6306] | loss: 0.006061741150915623\n",
      "Iteration [6307] | loss: 0.00607264181599021\n",
      "Iteration [6308] | loss: 0.006083542946726084\n",
      "Iteration [6309] | loss: 0.006094324868172407\n",
      "Iteration [6310] | loss: 0.006105462554842234\n",
      "Iteration [6311] | loss: 0.006116481497883797\n",
      "Iteration [6312] | loss: 0.00612750044092536\n",
      "Iteration [6313] | loss: 0.006138637196272612\n",
      "Iteration [6314] | loss: 0.006149656139314175\n",
      "Iteration [6315] | loss: 0.006160911172628403\n",
      "Iteration [6316] | loss: 0.006172047927975655\n",
      "Iteration [6317] | loss: 0.00618330342695117\n",
      "Iteration [6318] | loss: 0.006194676738232374\n",
      "Iteration [6319] | loss: 0.006205931771546602\n",
      "Iteration [6320] | loss: 0.006217423360794783\n",
      "Iteration [6321] | loss: 0.0062287962064146996\n",
      "Iteration [6322] | loss: 0.00624028779566288\n",
      "Iteration [6323] | loss: 0.006251778919249773\n",
      "Iteration [6324] | loss: 0.006263270508497953\n",
      "Iteration [6325] | loss: 0.006274879910051823\n",
      "Iteration [6326] | loss: 0.006286607589572668\n",
      "Iteration [6327] | loss: 0.0062983352690935135\n",
      "Iteration [6328] | loss: 0.006309944204986095\n",
      "Iteration [6329] | loss: 0.006321790162473917\n",
      "Iteration [6330] | loss: 0.006333517376333475\n",
      "Iteration [6331] | loss: 0.006345481611788273\n",
      "Iteration [6332] | loss: 0.006357327103614807\n",
      "Iteration [6333] | loss: 0.006369409151375294\n",
      "Iteration [6334] | loss: 0.006381254643201828\n",
      "Iteration [6335] | loss: 0.006393454968929291\n",
      "Iteration [6336] | loss: 0.00640553655102849\n",
      "Iteration [6337] | loss: 0.006417618133127689\n",
      "Iteration [6338] | loss: 0.006429699715226889\n",
      "Iteration [6339] | loss: 0.00644201785326004\n",
      "Iteration [6340] | loss: 0.006454335991293192\n",
      "Iteration [6341] | loss: 0.0064666541293263435\n",
      "Iteration [6342] | loss: 0.006479090079665184\n",
      "Iteration [6343] | loss: 0.006491407752037048\n",
      "Iteration [6344] | loss: 0.006503843702375889\n",
      "Iteration [6345] | loss: 0.006516516208648682\n",
      "Iteration [6346] | loss: 0.006529070436954498\n",
      "Iteration [6347] | loss: 0.006541624199599028\n",
      "Iteration [6348] | loss: 0.006554296240210533\n",
      "Iteration [6349] | loss: 0.006567087024450302\n",
      "Iteration [6350] | loss: 0.00657975859940052\n",
      "Iteration [6351] | loss: 0.006592667195945978\n",
      "Iteration [6352] | loss: 0.006605575326830149\n",
      "Iteration [6353] | loss: 0.0066184839233756065\n",
      "Iteration [6354] | loss: 0.006631510332226753\n",
      "Iteration [6355] | loss: 0.0066444179974496365\n",
      "Iteration [6356] | loss: 0.006657562684267759\n",
      "Iteration [6357] | loss: 0.006670588627457619\n",
      "Iteration [6358] | loss: 0.0066838511265814304\n",
      "Iteration [6359] | loss: 0.006697113625705242\n",
      "Iteration [6360] | loss: 0.006710375659167767\n",
      "Iteration [6361] | loss: 0.006723637692630291\n",
      "Iteration [6362] | loss: 0.006737255025655031\n",
      "Iteration [6363] | loss: 0.0067506348714232445\n",
      "Iteration [6364] | loss: 0.006764133460819721\n",
      "Iteration [6365] | loss: 0.006777631584554911\n",
      "Iteration [6366] | loss: 0.006791366264224052\n",
      "Iteration [6367] | loss: 0.006805100943893194\n",
      "Iteration [6368] | loss: 0.0068187168799340725\n",
      "Iteration [6369] | loss: 0.006832569371908903\n",
      "Iteration [6370] | loss: 0.006846421863883734\n",
      "Iteration [6371] | loss: 0.0068603926338255405\n",
      "Iteration [6372] | loss: 0.00687412591651082\n",
      "Iteration [6373] | loss: 0.006888214498758316\n",
      "Iteration [6374] | loss: 0.006902303081005812\n",
      "Iteration [6375] | loss: 0.00691662821918726\n",
      "Iteration [6376] | loss: 0.006930716335773468\n",
      "Iteration [6377] | loss: 0.006944803986698389\n",
      "Iteration [6378] | loss: 0.006959246937185526\n",
      "Iteration [6379] | loss: 0.006973689422011375\n",
      "Iteration [6380] | loss: 0.006988131906837225\n",
      "Iteration [6381] | loss: 0.007002573926001787\n",
      "Iteration [6382] | loss: 0.007017134223133326\n",
      "Iteration [6383] | loss: 0.00703181279823184\n",
      "Iteration [6384] | loss: 0.007046490907669067\n",
      "Iteration [6385] | loss: 0.0070610507391393185\n",
      "Iteration [6386] | loss: 0.007076083682477474\n",
      "Iteration [6387] | loss: 0.00709087960422039\n",
      "Iteration [6388] | loss: 0.007105793803930283\n",
      "Iteration [6389] | loss: 0.00712094409391284\n",
      "Iteration [6390] | loss: 0.007135857827961445\n",
      "Iteration [6391] | loss: 0.007151008117944002\n",
      "Iteration [6392] | loss: 0.007166276220232248\n",
      "Iteration [6393] | loss: 0.007181426044553518\n",
      "Iteration [6394] | loss: 0.007196930702775717\n",
      "Iteration [6395] | loss: 0.007212198339402676\n",
      "Iteration [6396] | loss: 0.007227702531963587\n",
      "Iteration [6397] | loss: 0.007243206258863211\n",
      "Iteration [6398] | loss: 0.007258828263729811\n",
      "Iteration [6399] | loss: 0.0072744498029351234\n",
      "Iteration [6400] | loss: 0.0072903078980743885\n",
      "Iteration [6401] | loss: 0.007306047715246677\n",
      "Iteration [6402] | loss: 0.007321905344724655\n",
      "Iteration [6403] | loss: 0.007337762508541346\n",
      "Iteration [6404] | loss: 0.007353856228291988\n",
      "Iteration [6405] | loss: 0.007369949948042631\n",
      "Iteration [6406] | loss: 0.007386161480098963\n",
      "Iteration [6407] | loss: 0.007402254734188318\n",
      "Iteration [6408] | loss: 0.007418584078550339\n",
      "Iteration [6409] | loss: 0.007435031700879335\n",
      "Iteration [6410] | loss: 0.007451597135514021\n",
      "Iteration [6411] | loss: 0.007468043826520443\n",
      "Iteration [6412] | loss: 0.007484608795493841\n",
      "Iteration [6413] | loss: 0.0075012920424342155\n",
      "Iteration [6414] | loss: 0.007517974823713303\n",
      "Iteration [6415] | loss: 0.007534893695265055\n",
      "Iteration [6416] | loss: 0.007551694288849831\n",
      "Iteration [6417] | loss: 0.007568730972707272\n",
      "Iteration [6418] | loss: 0.0075857676565647125\n",
      "Iteration [6419] | loss: 0.007602922152727842\n",
      "Iteration [6420] | loss: 0.007620194926857948\n",
      "Iteration [6421] | loss: 0.007637348957359791\n",
      "Iteration [6422] | loss: 0.0076548573561012745\n",
      "Iteration [6423] | loss: 0.007672247476875782\n",
      "Iteration [6424] | loss: 0.007689755409955978\n",
      "Iteration [6425] | loss: 0.007707499898970127\n",
      "Iteration [6426] | loss: 0.007725243456661701\n",
      "Iteration [6427] | loss: 0.0077428692020475864\n",
      "Iteration [6428] | loss: 0.0077608488500118256\n",
      "Iteration [6429] | loss: 0.007778828497976065\n",
      "Iteration [6430] | loss: 0.007796925958245993\n",
      "Iteration [6431] | loss: 0.007814904674887657\n",
      "Iteration [6432] | loss: 0.007833120413124561\n",
      "Iteration [6433] | loss: 0.007851453498005867\n",
      "Iteration [6434] | loss: 0.007869785651564598\n",
      "Iteration [6435] | loss: 0.007888355292379856\n",
      "Iteration [6436] | loss: 0.007906923070549965\n",
      "Iteration [6437] | loss: 0.007925610058009624\n",
      "Iteration [6438] | loss: 0.00794429611414671\n",
      "Iteration [6439] | loss: 0.007963218726217747\n",
      "Iteration [6440] | loss: 0.007982022128999233\n",
      "Iteration [6441] | loss: 0.008001062087714672\n",
      "Iteration [6442] | loss: 0.008020102046430111\n",
      "Iteration [6443] | loss: 0.008039377629756927\n",
      "Iteration [6444] | loss: 0.008058653213083744\n",
      "Iteration [6445] | loss: 0.008078046143054962\n",
      "Iteration [6446] | loss: 0.00809743907302618\n",
      "Iteration [6447] | loss: 0.008117067627608776\n",
      "Iteration [6448] | loss: 0.008136814460158348\n",
      "Iteration [6449] | loss: 0.00815656129270792\n",
      "Iteration [6450] | loss: 0.008176425471901894\n",
      "Iteration [6451] | loss: 0.008196407929062843\n",
      "Iteration [6452] | loss: 0.008216507732868195\n",
      "Iteration [6453] | loss: 0.008236607536673546\n",
      "Iteration [6454] | loss: 0.008256824687123299\n",
      "Iteration [6455] | loss: 0.00827727746218443\n",
      "Iteration [6456] | loss: 0.008297731168568134\n",
      "Iteration [6457] | loss: 0.008318301290273666\n",
      "Iteration [6458] | loss: 0.008338872343301773\n",
      "Iteration [6459] | loss: 0.008359796367585659\n",
      "Iteration [6460] | loss: 0.008380603045225143\n",
      "Iteration [6461] | loss: 0.008401526138186455\n",
      "Iteration [6462] | loss: 0.008422804065048695\n",
      "Iteration [6463] | loss: 0.008443963713943958\n",
      "Iteration [6464] | loss: 0.008465240709483624\n",
      "Iteration [6465] | loss: 0.00848663505166769\n",
      "Iteration [6466] | loss: 0.00850826594978571\n",
      "Iteration [6467] | loss: 0.008529895916581154\n",
      "Iteration [6468] | loss: 0.008551525883376598\n",
      "Iteration [6469] | loss: 0.008573509752750397\n",
      "Iteration [6470] | loss: 0.008595493622124195\n",
      "Iteration [6471] | loss: 0.008617594838142395\n",
      "Iteration [6472] | loss: 0.008639813400804996\n",
      "Iteration [6473] | loss: 0.008662149310112\n",
      "Iteration [6474] | loss: 0.008684603497385979\n",
      "Iteration [6475] | loss: 0.008707175962626934\n",
      "Iteration [6476] | loss: 0.008729864843189716\n",
      "Iteration [6477] | loss: 0.008752672001719475\n",
      "Iteration [6478] | loss: 0.008775715716183186\n",
      "Iteration [6479] | loss: 0.008798757568001747\n",
      "Iteration [6480] | loss: 0.008821917697787285\n",
      "Iteration [6481] | loss: 0.008845195174217224\n",
      "Iteration [6482] | loss: 0.00886859092861414\n",
      "Iteration [6483] | loss: 0.008892222307622433\n",
      "Iteration [6484] | loss: 0.008915971033275127\n",
      "Iteration [6485] | loss: 0.008939718827605247\n",
      "Iteration [6486] | loss: 0.00896358396857977\n",
      "Iteration [6487] | loss: 0.008987685665488243\n",
      "Iteration [6488] | loss: 0.009011904709041119\n",
      "Iteration [6489] | loss: 0.009036241099238396\n",
      "Iteration [6490] | loss: 0.009060695767402649\n",
      "Iteration [6491] | loss: 0.009085385128855705\n",
      "Iteration [6492] | loss: 0.009110191836953163\n",
      "Iteration [6493] | loss: 0.009134998545050621\n",
      "Iteration [6494] | loss: 0.009160040877759457\n",
      "Iteration [6495] | loss: 0.009185200557112694\n",
      "Iteration [6496] | loss: 0.009210360236465931\n",
      "Iteration [6497] | loss: 0.009235991165041924\n",
      "Iteration [6498] | loss: 0.009261502884328365\n",
      "Iteration [6499] | loss: 0.00928725115954876\n",
      "Iteration [6500] | loss: 0.009313234128057957\n",
      "Iteration [6501] | loss: 0.00933921616524458\n",
      "Iteration [6502] | loss: 0.009365316480398178\n",
      "Iteration [6503] | loss: 0.009391652420163155\n",
      "Iteration [6504] | loss: 0.009418104775249958\n",
      "Iteration [6505] | loss: 0.009444675408303738\n",
      "Iteration [6506] | loss: 0.009471599012613297\n",
      "Iteration [6507] | loss: 0.009498522616922855\n",
      "Iteration [6508] | loss: 0.009525562636554241\n",
      "Iteration [6509] | loss: 0.00955283921211958\n",
      "Iteration [6510] | loss: 0.009580113925039768\n",
      "Iteration [6511] | loss: 0.009607742540538311\n",
      "Iteration [6512] | loss: 0.009635488502681255\n",
      "Iteration [6513] | loss: 0.009663470089435577\n",
      "Iteration [6514] | loss: 0.009691332466900349\n",
      "Iteration [6515] | loss: 0.009719666093587875\n",
      "Iteration [6516] | loss: 0.009748117066919804\n",
      "Iteration [6517] | loss: 0.009776568040251732\n",
      "Iteration [6518] | loss: 0.009805489331483841\n",
      "Iteration [6519] | loss: 0.009834174066781998\n",
      "Iteration [6520] | loss: 0.00986333005130291\n",
      "Iteration [6521] | loss: 0.009892603382468224\n",
      "Iteration [6522] | loss: 0.009921994060277939\n",
      "Iteration [6523] | loss: 0.009951619431376457\n",
      "Iteration [6524] | loss: 0.009981480427086353\n",
      "Iteration [6525] | loss: 0.010011222213506699\n",
      "Iteration [6526] | loss: 0.0100414352491498\n",
      "Iteration [6527] | loss: 0.010071765631437302\n",
      "Iteration [6528] | loss: 0.010102331638336182\n",
      "Iteration [6529] | loss: 0.01013313140720129\n",
      "Iteration [6530] | loss: 0.010163931176066399\n",
      "Iteration [6531] | loss: 0.01019496563822031\n",
      "Iteration [6532] | loss: 0.010226353071630001\n",
      "Iteration [6533] | loss: 0.010257857851684093\n",
      "Iteration [6534] | loss: 0.010289597325026989\n",
      "Iteration [6535] | loss: 0.010321454145014286\n",
      "Iteration [6536] | loss: 0.010353545658290386\n",
      "Iteration [6537] | loss: 0.010385753586888313\n",
      "Iteration [6538] | loss: 0.010418197140097618\n",
      "Iteration [6539] | loss: 0.010450992733240128\n",
      "Iteration [6540] | loss: 0.010483788326382637\n",
      "Iteration [6541] | loss: 0.010516935959458351\n",
      "Iteration [6542] | loss: 0.010550200939178467\n",
      "Iteration [6543] | loss: 0.010583817958831787\n",
      "Iteration [6544] | loss: 0.010617670603096485\n",
      "Iteration [6545] | loss: 0.010651522316038609\n",
      "Iteration [6546] | loss: 0.010685726068913937\n",
      "Iteration [6547] | loss: 0.010720164515078068\n",
      "Iteration [6548] | loss: 0.010754837654531002\n",
      "Iteration [6549] | loss: 0.010789627209305763\n",
      "Iteration [6550] | loss: 0.010824769735336304\n",
      "Iteration [6551] | loss: 0.010860146954655647\n",
      "Iteration [6552] | loss: 0.010895758867263794\n",
      "Iteration [6553] | loss: 0.010931487195193768\n",
      "Iteration [6554] | loss: 0.010967567563056946\n",
      "Iteration [6555] | loss: 0.011003883555531502\n",
      "Iteration [6556] | loss: 0.011040433309972286\n",
      "Iteration [6557] | loss: 0.0110772168263793\n",
      "Iteration [6558] | loss: 0.011114117689430714\n",
      "Iteration [6559] | loss: 0.011151488870382309\n",
      "Iteration [6560] | loss: 0.011188976466655731\n",
      "Iteration [6561] | loss: 0.011226934380829334\n",
      "Iteration [6562] | loss: 0.011264772154390812\n",
      "Iteration [6563] | loss: 0.011303198523819447\n",
      "Iteration [6564] | loss: 0.011341741308569908\n",
      "Iteration [6565] | loss: 0.011380636133253574\n",
      "Iteration [6566] | loss: 0.011419764719903469\n",
      "Iteration [6567] | loss: 0.011459127999842167\n",
      "Iteration [6568] | loss: 0.011498842388391495\n",
      "Iteration [6569] | loss: 0.011538674123585224\n",
      "Iteration [6570] | loss: 0.011579210869967937\n",
      "Iteration [6571] | loss: 0.011621513403952122\n",
      "Iteration [6572] | loss: 0.011664167046546936\n",
      "Iteration [6573] | loss: 0.01170717366039753\n",
      "Iteration [6574] | loss: 0.011750530451536179\n",
      "Iteration [6575] | loss: 0.01179412193596363\n",
      "Iteration [6576] | loss: 0.011838182806968689\n",
      "Iteration [6577] | loss: 0.011882476508617401\n",
      "Iteration [6578] | loss: 0.011927122250199318\n",
      "Iteration [6579] | loss: 0.011972001753747463\n",
      "Iteration [6580] | loss: 0.01201734971255064\n",
      "Iteration [6581] | loss: 0.01206293236464262\n",
      "Iteration [6582] | loss: 0.012109100818634033\n",
      "Iteration [6583] | loss: 0.012155384756624699\n",
      "Iteration [6584] | loss: 0.01220213808119297\n",
      "Iteration [6585] | loss: 0.012249242514371872\n",
      "Iteration [6586] | loss: 0.012296698056161404\n",
      "Iteration [6587] | loss: 0.012344622053205967\n",
      "Iteration [6588] | loss: 0.01239289715886116\n",
      "Iteration [6589] | loss: 0.012441522441804409\n",
      "Iteration [6590] | loss: 0.012490499764680862\n",
      "Iteration [6591] | loss: 0.012540061958134174\n",
      "Iteration [6592] | loss: 0.012589857913553715\n",
      "Iteration [6593] | loss: 0.012640004977583885\n",
      "Iteration [6594] | loss: 0.012690502218902111\n",
      "Iteration [6595] | loss: 0.01274158526211977\n",
      "Iteration [6596] | loss: 0.012793018482625484\n",
      "Iteration [6597] | loss: 0.012844802811741829\n",
      "Iteration [6598] | loss: 0.012897172011435032\n",
      "Iteration [6599] | loss: 0.012949892319738865\n",
      "Iteration [6600] | loss: 0.013003197498619556\n",
      "Iteration [6601] | loss: 0.0130566181614995\n",
      "Iteration [6602] | loss: 0.013110741972923279\n",
      "Iteration [6603] | loss: 0.013165333308279514\n",
      "Iteration [6604] | loss: 0.01322027388960123\n",
      "Iteration [6605] | loss: 0.01327580027282238\n",
      "Iteration [6606] | loss: 0.013331676833331585\n",
      "Iteration [6607] | loss: 0.013388019986450672\n",
      "Iteration [6608] | loss: 0.013444830663502216\n",
      "Iteration [6609] | loss: 0.013502226211130619\n",
      "Iteration [6610] | loss: 0.01356020662933588\n",
      "Iteration [6611] | loss: 0.013618536293506622\n",
      "Iteration [6612] | loss: 0.013677568174898624\n",
      "Iteration [6613] | loss: 0.013736831955611706\n",
      "Iteration [6614] | loss: 0.013796797022223473\n",
      "Iteration [6615] | loss: 0.013857346959412098\n",
      "Iteration [6616] | loss: 0.01391824521124363\n",
      "Iteration [6617] | loss: 0.01397972833365202\n",
      "Iteration [6618] | loss: 0.014041912741959095\n",
      "Iteration [6619] | loss: 0.014104563742876053\n",
      "Iteration [6620] | loss: 0.01416779775172472\n",
      "Iteration [6621] | loss: 0.014231615699827671\n",
      "Iteration [6622] | loss: 0.014296017587184906\n",
      "Iteration [6623] | loss: 0.014360884204506874\n",
      "Iteration [6624] | loss: 0.014426570385694504\n",
      "Iteration [6625] | loss: 0.014492839574813843\n",
      "Iteration [6626] | loss: 0.014559690840542316\n",
      "Iteration [6627] | loss: 0.014627008698880672\n",
      "Iteration [6628] | loss: 0.014695143327116966\n",
      "Iteration [6629] | loss: 0.01476386096328497\n",
      "Iteration [6630] | loss: 0.014833278954029083\n",
      "Iteration [6631] | loss: 0.014903279021382332\n",
      "Iteration [6632] | loss: 0.01497386209666729\n",
      "Iteration [6633] | loss: 0.01504526101052761\n",
      "Iteration [6634] | loss: 0.015117477625608444\n",
      "Iteration [6635] | loss: 0.015190275385975838\n",
      "Iteration [6636] | loss: 0.015263772569596767\n",
      "Iteration [6637] | loss: 0.015338086523115635\n",
      "Iteration [6638] | loss: 0.015412981621921062\n",
      "Iteration [6639] | loss: 0.015488575212657452\n",
      "Iteration [6640] | loss: 0.01556510291993618\n",
      "Iteration [6641] | loss: 0.015642328187823296\n",
      "Iteration [6642] | loss: 0.015720369294285774\n",
      "Iteration [6643] | loss: 0.015799107030034065\n",
      "Iteration [6644] | loss: 0.01587866060435772\n",
      "Iteration [6645] | loss: 0.01595914736390114\n",
      "Iteration [6646] | loss: 0.01604033075273037\n",
      "Iteration [6647] | loss: 0.016122445464134216\n",
      "Iteration [6648] | loss: 0.016205376014113426\n",
      "Iteration [6649] | loss: 0.016289236024022102\n",
      "Iteration [6650] | loss: 0.016374029219150543\n",
      "Iteration [6651] | loss: 0.016459399834275246\n",
      "Iteration [6652] | loss: 0.016547929495573044\n",
      "Iteration [6653] | loss: 0.016640907153487206\n",
      "Iteration [6654] | loss: 0.016735047101974487\n",
      "Iteration [6655] | loss: 0.016830233857035637\n",
      "Iteration [6656] | loss: 0.016926584765315056\n",
      "Iteration [6657] | loss: 0.017023980617523193\n",
      "Iteration [6658] | loss: 0.01712253876030445\n",
      "Iteration [6659] | loss: 0.01722225733101368\n",
      "Iteration [6660] | loss: 0.017323140054941177\n",
      "Iteration [6661] | loss: 0.017425183206796646\n",
      "Iteration [6662] | loss: 0.01752862147986889\n",
      "Iteration [6663] | loss: 0.0176331028342247\n",
      "Iteration [6664] | loss: 0.017738977447152138\n",
      "Iteration [6665] | loss: 0.017846131697297096\n",
      "Iteration [6666] | loss: 0.017954442650079727\n",
      "Iteration [6667] | loss: 0.018064266070723534\n",
      "Iteration [6668] | loss: 0.018175363540649414\n",
      "Iteration [6669] | loss: 0.01828785426914692\n",
      "Iteration [6670] | loss: 0.0184017363935709\n",
      "Iteration [6671] | loss: 0.018517127260565758\n",
      "Iteration [6672] | loss: 0.01863390952348709\n",
      "Iteration [6673] | loss: 0.018752314150333405\n",
      "Iteration [6674] | loss: 0.01887117326259613\n",
      "Iteration [6675] | loss: 0.01899001933634281\n",
      "Iteration [6676] | loss: 0.01911037042737007\n",
      "Iteration [6677] | loss: 0.01923234574496746\n",
      "Iteration [6678] | loss: 0.01935594156384468\n",
      "Iteration [6679] | loss: 0.019480925053358078\n",
      "Iteration [6680] | loss: 0.019607648253440857\n",
      "Iteration [6681] | loss: 0.01973610557615757\n",
      "Iteration [6682] | loss: 0.01986606791615486\n",
      "Iteration [6683] | loss: 0.01999811641871929\n",
      "Iteration [6684] | loss: 0.020131666213274002\n",
      "Iteration [6685] | loss: 0.0202670656144619\n",
      "Iteration [6686] | loss: 0.020404433831572533\n",
      "Iteration [6687] | loss: 0.0205435361713171\n",
      "Iteration [6688] | loss: 0.020684601739048958\n",
      "Iteration [6689] | loss: 0.020827749744057655\n",
      "Iteration [6690] | loss: 0.02097286283969879\n",
      "Iteration [6691] | loss: 0.021119937300682068\n",
      "Iteration [6692] | loss: 0.021269092336297035\n",
      "Iteration [6693] | loss: 0.021420441567897797\n",
      "Iteration [6694] | loss: 0.021573984995484352\n",
      "Iteration [6695] | loss: 0.021729720756411552\n",
      "Iteration [6696] | loss: 0.021890096366405487\n",
      "Iteration [6697] | loss: 0.02205919288098812\n",
      "Iteration [6698] | loss: 0.02223105914890766\n",
      "Iteration [6699] | loss: 0.022405460476875305\n",
      "Iteration [6700] | loss: 0.02258274517953396\n",
      "Iteration [6701] | loss: 0.02276279404759407\n",
      "Iteration [6702] | loss: 0.022945839911699295\n",
      "Iteration [6703] | loss: 0.02313176542520523\n",
      "Iteration [6704] | loss: 0.023320799693465233\n",
      "Iteration [6705] | loss: 0.023513175547122955\n",
      "Iteration [6706] | loss: 0.023706795647740364\n",
      "Iteration [6707] | loss: 0.02390223927795887\n",
      "Iteration [6708] | loss: 0.024100787937641144\n",
      "Iteration [6709] | loss: 0.024303019046783447\n",
      "Iteration [6710] | loss: 0.024508466944098473\n",
      "Iteration [6711] | loss: 0.024717360734939575\n",
      "Iteration [6712] | loss: 0.02493004873394966\n",
      "Iteration [6713] | loss: 0.02514641173183918\n",
      "Iteration [6714] | loss: 0.0253703985363245\n",
      "Iteration [6715] | loss: 0.025606419891119003\n",
      "Iteration [6716] | loss: 0.025847263634204865\n",
      "Iteration [6717] | loss: 0.026092464104294777\n",
      "Iteration [6718] | loss: 0.02634247951209545\n",
      "Iteration [6719] | loss: 0.02659730799496174\n",
      "Iteration [6720] | loss: 0.02685706317424774\n",
      "Iteration [6721] | loss: 0.02712208591401577\n",
      "Iteration [6722] | loss: 0.027392376214265823\n",
      "Iteration [6723] | loss: 0.027668042108416557\n",
      "Iteration [6724] | loss: 0.02794942818582058\n",
      "Iteration [6725] | loss: 0.028236644342541695\n",
      "Iteration [6726] | loss: 0.0285295732319355\n",
      "Iteration [6727] | loss: 0.028828900307416916\n",
      "Iteration [6728] | loss: 0.029134508222341537\n",
      "Iteration [6729] | loss: 0.029446618631482124\n",
      "Iteration [6730] | loss: 0.029765460640192032\n",
      "Iteration [6731] | loss: 0.030091486871242523\n",
      "Iteration [6732] | loss: 0.0304245762526989\n",
      "Iteration [6733] | loss: 0.03076518513262272\n",
      "Iteration [6734] | loss: 0.03111341968178749\n",
      "Iteration [6735] | loss: 0.031469736248254776\n",
      "Iteration [6736] | loss: 0.031834352761507034\n",
      "Iteration [6737] | loss: 0.03220738098025322\n",
      "Iteration [6738] | loss: 0.03258926793932915\n",
      "Iteration [6739] | loss: 0.032980356365442276\n",
      "Iteration [6740] | loss: 0.03338097408413887\n",
      "Iteration [6741] | loss: 0.03379134088754654\n",
      "Iteration [6742] | loss: 0.034211792051792145\n",
      "Iteration [6743] | loss: 0.034643009305000305\n",
      "Iteration [6744] | loss: 0.03508497402071953\n",
      "Iteration [6745] | loss: 0.03553859144449234\n",
      "Iteration [6746] | loss: 0.0360039658844471\n",
      "Iteration [6747] | loss: 0.03648153319954872\n",
      "Iteration [6748] | loss: 0.03697185590863228\n",
      "Iteration [6749] | loss: 0.03747560456395149\n",
      "Iteration [6750] | loss: 0.037993330508470535\n",
      "Iteration [6751] | loss: 0.03852523863315582\n",
      "Iteration [6752] | loss: 0.039072226732969284\n",
      "Iteration [6753] | loss: 0.039634957909584045\n",
      "Iteration [6754] | loss: 0.04021397605538368\n",
      "Iteration [6755] | loss: 0.04080994054675102\n",
      "Iteration [6756] | loss: 0.0414237342774868\n",
      "Iteration [6757] | loss: 0.042056355625391006\n",
      "Iteration [6758] | loss: 0.04270833730697632\n",
      "Iteration [6759] | loss: 0.04338067024946213\n",
      "Iteration [6760] | loss: 0.044074222445487976\n",
      "Iteration [6761] | loss: 0.04479020833969116\n",
      "Iteration [6762] | loss: 0.04552937299013138\n",
      "Iteration [6763] | loss: 0.04629337415099144\n",
      "Iteration [6764] | loss: 0.04708294942975044\n",
      "Iteration [6765] | loss: 0.047899626195430756\n",
      "Iteration [6766] | loss: 0.0487450435757637\n",
      "Iteration [6767] | loss: 0.0496203787624836\n",
      "Iteration [6768] | loss: 0.05052724853157997\n",
      "Iteration [6769] | loss: 0.051467377692461014\n",
      "Iteration [6770] | loss: 0.05244271084666252\n",
      "Iteration [6771] | loss: 0.053455062210559845\n",
      "Iteration [6772] | loss: 0.05450669303536415\n",
      "Iteration [6773] | loss: 0.055599842220544815\n",
      "Iteration [6774] | loss: 0.05673696845769882\n",
      "Iteration [6775] | loss: 0.05792072415351868\n",
      "Iteration [6776] | loss: 0.059154096990823746\n",
      "Iteration [6777] | loss: 0.06044004112482071\n",
      "Iteration [6778] | loss: 0.061782050877809525\n",
      "Iteration [6779] | loss: 0.06318359076976776\n",
      "Iteration [6780] | loss: 0.06464887410402298\n",
      "Iteration [6781] | loss: 0.06618208438158035\n",
      "Iteration [6782] | loss: 0.06778814643621445\n",
      "Iteration [6783] | loss: 0.0694720447063446\n",
      "Iteration [6784] | loss: 0.07123936712741852\n",
      "Iteration [6785] | loss: 0.07309675961732864\n",
      "Iteration [6786] | loss: 0.07508674263954163\n",
      "Iteration [6787] | loss: 0.07727781683206558\n",
      "Iteration [6788] | loss: 0.07959643751382828\n",
      "Iteration [6789] | loss: 0.0820527896285057\n",
      "Iteration [6790] | loss: 0.08466020971536636\n",
      "Iteration [6791] | loss: 0.08743257075548172\n",
      "Iteration [6792] | loss: 0.09038536995649338\n",
      "Iteration [6793] | loss: 0.09353642165660858\n",
      "Iteration [6794] | loss: 0.09690557420253754\n",
      "Iteration [6795] | loss: 0.10051517933607101\n",
      "Iteration [6796] | loss: 0.10439127683639526\n",
      "Iteration [6797] | loss: 0.10856302827596664\n",
      "Iteration [6798] | loss: 0.1130644753575325\n",
      "Iteration [6799] | loss: 0.11793391406536102\n",
      "Iteration [6800] | loss: 0.12321637570858002\n",
      "Iteration [6801] | loss: 0.12896442413330078\n",
      "Iteration [6802] | loss: 0.1352388560771942\n",
      "Iteration [6803] | loss: 0.1421114206314087\n",
      "Iteration [6804] | loss: 0.1496666818857193\n",
      "Iteration [6805] | loss: 0.1579403430223465\n",
      "Iteration [6806] | loss: 0.16689051687717438\n",
      "Iteration [6807] | loss: 0.1768205761909485\n",
      "Iteration [6808] | loss: 0.18788884580135345\n",
      "Iteration [6809] | loss: 0.2002856433391571\n",
      "Iteration [6810] | loss: 0.21424424648284912\n",
      "Iteration [6811] | loss: 0.23004990816116333\n",
      "Iteration [6812] | loss: 0.2480568289756775\n",
      "Iteration [6813] | loss: 0.2687034606933594\n",
      "Iteration [6814] | loss: 0.29254278540611267\n",
      "Iteration [6815] | loss: 0.32027125358581543\n",
      "Iteration [6816] | loss: 0.3527732491493225\n",
      "Iteration [6817] | loss: 0.39115747809410095\n",
      "Iteration [6818] | loss: 0.4373839199542999\n",
      "Iteration [6819] | loss: 0.4961056113243103\n",
      "Iteration [6820] | loss: 0.5681083798408508\n",
      "Iteration [6821] | loss: 0.6570616364479065\n",
      "Iteration [6822] | loss: 0.7675279378890991\n",
      "Iteration [6823] | loss: 0.9049224853515625\n",
      "Iteration [6824] | loss: 1.0752012729644775\n",
      "Iteration [6825] | loss: 1.284114122390747\n",
      "Iteration [6826] | loss: 1.5360407829284668\n",
      "Iteration [6827] | loss: 1.8232007026672363\n",
      "Iteration [6828] | loss: 2.116863965988159\n",
      "Iteration [6829] | loss: 2.3318986892700195\n",
      "Iteration [6830] | loss: 2.3535754680633545\n",
      "Iteration [6831] | loss: 2.3741862773895264\n",
      "Iteration [6832] | loss: 2.393763780593872\n",
      "Iteration [6833] | loss: 2.4123430252075195\n",
      "Iteration [6834] | loss: 2.4299559593200684\n",
      "Iteration [6835] | loss: 2.4466392993927\n",
      "Iteration [6836] | loss: 2.4624273777008057\n",
      "Iteration [6837] | loss: 2.4747936725616455\n",
      "Iteration [6838] | loss: 2.486762523651123\n",
      "Iteration [6839] | loss: 2.4973793029785156\n",
      "Iteration [6840] | loss: 2.5084714889526367\n",
      "Iteration [6841] | loss: 2.5173611640930176\n",
      "Iteration [6842] | loss: 2.5277934074401855\n",
      "Iteration [6843] | loss: 2.5349888801574707\n",
      "Iteration [6844] | loss: 2.5449540615081787\n",
      "Iteration [6845] | loss: 2.550497531890869\n",
      "Iteration [6846] | loss: 2.5597262382507324\n",
      "Iteration [6847] | loss: 2.5645666122436523\n",
      "Iteration [6848] | loss: 2.572324275970459\n",
      "Iteration [6849] | loss: 2.577411413192749\n",
      "Iteration [6850] | loss: 2.5833349227905273\n",
      "Iteration [6851] | loss: 2.5887622833251953\n",
      "Iteration [6852] | loss: 2.592942953109741\n",
      "Iteration [6853] | loss: 2.598782539367676\n",
      "Iteration [6854] | loss: 2.6013054847717285\n",
      "Iteration [6855] | loss: 2.607217788696289\n",
      "Iteration [6856] | loss: 2.608999490737915\n",
      "Iteration [6857] | loss: 2.61384916305542\n",
      "Iteration [6858] | loss: 2.616513729095459\n",
      "Iteration [6859] | loss: 2.6195874214172363\n",
      "Iteration [6860] | loss: 2.6231391429901123\n",
      "Iteration [6861] | loss: 2.6245391368865967\n",
      "Iteration [6862] | loss: 2.6288750171661377\n",
      "Iteration [6863] | loss: 2.6289381980895996\n",
      "Iteration [6864] | loss: 2.6326992511749268\n",
      "Iteration [6865] | loss: 2.6339809894561768\n",
      "Iteration [6866] | loss: 2.635974168777466\n",
      "Iteration [6867] | loss: 2.638429641723633\n",
      "Iteration [6868] | loss: 2.638770580291748\n",
      "Iteration [6869] | loss: 2.642119884490967\n",
      "Iteration [6870] | loss: 2.6414248943328857\n",
      "Iteration [6871] | loss: 2.6441867351531982\n",
      "Iteration [6872] | loss: 2.644901990890503\n",
      "Iteration [6873] | loss: 2.6459264755249023\n",
      "Iteration [6874] | loss: 2.647980213165283\n",
      "Iteration [6875] | loss: 2.647386074066162\n",
      "Iteration [6876] | loss: 2.650128126144409\n",
      "Iteration [6877] | loss: 2.6492180824279785\n",
      "Iteration [6878] | loss: 2.651108741760254\n",
      "Iteration [6879] | loss: 2.651716470718384\n",
      "Iteration [6880] | loss: 2.6519036293029785\n",
      "Iteration [6881] | loss: 2.6539371013641357\n",
      "Iteration [6882] | loss: 2.6525206565856934\n",
      "Iteration [6883] | loss: 2.654939889907837\n",
      "Iteration [6884] | loss: 2.6540279388427734\n",
      "Iteration [6885] | loss: 2.655258893966675\n",
      "Iteration [6886] | loss: 2.655923843383789\n",
      "Iteration [6887] | loss: 2.6554782390594482\n",
      "Iteration [6888] | loss: 2.657618999481201\n",
      "Iteration [6889] | loss: 2.6556196212768555\n",
      "Iteration [6890] | loss: 2.65777587890625\n",
      "Iteration [6891] | loss: 2.6570911407470703\n",
      "Iteration [6892] | loss: 2.6576836109161377\n",
      "Iteration [6893] | loss: 2.6585488319396973\n",
      "Iteration [6894] | loss: 2.6575846672058105\n",
      "Iteration [6895] | loss: 2.6595964431762695\n",
      "Iteration [6896] | loss: 2.6577882766723633\n",
      "Iteration [6897] | loss: 2.659294605255127\n",
      "Iteration [6898] | loss: 2.6591885089874268\n",
      "Iteration [6899] | loss: 2.658979892730713\n",
      "Iteration [6900] | loss: 2.660320520401001\n",
      "Iteration [6901] | loss: 2.6581358909606934\n",
      "Iteration [6902] | loss: 2.6601366996765137\n",
      "Iteration [6903] | loss: 2.659182548522949\n",
      "Iteration [6904] | loss: 2.659731149673462\n",
      "Iteration [6905] | loss: 2.6604294776916504\n",
      "Iteration [6906] | loss: 2.659327983856201\n",
      "Iteration [6907] | loss: 2.661212205886841\n",
      "Iteration [6908] | loss: 2.6593165397644043\n",
      "Iteration [6909] | loss: 2.6606876850128174\n",
      "Iteration [6910] | loss: 2.6605255603790283\n",
      "Iteration [6911] | loss: 2.6601767539978027\n",
      "Iteration [6912] | loss: 2.6615676879882812\n",
      "Iteration [6913] | loss: 2.659705400466919\n",
      "Iteration [6914] | loss: 2.6614623069763184\n",
      "Iteration [6915] | loss: 2.6601037979125977\n",
      "Iteration [6916] | loss: 2.660487651824951\n",
      "Iteration [6917] | loss: 2.661241054534912\n",
      "Iteration [6918] | loss: 2.659975290298462\n",
      "Iteration [6919] | loss: 2.6618120670318604\n",
      "Iteration [6920] | loss: 2.6599838733673096\n",
      "Iteration [6921] | loss: 2.6612024307250977\n",
      "Iteration [6922] | loss: 2.6611130237579346\n",
      "Iteration [6923] | loss: 2.6605961322784424\n",
      "Iteration [6924] | loss: 2.6620872020721436\n",
      "Iteration [6925] | loss: 2.6600804328918457\n",
      "Iteration [6926] | loss: 2.661909341812134\n",
      "Iteration [6927] | loss: 2.660682201385498\n",
      "Iteration [6928] | loss: 2.6607377529144287\n",
      "Iteration [6929] | loss: 2.6615915298461914\n",
      "Iteration [6930] | loss: 2.660188674926758\n",
      "Iteration [6931] | loss: 2.6620092391967773\n",
      "Iteration [6932] | loss: 2.6602790355682373\n",
      "Iteration [6933] | loss: 2.6613667011260986\n",
      "Iteration [6934] | loss: 2.661320924758911\n",
      "Iteration [6935] | loss: 2.6607749462127686\n",
      "Iteration [6936] | loss: 2.6623291969299316\n",
      "Iteration [6937] | loss: 2.6601929664611816\n",
      "Iteration [6938] | loss: 2.662015199661255\n",
      "Iteration [6939] | loss: 2.6609129905700684\n",
      "Iteration [6940] | loss: 2.6613502502441406\n",
      "Iteration [6941] | loss: 2.661762237548828\n",
      "Iteration [6942] | loss: 2.660193920135498\n",
      "Iteration [6943] | loss: 2.662036418914795\n",
      "Iteration [6944] | loss: 2.6603548526763916\n",
      "Iteration [6945] | loss: 2.6613895893096924\n",
      "Iteration [6946] | loss: 2.6614465713500977\n",
      "Iteration [6947] | loss: 2.660773515701294\n",
      "Iteration [6948] | loss: 2.662442684173584\n",
      "Iteration [6949] | loss: 2.6601903438568115\n",
      "Iteration [6950] | loss: 2.6620118618011475\n",
      "Iteration [6951] | loss: 2.6610147953033447\n",
      "Iteration [6952] | loss: 2.661341905593872\n",
      "Iteration [6953] | loss: 2.662038564682007\n",
      "Iteration [6954] | loss: 2.6606807708740234\n",
      "Iteration [6955] | loss: 2.662230968475342\n",
      "Iteration [6956] | loss: 2.6604042053222656\n",
      "Iteration [6957] | loss: 2.6613197326660156\n",
      "Iteration [6958] | loss: 2.661496162414551\n",
      "Iteration [6959] | loss: 2.6607112884521484\n",
      "Iteration [6960] | loss: 2.6624910831451416\n",
      "Iteration [6961] | loss: 2.660130262374878\n",
      "Iteration [6962] | loss: 2.661956787109375\n",
      "Iteration [6963] | loss: 2.6610615253448486\n",
      "Iteration [6964] | loss: 2.6612844467163086\n",
      "Iteration [6965] | loss: 2.6620280742645264\n",
      "Iteration [6966] | loss: 2.6606829166412354\n",
      "Iteration [6967] | loss: 2.6624674797058105\n",
      "Iteration [6968] | loss: 2.6604087352752686\n",
      "Iteration [6969] | loss: 2.6612393856048584\n",
      "Iteration [6970] | loss: 2.661513090133667\n",
      "Iteration [6971] | loss: 2.660640001296997\n",
      "Iteration [6972] | loss: 2.6624224185943604\n",
      "Iteration [6973] | loss: 2.660191059112549\n",
      "Iteration [6974] | loss: 2.6617391109466553\n",
      "Iteration [6975] | loss: 2.6612939834594727\n",
      "Iteration [6976] | loss: 2.6610677242279053\n",
      "Iteration [6977] | loss: 2.6622443199157715\n",
      "Iteration [6978] | loss: 2.660496950149536\n",
      "Iteration [6979] | loss: 2.662294626235962\n",
      "Iteration [6980] | loss: 2.66082763671875\n",
      "Iteration [6981] | loss: 2.6616039276123047\n",
      "Iteration [6982] | loss: 2.6615190505981445\n",
      "Iteration [6983] | loss: 2.6604270935058594\n",
      "Iteration [6984] | loss: 2.662228584289551\n",
      "Iteration [6985] | loss: 2.6603336334228516\n",
      "Iteration [6986] | loss: 2.661560535430908\n",
      "Iteration [6987] | loss: 2.661369562149048\n",
      "Iteration [6988] | loss: 2.6609489917755127\n",
      "Iteration [6989] | loss: 2.6623716354370117\n",
      "Iteration [6990] | loss: 2.660348415374756\n",
      "Iteration [6991] | loss: 2.662155866622925\n",
      "Iteration [6992] | loss: 2.660947799682617\n",
      "Iteration [6993] | loss: 2.6614737510681152\n",
      "Iteration [6994] | loss: 2.6619768142700195\n",
      "Iteration [6995] | loss: 2.660818576812744\n",
      "Iteration [6996] | loss: 2.662214756011963\n",
      "Iteration [6997] | loss: 2.6603498458862305\n",
      "Iteration [6998] | loss: 2.6614205837249756\n",
      "Iteration [6999] | loss: 2.661447525024414\n",
      "Iteration [7000] | loss: 2.660801887512207\n",
      "Iteration [7001] | loss: 2.662444829940796\n",
      "Iteration [7002] | loss: 2.6602165699005127\n",
      "Iteration [7003] | loss: 2.6620335578918457\n",
      "Iteration [7004] | loss: 2.6610183715820312\n",
      "Iteration [7005] | loss: 2.661363363265991\n",
      "Iteration [7006] | loss: 2.662044048309326\n",
      "Iteration [7007] | loss: 2.6606979370117188\n",
      "Iteration [7008] | loss: 2.6625256538391113\n",
      "Iteration [7009] | loss: 2.6602768898010254\n",
      "Iteration [7010] | loss: 2.661294937133789\n",
      "Iteration [7011] | loss: 2.6614818572998047\n",
      "Iteration [7012] | loss: 2.660688638687134\n",
      "Iteration [7013] | loss: 2.6624701023101807\n",
      "Iteration [7014] | loss: 2.660165548324585\n",
      "Iteration [7015] | loss: 2.6617820262908936\n",
      "Iteration [7016] | loss: 2.661266565322876\n",
      "Iteration [7017] | loss: 2.661119222640991\n",
      "Iteration [7018] | loss: 2.6622183322906494\n",
      "Iteration [7019] | loss: 2.660533905029297\n",
      "Iteration [7020] | loss: 2.6623282432556152\n",
      "Iteration [7021] | loss: 2.660804271697998\n",
      "Iteration [7022] | loss: 2.661630868911743\n",
      "Iteration [7023] | loss: 2.66143536567688\n",
      "Iteration [7024] | loss: 2.6604537963867188\n",
      "Iteration [7025] | loss: 2.6622531414031982\n",
      "Iteration [7026] | loss: 2.6603167057037354\n",
      "Iteration [7027] | loss: 2.6615865230560303\n",
      "Iteration [7028] | loss: 2.6614129543304443\n",
      "Iteration [7029] | loss: 2.6609251499176025\n",
      "Iteration [7030] | loss: 2.662353754043579\n",
      "Iteration [7031] | loss: 2.6603684425354004\n",
      "Iteration [7032] | loss: 2.662175178527832\n",
      "Iteration [7033] | loss: 2.660933256149292\n",
      "Iteration [7034] | loss: 2.661492347717285\n",
      "Iteration [7035] | loss: 2.6619627475738525\n",
      "Iteration [7036] | loss: 2.660825252532959\n",
      "Iteration [7037] | loss: 2.662095069885254\n",
      "Iteration [7038] | loss: 2.6604044437408447\n",
      "Iteration [7039] | loss: 2.661437511444092\n",
      "Iteration [7040] | loss: 2.6614387035369873\n",
      "Iteration [7041] | loss: 2.660839796066284\n",
      "Iteration [7042] | loss: 2.662435531616211\n",
      "Iteration [7043] | loss: 2.6602461338043213\n",
      "Iteration [7044] | loss: 2.662062883377075\n",
      "Iteration [7045] | loss: 2.6610095500946045\n",
      "Iteration [7046] | loss: 2.6613898277282715\n",
      "Iteration [7047] | loss: 2.6620357036590576\n",
      "Iteration [7048] | loss: 2.6607398986816406\n",
      "Iteration [7049] | loss: 2.662510633468628\n",
      "Iteration [7050] | loss: 2.6603927612304688\n",
      "Iteration [7051] | loss: 2.6613426208496094\n",
      "Iteration [7052] | loss: 2.661489486694336\n",
      "Iteration [7053] | loss: 2.660731792449951\n",
      "Iteration [7054] | loss: 2.6624839305877686\n",
      "Iteration [7055] | loss: 2.6601500511169434\n",
      "Iteration [7056] | loss: 2.6619739532470703\n",
      "Iteration [7057] | loss: 2.661058187484741\n",
      "Iteration [7058] | loss: 2.661306858062744\n",
      "Iteration [7059] | loss: 2.6620826721191406\n",
      "Iteration [7060] | loss: 2.660646438598633\n",
      "Iteration [7061] | loss: 2.6624796390533447\n",
      "Iteration [7062] | loss: 2.6605279445648193\n",
      "Iteration [7063] | loss: 2.6612424850463867\n",
      "Iteration [7064] | loss: 2.6615076065063477\n",
      "Iteration [7065] | loss: 2.6606411933898926\n",
      "Iteration [7066] | loss: 2.6624276638031006\n",
      "Iteration [7067] | loss: 2.660188674926758\n",
      "Iteration [7068] | loss: 2.661743402481079\n",
      "Iteration [7069] | loss: 2.661292791366577\n",
      "Iteration [7070] | loss: 2.661083936691284\n",
      "Iteration [7071] | loss: 2.662243366241455\n",
      "Iteration [7072] | loss: 2.660498857498169\n",
      "Iteration [7073] | loss: 2.6622955799102783\n",
      "Iteration [7074] | loss: 2.660827875137329\n",
      "Iteration [7075] | loss: 2.661602735519409\n",
      "Iteration [7076] | loss: 2.661576747894287\n",
      "Iteration [7077] | loss: 2.660423994064331\n",
      "Iteration [7078] | loss: 2.66222882270813\n",
      "Iteration [7079] | loss: 2.6603329181671143\n",
      "Iteration [7080] | loss: 2.6615614891052246\n",
      "Iteration [7081] | loss: 2.661428928375244\n",
      "Iteration [7082] | loss: 2.6609015464782715\n",
      "Iteration [7083] | loss: 2.662370204925537\n",
      "Iteration [7084] | loss: 2.660346508026123\n",
      "Iteration [7085] | loss: 2.662155866622925\n",
      "Iteration [7086] | loss: 2.6609461307525635\n",
      "Iteration [7087] | loss: 2.661473512649536\n",
      "Iteration [7088] | loss: 2.661977529525757\n",
      "Iteration [7089] | loss: 2.66082763671875\n",
      "Iteration [7090] | loss: 2.6622140407562256\n",
      "Iteration [7091] | loss: 2.660402297973633\n",
      "Iteration [7092] | loss: 2.6613974571228027\n",
      "Iteration [7093] | loss: 2.661440134048462\n",
      "Iteration [7094] | loss: 2.6608028411865234\n",
      "Iteration [7095] | loss: 2.6624393463134766\n",
      "Iteration [7096] | loss: 2.6602139472961426\n",
      "Iteration [7097] | loss: 2.662033796310425\n",
      "Iteration [7098] | loss: 2.661015033721924\n",
      "Iteration [7099] | loss: 2.661362648010254\n",
      "Iteration [7100] | loss: 2.662041425704956\n",
      "Iteration [7101] | loss: 2.660717487335205\n",
      "Iteration [7102] | loss: 2.6625263690948486\n",
      "Iteration [7103] | loss: 2.660285234451294\n",
      "Iteration [7104] | loss: 2.661294937133789\n",
      "Iteration [7105] | loss: 2.6614768505096436\n",
      "Iteration [7106] | loss: 2.66068696975708\n",
      "Iteration [7107] | loss: 2.6624703407287598\n",
      "Iteration [7108] | loss: 2.660160541534424\n",
      "Iteration [7109] | loss: 2.6617817878723145\n",
      "Iteration [7110] | loss: 2.6612672805786133\n",
      "Iteration [7111] | loss: 2.661128520965576\n",
      "Iteration [7112] | loss: 2.662276029586792\n",
      "Iteration [7113] | loss: 2.6604807376861572\n",
      "Iteration [7114] | loss: 2.6623263359069824\n",
      "Iteration [7115] | loss: 2.660801410675049\n",
      "Iteration [7116] | loss: 2.6616313457489014\n",
      "Iteration [7117] | loss: 2.661433219909668\n",
      "Iteration [7118] | loss: 2.6604549884796143\n",
      "Iteration [7119] | loss: 2.6622564792633057\n",
      "Iteration [7120] | loss: 2.660313129425049\n",
      "Iteration [7121] | loss: 2.6615853309631348\n",
      "Iteration [7122] | loss: 2.6614081859588623\n",
      "Iteration [7123] | loss: 2.660938262939453\n",
      "Iteration [7124] | loss: 2.662353038787842\n",
      "Iteration [7125] | loss: 2.660367727279663\n",
      "Iteration [7126] | loss: 2.6621761322021484\n",
      "Iteration [7127] | loss: 2.660930871963501\n",
      "Iteration [7128] | loss: 2.6614904403686523\n",
      "Iteration [7129] | loss: 2.661961555480957\n",
      "Iteration [7130] | loss: 2.6608262062072754\n",
      "Iteration [7131] | loss: 2.6620965003967285\n",
      "Iteration [7132] | loss: 2.660400390625\n",
      "Iteration [7133] | loss: 2.6614415645599365\n",
      "Iteration [7134] | loss: 2.6614904403686523\n",
      "Iteration [7135] | loss: 2.66079044342041\n",
      "Iteration [7136] | loss: 2.66243052482605\n",
      "Iteration [7137] | loss: 2.660247325897217\n",
      "Iteration [7138] | loss: 2.662062168121338\n",
      "Iteration [7139] | loss: 2.6610052585601807\n",
      "Iteration [7140] | loss: 2.661389112472534\n",
      "Iteration [7141] | loss: 2.662032127380371\n",
      "Iteration [7142] | loss: 2.6607518196105957\n",
      "Iteration [7143] | loss: 2.6624855995178223\n",
      "Iteration [7144] | loss: 2.6604418754577637\n",
      "Iteration [7145] | loss: 2.6613218784332275\n",
      "Iteration [7146] | loss: 2.661478042602539\n",
      "Iteration [7147] | loss: 2.6607329845428467\n",
      "Iteration [7148] | loss: 2.662475824356079\n",
      "Iteration [7149] | loss: 2.660151720046997\n",
      "Iteration [7150] | loss: 2.6619760990142822\n",
      "Iteration [7151] | loss: 2.6610522270202637\n",
      "Iteration [7152] | loss: 2.6613101959228516\n",
      "Iteration [7153] | loss: 2.6620748043060303\n",
      "Iteration [7154] | loss: 2.660668134689331\n",
      "Iteration [7155] | loss: 2.662480354309082\n",
      "Iteration [7156] | loss: 2.66051983833313\n",
      "Iteration [7157] | loss: 2.661245107650757\n",
      "Iteration [7158] | loss: 2.661501407623291\n",
      "Iteration [7159] | loss: 2.6606435775756836\n",
      "Iteration [7160] | loss: 2.6624300479888916\n",
      "Iteration [7161] | loss: 2.660184383392334\n",
      "Iteration [7162] | loss: 2.6617462635040283\n",
      "Iteration [7163] | loss: 2.661288022994995\n",
      "Iteration [7164] | loss: 2.661097764968872\n",
      "Iteration [7165] | loss: 2.6622931957244873\n",
      "Iteration [7166] | loss: 2.6604502201080322\n",
      "Iteration [7167] | loss: 2.662299394607544\n",
      "Iteration [7168] | loss: 2.660820722579956\n",
      "Iteration [7169] | loss: 2.661604404449463\n",
      "Iteration [7170] | loss: 2.6615688800811768\n",
      "Iteration [7171] | loss: 2.6604268550872803\n",
      "Iteration [7172] | loss: 2.6622297763824463\n",
      "Iteration [7173] | loss: 2.6603260040283203\n",
      "Iteration [7174] | loss: 2.6615631580352783\n",
      "Iteration [7175] | loss: 2.661421775817871\n",
      "Iteration [7176] | loss: 2.6609158515930176\n",
      "Iteration [7177] | loss: 2.662363052368164\n",
      "Iteration [7178] | loss: 2.6603474617004395\n",
      "Iteration [7179] | loss: 2.662156105041504\n",
      "Iteration [7180] | loss: 2.660942554473877\n",
      "Iteration [7181] | loss: 2.6614744663238525\n",
      "Iteration [7182] | loss: 2.661972999572754\n",
      "Iteration [7183] | loss: 2.6608290672302246\n",
      "Iteration [7184] | loss: 2.662214756011963\n",
      "Iteration [7185] | loss: 2.6603994369506836\n",
      "Iteration [7186] | loss: 2.6614022254943848\n",
      "Iteration [7187] | loss: 2.661491632461548\n",
      "Iteration [7188] | loss: 2.6607561111450195\n",
      "Iteration [7189] | loss: 2.662431478500366\n",
      "Iteration [7190] | loss: 2.6602182388305664\n",
      "Iteration [7191] | loss: 2.662034273147583\n",
      "Iteration [7192] | loss: 2.6610074043273926\n",
      "Iteration [7193] | loss: 2.661365509033203\n",
      "Iteration [7194] | loss: 2.6620354652404785\n",
      "Iteration [7195] | loss: 2.660728931427002\n",
      "Iteration [7196] | loss: 2.662506580352783\n",
      "Iteration [7197] | loss: 2.6603262424468994\n",
      "Iteration [7198] | loss: 2.661273241043091\n",
      "Iteration [7199] | loss: 2.6614673137664795\n",
      "Iteration [7200] | loss: 2.660691738128662\n",
      "Iteration [7201] | loss: 2.6624679565429688\n",
      "Iteration [7202] | loss: 2.6601150035858154\n",
      "Iteration [7203] | loss: 2.6619412899017334\n",
      "Iteration [7204] | loss: 2.661044120788574\n",
      "Iteration [7205] | loss: 2.6612792015075684\n",
      "Iteration [7206] | loss: 2.6620733737945557\n",
      "Iteration [7207] | loss: 2.660641670227051\n",
      "Iteration [7208] | loss: 2.6624555587768555\n",
      "Iteration [7209] | loss: 2.660619020462036\n",
      "Iteration [7210] | loss: 2.661752700805664\n",
      "Iteration [7211] | loss: 2.6612589359283447\n",
      "Iteration [7212] | loss: 2.6605708599090576\n",
      "Iteration [7213] | loss: 2.662360191345215\n",
      "Iteration [7214] | loss: 2.660163164138794\n",
      "Iteration [7215] | loss: 2.6616876125335693\n",
      "Iteration [7216] | loss: 2.661271810531616\n",
      "Iteration [7217] | loss: 2.6610453128814697\n",
      "Iteration [7218] | loss: 2.6622815132141113\n",
      "Iteration [7219] | loss: 2.660403251647949\n",
      "Iteration [7220] | loss: 2.6622560024261475\n",
      "Iteration [7221] | loss: 2.6608128547668457\n",
      "Iteration [7222] | loss: 2.6615681648254395\n",
      "Iteration [7223] | loss: 2.6618545055389404\n",
      "Iteration [7224] | loss: 2.660893678665161\n",
      "Iteration [7225] | loss: 2.6621627807617188\n",
      "Iteration [7226] | loss: 2.660303831100464\n",
      "Iteration [7227] | loss: 2.6615049839019775\n",
      "Iteration [7228] | loss: 2.6614034175872803\n",
      "Iteration [7229] | loss: 2.66086483001709\n",
      "Iteration [7230] | loss: 2.662349224090576\n",
      "Iteration [7231] | loss: 2.660304546356201\n",
      "Iteration [7232] | loss: 2.6621158123016357\n",
      "Iteration [7233] | loss: 2.660930871963501\n",
      "Iteration [7234] | loss: 2.6614394187927246\n",
      "Iteration [7235] | loss: 2.661966323852539\n",
      "Iteration [7236] | loss: 2.6607964038848877\n",
      "Iteration [7237] | loss: 2.6625256538391113\n",
      "Iteration [7238] | loss: 2.6603822708129883\n",
      "Iteration [7239] | loss: 2.6613636016845703\n",
      "Iteration [7240] | loss: 2.661478042602539\n",
      "Iteration [7241] | loss: 2.66072416305542\n",
      "Iteration [7242] | loss: 2.662421703338623\n",
      "Iteration [7243] | loss: 2.660189628601074\n",
      "Iteration [7244] | loss: 2.662008762359619\n",
      "Iteration [7245] | loss: 2.6610004901885986\n",
      "Iteration [7246] | loss: 2.6613407135009766\n",
      "Iteration [7247] | loss: 2.6620309352874756\n",
      "Iteration [7248] | loss: 2.660709857940674\n",
      "Iteration [7249] | loss: 2.6624884605407715\n",
      "Iteration [7250] | loss: 2.6605279445648193\n",
      "Iteration [7251] | loss: 2.6612489223480225\n",
      "Iteration [7252] | loss: 2.661456346511841\n",
      "Iteration [7253] | loss: 2.6606695652008057\n",
      "Iteration [7254] | loss: 2.662451982498169\n",
      "Iteration [7255] | loss: 2.660144090652466\n",
      "Iteration [7256] | loss: 2.6617698669433594\n",
      "Iteration [7257] | loss: 2.66125226020813\n",
      "Iteration [7258] | loss: 2.661119222640991\n",
      "Iteration [7259] | loss: 2.6622612476348877\n",
      "Iteration [7260] | loss: 2.6604888439178467\n",
      "Iteration [7261] | loss: 2.6623151302337646\n",
      "Iteration [7262] | loss: 2.66079044342041\n",
      "Iteration [7263] | loss: 2.66162371635437\n",
      "Iteration [7264] | loss: 2.6615405082702637\n",
      "Iteration [7265] | loss: 2.660444736480713\n",
      "Iteration [7266] | loss: 2.6622445583343506\n",
      "Iteration [7267] | loss: 2.6603007316589355\n",
      "Iteration [7268] | loss: 2.661576271057129\n",
      "Iteration [7269] | loss: 2.6613988876342773\n",
      "Iteration [7270] | loss: 2.6609408855438232\n",
      "Iteration [7271] | loss: 2.662342071533203\n",
      "Iteration [7272] | loss: 2.6603612899780273\n",
      "Iteration [7273] | loss: 2.6621687412261963\n",
      "Iteration [7274] | loss: 2.660923480987549\n",
      "Iteration [7275] | loss: 2.661484956741333\n",
      "Iteration [7276] | loss: 2.661953926086426\n",
      "Iteration [7277] | loss: 2.6608388423919678\n",
      "Iteration [7278] | loss: 2.662224054336548\n",
      "Iteration [7279] | loss: 2.660384178161621\n",
      "Iteration [7280] | loss: 2.661412239074707\n",
      "Iteration [7281] | loss: 2.6614792346954346\n",
      "Iteration [7282] | loss: 2.6607730388641357\n",
      "Iteration [7283] | loss: 2.662419557571411\n",
      "Iteration [7284] | loss: 2.6602249145507812\n",
      "Iteration [7285] | loss: 2.6620419025421143\n",
      "Iteration [7286] | loss: 2.6609954833984375\n",
      "Iteration [7287] | loss: 2.661370038986206\n",
      "Iteration [7288] | loss: 2.662024736404419\n",
      "Iteration [7289] | loss: 2.660735607147217\n",
      "Iteration [7290] | loss: 2.6625146865844727\n",
      "Iteration [7291] | loss: 2.660315752029419\n",
      "Iteration [7292] | loss: 2.6612813472747803\n",
      "Iteration [7293] | loss: 2.661515712738037\n",
      "Iteration [7294] | loss: 2.660645008087158\n",
      "Iteration [7295] | loss: 2.6624562740325928\n",
      "Iteration [7296] | loss: 2.6601204872131348\n",
      "Iteration [7297] | loss: 2.661947250366211\n",
      "Iteration [7298] | loss: 2.6610350608825684\n",
      "Iteration [7299] | loss: 2.6612861156463623\n",
      "Iteration [7300] | loss: 2.662062644958496\n",
      "Iteration [7301] | loss: 2.6606569290161133\n",
      "Iteration [7302] | loss: 2.6624596118927\n",
      "Iteration [7303] | loss: 2.6606085300445557\n",
      "Iteration [7304] | loss: 2.6617562770843506\n",
      "Iteration [7305] | loss: 2.6612493991851807\n",
      "Iteration [7306] | loss: 2.6605758666992188\n",
      "Iteration [7307] | loss: 2.6623644828796387\n",
      "Iteration [7308] | loss: 2.660155773162842\n",
      "Iteration [7309] | loss: 2.6616902351379395\n",
      "Iteration [7310] | loss: 2.6612658500671387\n",
      "Iteration [7311] | loss: 2.6610488891601562\n",
      "Iteration [7312] | loss: 2.6622745990753174\n",
      "Iteration [7313] | loss: 2.660414457321167\n",
      "Iteration [7314] | loss: 2.662260055541992\n",
      "Iteration [7315] | loss: 2.6608078479766846\n",
      "Iteration [7316] | loss: 2.6615710258483887\n",
      "Iteration [7317] | loss: 2.661850690841675\n",
      "Iteration [7318] | loss: 2.6608967781066895\n",
      "Iteration [7319] | loss: 2.6621642112731934\n",
      "Iteration [7320] | loss: 2.6602978706359863\n",
      "Iteration [7321] | loss: 2.6615066528320312\n",
      "Iteration [7322] | loss: 2.6614010334014893\n",
      "Iteration [7323] | loss: 2.6608726978302\n",
      "Iteration [7324] | loss: 2.662346363067627\n",
      "Iteration [7325] | loss: 2.6603047847747803\n",
      "Iteration [7326] | loss: 2.6621177196502686\n",
      "Iteration [7327] | loss: 2.660928726196289\n",
      "Iteration [7328] | loss: 2.661439895629883\n",
      "Iteration [7329] | loss: 2.661961317062378\n",
      "Iteration [7330] | loss: 2.660796880722046\n",
      "Iteration [7331] | loss: 2.6625261306762695\n",
      "Iteration [7332] | loss: 2.6603810787200928\n",
      "Iteration [7333] | loss: 2.661365509033203\n",
      "Iteration [7334] | loss: 2.661477565765381\n",
      "Iteration [7335] | loss: 2.6607301235198975\n",
      "Iteration [7336] | loss: 2.6624183654785156\n",
      "Iteration [7337] | loss: 2.6601874828338623\n",
      "Iteration [7338] | loss: 2.6620097160339355\n",
      "Iteration [7339] | loss: 2.6609981060028076\n",
      "Iteration [7340] | loss: 2.661341905593872\n",
      "Iteration [7341] | loss: 2.6620285511016846\n",
      "Iteration [7342] | loss: 2.660710096359253\n",
      "Iteration [7343] | loss: 2.6624889373779297\n",
      "Iteration [7344] | loss: 2.660524368286133\n",
      "Iteration [7345] | loss: 2.6612515449523926\n",
      "Iteration [7346] | loss: 2.661456346511841\n",
      "Iteration [7347] | loss: 2.660670757293701\n",
      "Iteration [7348] | loss: 2.6624536514282227\n",
      "Iteration [7349] | loss: 2.660144329071045\n",
      "Iteration [7350] | loss: 2.6617701053619385\n",
      "Iteration [7351] | loss: 2.6612510681152344\n",
      "Iteration [7352] | loss: 2.6611180305480957\n",
      "Iteration [7353] | loss: 2.662260055541992\n",
      "Iteration [7354] | loss: 2.660490036010742\n",
      "Iteration [7355] | loss: 2.662316083908081\n",
      "Iteration [7356] | loss: 2.66078782081604\n",
      "Iteration [7357] | loss: 2.661623239517212\n",
      "Iteration [7358] | loss: 2.6615405082702637\n",
      "Iteration [7359] | loss: 2.6604435443878174\n",
      "Iteration [7360] | loss: 2.6622440814971924\n",
      "Iteration [7361] | loss: 2.6603024005889893\n",
      "Iteration [7362] | loss: 2.6615779399871826\n",
      "Iteration [7363] | loss: 2.6613988876342773\n",
      "Iteration [7364] | loss: 2.660942316055298\n",
      "Iteration [7365] | loss: 2.6623969078063965\n",
      "Iteration [7366] | loss: 2.660309076309204\n",
      "Iteration [7367] | loss: 2.6621689796447754\n",
      "Iteration [7368] | loss: 2.660918951034546\n",
      "Iteration [7369] | loss: 2.661487102508545\n",
      "Iteration [7370] | loss: 2.661951780319214\n",
      "Iteration [7371] | loss: 2.6608402729034424\n",
      "Iteration [7372] | loss: 2.662226438522339\n",
      "Iteration [7373] | loss: 2.6603798866271973\n",
      "Iteration [7374] | loss: 2.6614136695861816\n",
      "Iteration [7375] | loss: 2.6614744663238525\n",
      "Iteration [7376] | loss: 2.660783052444458\n",
      "Iteration [7377] | loss: 2.6624152660369873\n",
      "Iteration [7378] | loss: 2.6602258682250977\n",
      "Iteration [7379] | loss: 2.662045478820801\n",
      "Iteration [7380] | loss: 2.6609952449798584\n",
      "Iteration [7381] | loss: 2.6613731384277344\n",
      "Iteration [7382] | loss: 2.6620230674743652\n",
      "Iteration [7383] | loss: 2.6607377529144287\n",
      "Iteration [7384] | loss: 2.662515640258789\n",
      "Iteration [7385] | loss: 2.660316228866577\n",
      "Iteration [7386] | loss: 2.6612837314605713\n",
      "Iteration [7387] | loss: 2.6615123748779297\n",
      "Iteration [7388] | loss: 2.660651683807373\n",
      "Iteration [7389] | loss: 2.6624536514282227\n",
      "Iteration [7390] | loss: 2.6601216793060303\n",
      "Iteration [7391] | loss: 2.6619467735290527\n",
      "Iteration [7392] | loss: 2.66103458404541\n",
      "Iteration [7393] | loss: 2.6612863540649414\n",
      "Iteration [7394] | loss: 2.6620616912841797\n",
      "Iteration [7395] | loss: 2.6606569290161133\n",
      "Iteration [7396] | loss: 2.662442445755005\n",
      "Iteration [7397] | loss: 2.660661458969116\n",
      "Iteration [7398] | loss: 2.661729097366333\n",
      "Iteration [7399] | loss: 2.6612415313720703\n",
      "Iteration [7400] | loss: 2.660578489303589\n",
      "Iteration [7401] | loss: 2.662367343902588\n",
      "Iteration [7402] | loss: 2.660149097442627\n",
      "Iteration [7403] | loss: 2.6616923809051514\n",
      "Iteration [7404] | loss: 2.6612601280212402\n",
      "Iteration [7405] | loss: 2.6610512733459473\n",
      "Iteration [7406] | loss: 2.662269353866577\n",
      "Iteration [7407] | loss: 2.660426378250122\n",
      "Iteration [7408] | loss: 2.6622610092163086\n",
      "Iteration [7409] | loss: 2.66080379486084\n",
      "Iteration [7410] | loss: 2.661571979522705\n",
      "Iteration [7411] | loss: 2.6618454456329346\n",
      "Iteration [7412] | loss: 2.6608989238739014\n",
      "Iteration [7413] | loss: 2.6621651649475098\n",
      "Iteration [7414] | loss: 2.660294771194458\n",
      "Iteration [7415] | loss: 2.661508083343506\n",
      "Iteration [7416] | loss: 2.661395788192749\n",
      "Iteration [7417] | loss: 2.660881757736206\n",
      "Iteration [7418] | loss: 2.662397623062134\n",
      "Iteration [7419] | loss: 2.6602542400360107\n",
      "Iteration [7420] | loss: 2.6621198654174805\n",
      "Iteration [7421] | loss: 2.660921335220337\n",
      "Iteration [7422] | loss: 2.6614413261413574\n",
      "Iteration [7423] | loss: 2.661954879760742\n",
      "Iteration [7424] | loss: 2.660801410675049\n",
      "Iteration [7425] | loss: 2.662529945373535\n",
      "Iteration [7426] | loss: 2.6603739261627197\n",
      "Iteration [7427] | loss: 2.661367654800415\n",
      "Iteration [7428] | loss: 2.6614696979522705\n",
      "Iteration [7429] | loss: 2.66074275970459\n",
      "Iteration [7430] | loss: 2.6624152660369873\n",
      "Iteration [7431] | loss: 2.660191059112549\n",
      "Iteration [7432] | loss: 2.6620113849639893\n",
      "Iteration [7433] | loss: 2.660994291305542\n",
      "Iteration [7434] | loss: 2.661344289779663\n",
      "Iteration [7435] | loss: 2.6620242595672607\n",
      "Iteration [7436] | loss: 2.6607115268707275\n",
      "Iteration [7437] | loss: 2.6624910831451416\n",
      "Iteration [7438] | loss: 2.660522699356079\n",
      "Iteration [7439] | loss: 2.6612539291381836\n",
      "Iteration [7440] | loss: 2.6615071296691895\n",
      "Iteration [7441] | loss: 2.660625696182251\n",
      "Iteration [7442] | loss: 2.662451982498169\n",
      "Iteration [7443] | loss: 2.660100221633911\n",
      "Iteration [7444] | loss: 2.6619279384613037\n",
      "Iteration [7445] | loss: 2.6610305309295654\n",
      "Iteration [7446] | loss: 2.6612675189971924\n",
      "Iteration [7447] | loss: 2.662060260772705\n",
      "Iteration [7448] | loss: 2.6606431007385254\n",
      "Iteration [7449] | loss: 2.662428379058838\n",
      "Iteration [7450] | loss: 2.6606621742248535\n",
      "Iteration [7451] | loss: 2.661715030670166\n",
      "Iteration [7452] | loss: 2.6613621711730957\n",
      "Iteration [7453] | loss: 2.6605615615844727\n",
      "Iteration [7454] | loss: 2.6623520851135254\n",
      "Iteration [7455] | loss: 2.6601486206054688\n",
      "Iteration [7456] | loss: 2.661679267883301\n",
      "Iteration [7457] | loss: 2.6612563133239746\n",
      "Iteration [7458] | loss: 2.661038398742676\n",
      "Iteration [7459] | loss: 2.66226863861084\n",
      "Iteration [7460] | loss: 2.660417079925537\n",
      "Iteration [7461] | loss: 2.662250518798828\n",
      "Iteration [7462] | loss: 2.660801649093628\n",
      "Iteration [7463] | loss: 2.6615633964538574\n",
      "Iteration [7464] | loss: 2.661846160888672\n",
      "Iteration [7465] | loss: 2.660914659500122\n",
      "Iteration [7466] | loss: 2.662289619445801\n",
      "Iteration [7467] | loss: 2.6602859497070312\n",
      "Iteration [7468] | loss: 2.661475896835327\n",
      "Iteration [7469] | loss: 2.661388397216797\n",
      "Iteration [7470] | loss: 2.6608541011810303\n",
      "Iteration [7471] | loss: 2.662390947341919\n",
      "Iteration [7472] | loss: 2.6602280139923096\n",
      "Iteration [7473] | loss: 2.6620962619781494\n",
      "Iteration [7474] | loss: 2.6609160900115967\n",
      "Iteration [7475] | loss: 2.66142201423645\n",
      "Iteration [7476] | loss: 2.661952257156372\n",
      "Iteration [7477] | loss: 2.6607825756073\n",
      "Iteration [7478] | loss: 2.6625564098358154\n",
      "Iteration [7479] | loss: 2.660250425338745\n",
      "Iteration [7480] | loss: 2.661323070526123\n",
      "Iteration [7481] | loss: 2.661454439163208\n",
      "Iteration [7482] | loss: 2.6607024669647217\n",
      "Iteration [7483] | loss: 2.6623995304107666\n",
      "Iteration [7484] | loss: 2.6601579189300537\n",
      "Iteration [7485] | loss: 2.661980152130127\n",
      "Iteration [7486] | loss: 2.6609838008880615\n",
      "Iteration [7487] | loss: 2.661316394805908\n",
      "Iteration [7488] | loss: 2.6620171070098877\n",
      "Iteration [7489] | loss: 2.6606876850128174\n",
      "Iteration [7490] | loss: 2.6624698638916016\n",
      "Iteration [7491] | loss: 2.660623073577881\n",
      "Iteration [7492] | loss: 2.6617634296417236\n",
      "Iteration [7493] | loss: 2.661254644393921\n",
      "Iteration [7494] | loss: 2.660555362701416\n",
      "Iteration [7495] | loss: 2.6623899936676025\n",
      "Iteration [7496] | loss: 2.660111904144287\n",
      "Iteration [7497] | loss: 2.6617159843444824\n",
      "Iteration [7498] | loss: 2.6612255573272705\n",
      "Iteration [7499] | loss: 2.661069869995117\n",
      "Iteration [7500] | loss: 2.662238121032715\n",
      "Iteration [7501] | loss: 2.660461187362671\n",
      "Iteration [7502] | loss: 2.6622588634490967\n",
      "Iteration [7503] | loss: 2.6608283519744873\n",
      "Iteration [7504] | loss: 2.66156005859375\n",
      "Iteration [7505] | loss: 2.661811590194702\n",
      "Iteration [7506] | loss: 2.660917043685913\n",
      "Iteration [7507] | loss: 2.6621816158294678\n",
      "Iteration [7508] | loss: 2.660266637802124\n",
      "Iteration [7509] | loss: 2.661524772644043\n",
      "Iteration [7510] | loss: 2.661369800567627\n",
      "Iteration [7511] | loss: 2.6608972549438477\n",
      "Iteration [7512] | loss: 2.6623754501342773\n",
      "Iteration [7513] | loss: 2.6602861881256104\n",
      "Iteration [7514] | loss: 2.662132740020752\n",
      "Iteration [7515] | loss: 2.6608996391296387\n",
      "Iteration [7516] | loss: 2.6614553928375244\n",
      "Iteration [7517] | loss: 2.6619374752044678\n",
      "Iteration [7518] | loss: 2.6608128547668457\n",
      "Iteration [7519] | loss: 2.6625399589538574\n",
      "Iteration [7520] | loss: 2.6603569984436035\n",
      "Iteration [7521] | loss: 2.6613783836364746\n",
      "Iteration [7522] | loss: 2.6614532470703125\n",
      "Iteration [7523] | loss: 2.660762071609497\n",
      "Iteration [7524] | loss: 2.6623964309692383\n",
      "Iteration [7525] | loss: 2.66019868850708\n",
      "Iteration [7526] | loss: 2.662019968032837\n",
      "Iteration [7527] | loss: 2.660980463027954\n",
      "Iteration [7528] | loss: 2.6613523960113525\n",
      "Iteration [7529] | loss: 2.6620121002197266\n",
      "Iteration [7530] | loss: 2.6607182025909424\n",
      "Iteration [7531] | loss: 2.662497043609619\n",
      "Iteration [7532] | loss: 2.6605117321014404\n",
      "Iteration [7533] | loss: 2.6612603664398193\n",
      "Iteration [7534] | loss: 2.6614980697631836\n",
      "Iteration [7535] | loss: 2.6606380939483643\n",
      "Iteration [7536] | loss: 2.662440776824951\n",
      "Iteration [7537] | loss: 2.660104513168335\n",
      "Iteration [7538] | loss: 2.661932945251465\n",
      "Iteration [7539] | loss: 2.6610238552093506\n",
      "Iteration [7540] | loss: 2.6612725257873535\n",
      "Iteration [7541] | loss: 2.6620538234710693\n",
      "Iteration [7542] | loss: 2.6606454849243164\n",
      "Iteration [7543] | loss: 2.662430763244629\n",
      "Iteration [7544] | loss: 2.6606557369232178\n",
      "Iteration [7545] | loss: 2.6617255210876465\n",
      "Iteration [7546] | loss: 2.661356210708618\n",
      "Iteration [7547] | loss: 2.6605634689331055\n",
      "Iteration [7548] | loss: 2.662356376647949\n",
      "Iteration [7549] | loss: 2.6601412296295166\n",
      "Iteration [7550] | loss: 2.661681890487671\n",
      "Iteration [7551] | loss: 2.6612534523010254\n",
      "Iteration [7552] | loss: 2.6610398292541504\n",
      "Iteration [7553] | loss: 2.6622633934020996\n",
      "Iteration [7554] | loss: 2.660423994064331\n",
      "Iteration [7555] | loss: 2.662252426147461\n",
      "Iteration [7556] | loss: 2.6607978343963623\n",
      "Iteration [7557] | loss: 2.661566972732544\n",
      "Iteration [7558] | loss: 2.661842107772827\n",
      "Iteration [7559] | loss: 2.6609137058258057\n",
      "Iteration [7560] | loss: 2.6622910499572754\n",
      "Iteration [7561] | loss: 2.660280227661133\n",
      "Iteration [7562] | loss: 2.661475658416748\n",
      "Iteration [7563] | loss: 2.661383628845215\n",
      "Iteration [7564] | loss: 2.6608541011810303\n",
      "Iteration [7565] | loss: 2.662388801574707\n",
      "Iteration [7566] | loss: 2.6602344512939453\n",
      "Iteration [7567] | loss: 2.662100076675415\n",
      "Iteration [7568] | loss: 2.6609129905700684\n",
      "Iteration [7569] | loss: 2.6614227294921875\n",
      "Iteration [7570] | loss: 2.661949872970581\n",
      "Iteration [7571] | loss: 2.6607842445373535\n",
      "Iteration [7572] | loss: 2.662559986114502\n",
      "Iteration [7573] | loss: 2.6602487564086914\n",
      "Iteration [7574] | loss: 2.6613247394561768\n",
      "Iteration [7575] | loss: 2.661452293395996\n",
      "Iteration [7576] | loss: 2.660705089569092\n",
      "Iteration [7577] | loss: 2.6623988151550293\n",
      "Iteration [7578] | loss: 2.6601579189300537\n",
      "Iteration [7579] | loss: 2.6619818210601807\n",
      "Iteration [7580] | loss: 2.6609842777252197\n",
      "Iteration [7581] | loss: 2.661318063735962\n",
      "Iteration [7582] | loss: 2.662015438079834\n",
      "Iteration [7583] | loss: 2.660688638687134\n",
      "Iteration [7584] | loss: 2.6624698638916016\n",
      "Iteration [7585] | loss: 2.660620927810669\n",
      "Iteration [7586] | loss: 2.6617650985717773\n",
      "Iteration [7587] | loss: 2.6612541675567627\n",
      "Iteration [7588] | loss: 2.660555601119995\n",
      "Iteration [7589] | loss: 2.6623919010162354\n",
      "Iteration [7590] | loss: 2.6601107120513916\n",
      "Iteration [7591] | loss: 2.661715507507324\n",
      "Iteration [7592] | loss: 2.6612255573272705\n",
      "Iteration [7593] | loss: 2.661072015762329\n",
      "Iteration [7594] | loss: 2.662238121032715\n",
      "Iteration [7595] | loss: 2.6604623794555664\n",
      "Iteration [7596] | loss: 2.6622602939605713\n",
      "Iteration [7597] | loss: 2.6608288288116455\n",
      "Iteration [7598] | loss: 2.661559820175171\n",
      "Iteration [7599] | loss: 2.6618123054504395\n",
      "Iteration [7600] | loss: 2.660917282104492\n",
      "Iteration [7601] | loss: 2.6621832847595215\n",
      "Iteration [7602] | loss: 2.660266160964966\n",
      "Iteration [7603] | loss: 2.6615242958068848\n",
      "Iteration [7604] | loss: 2.661367893218994\n",
      "Iteration [7605] | loss: 2.6608989238739014\n",
      "Iteration [7606] | loss: 2.6623733043670654\n",
      "Iteration [7607] | loss: 2.6602859497070312\n",
      "Iteration [7608] | loss: 2.662132740020752\n",
      "Iteration [7609] | loss: 2.660898447036743\n",
      "Iteration [7610] | loss: 2.661454439163208\n",
      "Iteration [7611] | loss: 2.6619365215301514\n",
      "Iteration [7612] | loss: 2.660813331604004\n",
      "Iteration [7613] | loss: 2.6625404357910156\n",
      "Iteration [7614] | loss: 2.6603567600250244\n",
      "Iteration [7615] | loss: 2.6613781452178955\n",
      "Iteration [7616] | loss: 2.66145396232605\n",
      "Iteration [7617] | loss: 2.6607625484466553\n",
      "Iteration [7618] | loss: 2.662397861480713\n",
      "Iteration [7619] | loss: 2.6601996421813965\n",
      "Iteration [7620] | loss: 2.662019729614258\n",
      "Iteration [7621] | loss: 2.660979986190796\n",
      "Iteration [7622] | loss: 2.6613519191741943\n",
      "Iteration [7623] | loss: 2.6620116233825684\n",
      "Iteration [7624] | loss: 2.660717725753784\n",
      "Iteration [7625] | loss: 2.662497043609619\n",
      "Iteration [7626] | loss: 2.660510540008545\n",
      "Iteration [7627] | loss: 2.661261558532715\n",
      "Iteration [7628] | loss: 2.6614980697631836\n",
      "Iteration [7629] | loss: 2.6606385707855225\n",
      "Iteration [7630] | loss: 2.6624417304992676\n",
      "Iteration [7631] | loss: 2.6601054668426514\n",
      "Iteration [7632] | loss: 2.6619324684143066\n",
      "Iteration [7633] | loss: 2.661025285720825\n",
      "Iteration [7634] | loss: 2.661271333694458\n",
      "Iteration [7635] | loss: 2.6620540618896484\n",
      "Iteration [7636] | loss: 2.66064715385437\n",
      "Iteration [7637] | loss: 2.662430763244629\n",
      "Iteration [7638] | loss: 2.6606557369232178\n",
      "Iteration [7639] | loss: 2.6617228984832764\n",
      "Iteration [7640] | loss: 2.66135573387146\n",
      "Iteration [7641] | loss: 2.6605634689331055\n",
      "Iteration [7642] | loss: 2.6623551845550537\n",
      "Iteration [7643] | loss: 2.6601405143737793\n",
      "Iteration [7644] | loss: 2.6616809368133545\n",
      "Iteration [7645] | loss: 2.661252975463867\n",
      "Iteration [7646] | loss: 2.661040782928467\n",
      "Iteration [7647] | loss: 2.662264347076416\n",
      "Iteration [7648] | loss: 2.660423517227173\n",
      "Iteration [7649] | loss: 2.662252902984619\n",
      "Iteration [7650] | loss: 2.660796880722046\n",
      "Iteration [7651] | loss: 2.661566734313965\n",
      "Iteration [7652] | loss: 2.6618411540985107\n",
      "Iteration [7653] | loss: 2.6609132289886475\n",
      "Iteration [7654] | loss: 2.662292003631592\n",
      "Iteration [7655] | loss: 2.6602797508239746\n",
      "Iteration [7656] | loss: 2.6614768505096436\n",
      "Iteration [7657] | loss: 2.6613850593566895\n",
      "Iteration [7658] | loss: 2.6608545780181885\n",
      "Iteration [7659] | loss: 2.662386655807495\n",
      "Iteration [7660] | loss: 2.6602346897125244\n",
      "Iteration [7661] | loss: 2.6621005535125732\n",
      "Iteration [7662] | loss: 2.660914659500122\n",
      "Iteration [7663] | loss: 2.6614232063293457\n",
      "Iteration [7664] | loss: 2.6619505882263184\n",
      "Iteration [7665] | loss: 2.6607842445373535\n",
      "Iteration [7666] | loss: 2.662559986114502\n",
      "Iteration [7667] | loss: 2.660247325897217\n",
      "Iteration [7668] | loss: 2.6613247394561768\n",
      "Iteration [7669] | loss: 2.661451816558838\n",
      "Iteration [7670] | loss: 2.66070556640625\n",
      "Iteration [7671] | loss: 2.6623988151550293\n",
      "Iteration [7672] | loss: 2.66015625\n",
      "Iteration [7673] | loss: 2.6619813442230225\n",
      "Iteration [7674] | loss: 2.660982608795166\n",
      "Iteration [7675] | loss: 2.6613175868988037\n",
      "Iteration [7676] | loss: 2.662015914916992\n",
      "Iteration [7677] | loss: 2.660688638687134\n",
      "Iteration [7678] | loss: 2.6624703407287598\n",
      "Iteration [7679] | loss: 2.6606204509735107\n",
      "Iteration [7680] | loss: 2.6617650985717773\n",
      "Iteration [7681] | loss: 2.6612541675567627\n",
      "Iteration [7682] | loss: 2.6605546474456787\n",
      "Iteration [7683] | loss: 2.6623921394348145\n",
      "Iteration [7684] | loss: 2.660111904144287\n",
      "Iteration [7685] | loss: 2.661715507507324\n",
      "Iteration [7686] | loss: 2.661224603652954\n",
      "Iteration [7687] | loss: 2.6610705852508545\n",
      "Iteration [7688] | loss: 2.662238121032715\n",
      "Iteration [7689] | loss: 2.660461187362671\n",
      "Iteration [7690] | loss: 2.662259817123413\n",
      "Iteration [7691] | loss: 2.660827875137329\n",
      "Iteration [7692] | loss: 2.661559820175171\n",
      "Iteration [7693] | loss: 2.661811113357544\n",
      "Iteration [7694] | loss: 2.6609179973602295\n",
      "Iteration [7695] | loss: 2.6621832847595215\n",
      "Iteration [7696] | loss: 2.6602675914764404\n",
      "Iteration [7697] | loss: 2.661525249481201\n",
      "Iteration [7698] | loss: 2.661367893218994\n",
      "Iteration [7699] | loss: 2.6608972549438477\n",
      "Iteration [7700] | loss: 2.6623728275299072\n",
      "Iteration [7701] | loss: 2.6602859497070312\n",
      "Iteration [7702] | loss: 2.6621334552764893\n",
      "Iteration [7703] | loss: 2.660900115966797\n",
      "Iteration [7704] | loss: 2.6614558696746826\n",
      "Iteration [7705] | loss: 2.661935806274414\n",
      "Iteration [7706] | loss: 2.6608119010925293\n",
      "Iteration [7707] | loss: 2.6625404357910156\n",
      "Iteration [7708] | loss: 2.6603567600250244\n",
      "Iteration [7709] | loss: 2.661378860473633\n",
      "Iteration [7710] | loss: 2.6614532470703125\n",
      "Iteration [7711] | loss: 2.6607635021209717\n",
      "Iteration [7712] | loss: 2.662397861480713\n",
      "Iteration [7713] | loss: 2.66020131111145\n",
      "Iteration [7714] | loss: 2.6620209217071533\n",
      "Iteration [7715] | loss: 2.6609814167022705\n",
      "Iteration [7716] | loss: 2.6613523960113525\n",
      "Iteration [7717] | loss: 2.6620116233825684\n",
      "Iteration [7718] | loss: 2.6607182025909424\n",
      "Iteration [7719] | loss: 2.662496566772461\n",
      "Iteration [7720] | loss: 2.6605114936828613\n",
      "Iteration [7721] | loss: 2.6612610816955566\n",
      "Iteration [7722] | loss: 2.6614980697631836\n",
      "Iteration [7723] | loss: 2.660639524459839\n",
      "Iteration [7724] | loss: 2.6624417304992676\n",
      "Iteration [7725] | loss: 2.6601054668426514\n",
      "Iteration [7726] | loss: 2.6619324684143066\n",
      "Iteration [7727] | loss: 2.661024808883667\n",
      "Iteration [7728] | loss: 2.661271333694458\n",
      "Iteration [7729] | loss: 2.6620540618896484\n",
      "Iteration [7730] | loss: 2.6606481075286865\n",
      "Iteration [7731] | loss: 2.662430763244629\n",
      "Iteration [7732] | loss: 2.6606571674346924\n",
      "Iteration [7733] | loss: 2.661724328994751\n",
      "Iteration [7734] | loss: 2.661356210708618\n",
      "Iteration [7735] | loss: 2.660564422607422\n",
      "Iteration [7736] | loss: 2.6623551845550537\n",
      "Iteration [7737] | loss: 2.6601405143737793\n",
      "Iteration [7738] | loss: 2.6616809368133545\n",
      "Iteration [7739] | loss: 2.6612536907196045\n",
      "Iteration [7740] | loss: 2.6610398292541504\n",
      "Iteration [7741] | loss: 2.662264347076416\n",
      "Iteration [7742] | loss: 2.660423994064331\n",
      "Iteration [7743] | loss: 2.6622533798217773\n",
      "Iteration [7744] | loss: 2.660795211791992\n",
      "Iteration [7745] | loss: 2.661566734313965\n",
      "Iteration [7746] | loss: 2.6618411540985107\n",
      "Iteration [7747] | loss: 2.6609151363372803\n",
      "Iteration [7748] | loss: 2.662292718887329\n",
      "Iteration [7749] | loss: 2.6602816581726074\n",
      "Iteration [7750] | loss: 2.6614768505096436\n",
      "Iteration [7751] | loss: 2.661383628845215\n",
      "Iteration [7752] | loss: 2.6608550548553467\n",
      "Iteration [7753] | loss: 2.6623876094818115\n",
      "Iteration [7754] | loss: 2.660233974456787\n",
      "Iteration [7755] | loss: 2.662100076675415\n",
      "Iteration [7756] | loss: 2.6609137058258057\n",
      "Iteration [7757] | loss: 2.661423683166504\n",
      "Iteration [7758] | loss: 2.6619505882263184\n",
      "Iteration [7759] | loss: 2.6607842445373535\n",
      "Iteration [7760] | loss: 2.6625590324401855\n",
      "Iteration [7761] | loss: 2.660248279571533\n",
      "Iteration [7762] | loss: 2.6613247394561768\n",
      "Iteration [7763] | loss: 2.6614532470703125\n",
      "Iteration [7764] | loss: 2.660705089569092\n",
      "Iteration [7765] | loss: 2.662398338317871\n",
      "Iteration [7766] | loss: 2.66015887260437\n",
      "Iteration [7767] | loss: 2.6619818210601807\n",
      "Iteration [7768] | loss: 2.6609842777252197\n",
      "Iteration [7769] | loss: 2.6613171100616455\n",
      "Iteration [7770] | loss: 2.662014961242676\n",
      "Iteration [7771] | loss: 2.6606881618499756\n",
      "Iteration [7772] | loss: 2.662470817565918\n",
      "Iteration [7773] | loss: 2.660620927810669\n",
      "Iteration [7774] | loss: 2.6617660522460938\n",
      "Iteration [7775] | loss: 2.661254644393921\n",
      "Iteration [7776] | loss: 2.6605546474456787\n",
      "Iteration [7777] | loss: 2.6623921394348145\n",
      "Iteration [7778] | loss: 2.660111427307129\n",
      "Iteration [7779] | loss: 2.661714792251587\n",
      "Iteration [7780] | loss: 2.6612250804901123\n",
      "Iteration [7781] | loss: 2.661071538925171\n",
      "Iteration [7782] | loss: 2.6622376441955566\n",
      "Iteration [7783] | loss: 2.660461187362671\n",
      "Iteration [7784] | loss: 2.662259817123413\n",
      "Iteration [7785] | loss: 2.660827875137329\n",
      "Iteration [7786] | loss: 2.6615588665008545\n",
      "Iteration [7787] | loss: 2.6618125438690186\n",
      "Iteration [7788] | loss: 2.660917282104492\n",
      "Iteration [7789] | loss: 2.6621832847595215\n",
      "Iteration [7790] | loss: 2.660266160964966\n",
      "Iteration [7791] | loss: 2.6615233421325684\n",
      "Iteration [7792] | loss: 2.661369800567627\n",
      "Iteration [7793] | loss: 2.660898447036743\n",
      "Iteration [7794] | loss: 2.6623737812042236\n",
      "Iteration [7795] | loss: 2.6602864265441895\n",
      "Iteration [7796] | loss: 2.6621339321136475\n",
      "Iteration [7797] | loss: 2.6608996391296387\n",
      "Iteration [7798] | loss: 2.66145396232605\n",
      "Iteration [7799] | loss: 2.6619365215301514\n",
      "Iteration [7800] | loss: 2.6608119010925293\n",
      "Iteration [7801] | loss: 2.662539482116699\n",
      "Iteration [7802] | loss: 2.6603567600250244\n",
      "Iteration [7803] | loss: 2.661378860473633\n",
      "Iteration [7804] | loss: 2.661454916000366\n",
      "Iteration [7805] | loss: 2.6607630252838135\n",
      "Iteration [7806] | loss: 2.662397861480713\n",
      "Iteration [7807] | loss: 2.6601996421813965\n",
      "Iteration [7808] | loss: 2.6620192527770996\n",
      "Iteration [7809] | loss: 2.660980463027954\n",
      "Iteration [7810] | loss: 2.6613523960113525\n",
      "Iteration [7811] | loss: 2.662010908126831\n",
      "Iteration [7812] | loss: 2.6607186794281006\n",
      "Iteration [7813] | loss: 2.662496566772461\n",
      "Iteration [7814] | loss: 2.660510540008545\n",
      "Iteration [7815] | loss: 2.6612603664398193\n",
      "Iteration [7816] | loss: 2.6614975929260254\n",
      "Iteration [7817] | loss: 2.6606385707855225\n",
      "Iteration [7818] | loss: 2.6624412536621094\n",
      "Iteration [7819] | loss: 2.660104990005493\n",
      "Iteration [7820] | loss: 2.6619319915771484\n",
      "Iteration [7821] | loss: 2.661024808883667\n",
      "Iteration [7822] | loss: 2.6612708568573\n",
      "Iteration [7823] | loss: 2.662052869796753\n",
      "Iteration [7824] | loss: 2.66064715385437\n",
      "Iteration [7825] | loss: 2.6624317169189453\n",
      "Iteration [7826] | loss: 2.6606547832489014\n",
      "Iteration [7827] | loss: 2.661724805831909\n",
      "Iteration [7828] | loss: 2.66135573387146\n",
      "Iteration [7829] | loss: 2.6605634689331055\n",
      "Iteration [7830] | loss: 2.662355661392212\n",
      "Iteration [7831] | loss: 2.6601409912109375\n",
      "Iteration [7832] | loss: 2.661682367324829\n",
      "Iteration [7833] | loss: 2.661252975463867\n",
      "Iteration [7834] | loss: 2.6610398292541504\n",
      "Iteration [7835] | loss: 2.6622633934020996\n",
      "Iteration [7836] | loss: 2.6604256629943848\n",
      "Iteration [7837] | loss: 2.662252426147461\n",
      "Iteration [7838] | loss: 2.660795211791992\n",
      "Iteration [7839] | loss: 2.661567449569702\n",
      "Iteration [7840] | loss: 2.6618411540985107\n",
      "Iteration [7841] | loss: 2.660914182662964\n",
      "Iteration [7842] | loss: 2.66229248046875\n",
      "Iteration [7843] | loss: 2.660280227661133\n",
      "Iteration [7844] | loss: 2.6614768505096436\n",
      "Iteration [7845] | loss: 2.661383628845215\n",
      "Iteration [7846] | loss: 2.6608541011810303\n",
      "Iteration [7847] | loss: 2.6623880863189697\n",
      "Iteration [7848] | loss: 2.6602346897125244\n",
      "Iteration [7849] | loss: 2.6620988845825195\n",
      "Iteration [7850] | loss: 2.6609137058258057\n",
      "Iteration [7851] | loss: 2.6614232063293457\n",
      "Iteration [7852] | loss: 2.6619503498077393\n",
      "Iteration [7853] | loss: 2.6607842445373535\n",
      "Iteration [7854] | loss: 2.6625590324401855\n",
      "Iteration [7855] | loss: 2.660247802734375\n",
      "Iteration [7856] | loss: 2.661325216293335\n",
      "Iteration [7857] | loss: 2.6614527702331543\n",
      "Iteration [7858] | loss: 2.660705089569092\n",
      "Iteration [7859] | loss: 2.662398099899292\n",
      "Iteration [7860] | loss: 2.6601579189300537\n",
      "Iteration [7861] | loss: 2.6619818210601807\n",
      "Iteration [7862] | loss: 2.660982608795166\n",
      "Iteration [7863] | loss: 2.661318063735962\n",
      "Iteration [7864] | loss: 2.662015438079834\n",
      "Iteration [7865] | loss: 2.6606874465942383\n",
      "Iteration [7866] | loss: 2.6624698638916016\n",
      "Iteration [7867] | loss: 2.660620927810669\n",
      "Iteration [7868] | loss: 2.66176438331604\n",
      "Iteration [7869] | loss: 2.661255121231079\n",
      "Iteration [7870] | loss: 2.6605546474456787\n",
      "Iteration [7871] | loss: 2.662393569946289\n",
      "Iteration [7872] | loss: 2.660111427307129\n",
      "Iteration [7873] | loss: 2.661714792251587\n",
      "Iteration [7874] | loss: 2.6612255573272705\n",
      "Iteration [7875] | loss: 2.6610705852508545\n",
      "Iteration [7876] | loss: 2.662238121032715\n",
      "Iteration [7877] | loss: 2.6604621410369873\n",
      "Iteration [7878] | loss: 2.6622607707977295\n",
      "Iteration [7879] | loss: 2.6608288288116455\n",
      "Iteration [7880] | loss: 2.661559820175171\n",
      "Iteration [7881] | loss: 2.661811113357544\n",
      "Iteration [7882] | loss: 2.660917282104492\n",
      "Iteration [7883] | loss: 2.662182092666626\n",
      "Iteration [7884] | loss: 2.6602671146392822\n",
      "Iteration [7885] | loss: 2.6615242958068848\n",
      "Iteration [7886] | loss: 2.661367893218994\n",
      "Iteration [7887] | loss: 2.660898208618164\n",
      "Iteration [7888] | loss: 2.6623737812042236\n",
      "Iteration [7889] | loss: 2.6602871417999268\n",
      "Iteration [7890] | loss: 2.6621334552764893\n",
      "Iteration [7891] | loss: 2.6608994007110596\n",
      "Iteration [7892] | loss: 2.661454916000366\n",
      "Iteration [7893] | loss: 2.6619365215301514\n",
      "Iteration [7894] | loss: 2.6608123779296875\n",
      "Iteration [7895] | loss: 2.6625399589538574\n",
      "Iteration [7896] | loss: 2.6603567600250244\n",
      "Iteration [7897] | loss: 2.661378860473633\n",
      "Iteration [7898] | loss: 2.661454439163208\n",
      "Iteration [7899] | loss: 2.6607635021209717\n",
      "Iteration [7900] | loss: 2.662398338317871\n",
      "Iteration [7901] | loss: 2.6601996421813965\n",
      "Iteration [7902] | loss: 2.6620192527770996\n",
      "Iteration [7903] | loss: 2.6609818935394287\n",
      "Iteration [7904] | loss: 2.6613523960113525\n",
      "Iteration [7905] | loss: 2.6620116233825684\n",
      "Iteration [7906] | loss: 2.6607182025909424\n",
      "Iteration [7907] | loss: 2.662497043609619\n",
      "Iteration [7908] | loss: 2.6605119705200195\n",
      "Iteration [7909] | loss: 2.661259889602661\n",
      "Iteration [7910] | loss: 2.6614980697631836\n",
      "Iteration [7911] | loss: 2.6606390476226807\n",
      "Iteration [7912] | loss: 2.662442207336426\n",
      "Iteration [7913] | loss: 2.6601054668426514\n",
      "Iteration [7914] | loss: 2.6619324684143066\n",
      "Iteration [7915] | loss: 2.661024808883667\n",
      "Iteration [7916] | loss: 2.661271333694458\n",
      "Iteration [7917] | loss: 2.6620540618896484\n",
      "Iteration [7918] | loss: 2.66064715385437\n",
      "Iteration [7919] | loss: 2.6624302864074707\n",
      "Iteration [7920] | loss: 2.6606552600860596\n",
      "Iteration [7921] | loss: 2.661724328994751\n",
      "Iteration [7922] | loss: 2.66135573387146\n",
      "Iteration [7923] | loss: 2.6605634689331055\n",
      "Iteration [7924] | loss: 2.6623551845550537\n",
      "Iteration [7925] | loss: 2.6601405143737793\n",
      "Iteration [7926] | loss: 2.6616809368133545\n",
      "Iteration [7927] | loss: 2.661255121231079\n",
      "Iteration [7928] | loss: 2.661040782928467\n",
      "Iteration [7929] | loss: 2.6622626781463623\n",
      "Iteration [7930] | loss: 2.66042423248291\n",
      "Iteration [7931] | loss: 2.6622519493103027\n",
      "Iteration [7932] | loss: 2.660796642303467\n",
      "Iteration [7933] | loss: 2.661566734313965\n",
      "Iteration [7934] | loss: 2.6618404388427734\n",
      "Iteration [7935] | loss: 2.6609151363372803\n",
      "Iteration [7936] | loss: 2.66229248046875\n",
      "Iteration [7937] | loss: 2.660280227661133\n",
      "Iteration [7938] | loss: 2.66147780418396\n",
      "Iteration [7939] | loss: 2.6613850593566895\n",
      "Iteration [7940] | loss: 2.6608545780181885\n",
      "Iteration [7941] | loss: 2.6623876094818115\n",
      "Iteration [7942] | loss: 2.660233974456787\n",
      "Iteration [7943] | loss: 2.662100076675415\n",
      "Iteration [7944] | loss: 2.6609132289886475\n",
      "Iteration [7945] | loss: 2.661423683166504\n",
      "Iteration [7946] | loss: 2.661949872970581\n",
      "Iteration [7947] | loss: 2.6607847213745117\n",
      "Iteration [7948] | loss: 2.6625607013702393\n",
      "Iteration [7949] | loss: 2.6602470874786377\n",
      "Iteration [7950] | loss: 2.6613247394561768\n",
      "Iteration [7951] | loss: 2.6614532470703125\n",
      "Iteration [7952] | loss: 2.6607048511505127\n",
      "Iteration [7953] | loss: 2.6623971462249756\n",
      "Iteration [7954] | loss: 2.66015887260437\n",
      "Iteration [7955] | loss: 2.6619832515716553\n",
      "Iteration [7956] | loss: 2.6609842777252197\n",
      "Iteration [7957] | loss: 2.661318063735962\n",
      "Iteration [7958] | loss: 2.6620163917541504\n",
      "Iteration [7959] | loss: 2.6606881618499756\n",
      "Iteration [7960] | loss: 2.662468910217285\n",
      "Iteration [7961] | loss: 2.6606202125549316\n",
      "Iteration [7962] | loss: 2.66176438331604\n",
      "Iteration [7963] | loss: 2.6612541675567627\n",
      "Iteration [7964] | loss: 2.660557270050049\n",
      "Iteration [7965] | loss: 2.6623919010162354\n",
      "Iteration [7966] | loss: 2.660111427307129\n",
      "Iteration [7967] | loss: 2.6617159843444824\n",
      "Iteration [7968] | loss: 2.6612250804901123\n",
      "Iteration [7969] | loss: 2.6610705852508545\n",
      "Iteration [7970] | loss: 2.6622376441955566\n",
      "Iteration [7971] | loss: 2.660461187362671\n",
      "Iteration [7972] | loss: 2.662259101867676\n",
      "Iteration [7973] | loss: 2.660827875137329\n",
      "Iteration [7974] | loss: 2.6615593433380127\n",
      "Iteration [7975] | loss: 2.661811590194702\n",
      "Iteration [7976] | loss: 2.660917282104492\n",
      "Iteration [7977] | loss: 2.662182092666626\n",
      "Iteration [7978] | loss: 2.660266637802124\n",
      "Iteration [7979] | loss: 2.6615233421325684\n",
      "Iteration [7980] | loss: 2.661370038986206\n",
      "Iteration [7981] | loss: 2.6608972549438477\n",
      "Iteration [7982] | loss: 2.6623733043670654\n",
      "Iteration [7983] | loss: 2.6602866649627686\n",
      "Iteration [7984] | loss: 2.6621344089508057\n",
      "Iteration [7985] | loss: 2.6608991622924805\n",
      "Iteration [7986] | loss: 2.6614532470703125\n",
      "Iteration [7987] | loss: 2.6619365215301514\n",
      "Iteration [7988] | loss: 2.660813808441162\n",
      "Iteration [7989] | loss: 2.662539005279541\n",
      "Iteration [7990] | loss: 2.6603567600250244\n",
      "Iteration [7991] | loss: 2.661379337310791\n",
      "Iteration [7992] | loss: 2.66145396232605\n",
      "Iteration [7993] | loss: 2.6607630252838135\n",
      "Iteration [7994] | loss: 2.662398338317871\n",
      "Iteration [7995] | loss: 2.6601996421813965\n",
      "Iteration [7996] | loss: 2.662019729614258\n",
      "Iteration [7997] | loss: 2.6609818935394287\n",
      "Iteration [7998] | loss: 2.661353349685669\n",
      "Iteration [7999] | loss: 2.6620116233825684\n",
      "Iteration [8000] | loss: 2.6607182025909424\n",
      "Iteration [8001] | loss: 2.6624975204467773\n",
      "Iteration [8002] | loss: 2.660510540008545\n",
      "Iteration [8003] | loss: 2.661261558532715\n",
      "Iteration [8004] | loss: 2.6614980697631836\n",
      "Iteration [8005] | loss: 2.6606390476226807\n",
      "Iteration [8006] | loss: 2.662440776824951\n",
      "Iteration [8007] | loss: 2.6601054668426514\n",
      "Iteration [8008] | loss: 2.6619324684143066\n",
      "Iteration [8009] | loss: 2.6610240936279297\n",
      "Iteration [8010] | loss: 2.6612708568573\n",
      "Iteration [8011] | loss: 2.6620540618896484\n",
      "Iteration [8012] | loss: 2.66064715385437\n",
      "Iteration [8013] | loss: 2.6624317169189453\n",
      "Iteration [8014] | loss: 2.6606547832489014\n",
      "Iteration [8015] | loss: 2.6617238521575928\n",
      "Iteration [8016] | loss: 2.6613571643829346\n",
      "Iteration [8017] | loss: 2.660564422607422\n",
      "Iteration [8018] | loss: 2.6623547077178955\n",
      "Iteration [8019] | loss: 2.6601405143737793\n",
      "Iteration [8020] | loss: 2.661681890487671\n",
      "Iteration [8021] | loss: 2.6612534523010254\n",
      "Iteration [8022] | loss: 2.6610398292541504\n",
      "Iteration [8023] | loss: 2.662264347076416\n",
      "Iteration [8024] | loss: 2.6604247093200684\n",
      "Iteration [8025] | loss: 2.6622536182403564\n",
      "Iteration [8026] | loss: 2.6607956886291504\n",
      "Iteration [8027] | loss: 2.661567211151123\n",
      "Iteration [8028] | loss: 2.661841630935669\n",
      "Iteration [8029] | loss: 2.6609151363372803\n",
      "Iteration [8030] | loss: 2.66229248046875\n",
      "Iteration [8031] | loss: 2.6602816581726074\n",
      "Iteration [8032] | loss: 2.6614768505096436\n",
      "Iteration [8033] | loss: 2.6613831520080566\n",
      "Iteration [8034] | loss: 2.6608550548553467\n",
      "Iteration [8035] | loss: 2.662386655807495\n",
      "Iteration [8036] | loss: 2.660233974456787\n",
      "Iteration [8037] | loss: 2.662097930908203\n",
      "Iteration [8038] | loss: 2.6609137058258057\n",
      "Iteration [8039] | loss: 2.6614232063293457\n",
      "Iteration [8040] | loss: 2.661949872970581\n",
      "Iteration [8041] | loss: 2.6607842445373535\n",
      "Iteration [8042] | loss: 2.6625590324401855\n",
      "Iteration [8043] | loss: 2.660247325897217\n",
      "Iteration [8044] | loss: 2.6613242626190186\n",
      "Iteration [8045] | loss: 2.6614532470703125\n",
      "Iteration [8046] | loss: 2.660705089569092\n",
      "Iteration [8047] | loss: 2.6623988151550293\n",
      "Iteration [8048] | loss: 2.66015887260437\n",
      "Iteration [8049] | loss: 2.6619834899902344\n",
      "Iteration [8050] | loss: 2.6609838008880615\n",
      "Iteration [8051] | loss: 2.6613175868988037\n",
      "Iteration [8052] | loss: 2.662014961242676\n",
      "Iteration [8053] | loss: 2.6606881618499756\n",
      "Iteration [8054] | loss: 2.6624698638916016\n",
      "Iteration [8055] | loss: 2.660621404647827\n",
      "Iteration [8056] | loss: 2.661764621734619\n",
      "Iteration [8057] | loss: 2.6612541675567627\n",
      "Iteration [8058] | loss: 2.6605567932128906\n",
      "Iteration [8059] | loss: 2.662391424179077\n",
      "Iteration [8060] | loss: 2.660111427307129\n",
      "Iteration [8061] | loss: 2.6617164611816406\n",
      "Iteration [8062] | loss: 2.661224603652954\n",
      "Iteration [8063] | loss: 2.6610727310180664\n",
      "Iteration [8064] | loss: 2.662238597869873\n",
      "Iteration [8065] | loss: 2.6604621410369873\n",
      "Iteration [8066] | loss: 2.662259817123413\n",
      "Iteration [8067] | loss: 2.6608288288116455\n",
      "Iteration [8068] | loss: 2.661559820175171\n",
      "Iteration [8069] | loss: 2.661811590194702\n",
      "Iteration [8070] | loss: 2.6609179973602295\n",
      "Iteration [8071] | loss: 2.6621816158294678\n",
      "Iteration [8072] | loss: 2.6602675914764404\n",
      "Iteration [8073] | loss: 2.661524772644043\n",
      "Iteration [8074] | loss: 2.6613688468933105\n",
      "Iteration [8075] | loss: 2.6608972549438477\n",
      "Iteration [8076] | loss: 2.6623733043670654\n",
      "Iteration [8077] | loss: 2.6602859497070312\n",
      "Iteration [8078] | loss: 2.6621317863464355\n",
      "Iteration [8079] | loss: 2.660900115966797\n",
      "Iteration [8080] | loss: 2.661454439163208\n",
      "Iteration [8081] | loss: 2.6619365215301514\n",
      "Iteration [8082] | loss: 2.660813331604004\n",
      "Iteration [8083] | loss: 2.662540912628174\n",
      "Iteration [8084] | loss: 2.6603567600250244\n",
      "Iteration [8085] | loss: 2.661379337310791\n",
      "Iteration [8086] | loss: 2.6614558696746826\n",
      "Iteration [8087] | loss: 2.6607630252838135\n",
      "Iteration [8088] | loss: 2.6623988151550293\n",
      "Iteration [8089] | loss: 2.6601996421813965\n",
      "Iteration [8090] | loss: 2.6620192527770996\n",
      "Iteration [8091] | loss: 2.6609809398651123\n",
      "Iteration [8092] | loss: 2.661353349685669\n",
      "Iteration [8093] | loss: 2.662013530731201\n",
      "Iteration [8094] | loss: 2.6607186794281006\n",
      "Iteration [8095] | loss: 2.662497043609619\n",
      "Iteration [8096] | loss: 2.660511016845703\n",
      "Iteration [8097] | loss: 2.661259412765503\n",
      "Iteration [8098] | loss: 2.6614980697631836\n",
      "Iteration [8099] | loss: 2.6606392860412598\n",
      "Iteration [8100] | loss: 2.6624417304992676\n",
      "Iteration [8101] | loss: 2.660104990005493\n",
      "Iteration [8102] | loss: 2.6619324684143066\n",
      "Iteration [8103] | loss: 2.661024332046509\n",
      "Iteration [8104] | loss: 2.661271333694458\n",
      "Iteration [8105] | loss: 2.6620545387268066\n",
      "Iteration [8106] | loss: 2.660646677017212\n",
      "Iteration [8107] | loss: 2.6624317169189453\n",
      "Iteration [8108] | loss: 2.6606547832489014\n",
      "Iteration [8109] | loss: 2.661724328994751\n",
      "Iteration [8110] | loss: 2.6613552570343018\n",
      "Iteration [8111] | loss: 2.6605634689331055\n",
      "Iteration [8112] | loss: 2.6623547077178955\n",
      "Iteration [8113] | loss: 2.660139560699463\n",
      "Iteration [8114] | loss: 2.6616806983947754\n",
      "Iteration [8115] | loss: 2.6612534523010254\n",
      "Iteration [8116] | loss: 2.661039352416992\n",
      "Iteration [8117] | loss: 2.6622633934020996\n",
      "Iteration [8118] | loss: 2.660423994064331\n",
      "Iteration [8119] | loss: 2.6622536182403564\n",
      "Iteration [8120] | loss: 2.660796880722046\n",
      "Iteration [8121] | loss: 2.661567211151123\n",
      "Iteration [8122] | loss: 2.6618411540985107\n",
      "Iteration [8123] | loss: 2.6609151363372803\n",
      "Iteration [8124] | loss: 2.6622931957244873\n",
      "Iteration [8125] | loss: 2.660280227661133\n",
      "Iteration [8126] | loss: 2.6614768505096436\n",
      "Iteration [8127] | loss: 2.6613850593566895\n",
      "Iteration [8128] | loss: 2.6608550548553467\n",
      "Iteration [8129] | loss: 2.6623871326446533\n",
      "Iteration [8130] | loss: 2.660233974456787\n",
      "Iteration [8131] | loss: 2.662100076675415\n",
      "Iteration [8132] | loss: 2.6609137058258057\n",
      "Iteration [8133] | loss: 2.661423683166504\n",
      "Iteration [8134] | loss: 2.6619505882263184\n",
      "Iteration [8135] | loss: 2.6607847213745117\n",
      "Iteration [8136] | loss: 2.6625590324401855\n",
      "Iteration [8137] | loss: 2.660247802734375\n",
      "Iteration [8138] | loss: 2.6613237857818604\n",
      "Iteration [8139] | loss: 2.661452293395996\n",
      "Iteration [8140] | loss: 2.66070556640625\n",
      "Iteration [8141] | loss: 2.6623988151550293\n",
      "Iteration [8142] | loss: 2.660158395767212\n",
      "Iteration [8143] | loss: 2.6619834899902344\n",
      "Iteration [8144] | loss: 2.6609842777252197\n",
      "Iteration [8145] | loss: 2.6613190174102783\n",
      "Iteration [8146] | loss: 2.6620163917541504\n",
      "Iteration [8147] | loss: 2.660688638687134\n",
      "Iteration [8148] | loss: 2.6624698638916016\n",
      "Iteration [8149] | loss: 2.660621404647827\n",
      "Iteration [8150] | loss: 2.6617629528045654\n",
      "Iteration [8151] | loss: 2.6612534523010254\n",
      "Iteration [8152] | loss: 2.660557746887207\n",
      "Iteration [8153] | loss: 2.6623916625976562\n",
      "Iteration [8154] | loss: 2.660111427307129\n",
      "Iteration [8155] | loss: 2.661714792251587\n",
      "Iteration [8156] | loss: 2.661224603652954\n",
      "Iteration [8157] | loss: 2.6610732078552246\n",
      "Iteration [8158] | loss: 2.662238121032715\n",
      "Iteration [8159] | loss: 2.660461187362671\n",
      "Iteration [8160] | loss: 2.6622586250305176\n",
      "Iteration [8161] | loss: 2.660827398300171\n",
      "Iteration [8162] | loss: 2.661560535430908\n",
      "Iteration [8163] | loss: 2.661811590194702\n",
      "Iteration [8164] | loss: 2.660917043685913\n",
      "Iteration [8165] | loss: 2.662182092666626\n",
      "Iteration [8166] | loss: 2.660266160964966\n",
      "Iteration [8167] | loss: 2.6615242958068848\n",
      "Iteration [8168] | loss: 2.661367893218994\n",
      "Iteration [8169] | loss: 2.660898447036743\n",
      "Iteration [8170] | loss: 2.6623728275299072\n",
      "Iteration [8171] | loss: 2.6602866649627686\n",
      "Iteration [8172] | loss: 2.6621334552764893\n",
      "Iteration [8173] | loss: 2.6608991622924805\n",
      "Iteration [8174] | loss: 2.6614537239074707\n",
      "Iteration [8175] | loss: 2.6619362831115723\n",
      "Iteration [8176] | loss: 2.6608119010925293\n",
      "Iteration [8177] | loss: 2.6625404357910156\n",
      "Iteration [8178] | loss: 2.6603567600250244\n",
      "Iteration [8179] | loss: 2.661378860473633\n",
      "Iteration [8180] | loss: 2.6614558696746826\n",
      "Iteration [8181] | loss: 2.6607635021209717\n",
      "Iteration [8182] | loss: 2.662397861480713\n",
      "Iteration [8183] | loss: 2.66020131111145\n",
      "Iteration [8184] | loss: 2.662020444869995\n",
      "Iteration [8185] | loss: 2.6609809398651123\n",
      "Iteration [8186] | loss: 2.6613523960113525\n",
      "Iteration [8187] | loss: 2.6620113849639893\n",
      "Iteration [8188] | loss: 2.6607186794281006\n",
      "Iteration [8189] | loss: 2.662497043609619\n",
      "Iteration [8190] | loss: 2.6605114936828613\n",
      "Iteration [8191] | loss: 2.6612610816955566\n",
      "Iteration [8192] | loss: 2.6614980697631836\n",
      "Iteration [8193] | loss: 2.6606390476226807\n",
      "Iteration [8194] | loss: 2.6624412536621094\n",
      "Iteration [8195] | loss: 2.660104513168335\n",
      "Iteration [8196] | loss: 2.6619319915771484\n",
      "Iteration [8197] | loss: 2.661024332046509\n",
      "Iteration [8198] | loss: 2.661271333694458\n",
      "Iteration [8199] | loss: 2.6620535850524902\n",
      "Iteration [8200] | loss: 2.66064715385437\n",
      "Iteration [8201] | loss: 2.6624317169189453\n",
      "Iteration [8202] | loss: 2.6606552600860596\n",
      "Iteration [8203] | loss: 2.661724328994751\n",
      "Iteration [8204] | loss: 2.661356210708618\n",
      "Iteration [8205] | loss: 2.6605634689331055\n",
      "Iteration [8206] | loss: 2.6623547077178955\n",
      "Iteration [8207] | loss: 2.6601409912109375\n",
      "Iteration [8208] | loss: 2.6616809368133545\n",
      "Iteration [8209] | loss: 2.6612534523010254\n",
      "Iteration [8210] | loss: 2.6610403060913086\n",
      "Iteration [8211] | loss: 2.662264347076416\n",
      "Iteration [8212] | loss: 2.660423517227173\n",
      "Iteration [8213] | loss: 2.6622540950775146\n",
      "Iteration [8214] | loss: 2.660796880722046\n",
      "Iteration [8215] | loss: 2.661567211151123\n",
      "Iteration [8216] | loss: 2.6618404388427734\n",
      "Iteration [8217] | loss: 2.6609151363372803\n",
      "Iteration [8218] | loss: 2.6622936725616455\n",
      "Iteration [8219] | loss: 2.6602816581726074\n",
      "Iteration [8220] | loss: 2.66147780418396\n",
      "Iteration [8221] | loss: 2.6613850593566895\n",
      "Iteration [8222] | loss: 2.6608541011810303\n",
      "Iteration [8223] | loss: 2.6623871326446533\n",
      "Iteration [8224] | loss: 2.6602351665496826\n",
      "Iteration [8225] | loss: 2.662100076675415\n",
      "Iteration [8226] | loss: 2.6609137058258057\n",
      "Iteration [8227] | loss: 2.6614227294921875\n",
      "Iteration [8228] | loss: 2.6619505882263184\n",
      "Iteration [8229] | loss: 2.6607842445373535\n",
      "Iteration [8230] | loss: 2.662559986114502\n",
      "Iteration [8231] | loss: 2.660247802734375\n",
      "Iteration [8232] | loss: 2.6613242626190186\n",
      "Iteration [8233] | loss: 2.6614532470703125\n",
      "Iteration [8234] | loss: 2.660705089569092\n",
      "Iteration [8235] | loss: 2.6623988151550293\n",
      "Iteration [8236] | loss: 2.6601579189300537\n",
      "Iteration [8237] | loss: 2.6619834899902344\n",
      "Iteration [8238] | loss: 2.660985231399536\n",
      "Iteration [8239] | loss: 2.661318063735962\n",
      "Iteration [8240] | loss: 2.6620163917541504\n",
      "Iteration [8241] | loss: 2.6606881618499756\n",
      "Iteration [8242] | loss: 2.6624698638916016\n",
      "Iteration [8243] | loss: 2.660620927810669\n",
      "Iteration [8244] | loss: 2.6617629528045654\n",
      "Iteration [8245] | loss: 2.6612534523010254\n",
      "Iteration [8246] | loss: 2.660557746887207\n",
      "Iteration [8247] | loss: 2.6623916625976562\n",
      "Iteration [8248] | loss: 2.660111427307129\n",
      "Iteration [8249] | loss: 2.6617159843444824\n",
      "Iteration [8250] | loss: 2.661224603652954\n",
      "Iteration [8251] | loss: 2.661072015762329\n",
      "Iteration [8252] | loss: 2.662238121032715\n",
      "Iteration [8253] | loss: 2.660461664199829\n",
      "Iteration [8254] | loss: 2.662259340286255\n",
      "Iteration [8255] | loss: 2.6608288288116455\n",
      "Iteration [8256] | loss: 2.6615588665008545\n",
      "Iteration [8257] | loss: 2.661813259124756\n",
      "Iteration [8258] | loss: 2.660917282104492\n",
      "Iteration [8259] | loss: 2.6621828079223633\n",
      "Iteration [8260] | loss: 2.660266637802124\n",
      "Iteration [8261] | loss: 2.6615242958068848\n",
      "Iteration [8262] | loss: 2.661367893218994\n",
      "Iteration [8263] | loss: 2.660897970199585\n",
      "Iteration [8264] | loss: 2.6623733043670654\n",
      "Iteration [8265] | loss: 2.6602866649627686\n",
      "Iteration [8266] | loss: 2.6621339321136475\n",
      "Iteration [8267] | loss: 2.6608996391296387\n",
      "Iteration [8268] | loss: 2.6614558696746826\n",
      "Iteration [8269] | loss: 2.6619365215301514\n",
      "Iteration [8270] | loss: 2.660813331604004\n",
      "Iteration [8271] | loss: 2.6625404357910156\n",
      "Iteration [8272] | loss: 2.6603567600250244\n",
      "Iteration [8273] | loss: 2.661378860473633\n",
      "Iteration [8274] | loss: 2.66145396232605\n",
      "Iteration [8275] | loss: 2.6607630252838135\n",
      "Iteration [8276] | loss: 2.662398338317871\n",
      "Iteration [8277] | loss: 2.6602001190185547\n",
      "Iteration [8278] | loss: 2.662019729614258\n",
      "Iteration [8279] | loss: 2.6609809398651123\n",
      "Iteration [8280] | loss: 2.6613523960113525\n",
      "Iteration [8281] | loss: 2.6620116233825684\n",
      "Iteration [8282] | loss: 2.6607182025909424\n",
      "Iteration [8283] | loss: 2.6624979972839355\n",
      "Iteration [8284] | loss: 2.6605100631713867\n",
      "Iteration [8285] | loss: 2.6612610816955566\n",
      "Iteration [8286] | loss: 2.6614980697631836\n",
      "Iteration [8287] | loss: 2.6606385707855225\n",
      "Iteration [8288] | loss: 2.6624417304992676\n",
      "Iteration [8289] | loss: 2.660104990005493\n",
      "Iteration [8290] | loss: 2.6619324684143066\n",
      "Iteration [8291] | loss: 2.661024332046509\n",
      "Iteration [8292] | loss: 2.661271333694458\n",
      "Iteration [8293] | loss: 2.662052869796753\n",
      "Iteration [8294] | loss: 2.66064715385437\n",
      "Iteration [8295] | loss: 2.6624326705932617\n",
      "Iteration [8296] | loss: 2.6606547832489014\n",
      "Iteration [8297] | loss: 2.6617238521575928\n",
      "Iteration [8298] | loss: 2.661356210708618\n",
      "Iteration [8299] | loss: 2.660564422607422\n",
      "Iteration [8300] | loss: 2.6623551845550537\n",
      "Iteration [8301] | loss: 2.6601405143737793\n",
      "Iteration [8302] | loss: 2.6616809368133545\n",
      "Iteration [8303] | loss: 2.661252975463867\n",
      "Iteration [8304] | loss: 2.6610403060913086\n",
      "Iteration [8305] | loss: 2.6622633934020996\n",
      "Iteration [8306] | loss: 2.6604247093200684\n",
      "Iteration [8307] | loss: 2.6622536182403564\n",
      "Iteration [8308] | loss: 2.660796880722046\n",
      "Iteration [8309] | loss: 2.661567449569702\n",
      "Iteration [8310] | loss: 2.6618411540985107\n",
      "Iteration [8311] | loss: 2.6609151363372803\n",
      "Iteration [8312] | loss: 2.662292957305908\n",
      "Iteration [8313] | loss: 2.660281181335449\n",
      "Iteration [8314] | loss: 2.66147780418396\n",
      "Iteration [8315] | loss: 2.661384105682373\n",
      "Iteration [8316] | loss: 2.6608545780181885\n",
      "Iteration [8317] | loss: 2.6623871326446533\n",
      "Iteration [8318] | loss: 2.660233974456787\n",
      "Iteration [8319] | loss: 2.6620984077453613\n",
      "Iteration [8320] | loss: 2.6609137058258057\n",
      "Iteration [8321] | loss: 2.661423683166504\n",
      "Iteration [8322] | loss: 2.6619505882263184\n",
      "Iteration [8323] | loss: 2.6607847213745117\n",
      "Iteration [8324] | loss: 2.6625590324401855\n",
      "Iteration [8325] | loss: 2.660248279571533\n",
      "Iteration [8326] | loss: 2.6613247394561768\n",
      "Iteration [8327] | loss: 2.661452293395996\n",
      "Iteration [8328] | loss: 2.660705089569092\n",
      "Iteration [8329] | loss: 2.662397623062134\n",
      "Iteration [8330] | loss: 2.660158395767212\n",
      "Iteration [8331] | loss: 2.6619834899902344\n",
      "Iteration [8332] | loss: 2.6609842777252197\n",
      "Iteration [8333] | loss: 2.6613175868988037\n",
      "Iteration [8334] | loss: 2.662015914916992\n",
      "Iteration [8335] | loss: 2.6606881618499756\n",
      "Iteration [8336] | loss: 2.662470817565918\n",
      "Iteration [8337] | loss: 2.660621404647827\n",
      "Iteration [8338] | loss: 2.6617650985717773\n",
      "Iteration [8339] | loss: 2.6612534523010254\n",
      "Iteration [8340] | loss: 2.660557746887207\n",
      "Iteration [8341] | loss: 2.6623919010162354\n",
      "Iteration [8342] | loss: 2.660111427307129\n",
      "Iteration [8343] | loss: 2.6617164611816406\n",
      "Iteration [8344] | loss: 2.6612250804901123\n",
      "Iteration [8345] | loss: 2.661071538925171\n",
      "Iteration [8346] | loss: 2.6622376441955566\n",
      "Iteration [8347] | loss: 2.6604621410369873\n",
      "Iteration [8348] | loss: 2.6622588634490967\n",
      "Iteration [8349] | loss: 2.660827875137329\n",
      "Iteration [8350] | loss: 2.661560535430908\n",
      "Iteration [8351] | loss: 2.661811113357544\n",
      "Iteration [8352] | loss: 2.6609179973602295\n",
      "Iteration [8353] | loss: 2.6621828079223633\n",
      "Iteration [8354] | loss: 2.6602675914764404\n",
      "Iteration [8355] | loss: 2.6615242958068848\n",
      "Iteration [8356] | loss: 2.661367893218994\n",
      "Iteration [8357] | loss: 2.6608989238739014\n",
      "Iteration [8358] | loss: 2.6623737812042236\n",
      "Iteration [8359] | loss: 2.6602859497070312\n",
      "Iteration [8360] | loss: 2.6621334552764893\n",
      "Iteration [8361] | loss: 2.6608996391296387\n",
      "Iteration [8362] | loss: 2.661454439163208\n",
      "Iteration [8363] | loss: 2.6619365215301514\n",
      "Iteration [8364] | loss: 2.660813331604004\n",
      "Iteration [8365] | loss: 2.662540912628174\n",
      "Iteration [8366] | loss: 2.6603567600250244\n",
      "Iteration [8367] | loss: 2.6613781452178955\n",
      "Iteration [8368] | loss: 2.6614553928375244\n",
      "Iteration [8369] | loss: 2.6607635021209717\n",
      "Iteration [8370] | loss: 2.662398338317871\n",
      "Iteration [8371] | loss: 2.6601996421813965\n",
      "Iteration [8372] | loss: 2.662019729614258\n",
      "Iteration [8373] | loss: 2.660980463027954\n",
      "Iteration [8374] | loss: 2.6613516807556152\n",
      "Iteration [8375] | loss: 2.662013530731201\n",
      "Iteration [8376] | loss: 2.6607186794281006\n",
      "Iteration [8377] | loss: 2.6624979972839355\n",
      "Iteration [8378] | loss: 2.660511016845703\n",
      "Iteration [8379] | loss: 2.6612606048583984\n",
      "Iteration [8380] | loss: 2.6614980697631836\n",
      "Iteration [8381] | loss: 2.6606385707855225\n",
      "Iteration [8382] | loss: 2.6624417304992676\n",
      "Iteration [8383] | loss: 2.6601054668426514\n",
      "Iteration [8384] | loss: 2.661932945251465\n",
      "Iteration [8385] | loss: 2.661024808883667\n",
      "Iteration [8386] | loss: 2.6612720489501953\n",
      "Iteration [8387] | loss: 2.6620540618896484\n",
      "Iteration [8388] | loss: 2.66064715385437\n",
      "Iteration [8389] | loss: 2.6624302864074707\n",
      "Iteration [8390] | loss: 2.6606557369232178\n",
      "Iteration [8391] | loss: 2.661724328994751\n",
      "Iteration [8392] | loss: 2.6613552570343018\n",
      "Iteration [8393] | loss: 2.6605634689331055\n",
      "Iteration [8394] | loss: 2.662355661392212\n",
      "Iteration [8395] | loss: 2.6601409912109375\n",
      "Iteration [8396] | loss: 2.661681890487671\n",
      "Iteration [8397] | loss: 2.661252975463867\n",
      "Iteration [8398] | loss: 2.661039352416992\n",
      "Iteration [8399] | loss: 2.6622633934020996\n",
      "Iteration [8400] | loss: 2.66042423248291\n",
      "Iteration [8401] | loss: 2.6622533798217773\n",
      "Iteration [8402] | loss: 2.6607978343963623\n",
      "Iteration [8403] | loss: 2.661567211151123\n",
      "Iteration [8404] | loss: 2.6618409156799316\n",
      "Iteration [8405] | loss: 2.6609132289886475\n",
      "Iteration [8406] | loss: 2.66229248046875\n",
      "Iteration [8407] | loss: 2.660280227661133\n",
      "Iteration [8408] | loss: 2.6614768505096436\n",
      "Iteration [8409] | loss: 2.661383628845215\n",
      "Iteration [8410] | loss: 2.6608541011810303\n",
      "Iteration [8411] | loss: 2.6623880863189697\n",
      "Iteration [8412] | loss: 2.660233974456787\n",
      "Iteration [8413] | loss: 2.6620984077453613\n",
      "Iteration [8414] | loss: 2.6609137058258057\n",
      "Iteration [8415] | loss: 2.6614227294921875\n",
      "Iteration [8416] | loss: 2.661949872970581\n",
      "Iteration [8417] | loss: 2.6607842445373535\n",
      "Iteration [8418] | loss: 2.662559986114502\n",
      "Iteration [8419] | loss: 2.660248279571533\n",
      "Iteration [8420] | loss: 2.6613242626190186\n",
      "Iteration [8421] | loss: 2.661452293395996\n",
      "Iteration [8422] | loss: 2.6607065200805664\n",
      "Iteration [8423] | loss: 2.6623988151550293\n",
      "Iteration [8424] | loss: 2.66015887260437\n",
      "Iteration [8425] | loss: 2.6619818210601807\n",
      "Iteration [8426] | loss: 2.6609835624694824\n",
      "Iteration [8427] | loss: 2.661318063735962\n",
      "Iteration [8428] | loss: 2.662015438079834\n",
      "Iteration [8429] | loss: 2.6606881618499756\n",
      "Iteration [8430] | loss: 2.6624698638916016\n",
      "Iteration [8431] | loss: 2.6606218814849854\n",
      "Iteration [8432] | loss: 2.661764621734619\n",
      "Iteration [8433] | loss: 2.6612536907196045\n",
      "Iteration [8434] | loss: 2.6605567932128906\n",
      "Iteration [8435] | loss: 2.6623919010162354\n",
      "Iteration [8436] | loss: 2.660111427307129\n",
      "Iteration [8437] | loss: 2.661715507507324\n",
      "Iteration [8438] | loss: 2.6612255573272705\n",
      "Iteration [8439] | loss: 2.6610705852508545\n",
      "Iteration [8440] | loss: 2.662238121032715\n",
      "Iteration [8441] | loss: 2.660461187362671\n",
      "Iteration [8442] | loss: 2.662259817123413\n",
      "Iteration [8443] | loss: 2.6608288288116455\n",
      "Iteration [8444] | loss: 2.6615593433380127\n",
      "Iteration [8445] | loss: 2.661813735961914\n",
      "Iteration [8446] | loss: 2.6609175205230713\n",
      "Iteration [8447] | loss: 2.6621832847595215\n",
      "Iteration [8448] | loss: 2.6602671146392822\n",
      "Iteration [8449] | loss: 2.6615242958068848\n",
      "Iteration [8450] | loss: 2.6613705158233643\n",
      "Iteration [8451] | loss: 2.660898208618164\n",
      "Iteration [8452] | loss: 2.6623733043670654\n",
      "Iteration [8453] | loss: 2.6602866649627686\n",
      "Iteration [8454] | loss: 2.6621334552764893\n",
      "Iteration [8455] | loss: 2.6608989238739014\n",
      "Iteration [8456] | loss: 2.66145396232605\n",
      "Iteration [8457] | loss: 2.661935329437256\n",
      "Iteration [8458] | loss: 2.6608123779296875\n",
      "Iteration [8459] | loss: 2.6625404357910156\n",
      "Iteration [8460] | loss: 2.6603567600250244\n",
      "Iteration [8461] | loss: 2.661379337310791\n",
      "Iteration [8462] | loss: 2.66145396232605\n",
      "Iteration [8463] | loss: 2.6607630252838135\n",
      "Iteration [8464] | loss: 2.662398338317871\n",
      "Iteration [8465] | loss: 2.6601996421813965\n",
      "Iteration [8466] | loss: 2.6620192527770996\n",
      "Iteration [8467] | loss: 2.660979986190796\n",
      "Iteration [8468] | loss: 2.6613523960113525\n",
      "Iteration [8469] | loss: 2.662010908126831\n",
      "Iteration [8470] | loss: 2.6607182025909424\n",
      "Iteration [8471] | loss: 2.6624975204467773\n",
      "Iteration [8472] | loss: 2.660510540008545\n",
      "Iteration [8473] | loss: 2.6612610816955566\n",
      "Iteration [8474] | loss: 2.661498546600342\n",
      "Iteration [8475] | loss: 2.6606385707855225\n",
      "Iteration [8476] | loss: 2.6624417304992676\n",
      "Iteration [8477] | loss: 2.660104990005493\n",
      "Iteration [8478] | loss: 2.6619324684143066\n",
      "Iteration [8479] | loss: 2.661024808883667\n",
      "Iteration [8480] | loss: 2.6612708568573\n",
      "Iteration [8481] | loss: 2.6620523929595947\n",
      "Iteration [8482] | loss: 2.6606459617614746\n",
      "Iteration [8483] | loss: 2.6624317169189453\n",
      "Iteration [8484] | loss: 2.6606547832489014\n",
      "Iteration [8485] | loss: 2.6617238521575928\n",
      "Iteration [8486] | loss: 2.6613566875457764\n",
      "Iteration [8487] | loss: 2.6605653762817383\n",
      "Iteration [8488] | loss: 2.6623551845550537\n",
      "Iteration [8489] | loss: 2.6601405143737793\n",
      "Iteration [8490] | loss: 2.6616804599761963\n",
      "Iteration [8491] | loss: 2.6612534523010254\n",
      "Iteration [8492] | loss: 2.6610403060913086\n",
      "Iteration [8493] | loss: 2.662263870239258\n",
      "Iteration [8494] | loss: 2.6604251861572266\n",
      "Iteration [8495] | loss: 2.6622536182403564\n",
      "Iteration [8496] | loss: 2.660795211791992\n",
      "Iteration [8497] | loss: 2.661566734313965\n",
      "Iteration [8498] | loss: 2.6618411540985107\n",
      "Iteration [8499] | loss: 2.6609151363372803\n",
      "Iteration [8500] | loss: 2.662292718887329\n",
      "Iteration [8501] | loss: 2.6602816581726074\n",
      "Iteration [8502] | loss: 2.66147780418396\n",
      "Iteration [8503] | loss: 2.6613831520080566\n",
      "Iteration [8504] | loss: 2.6608550548553467\n",
      "Iteration [8505] | loss: 2.662386655807495\n",
      "Iteration [8506] | loss: 2.660233974456787\n",
      "Iteration [8507] | loss: 2.662100076675415\n",
      "Iteration [8508] | loss: 2.6609137058258057\n",
      "Iteration [8509] | loss: 2.6614232063293457\n",
      "Iteration [8510] | loss: 2.661949872970581\n",
      "Iteration [8511] | loss: 2.6607842445373535\n",
      "Iteration [8512] | loss: 2.662559986114502\n",
      "Iteration [8513] | loss: 2.660247802734375\n",
      "Iteration [8514] | loss: 2.6613242626190186\n",
      "Iteration [8515] | loss: 2.6614532470703125\n",
      "Iteration [8516] | loss: 2.6607048511505127\n",
      "Iteration [8517] | loss: 2.662398338317871\n",
      "Iteration [8518] | loss: 2.66015887260437\n",
      "Iteration [8519] | loss: 2.6619832515716553\n",
      "Iteration [8520] | loss: 2.6609833240509033\n",
      "Iteration [8521] | loss: 2.6613175868988037\n",
      "Iteration [8522] | loss: 2.662015914916992\n",
      "Iteration [8523] | loss: 2.6606881618499756\n",
      "Iteration [8524] | loss: 2.6624703407287598\n",
      "Iteration [8525] | loss: 2.6606204509735107\n",
      "Iteration [8526] | loss: 2.66176438331604\n",
      "Iteration [8527] | loss: 2.6612536907196045\n",
      "Iteration [8528] | loss: 2.6605546474456787\n",
      "Iteration [8529] | loss: 2.6623916625976562\n",
      "Iteration [8530] | loss: 2.6601107120513916\n",
      "Iteration [8531] | loss: 2.661715269088745\n",
      "Iteration [8532] | loss: 2.661224603652954\n",
      "Iteration [8533] | loss: 2.6610727310180664\n",
      "Iteration [8534] | loss: 2.662238121032715\n",
      "Iteration [8535] | loss: 2.660459518432617\n",
      "Iteration [8536] | loss: 2.6622607707977295\n",
      "Iteration [8537] | loss: 2.6608288288116455\n",
      "Iteration [8538] | loss: 2.6615588665008545\n",
      "Iteration [8539] | loss: 2.661811113357544\n",
      "Iteration [8540] | loss: 2.660917282104492\n",
      "Iteration [8541] | loss: 2.662182331085205\n",
      "Iteration [8542] | loss: 2.660266160964966\n",
      "Iteration [8543] | loss: 2.6615233421325684\n",
      "Iteration [8544] | loss: 2.661367893218994\n",
      "Iteration [8545] | loss: 2.660897970199585\n",
      "Iteration [8546] | loss: 2.6623737812042236\n",
      "Iteration [8547] | loss: 2.6602866649627686\n",
      "Iteration [8548] | loss: 2.66213321685791\n",
      "Iteration [8549] | loss: 2.660900115966797\n",
      "Iteration [8550] | loss: 2.661454916000366\n",
      "Iteration [8551] | loss: 2.6619365215301514\n",
      "Iteration [8552] | loss: 2.660813808441162\n",
      "Iteration [8553] | loss: 2.662540912628174\n",
      "Iteration [8554] | loss: 2.6603567600250244\n",
      "Iteration [8555] | loss: 2.661379814147949\n",
      "Iteration [8556] | loss: 2.6614553928375244\n",
      "Iteration [8557] | loss: 2.6607630252838135\n",
      "Iteration [8558] | loss: 2.662397861480713\n",
      "Iteration [8559] | loss: 2.66020131111145\n",
      "Iteration [8560] | loss: 2.6620209217071533\n",
      "Iteration [8561] | loss: 2.6609818935394287\n",
      "Iteration [8562] | loss: 2.661353349685669\n",
      "Iteration [8563] | loss: 2.6620116233825684\n",
      "Iteration [8564] | loss: 2.6607182025909424\n",
      "Iteration [8565] | loss: 2.662497043609619\n",
      "Iteration [8566] | loss: 2.6605114936828613\n",
      "Iteration [8567] | loss: 2.661261558532715\n",
      "Iteration [8568] | loss: 2.6614980697631836\n",
      "Iteration [8569] | loss: 2.6606390476226807\n",
      "Iteration [8570] | loss: 2.6624417304992676\n",
      "Iteration [8571] | loss: 2.660104990005493\n",
      "Iteration [8572] | loss: 2.6619324684143066\n",
      "Iteration [8573] | loss: 2.661024808883667\n",
      "Iteration [8574] | loss: 2.661271333694458\n",
      "Iteration [8575] | loss: 2.662053346633911\n",
      "Iteration [8576] | loss: 2.66064715385437\n",
      "Iteration [8577] | loss: 2.6624298095703125\n",
      "Iteration [8578] | loss: 2.6606552600860596\n",
      "Iteration [8579] | loss: 2.6617255210876465\n",
      "Iteration [8580] | loss: 2.6613545417785645\n",
      "Iteration [8581] | loss: 2.6605629920959473\n",
      "Iteration [8582] | loss: 2.6623551845550537\n",
      "Iteration [8583] | loss: 2.6601412296295166\n",
      "Iteration [8584] | loss: 2.661681890487671\n",
      "Iteration [8585] | loss: 2.661252975463867\n",
      "Iteration [8586] | loss: 2.661040782928467\n",
      "Iteration [8587] | loss: 2.662264347076416\n",
      "Iteration [8588] | loss: 2.6604251861572266\n",
      "Iteration [8589] | loss: 2.6622514724731445\n",
      "Iteration [8590] | loss: 2.660796880722046\n",
      "Iteration [8591] | loss: 2.661567211151123\n",
      "Iteration [8592] | loss: 2.661841630935669\n",
      "Iteration [8593] | loss: 2.6609134674072266\n",
      "Iteration [8594] | loss: 2.662292003631592\n",
      "Iteration [8595] | loss: 2.6602797508239746\n",
      "Iteration [8596] | loss: 2.66147780418396\n",
      "Iteration [8597] | loss: 2.6613845825195312\n",
      "Iteration [8598] | loss: 2.6608545780181885\n",
      "Iteration [8599] | loss: 2.6623878479003906\n",
      "Iteration [8600] | loss: 2.660233974456787\n",
      "Iteration [8601] | loss: 2.662100076675415\n",
      "Iteration [8602] | loss: 2.6609132289886475\n",
      "Iteration [8603] | loss: 2.661423683166504\n",
      "Iteration [8604] | loss: 2.6619489192962646\n",
      "Iteration [8605] | loss: 2.6607842445373535\n",
      "Iteration [8606] | loss: 2.662559986114502\n",
      "Iteration [8607] | loss: 2.660247802734375\n",
      "Iteration [8608] | loss: 2.6613242626190186\n",
      "Iteration [8609] | loss: 2.6614532470703125\n",
      "Iteration [8610] | loss: 2.660705089569092\n",
      "Iteration [8611] | loss: 2.6623988151550293\n",
      "Iteration [8612] | loss: 2.66015887260437\n",
      "Iteration [8613] | loss: 2.6619834899902344\n",
      "Iteration [8614] | loss: 2.660982608795166\n",
      "Iteration [8615] | loss: 2.6613171100616455\n",
      "Iteration [8616] | loss: 2.6620163917541504\n",
      "Iteration [8617] | loss: 2.660689115524292\n",
      "Iteration [8618] | loss: 2.6624693870544434\n",
      "Iteration [8619] | loss: 2.6606204509735107\n",
      "Iteration [8620] | loss: 2.66176438331604\n",
      "Iteration [8621] | loss: 2.6612541675567627\n",
      "Iteration [8622] | loss: 2.6605563163757324\n",
      "Iteration [8623] | loss: 2.6623919010162354\n",
      "Iteration [8624] | loss: 2.660111427307129\n",
      "Iteration [8625] | loss: 2.661715269088745\n",
      "Iteration [8626] | loss: 2.661224603652954\n",
      "Iteration [8627] | loss: 2.6610727310180664\n",
      "Iteration [8628] | loss: 2.662238121032715\n",
      "Iteration [8629] | loss: 2.660461187362671\n",
      "Iteration [8630] | loss: 2.662259817123413\n",
      "Iteration [8631] | loss: 2.660827875137329\n",
      "Iteration [8632] | loss: 2.6615588665008545\n",
      "Iteration [8633] | loss: 2.661811113357544\n",
      "Iteration [8634] | loss: 2.660918951034546\n",
      "Iteration [8635] | loss: 2.6621828079223633\n",
      "Iteration [8636] | loss: 2.6602671146392822\n",
      "Iteration [8637] | loss: 2.6615238189697266\n",
      "Iteration [8638] | loss: 2.661367893218994\n",
      "Iteration [8639] | loss: 2.660898447036743\n",
      "Iteration [8640] | loss: 2.6623733043670654\n",
      "Iteration [8641] | loss: 2.6602866649627686\n",
      "Iteration [8642] | loss: 2.6621339321136475\n",
      "Iteration [8643] | loss: 2.660900115966797\n",
      "Iteration [8644] | loss: 2.66145396232605\n",
      "Iteration [8645] | loss: 2.6619365215301514\n",
      "Iteration [8646] | loss: 2.660813331604004\n",
      "Iteration [8647] | loss: 2.662539482116699\n",
      "Iteration [8648] | loss: 2.6603567600250244\n",
      "Iteration [8649] | loss: 2.661379337310791\n",
      "Iteration [8650] | loss: 2.661454916000366\n",
      "Iteration [8651] | loss: 2.6607635021209717\n",
      "Iteration [8652] | loss: 2.662397861480713\n",
      "Iteration [8653] | loss: 2.6601996421813965\n",
      "Iteration [8654] | loss: 2.6620192527770996\n",
      "Iteration [8655] | loss: 2.6609809398651123\n",
      "Iteration [8656] | loss: 2.6613516807556152\n",
      "Iteration [8657] | loss: 2.6620113849639893\n",
      "Iteration [8658] | loss: 2.660719156265259\n",
      "Iteration [8659] | loss: 2.6624975204467773\n",
      "Iteration [8660] | loss: 2.660510540008545\n",
      "Iteration [8661] | loss: 2.6612610816955566\n",
      "Iteration [8662] | loss: 2.6614980697631836\n",
      "Iteration [8663] | loss: 2.660639524459839\n",
      "Iteration [8664] | loss: 2.6624412536621094\n",
      "Iteration [8665] | loss: 2.6601054668426514\n",
      "Iteration [8666] | loss: 2.6619319915771484\n",
      "Iteration [8667] | loss: 2.6610240936279297\n",
      "Iteration [8668] | loss: 2.661271333694458\n",
      "Iteration [8669] | loss: 2.6620535850524902\n",
      "Iteration [8670] | loss: 2.6606462001800537\n",
      "Iteration [8671] | loss: 2.6624317169189453\n",
      "Iteration [8672] | loss: 2.6606545448303223\n",
      "Iteration [8673] | loss: 2.6617255210876465\n",
      "Iteration [8674] | loss: 2.66135573387146\n",
      "Iteration [8675] | loss: 2.66056489944458\n",
      "Iteration [8676] | loss: 2.6623547077178955\n",
      "Iteration [8677] | loss: 2.6601405143737793\n",
      "Iteration [8678] | loss: 2.6616809368133545\n",
      "Iteration [8679] | loss: 2.6612534523010254\n",
      "Iteration [8680] | loss: 2.661040782928467\n",
      "Iteration [8681] | loss: 2.6622633934020996\n",
      "Iteration [8682] | loss: 2.6604247093200684\n",
      "Iteration [8683] | loss: 2.662252902984619\n",
      "Iteration [8684] | loss: 2.660796880722046\n",
      "Iteration [8685] | loss: 2.661567211151123\n",
      "Iteration [8686] | loss: 2.6618411540985107\n",
      "Iteration [8687] | loss: 2.660914182662964\n",
      "Iteration [8688] | loss: 2.662292957305908\n",
      "Iteration [8689] | loss: 2.660280704498291\n",
      "Iteration [8690] | loss: 2.66147780418396\n",
      "Iteration [8691] | loss: 2.661384105682373\n",
      "Iteration [8692] | loss: 2.6608550548553467\n",
      "Iteration [8693] | loss: 2.662386655807495\n",
      "Iteration [8694] | loss: 2.660233974456787\n",
      "Iteration [8695] | loss: 2.6620991230010986\n",
      "Iteration [8696] | loss: 2.6609137058258057\n",
      "Iteration [8697] | loss: 2.6614232063293457\n",
      "Iteration [8698] | loss: 2.661949872970581\n",
      "Iteration [8699] | loss: 2.6607840061187744\n",
      "Iteration [8700] | loss: 2.6625585556030273\n",
      "Iteration [8701] | loss: 2.660248279571533\n",
      "Iteration [8702] | loss: 2.661325216293335\n",
      "Iteration [8703] | loss: 2.661452293395996\n",
      "Iteration [8704] | loss: 2.660705089569092\n",
      "Iteration [8705] | loss: 2.662398099899292\n",
      "Iteration [8706] | loss: 2.6601579189300537\n",
      "Iteration [8707] | loss: 2.661982536315918\n",
      "Iteration [8708] | loss: 2.660985231399536\n",
      "Iteration [8709] | loss: 2.661318063735962\n",
      "Iteration [8710] | loss: 2.662014961242676\n",
      "Iteration [8711] | loss: 2.6606881618499756\n",
      "Iteration [8712] | loss: 2.6624698638916016\n",
      "Iteration [8713] | loss: 2.6606204509735107\n",
      "Iteration [8714] | loss: 2.66176438331604\n",
      "Iteration [8715] | loss: 2.6612541675567627\n",
      "Iteration [8716] | loss: 2.660555601119995\n",
      "Iteration [8717] | loss: 2.6623919010162354\n",
      "Iteration [8718] | loss: 2.660111427307129\n",
      "Iteration [8719] | loss: 2.6617164611816406\n",
      "Iteration [8720] | loss: 2.6612250804901123\n",
      "Iteration [8721] | loss: 2.661071538925171\n",
      "Iteration [8722] | loss: 2.662238121032715\n",
      "Iteration [8723] | loss: 2.660461187362671\n",
      "Iteration [8724] | loss: 2.6622586250305176\n",
      "Iteration [8725] | loss: 2.6608288288116455\n",
      "Iteration [8726] | loss: 2.661559820175171\n",
      "Iteration [8727] | loss: 2.661811590194702\n",
      "Iteration [8728] | loss: 2.660917282104492\n",
      "Iteration [8729] | loss: 2.662182331085205\n",
      "Iteration [8730] | loss: 2.660266160964966\n",
      "Iteration [8731] | loss: 2.6615242958068848\n",
      "Iteration [8732] | loss: 2.6613683700561523\n",
      "Iteration [8733] | loss: 2.6608989238739014\n",
      "Iteration [8734] | loss: 2.6623733043670654\n",
      "Iteration [8735] | loss: 2.6602859497070312\n",
      "Iteration [8736] | loss: 2.6621334552764893\n",
      "Iteration [8737] | loss: 2.6608996391296387\n",
      "Iteration [8738] | loss: 2.661454439163208\n",
      "Iteration [8739] | loss: 2.661935806274414\n",
      "Iteration [8740] | loss: 2.6608119010925293\n",
      "Iteration [8741] | loss: 2.662540912628174\n",
      "Iteration [8742] | loss: 2.6603567600250244\n",
      "Iteration [8743] | loss: 2.661379337310791\n",
      "Iteration [8744] | loss: 2.661454916000366\n",
      "Iteration [8745] | loss: 2.6607630252838135\n",
      "Iteration [8746] | loss: 2.662398338317871\n",
      "Iteration [8747] | loss: 2.6602001190185547\n",
      "Iteration [8748] | loss: 2.6620192527770996\n",
      "Iteration [8749] | loss: 2.660980463027954\n",
      "Iteration [8750] | loss: 2.6613519191741943\n",
      "Iteration [8751] | loss: 2.6620116233825684\n",
      "Iteration [8752] | loss: 2.6607186794281006\n",
      "Iteration [8753] | loss: 2.66249680519104\n",
      "Iteration [8754] | loss: 2.6605114936828613\n",
      "Iteration [8755] | loss: 2.6612610816955566\n",
      "Iteration [8756] | loss: 2.6614980697631836\n",
      "Iteration [8757] | loss: 2.6606390476226807\n",
      "Iteration [8758] | loss: 2.6624417304992676\n",
      "Iteration [8759] | loss: 2.6601054668426514\n",
      "Iteration [8760] | loss: 2.6619324684143066\n",
      "Iteration [8761] | loss: 2.661024332046509\n",
      "Iteration [8762] | loss: 2.661271333694458\n",
      "Iteration [8763] | loss: 2.6620523929595947\n",
      "Iteration [8764] | loss: 2.66064715385437\n",
      "Iteration [8765] | loss: 2.662430763244629\n",
      "Iteration [8766] | loss: 2.6606547832489014\n",
      "Iteration [8767] | loss: 2.6617238521575928\n",
      "Iteration [8768] | loss: 2.66135573387146\n",
      "Iteration [8769] | loss: 2.660564422607422\n",
      "Iteration [8770] | loss: 2.6623551845550537\n",
      "Iteration [8771] | loss: 2.660140037536621\n",
      "Iteration [8772] | loss: 2.661681890487671\n",
      "Iteration [8773] | loss: 2.6612534523010254\n",
      "Iteration [8774] | loss: 2.661040782928467\n",
      "Iteration [8775] | loss: 2.662263870239258\n",
      "Iteration [8776] | loss: 2.66042423248291\n",
      "Iteration [8777] | loss: 2.6622533798217773\n",
      "Iteration [8778] | loss: 2.660796880722046\n",
      "Iteration [8779] | loss: 2.661566734313965\n",
      "Iteration [8780] | loss: 2.6618411540985107\n",
      "Iteration [8781] | loss: 2.6609151363372803\n",
      "Iteration [8782] | loss: 2.66229248046875\n",
      "Iteration [8783] | loss: 2.660280704498291\n",
      "Iteration [8784] | loss: 2.6614768505096436\n",
      "Iteration [8785] | loss: 2.6613850593566895\n",
      "Iteration [8786] | loss: 2.6608550548553467\n",
      "Iteration [8787] | loss: 2.6623871326446533\n",
      "Iteration [8788] | loss: 2.6602346897125244\n",
      "Iteration [8789] | loss: 2.662100076675415\n",
      "Iteration [8790] | loss: 2.6609137058258057\n",
      "Iteration [8791] | loss: 2.6614246368408203\n",
      "Iteration [8792] | loss: 2.661949872970581\n",
      "Iteration [8793] | loss: 2.6607842445373535\n",
      "Iteration [8794] | loss: 2.6625583171844482\n",
      "Iteration [8795] | loss: 2.6602470874786377\n",
      "Iteration [8796] | loss: 2.6613242626190186\n",
      "Iteration [8797] | loss: 2.6614537239074707\n",
      "Iteration [8798] | loss: 2.6607048511505127\n",
      "Iteration [8799] | loss: 2.662397623062134\n",
      "Iteration [8800] | loss: 2.66015887260437\n",
      "Iteration [8801] | loss: 2.6619818210601807\n",
      "Iteration [8802] | loss: 2.6609842777252197\n",
      "Iteration [8803] | loss: 2.6613175868988037\n",
      "Iteration [8804] | loss: 2.6620163917541504\n",
      "Iteration [8805] | loss: 2.6606881618499756\n",
      "Iteration [8806] | loss: 2.6624698638916016\n",
      "Iteration [8807] | loss: 2.6606204509735107\n",
      "Iteration [8808] | loss: 2.6617650985717773\n",
      "Iteration [8809] | loss: 2.6612534523010254\n",
      "Iteration [8810] | loss: 2.660555601119995\n",
      "Iteration [8811] | loss: 2.6623919010162354\n",
      "Iteration [8812] | loss: 2.66011118888855\n",
      "Iteration [8813] | loss: 2.661715269088745\n",
      "Iteration [8814] | loss: 2.661224603652954\n",
      "Iteration [8815] | loss: 2.661071538925171\n",
      "Iteration [8816] | loss: 2.662238121032715\n",
      "Iteration [8817] | loss: 2.660461187362671\n",
      "Iteration [8818] | loss: 2.6622586250305176\n",
      "Iteration [8819] | loss: 2.6608288288116455\n",
      "Iteration [8820] | loss: 2.661559820175171\n",
      "Iteration [8821] | loss: 2.661811590194702\n",
      "Iteration [8822] | loss: 2.660917282104492\n",
      "Iteration [8823] | loss: 2.6621828079223633\n",
      "Iteration [8824] | loss: 2.6602675914764404\n",
      "Iteration [8825] | loss: 2.6615238189697266\n",
      "Iteration [8826] | loss: 2.661367893218994\n",
      "Iteration [8827] | loss: 2.6608989238739014\n",
      "Iteration [8828] | loss: 2.6623728275299072\n",
      "Iteration [8829] | loss: 2.6602859497070312\n",
      "Iteration [8830] | loss: 2.66213321685791\n",
      "Iteration [8831] | loss: 2.6608996391296387\n",
      "Iteration [8832] | loss: 2.6614558696746826\n",
      "Iteration [8833] | loss: 2.6619365215301514\n",
      "Iteration [8834] | loss: 2.660813808441162\n",
      "Iteration [8835] | loss: 2.6625404357910156\n",
      "Iteration [8836] | loss: 2.6603567600250244\n",
      "Iteration [8837] | loss: 2.661379337310791\n",
      "Iteration [8838] | loss: 2.6614532470703125\n",
      "Iteration [8839] | loss: 2.660764455795288\n",
      "Iteration [8840] | loss: 2.662397861480713\n",
      "Iteration [8841] | loss: 2.6601996421813965\n",
      "Iteration [8842] | loss: 2.662019729614258\n",
      "Iteration [8843] | loss: 2.660979747772217\n",
      "Iteration [8844] | loss: 2.6613519191741943\n",
      "Iteration [8845] | loss: 2.6620113849639893\n",
      "Iteration [8846] | loss: 2.6607182025909424\n",
      "Iteration [8847] | loss: 2.662497043609619\n",
      "Iteration [8848] | loss: 2.660510540008545\n",
      "Iteration [8849] | loss: 2.661261558532715\n",
      "Iteration [8850] | loss: 2.6614980697631836\n",
      "Iteration [8851] | loss: 2.660639762878418\n",
      "Iteration [8852] | loss: 2.6624417304992676\n",
      "Iteration [8853] | loss: 2.6601054668426514\n",
      "Iteration [8854] | loss: 2.6619324684143066\n",
      "Iteration [8855] | loss: 2.661024808883667\n",
      "Iteration [8856] | loss: 2.661271810531616\n",
      "Iteration [8857] | loss: 2.662052869796753\n",
      "Iteration [8858] | loss: 2.66064715385437\n",
      "Iteration [8859] | loss: 2.6624317169189453\n",
      "Iteration [8860] | loss: 2.6606545448303223\n",
      "Iteration [8861] | loss: 2.661724805831909\n",
      "Iteration [8862] | loss: 2.6613552570343018\n",
      "Iteration [8863] | loss: 2.6605634689331055\n",
      "Iteration [8864] | loss: 2.6623551845550537\n",
      "Iteration [8865] | loss: 2.6601405143737793\n",
      "Iteration [8866] | loss: 2.661681890487671\n",
      "Iteration [8867] | loss: 2.6612541675567627\n",
      "Iteration [8868] | loss: 2.661040782928467\n",
      "Iteration [8869] | loss: 2.662264347076416\n",
      "Iteration [8870] | loss: 2.660423517227173\n",
      "Iteration [8871] | loss: 2.6622540950775146\n",
      "Iteration [8872] | loss: 2.660796880722046\n",
      "Iteration [8873] | loss: 2.661566734313965\n",
      "Iteration [8874] | loss: 2.6618411540985107\n",
      "Iteration [8875] | loss: 2.6609151363372803\n",
      "Iteration [8876] | loss: 2.6622931957244873\n",
      "Iteration [8877] | loss: 2.6602816581726074\n",
      "Iteration [8878] | loss: 2.6614768505096436\n",
      "Iteration [8879] | loss: 2.661384105682373\n",
      "Iteration [8880] | loss: 2.660856246948242\n",
      "Iteration [8881] | loss: 2.662388801574707\n",
      "Iteration [8882] | loss: 2.6602351665496826\n",
      "Iteration [8883] | loss: 2.6621005535125732\n",
      "Iteration [8884] | loss: 2.6609137058258057\n",
      "Iteration [8885] | loss: 2.661423683166504\n",
      "Iteration [8886] | loss: 2.6619505882263184\n",
      "Iteration [8887] | loss: 2.6607847213745117\n",
      "Iteration [8888] | loss: 2.662558078765869\n",
      "Iteration [8889] | loss: 2.660247325897217\n",
      "Iteration [8890] | loss: 2.6613245010375977\n",
      "Iteration [8891] | loss: 2.6614532470703125\n",
      "Iteration [8892] | loss: 2.660705089569092\n",
      "Iteration [8893] | loss: 2.662398338317871\n",
      "Iteration [8894] | loss: 2.66015887260437\n",
      "Iteration [8895] | loss: 2.6619834899902344\n",
      "Iteration [8896] | loss: 2.6609833240509033\n",
      "Iteration [8897] | loss: 2.6613171100616455\n",
      "Iteration [8898] | loss: 2.662014961242676\n",
      "Iteration [8899] | loss: 2.660688638687134\n",
      "Iteration [8900] | loss: 2.6624698638916016\n",
      "Iteration [8901] | loss: 2.6606218814849854\n",
      "Iteration [8902] | loss: 2.6617624759674072\n",
      "Iteration [8903] | loss: 2.6612541675567627\n",
      "Iteration [8904] | loss: 2.660557746887207\n",
      "Iteration [8905] | loss: 2.6623926162719727\n",
      "Iteration [8906] | loss: 2.660111427307129\n",
      "Iteration [8907] | loss: 2.6617159843444824\n",
      "Iteration [8908] | loss: 2.661224603652954\n",
      "Iteration [8909] | loss: 2.6610705852508545\n",
      "Iteration [8910] | loss: 2.6622376441955566\n",
      "Iteration [8911] | loss: 2.660461187362671\n",
      "Iteration [8912] | loss: 2.662259817123413\n",
      "Iteration [8913] | loss: 2.6608288288116455\n",
      "Iteration [8914] | loss: 2.66156005859375\n",
      "Iteration [8915] | loss: 2.661811113357544\n",
      "Iteration [8916] | loss: 2.660917282104492\n",
      "Iteration [8917] | loss: 2.6621828079223633\n",
      "Iteration [8918] | loss: 2.6602675914764404\n",
      "Iteration [8919] | loss: 2.661525249481201\n",
      "Iteration [8920] | loss: 2.6613688468933105\n",
      "Iteration [8921] | loss: 2.6608972549438477\n",
      "Iteration [8922] | loss: 2.6623737812042236\n",
      "Iteration [8923] | loss: 2.6602859497070312\n",
      "Iteration [8924] | loss: 2.6621339321136475\n",
      "Iteration [8925] | loss: 2.660900115966797\n",
      "Iteration [8926] | loss: 2.661454916000366\n",
      "Iteration [8927] | loss: 2.6619365215301514\n",
      "Iteration [8928] | loss: 2.6608128547668457\n",
      "Iteration [8929] | loss: 2.6625404357910156\n",
      "Iteration [8930] | loss: 2.6603567600250244\n",
      "Iteration [8931] | loss: 2.661379337310791\n",
      "Iteration [8932] | loss: 2.66145396232605\n",
      "Iteration [8933] | loss: 2.6607630252838135\n",
      "Iteration [8934] | loss: 2.662397861480713\n",
      "Iteration [8935] | loss: 2.6601996421813965\n",
      "Iteration [8936] | loss: 2.6620209217071533\n",
      "Iteration [8937] | loss: 2.6609809398651123\n",
      "Iteration [8938] | loss: 2.661353349685669\n",
      "Iteration [8939] | loss: 2.6620116233825684\n",
      "Iteration [8940] | loss: 2.6607186794281006\n",
      "Iteration [8941] | loss: 2.6624979972839355\n",
      "Iteration [8942] | loss: 2.660510540008545\n",
      "Iteration [8943] | loss: 2.6612610816955566\n",
      "Iteration [8944] | loss: 2.6614980697631836\n",
      "Iteration [8945] | loss: 2.6606390476226807\n",
      "Iteration [8946] | loss: 2.662440776824951\n",
      "Iteration [8947] | loss: 2.660104513168335\n",
      "Iteration [8948] | loss: 2.661932945251465\n",
      "Iteration [8949] | loss: 2.661025285720825\n",
      "Iteration [8950] | loss: 2.6612708568573\n",
      "Iteration [8951] | loss: 2.662053346633911\n",
      "Iteration [8952] | loss: 2.66064715385437\n",
      "Iteration [8953] | loss: 2.6624317169189453\n",
      "Iteration [8954] | loss: 2.6606545448303223\n",
      "Iteration [8955] | loss: 2.661724328994751\n",
      "Iteration [8956] | loss: 2.661356210708618\n",
      "Iteration [8957] | loss: 2.6605653762817383\n",
      "Iteration [8958] | loss: 2.6623551845550537\n",
      "Iteration [8959] | loss: 2.6601405143737793\n",
      "Iteration [8960] | loss: 2.661681890487671\n",
      "Iteration [8961] | loss: 2.661252975463867\n",
      "Iteration [8962] | loss: 2.6610398292541504\n",
      "Iteration [8963] | loss: 2.6622633934020996\n",
      "Iteration [8964] | loss: 2.6604247093200684\n",
      "Iteration [8965] | loss: 2.6622533798217773\n",
      "Iteration [8966] | loss: 2.660795211791992\n",
      "Iteration [8967] | loss: 2.661567211151123\n",
      "Iteration [8968] | loss: 2.661841630935669\n",
      "Iteration [8969] | loss: 2.6609137058258057\n",
      "Iteration [8970] | loss: 2.66229248046875\n",
      "Iteration [8971] | loss: 2.660280704498291\n",
      "Iteration [8972] | loss: 2.6614768505096436\n",
      "Iteration [8973] | loss: 2.661384105682373\n",
      "Iteration [8974] | loss: 2.6608550548553467\n",
      "Iteration [8975] | loss: 2.6623876094818115\n",
      "Iteration [8976] | loss: 2.660233974456787\n",
      "Iteration [8977] | loss: 2.662099599838257\n",
      "Iteration [8978] | loss: 2.6609137058258057\n",
      "Iteration [8979] | loss: 2.6614232063293457\n",
      "Iteration [8980] | loss: 2.661949872970581\n",
      "Iteration [8981] | loss: 2.6607847213745117\n",
      "Iteration [8982] | loss: 2.66256046295166\n",
      "Iteration [8983] | loss: 2.6602468490600586\n",
      "Iteration [8984] | loss: 2.6613242626190186\n",
      "Iteration [8985] | loss: 2.6614532470703125\n",
      "Iteration [8986] | loss: 2.660705089569092\n",
      "Iteration [8987] | loss: 2.662398099899292\n",
      "Iteration [8988] | loss: 2.6601579189300537\n",
      "Iteration [8989] | loss: 2.6619832515716553\n",
      "Iteration [8990] | loss: 2.6609842777252197\n",
      "Iteration [8991] | loss: 2.6613171100616455\n",
      "Iteration [8992] | loss: 2.662015914916992\n",
      "Iteration [8993] | loss: 2.660688638687134\n",
      "Iteration [8994] | loss: 2.6624698638916016\n",
      "Iteration [8995] | loss: 2.6606204509735107\n",
      "Iteration [8996] | loss: 2.661764621734619\n",
      "Iteration [8997] | loss: 2.6612534523010254\n",
      "Iteration [8998] | loss: 2.660557746887207\n",
      "Iteration [8999] | loss: 2.662393569946289\n",
      "Iteration [9000] | loss: 2.660111427307129\n",
      "Iteration [9001] | loss: 2.661715507507324\n",
      "Iteration [9002] | loss: 2.661224603652954\n",
      "Iteration [9003] | loss: 2.6610724925994873\n",
      "Iteration [9004] | loss: 2.662238121032715\n",
      "Iteration [9005] | loss: 2.660461664199829\n",
      "Iteration [9006] | loss: 2.6622581481933594\n",
      "Iteration [9007] | loss: 2.6608288288116455\n",
      "Iteration [9008] | loss: 2.6615588665008545\n",
      "Iteration [9009] | loss: 2.661813735961914\n",
      "Iteration [9010] | loss: 2.660917282104492\n",
      "Iteration [9011] | loss: 2.6621837615966797\n",
      "Iteration [9012] | loss: 2.660266637802124\n",
      "Iteration [9013] | loss: 2.6615242958068848\n",
      "Iteration [9014] | loss: 2.661369800567627\n",
      "Iteration [9015] | loss: 2.6608972549438477\n",
      "Iteration [9016] | loss: 2.6623754501342773\n",
      "Iteration [9017] | loss: 2.660287618637085\n",
      "Iteration [9018] | loss: 2.6621344089508057\n",
      "Iteration [9019] | loss: 2.6608991622924805\n",
      "Iteration [9020] | loss: 2.661454439163208\n",
      "Iteration [9021] | loss: 2.6619365215301514\n",
      "Iteration [9022] | loss: 2.6608123779296875\n",
      "Iteration [9023] | loss: 2.6625399589538574\n",
      "Iteration [9024] | loss: 2.6603569984436035\n",
      "Iteration [9025] | loss: 2.661378860473633\n",
      "Iteration [9026] | loss: 2.661454439163208\n",
      "Iteration [9027] | loss: 2.6607630252838135\n",
      "Iteration [9028] | loss: 2.662397861480713\n",
      "Iteration [9029] | loss: 2.6601996421813965\n",
      "Iteration [9030] | loss: 2.662019968032837\n",
      "Iteration [9031] | loss: 2.6609809398651123\n",
      "Iteration [9032] | loss: 2.6613523960113525\n",
      "Iteration [9033] | loss: 2.6620116233825684\n",
      "Iteration [9034] | loss: 2.6607182025909424\n",
      "Iteration [9035] | loss: 2.6624975204467773\n",
      "Iteration [9036] | loss: 2.6605114936828613\n",
      "Iteration [9037] | loss: 2.661261558532715\n",
      "Iteration [9038] | loss: 2.6614980697631836\n",
      "Iteration [9039] | loss: 2.6606385707855225\n",
      "Iteration [9040] | loss: 2.662442207336426\n",
      "Iteration [9041] | loss: 2.660104513168335\n",
      "Iteration [9042] | loss: 2.6619324684143066\n",
      "Iteration [9043] | loss: 2.661024808883667\n",
      "Iteration [9044] | loss: 2.661271333694458\n",
      "Iteration [9045] | loss: 2.662053346633911\n",
      "Iteration [9046] | loss: 2.6606454849243164\n",
      "Iteration [9047] | loss: 2.6624317169189453\n",
      "Iteration [9048] | loss: 2.6606552600860596\n",
      "Iteration [9049] | loss: 2.6617238521575928\n",
      "Iteration [9050] | loss: 2.66135573387146\n",
      "Iteration [9051] | loss: 2.660564422607422\n",
      "Iteration [9052] | loss: 2.6623551845550537\n",
      "Iteration [9053] | loss: 2.6601414680480957\n",
      "Iteration [9054] | loss: 2.6616809368133545\n",
      "Iteration [9055] | loss: 2.6612534523010254\n",
      "Iteration [9056] | loss: 2.6610398292541504\n",
      "Iteration [9057] | loss: 2.662264347076416\n",
      "Iteration [9058] | loss: 2.66042423248291\n",
      "Iteration [9059] | loss: 2.662252902984619\n",
      "Iteration [9060] | loss: 2.6607961654663086\n",
      "Iteration [9061] | loss: 2.661567211151123\n",
      "Iteration [9062] | loss: 2.6618411540985107\n",
      "Iteration [9063] | loss: 2.6609132289886475\n",
      "Iteration [9064] | loss: 2.6622915267944336\n",
      "Iteration [9065] | loss: 2.6602797508239746\n",
      "Iteration [9066] | loss: 2.6614768505096436\n",
      "Iteration [9067] | loss: 2.661384105682373\n",
      "Iteration [9068] | loss: 2.6608545780181885\n",
      "Iteration [9069] | loss: 2.6623871326446533\n",
      "Iteration [9070] | loss: 2.660233497619629\n",
      "Iteration [9071] | loss: 2.662099599838257\n",
      "Iteration [9072] | loss: 2.6609137058258057\n",
      "Iteration [9073] | loss: 2.6614232063293457\n",
      "Iteration [9074] | loss: 2.661949872970581\n",
      "Iteration [9075] | loss: 2.6607842445373535\n",
      "Iteration [9076] | loss: 2.662559986114502\n",
      "Iteration [9077] | loss: 2.660247802734375\n",
      "Iteration [9078] | loss: 2.6613247394561768\n",
      "Iteration [9079] | loss: 2.661451578140259\n",
      "Iteration [9080] | loss: 2.6607048511505127\n",
      "Iteration [9081] | loss: 2.662398338317871\n",
      "Iteration [9082] | loss: 2.6601579189300537\n",
      "Iteration [9083] | loss: 2.6619834899902344\n",
      "Iteration [9084] | loss: 2.660982608795166\n",
      "Iteration [9085] | loss: 2.6613175868988037\n",
      "Iteration [9086] | loss: 2.662015914916992\n",
      "Iteration [9087] | loss: 2.6606881618499756\n",
      "Iteration [9088] | loss: 2.6624698638916016\n",
      "Iteration [9089] | loss: 2.660621404647827\n",
      "Iteration [9090] | loss: 2.6617650985717773\n",
      "Iteration [9091] | loss: 2.661254644393921\n",
      "Iteration [9092] | loss: 2.6605567932128906\n",
      "Iteration [9093] | loss: 2.6623919010162354\n",
      "Iteration [9094] | loss: 2.660111427307129\n",
      "Iteration [9095] | loss: 2.661714792251587\n",
      "Iteration [9096] | loss: 2.6612250804901123\n",
      "Iteration [9097] | loss: 2.6610710620880127\n",
      "Iteration [9098] | loss: 2.662238121032715\n",
      "Iteration [9099] | loss: 2.660461187362671\n",
      "Iteration [9100] | loss: 2.662259340286255\n",
      "Iteration [9101] | loss: 2.660827875137329\n",
      "Iteration [9102] | loss: 2.661559820175171\n",
      "Iteration [9103] | loss: 2.661811590194702\n",
      "Iteration [9104] | loss: 2.660917282104492\n",
      "Iteration [9105] | loss: 2.662182331085205\n",
      "Iteration [9106] | loss: 2.6602656841278076\n",
      "Iteration [9107] | loss: 2.6615238189697266\n",
      "Iteration [9108] | loss: 2.661369800567627\n",
      "Iteration [9109] | loss: 2.660897970199585\n",
      "Iteration [9110] | loss: 2.662374496459961\n",
      "Iteration [9111] | loss: 2.6602859497070312\n",
      "Iteration [9112] | loss: 2.6621339321136475\n",
      "Iteration [9113] | loss: 2.6608996391296387\n",
      "Iteration [9114] | loss: 2.661454916000366\n",
      "Iteration [9115] | loss: 2.6619365215301514\n",
      "Iteration [9116] | loss: 2.660813331604004\n",
      "Iteration [9117] | loss: 2.662539005279541\n",
      "Iteration [9118] | loss: 2.6603567600250244\n",
      "Iteration [9119] | loss: 2.661378860473633\n",
      "Iteration [9120] | loss: 2.661454439163208\n",
      "Iteration [9121] | loss: 2.660764694213867\n",
      "Iteration [9122] | loss: 2.662398338317871\n",
      "Iteration [9123] | loss: 2.660200834274292\n",
      "Iteration [9124] | loss: 2.662019968032837\n",
      "Iteration [9125] | loss: 2.660980463027954\n",
      "Iteration [9126] | loss: 2.6613523960113525\n",
      "Iteration [9127] | loss: 2.6620116233825684\n",
      "Iteration [9128] | loss: 2.660719156265259\n",
      "Iteration [9129] | loss: 2.662497043609619\n",
      "Iteration [9130] | loss: 2.660510540008545\n",
      "Iteration [9131] | loss: 2.6612610816955566\n",
      "Iteration [9132] | loss: 2.6614980697631836\n",
      "Iteration [9133] | loss: 2.6606390476226807\n",
      "Iteration [9134] | loss: 2.6624417304992676\n",
      "Iteration [9135] | loss: 2.660104513168335\n",
      "Iteration [9136] | loss: 2.6619324684143066\n",
      "Iteration [9137] | loss: 2.6610240936279297\n",
      "Iteration [9138] | loss: 2.661271333694458\n",
      "Iteration [9139] | loss: 2.662053346633911\n",
      "Iteration [9140] | loss: 2.66064715385437\n",
      "Iteration [9141] | loss: 2.662430763244629\n",
      "Iteration [9142] | loss: 2.6606557369232178\n",
      "Iteration [9143] | loss: 2.6617238521575928\n",
      "Iteration [9144] | loss: 2.6613552570343018\n",
      "Iteration [9145] | loss: 2.6605634689331055\n",
      "Iteration [9146] | loss: 2.6623547077178955\n",
      "Iteration [9147] | loss: 2.6601405143737793\n",
      "Iteration [9148] | loss: 2.6616809368133545\n",
      "Iteration [9149] | loss: 2.6612534523010254\n",
      "Iteration [9150] | loss: 2.661040782928467\n",
      "Iteration [9151] | loss: 2.662264347076416\n",
      "Iteration [9152] | loss: 2.6604256629943848\n",
      "Iteration [9153] | loss: 2.662252902984619\n",
      "Iteration [9154] | loss: 2.660796880722046\n",
      "Iteration [9155] | loss: 2.661567211151123\n",
      "Iteration [9156] | loss: 2.6618411540985107\n",
      "Iteration [9157] | loss: 2.6609151363372803\n",
      "Iteration [9158] | loss: 2.66229248046875\n",
      "Iteration [9159] | loss: 2.6602816581726074\n",
      "Iteration [9160] | loss: 2.6614768505096436\n",
      "Iteration [9161] | loss: 2.6613850593566895\n",
      "Iteration [9162] | loss: 2.6608550548553467\n",
      "Iteration [9163] | loss: 2.6623880863189697\n",
      "Iteration [9164] | loss: 2.6602346897125244\n",
      "Iteration [9165] | loss: 2.662100076675415\n",
      "Iteration [9166] | loss: 2.6609137058258057\n",
      "Iteration [9167] | loss: 2.661423683166504\n",
      "Iteration [9168] | loss: 2.661949872970581\n",
      "Iteration [9169] | loss: 2.6607842445373535\n",
      "Iteration [9170] | loss: 2.6625590324401855\n",
      "Iteration [9171] | loss: 2.660247802734375\n",
      "Iteration [9172] | loss: 2.661325216293335\n",
      "Iteration [9173] | loss: 2.661451578140259\n",
      "Iteration [9174] | loss: 2.660705089569092\n",
      "Iteration [9175] | loss: 2.6623971462249756\n",
      "Iteration [9176] | loss: 2.66015887260437\n",
      "Iteration [9177] | loss: 2.6619820594787598\n",
      "Iteration [9178] | loss: 2.6609842777252197\n",
      "Iteration [9179] | loss: 2.6613190174102783\n",
      "Iteration [9180] | loss: 2.662015914916992\n",
      "Iteration [9181] | loss: 2.6606881618499756\n",
      "Iteration [9182] | loss: 2.6624698638916016\n",
      "Iteration [9183] | loss: 2.6606204509735107\n",
      "Iteration [9184] | loss: 2.6617660522460938\n",
      "Iteration [9185] | loss: 2.661255121231079\n",
      "Iteration [9186] | loss: 2.660555601119995\n",
      "Iteration [9187] | loss: 2.6623919010162354\n",
      "Iteration [9188] | loss: 2.66011118888855\n",
      "Iteration [9189] | loss: 2.661715269088745\n",
      "Iteration [9190] | loss: 2.6612250804901123\n",
      "Iteration [9191] | loss: 2.661071538925171\n",
      "Iteration [9192] | loss: 2.6622376441955566\n",
      "Iteration [9193] | loss: 2.6604602336883545\n",
      "Iteration [9194] | loss: 2.662259817123413\n",
      "Iteration [9195] | loss: 2.660827875137329\n",
      "Iteration [9196] | loss: 2.6615593433380127\n",
      "Iteration [9197] | loss: 2.6618125438690186\n",
      "Iteration [9198] | loss: 2.660917282104492\n",
      "Iteration [9199] | loss: 2.6621832847595215\n",
      "Iteration [9200] | loss: 2.6602678298950195\n",
      "Iteration [9201] | loss: 2.6615242958068848\n",
      "Iteration [9202] | loss: 2.6613683700561523\n",
      "Iteration [9203] | loss: 2.6608972549438477\n",
      "Iteration [9204] | loss: 2.6623737812042236\n",
      "Iteration [9205] | loss: 2.6602859497070312\n",
      "Iteration [9206] | loss: 2.6621344089508057\n",
      "Iteration [9207] | loss: 2.6608989238739014\n",
      "Iteration [9208] | loss: 2.661454916000366\n",
      "Iteration [9209] | loss: 2.661935806274414\n",
      "Iteration [9210] | loss: 2.6608128547668457\n",
      "Iteration [9211] | loss: 2.662540912628174\n",
      "Iteration [9212] | loss: 2.6603569984436035\n",
      "Iteration [9213] | loss: 2.6613786220550537\n",
      "Iteration [9214] | loss: 2.6614532470703125\n",
      "Iteration [9215] | loss: 2.6607635021209717\n",
      "Iteration [9216] | loss: 2.662398338317871\n",
      "Iteration [9217] | loss: 2.6601996421813965\n",
      "Iteration [9218] | loss: 2.662020444869995\n",
      "Iteration [9219] | loss: 2.6609809398651123\n",
      "Iteration [9220] | loss: 2.6613523960113525\n",
      "Iteration [9221] | loss: 2.6620113849639893\n",
      "Iteration [9222] | loss: 2.6607182025909424\n",
      "Iteration [9223] | loss: 2.662496328353882\n",
      "Iteration [9224] | loss: 2.6605117321014404\n",
      "Iteration [9225] | loss: 2.661259889602661\n",
      "Iteration [9226] | loss: 2.6614980697631836\n",
      "Iteration [9227] | loss: 2.6606380939483643\n",
      "Iteration [9228] | loss: 2.6624417304992676\n",
      "Iteration [9229] | loss: 2.660104990005493\n",
      "Iteration [9230] | loss: 2.6619324684143066\n",
      "Iteration [9231] | loss: 2.6610240936279297\n",
      "Iteration [9232] | loss: 2.661271333694458\n",
      "Iteration [9233] | loss: 2.662053346633911\n",
      "Iteration [9234] | loss: 2.6606476306915283\n",
      "Iteration [9235] | loss: 2.6624317169189453\n",
      "Iteration [9236] | loss: 2.6606547832489014\n",
      "Iteration [9237] | loss: 2.6617238521575928\n",
      "Iteration [9238] | loss: 2.66135573387146\n",
      "Iteration [9239] | loss: 2.66056489944458\n",
      "Iteration [9240] | loss: 2.6623551845550537\n",
      "Iteration [9241] | loss: 2.6601405143737793\n",
      "Iteration [9242] | loss: 2.661681890487671\n",
      "Iteration [9243] | loss: 2.661252975463867\n",
      "Iteration [9244] | loss: 2.6610398292541504\n",
      "Iteration [9245] | loss: 2.662264823913574\n",
      "Iteration [9246] | loss: 2.660423517227173\n",
      "Iteration [9247] | loss: 2.6622533798217773\n",
      "Iteration [9248] | loss: 2.660796642303467\n",
      "Iteration [9249] | loss: 2.661567211151123\n",
      "Iteration [9250] | loss: 2.6618404388427734\n",
      "Iteration [9251] | loss: 2.6609151363372803\n",
      "Iteration [9252] | loss: 2.6622931957244873\n",
      "Iteration [9253] | loss: 2.660281181335449\n",
      "Iteration [9254] | loss: 2.6614768505096436\n",
      "Iteration [9255] | loss: 2.661384105682373\n",
      "Iteration [9256] | loss: 2.6608550548553467\n",
      "Iteration [9257] | loss: 2.662388801574707\n",
      "Iteration [9258] | loss: 2.660233974456787\n",
      "Iteration [9259] | loss: 2.6620984077453613\n",
      "Iteration [9260] | loss: 2.6609132289886475\n",
      "Iteration [9261] | loss: 2.6614227294921875\n",
      "Iteration [9262] | loss: 2.661949872970581\n",
      "Iteration [9263] | loss: 2.6607847213745117\n",
      "Iteration [9264] | loss: 2.66256046295166\n",
      "Iteration [9265] | loss: 2.660248279571533\n",
      "Iteration [9266] | loss: 2.6613242626190186\n",
      "Iteration [9267] | loss: 2.6614537239074707\n",
      "Iteration [9268] | loss: 2.660705089569092\n",
      "Iteration [9269] | loss: 2.6623988151550293\n",
      "Iteration [9270] | loss: 2.66015887260437\n",
      "Iteration [9271] | loss: 2.661983013153076\n",
      "Iteration [9272] | loss: 2.6609833240509033\n",
      "Iteration [9273] | loss: 2.6613171100616455\n",
      "Iteration [9274] | loss: 2.662015914916992\n",
      "Iteration [9275] | loss: 2.6606881618499756\n",
      "Iteration [9276] | loss: 2.6624698638916016\n",
      "Iteration [9277] | loss: 2.6606199741363525\n",
      "Iteration [9278] | loss: 2.66176438331604\n",
      "Iteration [9279] | loss: 2.661255121231079\n",
      "Iteration [9280] | loss: 2.6605567932128906\n",
      "Iteration [9281] | loss: 2.6623919010162354\n",
      "Iteration [9282] | loss: 2.6601107120513916\n",
      "Iteration [9283] | loss: 2.661715269088745\n",
      "Iteration [9284] | loss: 2.661224603652954\n",
      "Iteration [9285] | loss: 2.6610727310180664\n",
      "Iteration [9286] | loss: 2.6622376441955566\n",
      "Iteration [9287] | loss: 2.6604623794555664\n",
      "Iteration [9288] | loss: 2.662259340286255\n",
      "Iteration [9289] | loss: 2.6608283519744873\n",
      "Iteration [9290] | loss: 2.6615610122680664\n",
      "Iteration [9291] | loss: 2.661811590194702\n",
      "Iteration [9292] | loss: 2.660916805267334\n",
      "Iteration [9293] | loss: 2.6621828079223633\n",
      "Iteration [9294] | loss: 2.660266160964966\n",
      "Iteration [9295] | loss: 2.6615233421325684\n",
      "Iteration [9296] | loss: 2.661367893218994\n",
      "Iteration [9297] | loss: 2.660897970199585\n",
      "Iteration [9298] | loss: 2.6623737812042236\n",
      "Iteration [9299] | loss: 2.6602871417999268\n",
      "Iteration [9300] | loss: 2.6621339321136475\n",
      "Iteration [9301] | loss: 2.660900115966797\n",
      "Iteration [9302] | loss: 2.661454439163208\n",
      "Iteration [9303] | loss: 2.6619365215301514\n",
      "Iteration [9304] | loss: 2.660813331604004\n",
      "Iteration [9305] | loss: 2.662540912628174\n",
      "Iteration [9306] | loss: 2.6603567600250244\n",
      "Iteration [9307] | loss: 2.661379814147949\n",
      "Iteration [9308] | loss: 2.66145396232605\n",
      "Iteration [9309] | loss: 2.6607630252838135\n",
      "Iteration [9310] | loss: 2.662397861480713\n",
      "Iteration [9311] | loss: 2.6601996421813965\n",
      "Iteration [9312] | loss: 2.6620192527770996\n",
      "Iteration [9313] | loss: 2.660979986190796\n",
      "Iteration [9314] | loss: 2.6613516807556152\n",
      "Iteration [9315] | loss: 2.6620121002197266\n",
      "Iteration [9316] | loss: 2.6607186794281006\n",
      "Iteration [9317] | loss: 2.6624975204467773\n",
      "Iteration [9318] | loss: 2.6605114936828613\n",
      "Iteration [9319] | loss: 2.661261558532715\n",
      "Iteration [9320] | loss: 2.6614980697631836\n",
      "Iteration [9321] | loss: 2.6606390476226807\n",
      "Iteration [9322] | loss: 2.6624417304992676\n",
      "Iteration [9323] | loss: 2.6601054668426514\n",
      "Iteration [9324] | loss: 2.6619324684143066\n",
      "Iteration [9325] | loss: 2.661024808883667\n",
      "Iteration [9326] | loss: 2.6612708568573\n",
      "Iteration [9327] | loss: 2.662053346633911\n",
      "Iteration [9328] | loss: 2.66064715385437\n",
      "Iteration [9329] | loss: 2.662430763244629\n",
      "Iteration [9330] | loss: 2.6606547832489014\n",
      "Iteration [9331] | loss: 2.661724805831909\n",
      "Iteration [9332] | loss: 2.661356210708618\n",
      "Iteration [9333] | loss: 2.660564422607422\n",
      "Iteration [9334] | loss: 2.662355661392212\n",
      "Iteration [9335] | loss: 2.6601405143737793\n",
      "Iteration [9336] | loss: 2.661681890487671\n",
      "Iteration [9337] | loss: 2.661252498626709\n",
      "Iteration [9338] | loss: 2.661041498184204\n",
      "Iteration [9339] | loss: 2.6622633934020996\n",
      "Iteration [9340] | loss: 2.6604247093200684\n",
      "Iteration [9341] | loss: 2.6622519493103027\n",
      "Iteration [9342] | loss: 2.660796880722046\n",
      "Iteration [9343] | loss: 2.661567211151123\n",
      "Iteration [9344] | loss: 2.6618404388427734\n",
      "Iteration [9345] | loss: 2.6609151363372803\n",
      "Iteration [9346] | loss: 2.6622931957244873\n",
      "Iteration [9347] | loss: 2.6602816581726074\n",
      "Iteration [9348] | loss: 2.6614768505096436\n",
      "Iteration [9349] | loss: 2.661383628845215\n",
      "Iteration [9350] | loss: 2.6608550548553467\n",
      "Iteration [9351] | loss: 2.662386655807495\n",
      "Iteration [9352] | loss: 2.660233974456787\n",
      "Iteration [9353] | loss: 2.662100076675415\n",
      "Iteration [9354] | loss: 2.6609137058258057\n",
      "Iteration [9355] | loss: 2.661423683166504\n",
      "Iteration [9356] | loss: 2.661949872970581\n",
      "Iteration [9357] | loss: 2.6607842445373535\n",
      "Iteration [9358] | loss: 2.66256046295166\n",
      "Iteration [9359] | loss: 2.6602492332458496\n",
      "Iteration [9360] | loss: 2.661325216293335\n",
      "Iteration [9361] | loss: 2.6614532470703125\n",
      "Iteration [9362] | loss: 2.660705089569092\n",
      "Iteration [9363] | loss: 2.6623992919921875\n",
      "Iteration [9364] | loss: 2.66015887260437\n",
      "Iteration [9365] | loss: 2.6619818210601807\n",
      "Iteration [9366] | loss: 2.6609838008880615\n",
      "Iteration [9367] | loss: 2.661318063735962\n",
      "Iteration [9368] | loss: 2.662015914916992\n",
      "Iteration [9369] | loss: 2.660689115524292\n",
      "Iteration [9370] | loss: 2.662468910217285\n",
      "Iteration [9371] | loss: 2.6606204509735107\n",
      "Iteration [9372] | loss: 2.661764621734619\n",
      "Iteration [9373] | loss: 2.6612541675567627\n",
      "Iteration [9374] | loss: 2.660555601119995\n",
      "Iteration [9375] | loss: 2.6623916625976562\n",
      "Iteration [9376] | loss: 2.660111427307129\n",
      "Iteration [9377] | loss: 2.661714792251587\n",
      "Iteration [9378] | loss: 2.6612250804901123\n",
      "Iteration [9379] | loss: 2.6610705852508545\n",
      "Iteration [9380] | loss: 2.662238121032715\n",
      "Iteration [9381] | loss: 2.6604602336883545\n",
      "Iteration [9382] | loss: 2.6622581481933594\n",
      "Iteration [9383] | loss: 2.660827875137329\n",
      "Iteration [9384] | loss: 2.661560535430908\n",
      "Iteration [9385] | loss: 2.661811113357544\n",
      "Iteration [9386] | loss: 2.6609175205230713\n",
      "Iteration [9387] | loss: 2.662182092666626\n",
      "Iteration [9388] | loss: 2.6602678298950195\n",
      "Iteration [9389] | loss: 2.6615242958068848\n",
      "Iteration [9390] | loss: 2.661367893218994\n",
      "Iteration [9391] | loss: 2.6608972549438477\n",
      "Iteration [9392] | loss: 2.6623733043670654\n",
      "Iteration [9393] | loss: 2.6602866649627686\n",
      "Iteration [9394] | loss: 2.6621334552764893\n",
      "Iteration [9395] | loss: 2.660900115966797\n",
      "Iteration [9396] | loss: 2.6614558696746826\n",
      "Iteration [9397] | loss: 2.6619348526000977\n",
      "Iteration [9398] | loss: 2.6608119010925293\n",
      "Iteration [9399] | loss: 2.662539482116699\n",
      "Iteration [9400] | loss: 2.6603567600250244\n",
      "Iteration [9401] | loss: 2.661378860473633\n",
      "Iteration [9402] | loss: 2.661454916000366\n",
      "Iteration [9403] | loss: 2.66076397895813\n",
      "Iteration [9404] | loss: 2.662397861480713\n",
      "Iteration [9405] | loss: 2.66020131111145\n",
      "Iteration [9406] | loss: 2.6620209217071533\n",
      "Iteration [9407] | loss: 2.660980463027954\n",
      "Iteration [9408] | loss: 2.661353349685669\n",
      "Iteration [9409] | loss: 2.6620113849639893\n",
      "Iteration [9410] | loss: 2.6607182025909424\n",
      "Iteration [9411] | loss: 2.662496328353882\n",
      "Iteration [9412] | loss: 2.6605119705200195\n",
      "Iteration [9413] | loss: 2.661261558532715\n",
      "Iteration [9414] | loss: 2.6614980697631836\n",
      "Iteration [9415] | loss: 2.6606385707855225\n",
      "Iteration [9416] | loss: 2.6624417304992676\n",
      "Iteration [9417] | loss: 2.660104990005493\n",
      "Iteration [9418] | loss: 2.6619324684143066\n",
      "Iteration [9419] | loss: 2.661024808883667\n",
      "Iteration [9420] | loss: 2.6612708568573\n",
      "Iteration [9421] | loss: 2.662052869796753\n",
      "Iteration [9422] | loss: 2.660646677017212\n",
      "Iteration [9423] | loss: 2.6624317169189453\n",
      "Iteration [9424] | loss: 2.6606545448303223\n",
      "Iteration [9425] | loss: 2.6617238521575928\n",
      "Iteration [9426] | loss: 2.6613571643829346\n",
      "Iteration [9427] | loss: 2.6605634689331055\n",
      "Iteration [9428] | loss: 2.662355661392212\n",
      "Iteration [9429] | loss: 2.6601405143737793\n",
      "Iteration [9430] | loss: 2.6616809368133545\n",
      "Iteration [9431] | loss: 2.6612536907196045\n",
      "Iteration [9432] | loss: 2.661040782928467\n",
      "Iteration [9433] | loss: 2.662264347076416\n",
      "Iteration [9434] | loss: 2.6604256629943848\n",
      "Iteration [9435] | loss: 2.662252902984619\n",
      "Iteration [9436] | loss: 2.660795211791992\n",
      "Iteration [9437] | loss: 2.661567211151123\n",
      "Iteration [9438] | loss: 2.6618411540985107\n",
      "Iteration [9439] | loss: 2.6609151363372803\n",
      "Iteration [9440] | loss: 2.6622931957244873\n",
      "Iteration [9441] | loss: 2.6602816581726074\n",
      "Iteration [9442] | loss: 2.6614768505096436\n",
      "Iteration [9443] | loss: 2.661383628845215\n",
      "Iteration [9444] | loss: 2.660855770111084\n",
      "Iteration [9445] | loss: 2.662388801574707\n",
      "Iteration [9446] | loss: 2.6602346897125244\n",
      "Iteration [9447] | loss: 2.662100076675415\n",
      "Iteration [9448] | loss: 2.6609137058258057\n",
      "Iteration [9449] | loss: 2.6614227294921875\n",
      "Iteration [9450] | loss: 2.661949872970581\n",
      "Iteration [9451] | loss: 2.6607842445373535\n",
      "Iteration [9452] | loss: 2.6625590324401855\n",
      "Iteration [9453] | loss: 2.6602487564086914\n",
      "Iteration [9454] | loss: 2.661325216293335\n",
      "Iteration [9455] | loss: 2.661452293395996\n",
      "Iteration [9456] | loss: 2.660705089569092\n",
      "Iteration [9457] | loss: 2.662398338317871\n",
      "Iteration [9458] | loss: 2.660158395767212\n",
      "Iteration [9459] | loss: 2.6619832515716553\n",
      "Iteration [9460] | loss: 2.6609838008880615\n",
      "Iteration [9461] | loss: 2.6613171100616455\n",
      "Iteration [9462] | loss: 2.6620163917541504\n",
      "Iteration [9463] | loss: 2.6606881618499756\n",
      "Iteration [9464] | loss: 2.6624698638916016\n",
      "Iteration [9465] | loss: 2.6606204509735107\n",
      "Iteration [9466] | loss: 2.6617660522460938\n",
      "Iteration [9467] | loss: 2.661255121231079\n",
      "Iteration [9468] | loss: 2.660557270050049\n",
      "Iteration [9469] | loss: 2.6623916625976562\n",
      "Iteration [9470] | loss: 2.660111427307129\n",
      "Iteration [9471] | loss: 2.6617164611816406\n",
      "Iteration [9472] | loss: 2.661224603652954\n",
      "Iteration [9473] | loss: 2.661071538925171\n",
      "Iteration [9474] | loss: 2.662238597869873\n",
      "Iteration [9475] | loss: 2.660461187362671\n",
      "Iteration [9476] | loss: 2.662259817123413\n",
      "Iteration [9477] | loss: 2.6608283519744873\n",
      "Iteration [9478] | loss: 2.6615593433380127\n",
      "Iteration [9479] | loss: 2.661811590194702\n",
      "Iteration [9480] | loss: 2.6609179973602295\n",
      "Iteration [9481] | loss: 2.6621828079223633\n",
      "Iteration [9482] | loss: 2.660266637802124\n",
      "Iteration [9483] | loss: 2.6615242958068848\n",
      "Iteration [9484] | loss: 2.661369800567627\n",
      "Iteration [9485] | loss: 2.660897970199585\n",
      "Iteration [9486] | loss: 2.6623737812042236\n",
      "Iteration [9487] | loss: 2.6602866649627686\n",
      "Iteration [9488] | loss: 2.66213321685791\n",
      "Iteration [9489] | loss: 2.660900115966797\n",
      "Iteration [9490] | loss: 2.66145396232605\n",
      "Iteration [9491] | loss: 2.6619365215301514\n",
      "Iteration [9492] | loss: 2.6608123779296875\n",
      "Iteration [9493] | loss: 2.6625404357910156\n",
      "Iteration [9494] | loss: 2.6603567600250244\n",
      "Iteration [9495] | loss: 2.661378860473633\n",
      "Iteration [9496] | loss: 2.661454439163208\n",
      "Iteration [9497] | loss: 2.660764694213867\n",
      "Iteration [9498] | loss: 2.662397861480713\n",
      "Iteration [9499] | loss: 2.66020131111145\n",
      "Iteration [9500] | loss: 2.662020444869995\n",
      "Iteration [9501] | loss: 2.660980463027954\n",
      "Iteration [9502] | loss: 2.6613523960113525\n",
      "Iteration [9503] | loss: 2.6620116233825684\n",
      "Iteration [9504] | loss: 2.6607179641723633\n",
      "Iteration [9505] | loss: 2.662497043609619\n",
      "Iteration [9506] | loss: 2.6605122089385986\n",
      "Iteration [9507] | loss: 2.6612606048583984\n",
      "Iteration [9508] | loss: 2.6614980697631836\n",
      "Iteration [9509] | loss: 2.660639524459839\n",
      "Iteration [9510] | loss: 2.6624417304992676\n",
      "Iteration [9511] | loss: 2.6601035594940186\n",
      "Iteration [9512] | loss: 2.661932945251465\n",
      "Iteration [9513] | loss: 2.661024808883667\n",
      "Iteration [9514] | loss: 2.6612703800201416\n",
      "Iteration [9515] | loss: 2.662055015563965\n",
      "Iteration [9516] | loss: 2.6606462001800537\n",
      "Iteration [9517] | loss: 2.6624302864074707\n",
      "Iteration [9518] | loss: 2.660656213760376\n",
      "Iteration [9519] | loss: 2.661724328994751\n",
      "Iteration [9520] | loss: 2.6613566875457764\n",
      "Iteration [9521] | loss: 2.660564422607422\n",
      "Iteration [9522] | loss: 2.6623547077178955\n",
      "Iteration [9523] | loss: 2.6601405143737793\n",
      "Iteration [9524] | loss: 2.661681890487671\n",
      "Iteration [9525] | loss: 2.6612534523010254\n",
      "Iteration [9526] | loss: 2.6610398292541504\n",
      "Iteration [9527] | loss: 2.662264823913574\n",
      "Iteration [9528] | loss: 2.66042423248291\n",
      "Iteration [9529] | loss: 2.6622533798217773\n",
      "Iteration [9530] | loss: 2.6607961654663086\n",
      "Iteration [9531] | loss: 2.661567211151123\n",
      "Iteration [9532] | loss: 2.6618411540985107\n",
      "Iteration [9533] | loss: 2.6609151363372803\n",
      "Iteration [9534] | loss: 2.662292957305908\n",
      "Iteration [9535] | loss: 2.6602816581726074\n",
      "Iteration [9536] | loss: 2.6614768505096436\n",
      "Iteration [9537] | loss: 2.6613845825195312\n",
      "Iteration [9538] | loss: 2.660855770111084\n",
      "Iteration [9539] | loss: 2.662388801574707\n",
      "Iteration [9540] | loss: 2.6602346897125244\n",
      "Iteration [9541] | loss: 2.662100076675415\n",
      "Iteration [9542] | loss: 2.6609137058258057\n",
      "Iteration [9543] | loss: 2.66142201423645\n",
      "Iteration [9544] | loss: 2.661949872970581\n",
      "Iteration [9545] | loss: 2.6607842445373535\n",
      "Iteration [9546] | loss: 2.6625590324401855\n",
      "Iteration [9547] | loss: 2.660247802734375\n",
      "Iteration [9548] | loss: 2.6613247394561768\n",
      "Iteration [9549] | loss: 2.6614532470703125\n",
      "Iteration [9550] | loss: 2.660706043243408\n",
      "Iteration [9551] | loss: 2.6623988151550293\n",
      "Iteration [9552] | loss: 2.6601579189300537\n",
      "Iteration [9553] | loss: 2.6619839668273926\n",
      "Iteration [9554] | loss: 2.6609842777252197\n",
      "Iteration [9555] | loss: 2.6613171100616455\n",
      "Iteration [9556] | loss: 2.662014961242676\n",
      "Iteration [9557] | loss: 2.6606881618499756\n",
      "Iteration [9558] | loss: 2.6624698638916016\n",
      "Iteration [9559] | loss: 2.660620927810669\n",
      "Iteration [9560] | loss: 2.6617650985717773\n",
      "Iteration [9561] | loss: 2.6612541675567627\n",
      "Iteration [9562] | loss: 2.6605563163757324\n",
      "Iteration [9563] | loss: 2.6623926162719727\n",
      "Iteration [9564] | loss: 2.660111427307129\n",
      "Iteration [9565] | loss: 2.661714792251587\n",
      "Iteration [9566] | loss: 2.6612250804901123\n",
      "Iteration [9567] | loss: 2.6610705852508545\n",
      "Iteration [9568] | loss: 2.662238121032715\n",
      "Iteration [9569] | loss: 2.660461664199829\n",
      "Iteration [9570] | loss: 2.6622588634490967\n",
      "Iteration [9571] | loss: 2.6608288288116455\n",
      "Iteration [9572] | loss: 2.661560297012329\n",
      "Iteration [9573] | loss: 2.6618120670318604\n",
      "Iteration [9574] | loss: 2.660917282104492\n",
      "Iteration [9575] | loss: 2.662182092666626\n",
      "Iteration [9576] | loss: 2.6602675914764404\n",
      "Iteration [9577] | loss: 2.6615242958068848\n",
      "Iteration [9578] | loss: 2.661369800567627\n",
      "Iteration [9579] | loss: 2.6608989238739014\n",
      "Iteration [9580] | loss: 2.6623737812042236\n",
      "Iteration [9581] | loss: 2.6602859497070312\n",
      "Iteration [9582] | loss: 2.6621339321136475\n",
      "Iteration [9583] | loss: 2.6608996391296387\n",
      "Iteration [9584] | loss: 2.66145396232605\n",
      "Iteration [9585] | loss: 2.6619362831115723\n",
      "Iteration [9586] | loss: 2.6608123779296875\n",
      "Iteration [9587] | loss: 2.662540912628174\n",
      "Iteration [9588] | loss: 2.6603567600250244\n",
      "Iteration [9589] | loss: 2.661379337310791\n",
      "Iteration [9590] | loss: 2.6614553928375244\n",
      "Iteration [9591] | loss: 2.6607635021209717\n",
      "Iteration [9592] | loss: 2.662396192550659\n",
      "Iteration [9593] | loss: 2.66020131111145\n",
      "Iteration [9594] | loss: 2.662020444869995\n",
      "Iteration [9595] | loss: 2.660980463027954\n",
      "Iteration [9596] | loss: 2.6613523960113525\n",
      "Iteration [9597] | loss: 2.6620113849639893\n",
      "Iteration [9598] | loss: 2.6607182025909424\n",
      "Iteration [9599] | loss: 2.662497043609619\n",
      "Iteration [9600] | loss: 2.660510540008545\n",
      "Iteration [9601] | loss: 2.661261558532715\n",
      "Iteration [9602] | loss: 2.6614980697631836\n",
      "Iteration [9603] | loss: 2.6606385707855225\n",
      "Iteration [9604] | loss: 2.662440776824951\n",
      "Iteration [9605] | loss: 2.6601054668426514\n",
      "Iteration [9606] | loss: 2.661932945251465\n",
      "Iteration [9607] | loss: 2.6610236167907715\n",
      "Iteration [9608] | loss: 2.6612703800201416\n",
      "Iteration [9609] | loss: 2.6620535850524902\n",
      "Iteration [9610] | loss: 2.6606462001800537\n",
      "Iteration [9611] | loss: 2.662431240081787\n",
      "Iteration [9612] | loss: 2.6606547832489014\n",
      "Iteration [9613] | loss: 2.6617238521575928\n",
      "Iteration [9614] | loss: 2.66135573387146\n",
      "Iteration [9615] | loss: 2.6605634689331055\n",
      "Iteration [9616] | loss: 2.6623547077178955\n",
      "Iteration [9617] | loss: 2.6601405143737793\n",
      "Iteration [9618] | loss: 2.6616814136505127\n",
      "Iteration [9619] | loss: 2.661252975463867\n",
      "Iteration [9620] | loss: 2.6610398292541504\n",
      "Iteration [9621] | loss: 2.662264347076416\n",
      "Iteration [9622] | loss: 2.6604251861572266\n",
      "Iteration [9623] | loss: 2.6622536182403564\n",
      "Iteration [9624] | loss: 2.660796880722046\n",
      "Iteration [9625] | loss: 2.661567211151123\n",
      "Iteration [9626] | loss: 2.6618411540985107\n",
      "Iteration [9627] | loss: 2.6609151363372803\n",
      "Iteration [9628] | loss: 2.6622931957244873\n",
      "Iteration [9629] | loss: 2.660281181335449\n",
      "Iteration [9630] | loss: 2.6614768505096436\n",
      "Iteration [9631] | loss: 2.6613850593566895\n",
      "Iteration [9632] | loss: 2.6608545780181885\n",
      "Iteration [9633] | loss: 2.662388801574707\n",
      "Iteration [9634] | loss: 2.6602346897125244\n",
      "Iteration [9635] | loss: 2.662100076675415\n",
      "Iteration [9636] | loss: 2.6609137058258057\n",
      "Iteration [9637] | loss: 2.661423683166504\n",
      "Iteration [9638] | loss: 2.661949872970581\n",
      "Iteration [9639] | loss: 2.6607842445373535\n",
      "Iteration [9640] | loss: 2.662559986114502\n",
      "Iteration [9641] | loss: 2.660247802734375\n",
      "Iteration [9642] | loss: 2.6613247394561768\n",
      "Iteration [9643] | loss: 2.661452293395996\n",
      "Iteration [9644] | loss: 2.660706043243408\n",
      "Iteration [9645] | loss: 2.6623988151550293\n",
      "Iteration [9646] | loss: 2.66015625\n",
      "Iteration [9647] | loss: 2.6619818210601807\n",
      "Iteration [9648] | loss: 2.660982608795166\n",
      "Iteration [9649] | loss: 2.6613175868988037\n",
      "Iteration [9650] | loss: 2.662014961242676\n",
      "Iteration [9651] | loss: 2.6606881618499756\n",
      "Iteration [9652] | loss: 2.6624698638916016\n",
      "Iteration [9653] | loss: 2.6606204509735107\n",
      "Iteration [9654] | loss: 2.6617650985717773\n",
      "Iteration [9655] | loss: 2.661255121231079\n",
      "Iteration [9656] | loss: 2.6605567932128906\n",
      "Iteration [9657] | loss: 2.6623921394348145\n",
      "Iteration [9658] | loss: 2.66011118888855\n",
      "Iteration [9659] | loss: 2.6617164611816406\n",
      "Iteration [9660] | loss: 2.6612255573272705\n",
      "Iteration [9661] | loss: 2.6610705852508545\n",
      "Iteration [9662] | loss: 2.662238121032715\n",
      "Iteration [9663] | loss: 2.6604621410369873\n",
      "Iteration [9664] | loss: 2.662259817123413\n",
      "Iteration [9665] | loss: 2.660827875137329\n",
      "Iteration [9666] | loss: 2.661557912826538\n",
      "Iteration [9667] | loss: 2.661813259124756\n",
      "Iteration [9668] | loss: 2.660916328430176\n",
      "Iteration [9669] | loss: 2.6621837615966797\n",
      "Iteration [9670] | loss: 2.660266637802124\n",
      "Iteration [9671] | loss: 2.6615238189697266\n",
      "Iteration [9672] | loss: 2.661369800567627\n",
      "Iteration [9673] | loss: 2.660898208618164\n",
      "Iteration [9674] | loss: 2.6623737812042236\n",
      "Iteration [9675] | loss: 2.6602859497070312\n",
      "Iteration [9676] | loss: 2.6621344089508057\n",
      "Iteration [9677] | loss: 2.660900115966797\n",
      "Iteration [9678] | loss: 2.661454916000366\n",
      "Iteration [9679] | loss: 2.661935806274414\n",
      "Iteration [9680] | loss: 2.6608123779296875\n",
      "Iteration [9681] | loss: 2.6625399589538574\n",
      "Iteration [9682] | loss: 2.6603567600250244\n",
      "Iteration [9683] | loss: 2.661379337310791\n",
      "Iteration [9684] | loss: 2.66145396232605\n",
      "Iteration [9685] | loss: 2.6607630252838135\n",
      "Iteration [9686] | loss: 2.662398338317871\n",
      "Iteration [9687] | loss: 2.6601996421813965\n",
      "Iteration [9688] | loss: 2.662019729614258\n",
      "Iteration [9689] | loss: 2.660980463027954\n",
      "Iteration [9690] | loss: 2.6613523960113525\n",
      "Iteration [9691] | loss: 2.6620125770568848\n",
      "Iteration [9692] | loss: 2.660717487335205\n",
      "Iteration [9693] | loss: 2.6624975204467773\n",
      "Iteration [9694] | loss: 2.660510540008545\n",
      "Iteration [9695] | loss: 2.6612610816955566\n",
      "Iteration [9696] | loss: 2.6614980697631836\n",
      "Iteration [9697] | loss: 2.660639762878418\n",
      "Iteration [9698] | loss: 2.6624417304992676\n",
      "Iteration [9699] | loss: 2.660104990005493\n",
      "Iteration [9700] | loss: 2.661932945251465\n",
      "Iteration [9701] | loss: 2.661024808883667\n",
      "Iteration [9702] | loss: 2.661271810531616\n",
      "Iteration [9703] | loss: 2.662053346633911\n",
      "Iteration [9704] | loss: 2.6606462001800537\n",
      "Iteration [9705] | loss: 2.662431240081787\n",
      "Iteration [9706] | loss: 2.6606552600860596\n",
      "Iteration [9707] | loss: 2.661724328994751\n",
      "Iteration [9708] | loss: 2.66135573387146\n",
      "Iteration [9709] | loss: 2.660564422607422\n",
      "Iteration [9710] | loss: 2.6623547077178955\n",
      "Iteration [9711] | loss: 2.6601409912109375\n",
      "Iteration [9712] | loss: 2.661681890487671\n",
      "Iteration [9713] | loss: 2.661252975463867\n",
      "Iteration [9714] | loss: 2.6610419750213623\n",
      "Iteration [9715] | loss: 2.6622633934020996\n",
      "Iteration [9716] | loss: 2.6604247093200684\n",
      "Iteration [9717] | loss: 2.6622533798217773\n",
      "Iteration [9718] | loss: 2.660796880722046\n",
      "Iteration [9719] | loss: 2.661567211151123\n",
      "Iteration [9720] | loss: 2.6618411540985107\n",
      "Iteration [9721] | loss: 2.6609156131744385\n",
      "Iteration [9722] | loss: 2.662292957305908\n",
      "Iteration [9723] | loss: 2.660281181335449\n",
      "Iteration [9724] | loss: 2.661478281021118\n",
      "Iteration [9725] | loss: 2.661384105682373\n",
      "Iteration [9726] | loss: 2.660853624343872\n",
      "Iteration [9727] | loss: 2.662386655807495\n",
      "Iteration [9728] | loss: 2.6602346897125244\n",
      "Iteration [9729] | loss: 2.662099599838257\n",
      "Iteration [9730] | loss: 2.6609132289886475\n",
      "Iteration [9731] | loss: 2.6614227294921875\n",
      "Iteration [9732] | loss: 2.6619510650634766\n",
      "Iteration [9733] | loss: 2.6607842445373535\n",
      "Iteration [9734] | loss: 2.662559986114502\n",
      "Iteration [9735] | loss: 2.6602487564086914\n",
      "Iteration [9736] | loss: 2.661325216293335\n",
      "Iteration [9737] | loss: 2.6614532470703125\n",
      "Iteration [9738] | loss: 2.660705089569092\n",
      "Iteration [9739] | loss: 2.662398338317871\n",
      "Iteration [9740] | loss: 2.6601579189300537\n",
      "Iteration [9741] | loss: 2.6619818210601807\n",
      "Iteration [9742] | loss: 2.6609838008880615\n",
      "Iteration [9743] | loss: 2.66131854057312\n",
      "Iteration [9744] | loss: 2.662015914916992\n",
      "Iteration [9745] | loss: 2.6606876850128174\n",
      "Iteration [9746] | loss: 2.6624693870544434\n",
      "Iteration [9747] | loss: 2.6606204509735107\n",
      "Iteration [9748] | loss: 2.6617655754089355\n",
      "Iteration [9749] | loss: 2.661254644393921\n",
      "Iteration [9750] | loss: 2.6605560779571533\n",
      "Iteration [9751] | loss: 2.6623919010162354\n",
      "Iteration [9752] | loss: 2.660111427307129\n",
      "Iteration [9753] | loss: 2.6617159843444824\n",
      "Iteration [9754] | loss: 2.661224126815796\n",
      "Iteration [9755] | loss: 2.661071538925171\n",
      "Iteration [9756] | loss: 2.662238121032715\n",
      "Iteration [9757] | loss: 2.6604602336883545\n",
      "Iteration [9758] | loss: 2.662259817123413\n",
      "Iteration [9759] | loss: 2.6608288288116455\n",
      "Iteration [9760] | loss: 2.6615593433380127\n",
      "Iteration [9761] | loss: 2.661811113357544\n",
      "Iteration [9762] | loss: 2.6609175205230713\n",
      "Iteration [9763] | loss: 2.6621828079223633\n",
      "Iteration [9764] | loss: 2.6602675914764404\n",
      "Iteration [9765] | loss: 2.661525249481201\n",
      "Iteration [9766] | loss: 2.6613693237304688\n",
      "Iteration [9767] | loss: 2.660897970199585\n",
      "Iteration [9768] | loss: 2.6623737812042236\n",
      "Iteration [9769] | loss: 2.6602864265441895\n",
      "Iteration [9770] | loss: 2.6621344089508057\n",
      "Iteration [9771] | loss: 2.660900115966797\n",
      "Iteration [9772] | loss: 2.661454916000366\n",
      "Iteration [9773] | loss: 2.6619365215301514\n",
      "Iteration [9774] | loss: 2.6608128547668457\n",
      "Iteration [9775] | loss: 2.662539482116699\n",
      "Iteration [9776] | loss: 2.6603567600250244\n",
      "Iteration [9777] | loss: 2.661378860473633\n",
      "Iteration [9778] | loss: 2.66145396232605\n",
      "Iteration [9779] | loss: 2.6607630252838135\n",
      "Iteration [9780] | loss: 2.662397861480713\n",
      "Iteration [9781] | loss: 2.6601996421813965\n",
      "Iteration [9782] | loss: 2.6620192527770996\n",
      "Iteration [9783] | loss: 2.660979986190796\n",
      "Iteration [9784] | loss: 2.6613516807556152\n",
      "Iteration [9785] | loss: 2.6620125770568848\n",
      "Iteration [9786] | loss: 2.6607182025909424\n",
      "Iteration [9787] | loss: 2.662497043609619\n",
      "Iteration [9788] | loss: 2.660510540008545\n",
      "Iteration [9789] | loss: 2.6612610816955566\n",
      "Iteration [9790] | loss: 2.6614990234375\n",
      "Iteration [9791] | loss: 2.660639762878418\n",
      "Iteration [9792] | loss: 2.6624412536621094\n",
      "Iteration [9793] | loss: 2.660104990005493\n",
      "Iteration [9794] | loss: 2.6619324684143066\n",
      "Iteration [9795] | loss: 2.661024332046509\n",
      "Iteration [9796] | loss: 2.6612720489501953\n",
      "Iteration [9797] | loss: 2.6620535850524902\n",
      "Iteration [9798] | loss: 2.66064715385437\n",
      "Iteration [9799] | loss: 2.6624298095703125\n",
      "Iteration [9800] | loss: 2.660656213760376\n",
      "Iteration [9801] | loss: 2.661724805831909\n",
      "Iteration [9802] | loss: 2.6613552570343018\n",
      "Iteration [9803] | loss: 2.6605634689331055\n",
      "Iteration [9804] | loss: 2.662355661392212\n",
      "Iteration [9805] | loss: 2.6601405143737793\n",
      "Iteration [9806] | loss: 2.6616809368133545\n",
      "Iteration [9807] | loss: 2.6612541675567627\n",
      "Iteration [9808] | loss: 2.661040782928467\n",
      "Iteration [9809] | loss: 2.6622626781463623\n",
      "Iteration [9810] | loss: 2.660423994064331\n",
      "Iteration [9811] | loss: 2.662252902984619\n",
      "Iteration [9812] | loss: 2.660796880722046\n",
      "Iteration [9813] | loss: 2.661567211151123\n",
      "Iteration [9814] | loss: 2.6618411540985107\n",
      "Iteration [9815] | loss: 2.6609151363372803\n",
      "Iteration [9816] | loss: 2.66229248046875\n",
      "Iteration [9817] | loss: 2.6602816581726074\n",
      "Iteration [9818] | loss: 2.6614768505096436\n",
      "Iteration [9819] | loss: 2.661383628845215\n",
      "Iteration [9820] | loss: 2.6608545780181885\n",
      "Iteration [9821] | loss: 2.6623876094818115\n",
      "Iteration [9822] | loss: 2.6602346897125244\n",
      "Iteration [9823] | loss: 2.662100076675415\n",
      "Iteration [9824] | loss: 2.6609137058258057\n",
      "Iteration [9825] | loss: 2.6614227294921875\n",
      "Iteration [9826] | loss: 2.6619505882263184\n",
      "Iteration [9827] | loss: 2.6607842445373535\n",
      "Iteration [9828] | loss: 2.6625590324401855\n",
      "Iteration [9829] | loss: 2.660247802734375\n",
      "Iteration [9830] | loss: 2.6613247394561768\n",
      "Iteration [9831] | loss: 2.6614532470703125\n",
      "Iteration [9832] | loss: 2.660705089569092\n",
      "Iteration [9833] | loss: 2.662397623062134\n",
      "Iteration [9834] | loss: 2.6601572036743164\n",
      "Iteration [9835] | loss: 2.6619834899902344\n",
      "Iteration [9836] | loss: 2.6609833240509033\n",
      "Iteration [9837] | loss: 2.661318063735962\n",
      "Iteration [9838] | loss: 2.662014961242676\n",
      "Iteration [9839] | loss: 2.6606876850128174\n",
      "Iteration [9840] | loss: 2.6624703407287598\n",
      "Iteration [9841] | loss: 2.660620927810669\n",
      "Iteration [9842] | loss: 2.661764621734619\n",
      "Iteration [9843] | loss: 2.6612541675567627\n",
      "Iteration [9844] | loss: 2.6605567932128906\n",
      "Iteration [9845] | loss: 2.6623921394348145\n",
      "Iteration [9846] | loss: 2.6601107120513916\n",
      "Iteration [9847] | loss: 2.661715269088745\n",
      "Iteration [9848] | loss: 2.6612255573272705\n",
      "Iteration [9849] | loss: 2.661072015762329\n",
      "Iteration [9850] | loss: 2.6622369289398193\n",
      "Iteration [9851] | loss: 2.6604621410369873\n",
      "Iteration [9852] | loss: 2.662259340286255\n",
      "Iteration [9853] | loss: 2.6608288288116455\n",
      "Iteration [9854] | loss: 2.661560535430908\n",
      "Iteration [9855] | loss: 2.661811590194702\n",
      "Iteration [9856] | loss: 2.660917282104492\n",
      "Iteration [9857] | loss: 2.662182092666626\n",
      "Iteration [9858] | loss: 2.6602675914764404\n",
      "Iteration [9859] | loss: 2.6615242958068848\n",
      "Iteration [9860] | loss: 2.6613705158233643\n",
      "Iteration [9861] | loss: 2.660897731781006\n",
      "Iteration [9862] | loss: 2.6623737812042236\n",
      "Iteration [9863] | loss: 2.6602859497070312\n",
      "Iteration [9864] | loss: 2.66213321685791\n",
      "Iteration [9865] | loss: 2.6608996391296387\n",
      "Iteration [9866] | loss: 2.661454439163208\n",
      "Iteration [9867] | loss: 2.6619365215301514\n",
      "Iteration [9868] | loss: 2.660813331604004\n",
      "Iteration [9869] | loss: 2.662539482116699\n",
      "Iteration [9870] | loss: 2.6603567600250244\n",
      "Iteration [9871] | loss: 2.661379337310791\n",
      "Iteration [9872] | loss: 2.6614553928375244\n",
      "Iteration [9873] | loss: 2.6607630252838135\n",
      "Iteration [9874] | loss: 2.662397861480713\n",
      "Iteration [9875] | loss: 2.660200595855713\n",
      "Iteration [9876] | loss: 2.6620192527770996\n",
      "Iteration [9877] | loss: 2.660980463027954\n",
      "Iteration [9878] | loss: 2.6613523960113525\n",
      "Iteration [9879] | loss: 2.6620113849639893\n",
      "Iteration [9880] | loss: 2.660719156265259\n",
      "Iteration [9881] | loss: 2.6624979972839355\n",
      "Iteration [9882] | loss: 2.660510540008545\n",
      "Iteration [9883] | loss: 2.6612608432769775\n",
      "Iteration [9884] | loss: 2.6614980697631836\n",
      "Iteration [9885] | loss: 2.6606390476226807\n",
      "Iteration [9886] | loss: 2.662440299987793\n",
      "Iteration [9887] | loss: 2.660104990005493\n",
      "Iteration [9888] | loss: 2.661932945251465\n",
      "Iteration [9889] | loss: 2.661025285720825\n",
      "Iteration [9890] | loss: 2.661271333694458\n",
      "Iteration [9891] | loss: 2.662053346633911\n",
      "Iteration [9892] | loss: 2.6606462001800537\n",
      "Iteration [9893] | loss: 2.662431240081787\n",
      "Iteration [9894] | loss: 2.6606547832489014\n",
      "Iteration [9895] | loss: 2.6617250442504883\n",
      "Iteration [9896] | loss: 2.6613566875457764\n",
      "Iteration [9897] | loss: 2.6605634689331055\n",
      "Iteration [9898] | loss: 2.6623547077178955\n",
      "Iteration [9899] | loss: 2.6601405143737793\n",
      "Iteration [9900] | loss: 2.661682367324829\n",
      "Iteration [9901] | loss: 2.661252975463867\n",
      "Iteration [9902] | loss: 2.6610398292541504\n",
      "Iteration [9903] | loss: 2.662264347076416\n",
      "Iteration [9904] | loss: 2.66042423248291\n",
      "Iteration [9905] | loss: 2.662252426147461\n",
      "Iteration [9906] | loss: 2.6607961654663086\n",
      "Iteration [9907] | loss: 2.661567211151123\n",
      "Iteration [9908] | loss: 2.661841630935669\n",
      "Iteration [9909] | loss: 2.6609151363372803\n",
      "Iteration [9910] | loss: 2.662292957305908\n",
      "Iteration [9911] | loss: 2.6602797508239746\n",
      "Iteration [9912] | loss: 2.6614768505096436\n",
      "Iteration [9913] | loss: 2.6613831520080566\n",
      "Iteration [9914] | loss: 2.6608545780181885\n",
      "Iteration [9915] | loss: 2.662388801574707\n",
      "Iteration [9916] | loss: 2.6602346897125244\n",
      "Iteration [9917] | loss: 2.6621005535125732\n",
      "Iteration [9918] | loss: 2.6609137058258057\n",
      "Iteration [9919] | loss: 2.6614246368408203\n",
      "Iteration [9920] | loss: 2.6619510650634766\n",
      "Iteration [9921] | loss: 2.6607847213745117\n",
      "Iteration [9922] | loss: 2.662559986114502\n",
      "Iteration [9923] | loss: 2.6602487564086914\n",
      "Iteration [9924] | loss: 2.661325216293335\n",
      "Iteration [9925] | loss: 2.6614532470703125\n",
      "Iteration [9926] | loss: 2.660705089569092\n",
      "Iteration [9927] | loss: 2.6623971462249756\n",
      "Iteration [9928] | loss: 2.6601574420928955\n",
      "Iteration [9929] | loss: 2.6619818210601807\n",
      "Iteration [9930] | loss: 2.660983085632324\n",
      "Iteration [9931] | loss: 2.661318063735962\n",
      "Iteration [9932] | loss: 2.6620163917541504\n",
      "Iteration [9933] | loss: 2.6606881618499756\n",
      "Iteration [9934] | loss: 2.6624698638916016\n",
      "Iteration [9935] | loss: 2.660620927810669\n",
      "Iteration [9936] | loss: 2.6617655754089355\n",
      "Iteration [9937] | loss: 2.661255121231079\n",
      "Iteration [9938] | loss: 2.6605546474456787\n",
      "Iteration [9939] | loss: 2.6623926162719727\n",
      "Iteration [9940] | loss: 2.66011118888855\n",
      "Iteration [9941] | loss: 2.661714792251587\n",
      "Iteration [9942] | loss: 2.6612250804901123\n",
      "Iteration [9943] | loss: 2.661071538925171\n",
      "Iteration [9944] | loss: 2.662238121032715\n",
      "Iteration [9945] | loss: 2.6604621410369873\n",
      "Iteration [9946] | loss: 2.662259817123413\n",
      "Iteration [9947] | loss: 2.6608288288116455\n",
      "Iteration [9948] | loss: 2.6615593433380127\n",
      "Iteration [9949] | loss: 2.6618127822875977\n",
      "Iteration [9950] | loss: 2.660917282104492\n",
      "Iteration [9951] | loss: 2.6621828079223633\n",
      "Iteration [9952] | loss: 2.660266637802124\n",
      "Iteration [9953] | loss: 2.6615238189697266\n",
      "Iteration [9954] | loss: 2.661370038986206\n",
      "Iteration [9955] | loss: 2.660898447036743\n",
      "Iteration [9956] | loss: 2.6623737812042236\n",
      "Iteration [9957] | loss: 2.6602866649627686\n",
      "Iteration [9958] | loss: 2.6621334552764893\n",
      "Iteration [9959] | loss: 2.6608994007110596\n",
      "Iteration [9960] | loss: 2.661454439163208\n",
      "Iteration [9961] | loss: 2.6619365215301514\n",
      "Iteration [9962] | loss: 2.660813331604004\n",
      "Iteration [9963] | loss: 2.6625404357910156\n",
      "Iteration [9964] | loss: 2.6603567600250244\n",
      "Iteration [9965] | loss: 2.661379337310791\n",
      "Iteration [9966] | loss: 2.661454916000366\n",
      "Iteration [9967] | loss: 2.6607635021209717\n",
      "Iteration [9968] | loss: 2.662397861480713\n",
      "Iteration [9969] | loss: 2.6601996421813965\n",
      "Iteration [9970] | loss: 2.6620192527770996\n",
      "Iteration [9971] | loss: 2.6609818935394287\n",
      "Iteration [9972] | loss: 2.6613523960113525\n",
      "Iteration [9973] | loss: 2.6620099544525146\n",
      "Iteration [9974] | loss: 2.660717725753784\n",
      "Iteration [9975] | loss: 2.662497043609619\n",
      "Iteration [9976] | loss: 2.6605119705200195\n",
      "Iteration [9977] | loss: 2.6612606048583984\n",
      "Iteration [9978] | loss: 2.6614980697631836\n",
      "Iteration [9979] | loss: 2.6606385707855225\n",
      "Iteration [9980] | loss: 2.662440776824951\n",
      "Iteration [9981] | loss: 2.660104990005493\n",
      "Iteration [9982] | loss: 2.6619324684143066\n",
      "Iteration [9983] | loss: 2.661024808883667\n",
      "Iteration [9984] | loss: 2.661271333694458\n",
      "Iteration [9985] | loss: 2.6620523929595947\n",
      "Iteration [9986] | loss: 2.6606462001800537\n",
      "Iteration [9987] | loss: 2.662430763244629\n",
      "Iteration [9988] | loss: 2.6606557369232178\n",
      "Iteration [9989] | loss: 2.6617250442504883\n",
      "Iteration [9990] | loss: 2.6613552570343018\n",
      "Iteration [9991] | loss: 2.660562515258789\n",
      "Iteration [9992] | loss: 2.662355661392212\n",
      "Iteration [9993] | loss: 2.6601405143737793\n",
      "Iteration [9994] | loss: 2.6616809368133545\n",
      "Iteration [9995] | loss: 2.661255121231079\n",
      "Iteration [9996] | loss: 2.661041498184204\n",
      "Iteration [9997] | loss: 2.6622631549835205\n",
      "Iteration [9998] | loss: 2.66042423248291\n",
      "Iteration [9999] | loss: 2.6622536182403564\n"
     ]
    }
   ],
   "source": [
    "attacked_sample = sample.clone()\n",
    "attacked_sample.requires_grad = True\n",
    "adversarial_optimizer = SGD([attacked_sample], lr=1e-1, maximize=True) # Increase loss instead of minimizing it!\n",
    "eps = 5\n",
    "n_iters = 10_000\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for iter in range(n_iters):\n",
    "    logits = model(attacked_sample)\n",
    "    loss = loss_fn(logits, true_y)\n",
    "\n",
    "    # Gradient step\n",
    "    adversarial_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    adversarial_optimizer.step()\n",
    "\n",
    "    # Projection!\n",
    "    delta = attacked_sample.data - sample.data\n",
    "    delta *= min(1,eps/torch.norm(delta))\n",
    "    attacked_sample.data = sample + delta\n",
    "\n",
    "    print(f'Iteration [{iter}] | loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth: 7\\nPrediction: 2, confidence: 89.97%')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the attacked sample\n",
    "\n",
    "with torch.no_grad():\n",
    "    logit = model(attacked_sample)[0]\n",
    "    proba = torch.softmax(logit, dim=0)\n",
    "    pred = torch.argmax(proba)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(attacked_sample.detach().numpy().reshape(28,28), cmap='gray', interpolation='none')\n",
    "plt.title(\"Ground Truth: {}\\nPrediction: {}, confidence: {:.2f}%\".format(true_y.item(), pred, proba[pred]*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As can be observed the model mistakes with very high confidence on the perturbated sample.\n",
    "* However, it is clear that the ground truth is still the same!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This tutorial was written by Mitchell Keren Taraday.\n",
    "* Image credits:\n",
    "    + [https://www.ruder.io/optimizing-gradient-descent/](https://www.ruder.io/optimizing-gradient-descent/)\n",
    "    + [https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/3dpoly.svg/1024px-3dpoly.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/3dpoly.svg/1024px-3dpoly.svg.png)\n",
    "    + [https://www.fromthegenesis.com/gradient-descent-part1/](https://www.fromthegenesis.com/gradient-descent-part1/)\n",
    "    + [https://huggingface.co/blog/assets/33_large_language_models/01_model_size.jpg](https://huggingface.co/blog/assets/33_large_language_models/01_model_size.jpg)\n",
    "    + [https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/batch-1-1024x670.png](https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/batch-1-1024x670.png)\n",
    "    + [https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
    "    + [https://www.researchgate.net/publication/332709751_Data-Driven_Neuron_Allocation_for_Scale_Aggregation_Networks](https://www.researchgate.net/publication/332709751_Data-Driven_Neuron_Allocation_for_Scale_Aggregation_Networks)\n",
    "    + [https://towardsdatascience.com/the-best-learning-rate-schedules-6b7b9fb72565](https://towardsdatascience.com/the-best-learning-rate-schedules-6b7b9fb72565)\n",
    "    + [https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png)\n",
    "    + [https://home.ttic.edu/~nati/Teaching/TTIC31070/2015/Lecture16.pdf](https://home.ttic.edu/~nati/Teaching/TTIC31070/2015/Lecture16.pdf)\n",
    "    + [https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Linalg_projection_onto_plane.png/223px-Linalg_projection_onto_plane.png](https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Linalg_projection_onto_plane.png/223px-Linalg_projection_onto_plane.png)\n",
    "    + [https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Linalg_projection_onto_plane.png/223px-Linalg_projection_onto_plane.png](https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Linalg_projection_onto_plane.png/223px-Linalg_projection_onto_plane.png)\n",
    "    + [https://i.stack.imgur.com/5rfe9.png](https://i.stack.imgur.com/5rfe9.png)\n",
    "    + [http://www.cohennadav.com/files/icermw19_slides.pdf](http://www.cohennadav.com/files/icermw19_slides.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
